# edgequake-litellm vs litellm â€” Compatibility Matrix

> Last updated: 2025-07  
> edgequake-litellm version: **0.3.x**  
> litellm reference version: **1.x (stable)**  
> litellm docs: https://docs.litellm.ai/

---

## Legend

| Symbol | Meaning |
|--------|---------|
| âœ… | Fully compatible â€” identical behaviour |
| âš ï¸ | Partial â€” works but with differences (noted below) |
| âŒ | Not implemented |
| ğŸ”§ | Silently dropped (no error, no effect) |
| ğŸ“‹ | On roadmap |

---

## 1. Top-Level Functions

| Function | Status | Notes |
|----------|--------|-------|
| `completion(model, messages, ...)` | âœ… | Core params supported. See Â§3. |
| `acompletion(model, messages, ...)` | âœ… | Async version â€” same params. |
| `completion(..., stream=True)` | âŒ | Only `stream()` async generator available; `stream=True` kwarg raises `NotImplementedError`. |
| `acompletion(..., stream=True)` | âš ï¸ | Returns `AsyncGenerator[StreamChunk, None]` â€” compatible usage but different object type. |
| `stream(model, messages, ...)` | âš ï¸ | **edgequake-only** function (not in litellm). litellm uses `stream=True` kwarg on `completion`. |
| `embedding(model, input, ...)` | âš ï¸ | Returns `List[List[float]]` â€” litellm returns `EmbeddingResponse` object. |
| `aembedding(model, input, ...)` | âš ï¸ | Same return-type difference as `embedding()`. |
| `text_completion(prompt, model, ...)` | âŒ | Old `/v1/completions` endpoint â€” not implemented. |
| `image_generation(prompt, model, ...)` | âŒ | Not implemented. |
| `transcription(file, model, ...)` | âŒ | Not implemented. |
| `speech(model, input, voice, ...)` | âŒ | Not implemented. |
| `stream_chunk_builder(chunks, messages)` | âœ… | Provided as `edgequake_litellm.stream_chunk_builder()`. |
| `get_supported_openai_params(model, ...)` | âŒ | Not implemented. |
| `utils.get_model_info(model)` | âŒ | Not implemented. |

---

## 2. Module-Level Globals

| Global | litellm | edgequake-litellm | Status |
|--------|---------|-------------------|--------|
| `litellm.api_key` | Global fallback API key | âŒ | Not implemented |
| `litellm.api_base` | Global base URL override | âŒ | Not implemented |
| `litellm.set_verbose` | Enable debug logging | âœ… | Wraps `_config.verbose` |
| `litellm.drop_params` | Silently drop unsupported params | âœ… | Always `True` (immutable for now) |
| `litellm.model_cost` | Dict of model pricing | âŒ | Not implemented |
| `litellm.callbacks` | List of callback handlers | âŒ | Not implemented |
| `litellm.success_callback` | On-success hooks | âŒ | Not implemented |
| `litellm.failure_callback` | On-failure hooks | âŒ | Not implemented |
| `litellm.REPEATED_STREAMING_CHUNK_LIMIT` | Streaming safety limit | âŒ | Not implemented |

---

## 3. `completion()` / `acompletion()` â€” Input Parameters

### Core Parameters

| Parameter | litellm | edgequake-litellm | Status |
|-----------|---------|-------------------|--------|
| `model` | âœ… Required | âœ… Required | âœ… |
| `messages` | âœ… Required | âœ… Required | âœ… |
| `max_tokens` | âœ… | âœ… | âœ… |
| `temperature` | âœ… | âœ… | âœ… |
| `top_p` | âœ… | âœ… | âœ… |
| `stop` | âœ… List[str] | âœ… List[str] | âœ… |
| `frequency_penalty` | âœ… | âœ… | âœ… |
| `presence_penalty` | âœ… | âœ… | âœ… |
| `response_format` | âœ… `str` or `dict` | âš ï¸ `str` only (`"json_object"`) | âš ï¸ |
| `tools` | âœ… | âœ… | âœ… |
| `tool_choice` | âœ… | âœ… | âœ… |
| `stream` | âœ… bool | âŒ Not accepted | âŒ |
| `n` | âœ… int | âŒ Silently dropped | ğŸ”§ |
| `seed` | âœ… int | âŒ Silently dropped | ğŸ”§ |
| `logit_bias` | âœ… dict | âŒ Silently dropped | ğŸ”§ |
| `logprobs` | âœ… bool | âŒ Silently dropped | ğŸ”§ |
| `top_logprobs` | âœ… int | âŒ Silently dropped | ğŸ”§ |
| `parallel_tool_calls` | âœ… bool | âŒ Silently dropped | ğŸ”§ |
| `user` | âœ… str | ğŸ”§ Accepted, silently dropped | ğŸ”§ |
| `timeout` | âœ… float/int | âš ï¸ Accepted in signature, not wired to Rust | âš ï¸ |
| `max_completion_tokens` | âœ… alias for max_tokens | âŒ Dropped | ğŸ”§ |

### litellm-Specific Parameters (Provider Overrides)

| Parameter | litellm | edgequake-litellm | Status |
|-----------|---------|-------------------|--------|
| `api_base` / `base_url` | âœ… Per-call URL override | âš ï¸ Accepted, not wired to Rust core | âš ï¸ |
| `api_key` | âœ… Per-call key override | âš ï¸ Accepted, not wired to Rust core | âš ï¸ |
| `api_version` | âœ… Azure version pin | âŒ Dropped | ğŸ”§ |
| `headers` / `extra_headers` | âœ… Custom HTTP headers | âŒ Dropped | ğŸ”§ |
| `num_retries` | âœ… Per-call retry count | âŒ Dropped (config default used) | ğŸ”§ |
| `fallbacks` | âœ… List of fallback models | âŒ Not implemented | âŒ |
| `metadata` | âœ… Arbitrary logging dict | âŒ Dropped | ğŸ”§ |
| `input_cost_per_token` | âœ… Cost override | âŒ Dropped | ğŸ”§ |
| `output_cost_per_token` | âœ… Cost override | âŒ Dropped | ğŸ”§ |
| `initial_prompt_value` | âœ… | âŒ Dropped | ğŸ”§ |
| `stream_options` | âœ… `{"include_usage": True}` | âŒ Dropped | ğŸ”§ |

### edgequake-litellm-Only Parameters

| Parameter | Description |
|-----------|-------------|
| `system` | Convenience shorthand for adding a system message. Not in litellm's API. |

---

## 4. `ModelResponse` â€” Output Object

### Field Comparison

| Field | litellm access | edgequake-litellm | Status |
|-------|---------------|-------------------|--------|
| Response ID | `resp.id` | âŒ Not exposed | âŒ |
| Created timestamp | `resp.created` | âŒ Not exposed | âŒ |
| Object type | `resp.object` | âŒ Not exposed | âŒ |
| System fingerprint | `resp.system_fingerprint` | âŒ Not exposed | âŒ |
| Model name | `resp.model` | âœ… `resp.model` | âœ… |
| Message content | `resp.choices[0].message.content` | âš ï¸ `resp.content` (shortcut) | âš ï¸ |
| Message role | `resp.choices[0].message.role` | âŒ Not exposed | âŒ |
| Finish reason | `resp.choices[0].finish_reason` | âŒ Not exposed | âŒ |
| Tool calls | `resp.choices[0].message.tool_calls` | âš ï¸ `resp.tool_calls` (shortcut) | âš ï¸ |
| Choices list | `resp.choices` (list, len = n) | âŒ No `choices` attribute | âŒ |
| Prompt tokens | `resp.usage.prompt_tokens` | âœ… `resp.usage.prompt_tokens` | âœ… |
| Completion tokens | `resp.usage.completion_tokens` | âœ… `resp.usage.completion_tokens` | âœ… |
| Total tokens | `resp.usage.total_tokens` | âœ… `resp.usage.total_tokens` | âœ… |
| Cached tokens | `resp.usage.prompt_tokens_details.cached_tokens` | âŒ Not exposed | âŒ |
| Cache creation tokens | `resp.usage.cache_creation_input_tokens` | âŒ Not exposed | âŒ |
| Cache read tokens | `resp.usage.cache_read_input_tokens` | âŒ Not exposed | âŒ |
| Completion token details | `resp.usage.completion_tokens_details` | âŒ Not exposed | âŒ |
| Latency | `resp.response_ms` | âŒ Not exposed | âŒ |
| Dict access | `resp["choices"][0]["message"]["content"]` | âŒ No `__getitem__` | âŒ |

### litellm Example vs edgequake-litellm

```python
# litellm
import litellm
resp = litellm.completion("gpt-4o-mini", messages)
print(resp.choices[0].message.content)   # standard OpenAI path
print(resp["choices"][0]["message"]["content"])  # dict path

# edgequake-litellm
import edgequake_litellm as litellm
resp = litellm.completion("openai/gpt-4o-mini", messages)
print(resp.content)                       # shortened accessor âœ…
# resp.choices[0].message.content         â† âŒ raises AttributeError
```

**Impact**: Any code that accesses `response.choices[0].message.content` will break.  
This is the **single largest compatibility gap** today.

---

## 5. Streaming API

### API Shape Difference

| Aspect | litellm | edgequake-litellm |
|--------|---------|-------------------|
| Sync streaming | `for chunk in completion(..., stream=True)` | âŒ Not supported |
| Async streaming | `async for chunk in acompletion(..., stream=True)` | âš ï¸ `async for chunk in stream(model, messages)` |
| Chunk type | `ModelResponse` with `choices[0].delta.content` | `StreamChunk` with `.content` |
| Finish detection | `choices[0].finish_reason == "stop"` | `chunk.is_finished == True` |
| Chunk helper | `stream_chunk_builder(chunks, messages)` | âœ… `stream_chunk_builder(chunks)` provided |

### Streaming Chunk Field Comparison

| Field | litellm chunk | edgequake-litellm StreamChunk |
|-------|--------------|-------------------------------|
| Delta content | `chunk.choices[0].delta.content` | âœ… `chunk.content` |
| Delta role | `chunk.choices[0].delta.role` | âŒ Not exposed |
| Delta tool calls | `chunk.choices[0].delta.tool_calls` | âŒ Not exposed |
| Finish reason | `chunk.choices[0].finish_reason` | âš ï¸ `chunk.finish_reason` |
| Is finished | (check finish_reason) | âœ… `chunk.is_finished` |
| Thinking/reasoning | N/A (litellm) | âœ… `chunk.thinking` (Anthropic extended) |
| Index | `chunk.choices[0].index` | âŒ Not exposed |

### Migration Pattern

```python
# litellm pattern
for chunk in litellm.completion("gpt-4o-mini", msgs, stream=True):
    print(chunk.choices[0].delta.content or "", end="")

# edgequake-litellm equivalent (must be async)
import asyncio
async def run():
    async for chunk in edgequake_litellm.stream("openai/gpt-4o-mini", msgs):
        print(chunk.content or "", end="")

asyncio.run(run())
```

---

## 6. `embedding()` / `aembedding()`

### Input Parameters

| Parameter | litellm | edgequake-litellm | Status |
|-----------|---------|-------------------|--------|
| `model` | âœ… | âœ… | âœ… |
| `input` | âœ… `str` or `List[str]` | âœ… | âœ… |
| `user` | âœ… | âŒ Dropped | ğŸ”§ |
| `dimensions` | âœ… | âŒ Dropped | ğŸ”§ |
| `encoding_format` | âœ… `"float"` / `"base64"` | âŒ Dropped | ğŸ”§ |
| `timeout` | âœ… | âŒ Dropped | ğŸ”§ |
| `api_base` | âœ… | âŒ Dropped | ğŸ”§ |
| `api_key` | âœ… | âŒ Dropped | ğŸ”§ |

### Return Type Difference

```python
# litellm â€” returns EmbeddingResponse
result = litellm.embedding("text-embedding-3-small", input=["hello"])
vectors = [item.embedding for item in result.data]   # List[List[float]]

# edgequake-litellm â€” returns List[List[float]] directly
vectors = edgequake_litellm.embedding("openai/text-embedding-3-small", input=["hello"])
# No .data / .model / .usage attributes available
```

**Impact**: Any code that accesses `result.data`, `result.model`, or `result.usage` will break.

---

## 7. Exception Hierarchy

| litellm exception | edgequake-litellm exception | Status |
|------------------|---------------------------|--------|
| `litellm.AuthenticationError` | `AuthenticationError` | âœ… |
| `litellm.RateLimitError` | `RateLimitError` | âœ… |
| `litellm.ContextWindowExceededError` | `ContextWindowExceededError` | âœ… |
| `litellm.Timeout` | `Timeout` | âœ… |
| `litellm.APIConnectionError` | `APIConnectionError` | âœ… |
| `litellm.APIError` | `APIError` | âœ… |
| `litellm.NotFoundError` | `ModelNotFoundError` | âš ï¸ Different name |
| `litellm.BadRequestError` | âŒ Not defined | âŒ |
| `litellm.ServiceUnavailableError` | âŒ Not defined | âŒ |
| `litellm.RouterErrors.RouterLLMNotFoundError` | âŒ Not defined | âŒ |
| `.status_code` attribute | âœ… | âœ… |
| `.llm_provider` attribute | âœ… | âœ… |
| `.model` attribute | âœ… | âœ… |

---

## 8. Router / Load Balancing

The `litellm.Router` class provides horizontal scaling across multiple model deployments â€” this is an **entirely separate subsystem** not present in edgequake-litellm.

| Feature | litellm.Router | edgequake-litellm |
|---------|---------------|-------------------|
| Multiple model deployments | âœ… | âŒ |
| Load balancing strategies | âœ… (simple-shuffle, latency-based, cost-based, etc.) | âŒ |
| Automatic retries | âœ… | âŒ |
| Deployment cooldowns | âœ… | âŒ |
| Redis caching | âœ… | âŒ |
| In-memory caching | âœ… | âŒ |
| Fallbacks | âœ… | âŒ |
| `router.completion()` | âœ… | âŒ |
| `router.acompletion()` | âœ… | âŒ |

---

## 9. Callbacks & Observability

litellm has a rich callback system for integrating with 30+ observability platforms. edgequake-litellm instead provides native OpenTelemetry instrumentation through the Rust core.

| Feature | litellm | edgequake-litellm |
|---------|---------|-------------------|
| `litellm.success_callback` | âœ… | âŒ |
| `litellm.failure_callback` | âœ… | âŒ |
| `litellm.callbacks = [handler]` | âœ… | âŒ |
| Langfuse integration | âœ… | âŒ |
| MLflow integration | âœ… | âŒ |
| Helicone integration | âœ… | âŒ |
| Lunary integration | âœ… | âŒ |
| OpenTelemetry (OTEL) | âœ… (via callback) | âœ… **Native** (Rust-level) |
| Cost tracking (per call) | âœ… (via `litellm.model_cost`) | âš ï¸ (Rust token tracking, no USD cost) |

---

## 10. Provider Coverage

### Text Completion Providers

| Provider | litellm | edgequake-litellm | Model string prefix |
|----------|---------|-------------------|---------------------|
| OpenAI | âœ… | âœ… | `openai/` |
| Anthropic | âœ… | âœ… | `anthropic/` |
| Google Gemini | âœ… | âœ… | `gemini/` |
| Mistral | âœ… | âœ… | `mistral/` |
| xAI (Grok) | âœ… | âœ… | `xai/` |
| OpenRouter | âœ… | âœ… | `openrouter/` |
| Ollama | âœ… | âœ… | `ollama/` |
| LM Studio | âœ… | âœ… | `lmstudio/` |
| Azure OpenAI | âœ… | âœ… | `azure/` |
| Hugging Face | âœ… | âœ… | `huggingface/` |
| Bedrock (AWS) | âœ… | âŒ | â€” |
| Vertex AI | âœ… | âŒ | â€” |
| Cohere | âœ… | âŒ | â€” |
| Together AI | âœ… | âŒ | â€” |
| Replicate | âœ… | âŒ | â€” |
| AI21 | âœ… | âŒ | â€” |
| Groq | âœ… | âŒ | â€” |

### Embedding Providers

| Provider | litellm | edgequake-litellm |
|----------|---------|-------------------|
| OpenAI | âœ… | âœ… (via `jina/` or `openai/`) |
| Jina AI | âœ… | âœ… |
| Mistral | âœ… | âœ… |
| Cohere | âœ… | âŒ |
| Bedrock | âœ… | âŒ |
| Vertex AI | âœ… | âŒ |

---

## 11. Overall Compatibility Summary

### Compatibility Score by Category

| Category | Score | Notes |
|----------|-------|-------|
| Core `completion()` params | 60% | Missing `stream`, `n`, `seed`, `timeout`, `api_key`, `api_base`, `user` |
| `ModelResponse` structure | 30% | Big gap: no `.choices`, no `.id`/`.created`, no dict access |
| Streaming API shape | 40% | Different function name, different chunk fields |
| `embedding()` return type | 50% | Data is there, wrapper object missing |
| Exception hierarchy | 85% | All critical ones present, a few missing |
| Module globals | 40% | `set_verbose`, `drop_params` present; no `api_key`, `model_cost` |
| Provider coverage | 65% | 10/17+ providers implemented |
| Router / Load balancing | 0% | Not in scope |
| Callbacks / Observability | 10% | OTEL native only |

### Drop-in Compatibility Assessment

**Scenario 1: Basic completion with `.content` access**
```python
resp = litellm.completion("openai/gpt-4o-mini", msgs)
print(resp.content)
```
â†’ **âœ… Works** (edgequake-litellm extends with `.content` shortcut)

**Scenario 2: Standard OpenAI response path**
```python
resp = litellm.completion("openai/gpt-4o-mini", msgs)
print(resp.choices[0].message.content)
```
â†’ **âŒ Fails** â€” `.choices` not available on `ModelResponse`

**Scenario 3: Synchronous streaming**
```python
for chunk in litellm.completion("openai/gpt-4o-mini", msgs, stream=True):
    print(chunk.choices[0].delta.content or "", end="")
```
â†’ **âŒ Fails** â€” `stream=True` raises `NotImplementedError`

**Scenario 4: Async streaming**
```python
async for chunk in litellm.acompletion("openai/gpt-4o-mini", msgs, stream=True):
    print(chunk.choices[0].delta.content or "", end="")
```
â†’ **âš ï¸ Partial** â€” works in edgequake-litellm as `stream()` but chunk field path differs

**Scenario 5: Tool calling**
```python
resp = litellm.completion("openai/gpt-4o-mini", msgs, tools=[...])
tool_calls = resp.choices[0].message.tool_calls
```
â†’ **âŒ Fails** â€” `.choices` not available; use `resp.tool_calls` instead

**Scenario 6: Embeddings with usage stats**
```python
result = litellm.embedding("openai/text-embedding-3-small", input=texts)
print(result.data[0].embedding, result.usage.total_tokens)
```
â†’ **âŒ Fails** â€” returns `List[List[float]]`, not `EmbeddingResponse`

---

## 12. Quick Migration Reference

For codebases migrating from litellm to edgequake-litellm:

```python
# OLD (litellm)
import litellm

resp = litellm.completion("gpt-4o-mini", messages)
content = resp.choices[0].message.content

# NEW (edgequake-litellm)
import edgequake_litellm as litellm

resp = litellm.completion("openai/gpt-4o-mini", messages)
content = resp.content  # shortcut accessor

# â”€â”€â”€ Streaming â”€â”€â”€
# OLD: for chunk in litellm.completion(..., stream=True):
# NEW: async for chunk in edgequake_litellm.stream(...):

# â”€â”€â”€ Embeddings â”€â”€â”€
# OLD: result = litellm.embedding(...); vectors = [d.embedding for d in result.data]
# NEW: vectors = edgequake_litellm.embedding(...)  # already List[List[float]]

# â”€â”€â”€ Model string â”€â”€â”€
# OLD: "gpt-4o-mini" (litellm resolves provider automatically)
# NEW: "openai/gpt-4o-mini" (explicit provider prefix required)
```
