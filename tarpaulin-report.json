{"files":[{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","examples","basic_completion.rs"],"content":"//! Basic LLM completion example\n//!\n//! Run with: cargo run --example basic_completion\n//! Requires: OPENAI_API_KEY environment variable\n\nuse edgequake_llm::{ChatMessage, LLMProvider, OpenAIProvider};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    // Get API key from environment\n    let api_key = std::env::var(\"OPENAI_API_KEY\").expect(\"OPENAI_API_KEY must be set\");\n\n    // Initialize provider\n    let provider = OpenAIProvider::new(&api_key).with_model(\"gpt-4\");\n\n    println!(\"ðŸ¤– EdgeQuake LLM - Basic Completion Example\\n\");\n\n    // Create message\n    let messages = vec![ChatMessage::user(\"What is the capital of France? Answer in one word.\")];\n\n    println!(\"Sending request to OpenAI...\");\n\n    // Get completion\n    let response = provider.chat(&messages, None).await?;\n\n    println!(\"\\nâœ¨ Response: {}\", response.content);\n    println!(\n        \"ðŸ“Š Tokens: {} prompt + {} completion = {} total\",\n        response.prompt_tokens,\n        response.completion_tokens,\n        response.total_tokens\n    );\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","examples","multi_provider.rs"],"content":"//! Multi-provider example showing provider abstraction\n//!\n//! Run with: cargo run --example multi_provider\n//! Requires: OPENAI_API_KEY, ANTHROPIC_API_KEY, GOOGLE_API_KEY\n\nuse edgequake_llm::{\n    AnthropicProvider, ChatMessage, GeminiProvider, LLMProvider, OpenAIProvider,\n};\n\n#[tokio::main]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    println!(\"ðŸ¤– EdgeQuake LLM - Multi-Provider Example\\n\");\n\n    let message = ChatMessage::user(\"Explain async/await in Rust in 2 sentences.\");\n\n    // Create providers (skip if API key not available)\n    let mut providers: Vec<Box<dyn LLMProvider>> = vec![];\n\n    if let Ok(api_key) = std::env::var(\"OPENAI_API_KEY\") {\n        providers.push(Box::new(OpenAIProvider::new(api_key)));\n    }\n\n    if let Ok(provider) = AnthropicProvider::from_env() {\n        providers.push(Box::new(provider));\n    }\n\n    if let Ok(provider) = GeminiProvider::from_env() {\n        providers.push(Box::new(provider));\n    }\n\n    if providers.is_empty() {\n        eprintln!(\"âŒ No API keys found. Please set at least one:\");\n        eprintln!(\"   - OPENAI_API_KEY\");\n        eprintln!(\"   - ANTHROPIC_API_KEY\");\n        eprintln!(\"   - GOOGLE_API_KEY\");\n        return Ok(());\n    }\n\n    // Try each provider\n    for provider in providers {\n        println!(\"ðŸ”„ Testing: {}\", provider.name());\n        println!(\"   Model: {}\", provider.model());\n\n        match provider.chat(&[message.clone()], None).await {\n            Ok(response) => {\n                println!(\"   âœ… Response: {}\", response.content.lines().next().unwrap_or(\"\"));\n                println!(\"   ðŸ“Š Tokens: {}\\n\", response.total_tokens);\n            }\n            Err(e) => {\n                println!(\"   âŒ Error: {}\\n\", e);\n            }\n        }\n    }\n\n    Ok(())\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","cache.rs"],"content":"//! LLM response caching for reducing API costs and latency.\n//!\n//! This module provides a caching layer for LLM completions and embeddings,\n//! significantly reducing costs for repeated queries and improving response times.\n//!\n//! ## Implements\n//!\n//! - **FEAT0019**: LLM Response Caching\n//! - **FEAT0772**: LRU eviction policy\n//! - **FEAT0773**: TTL-based expiration\n//!\n//! ## Enforces\n//!\n//! - **BR0772**: Cache hit does not modify original response\n//! - **BR0773**: Expired entries evicted on next access\n//!\n//! Based on LightRAG's caching approach with an in-memory LRU cache.\n\nuse crate::error::Result;\nuse crate::traits::{ChatMessage, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse};\nuse async_trait::async_trait;\nuse std::collections::HashMap;\nuse std::hash::{Hash, Hasher};\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::RwLock;\n\n/// Configuration for the LLM cache.\n#[derive(Debug, Clone)]\npub struct CacheConfig {\n    /// Maximum number of entries in the cache.\n    pub max_entries: usize,\n    /// Time-to-live for cache entries.\n    pub ttl: Duration,\n    /// Whether to cache completions.\n    pub cache_completions: bool,\n    /// Whether to cache embeddings.\n    pub cache_embeddings: bool,\n}\n\nimpl Default for CacheConfig {\n    fn default() -> Self {\n        Self {\n            max_entries: 1000,\n            ttl: Duration::from_secs(3600), // 1 hour\n            cache_completions: true,\n            cache_embeddings: true,\n        }\n    }\n}\n\nimpl CacheConfig {\n    /// Create a new cache config with specified max entries.\n    pub fn new(max_entries: usize) -> Self {\n        Self {\n            max_entries,\n            ..Default::default()\n        }\n    }\n\n    /// Set the TTL for cache entries.\n    pub fn with_ttl(mut self, ttl: Duration) -> Self {\n        self.ttl = ttl;\n        self\n    }\n\n    /// Enable or disable completion caching.\n    pub fn with_completion_caching(mut self, enabled: bool) -> Self {\n        self.cache_completions = enabled;\n        self\n    }\n\n    /// Enable or disable embedding caching.\n    pub fn with_embedding_caching(mut self, enabled: bool) -> Self {\n        self.cache_embeddings = enabled;\n        self\n    }\n}\n\n/// A cached entry with metadata.\n#[derive(Debug, Clone)]\nstruct CacheEntry<T> {\n    value: T,\n    created_at: Instant,\n    access_count: usize,\n}\n\nimpl<T: Clone> CacheEntry<T> {\n    fn new(value: T) -> Self {\n        Self {\n            value,\n            created_at: Instant::now(),\n            access_count: 0,\n        }\n    }\n\n    fn is_expired(&self, ttl: Duration) -> bool {\n        self.created_at.elapsed() > ttl\n    }\n\n    fn access(&mut self) -> T {\n        self.access_count += 1;\n        self.value.clone()\n    }\n}\n\n/// Cache key derived from prompt/input.\n#[derive(Debug, Clone, Eq, PartialEq, Hash)]\nstruct CacheKey {\n    hash: u64,\n}\n\nimpl CacheKey {\n    fn from_prompt(prompt: &str) -> Self {\n        let mut hasher = std::collections::hash_map::DefaultHasher::new();\n        prompt.hash(&mut hasher);\n        Self {\n            hash: hasher.finish(),\n        }\n    }\n\n    fn from_texts(texts: &[&str]) -> Self {\n        let mut hasher = std::collections::hash_map::DefaultHasher::new();\n        for text in texts {\n            text.hash(&mut hasher);\n        }\n        Self {\n            hash: hasher.finish(),\n        }\n    }\n}\n\n/// LLM cache statistics.\n#[derive(Debug, Clone, Default)]\npub struct CacheStats {\n    /// Number of cache hits.\n    pub hits: usize,\n    /// Number of cache misses.\n    pub misses: usize,\n    /// Current number of entries.\n    pub entries: usize,\n    /// Number of evictions.\n    pub evictions: usize,\n}\n\nimpl CacheStats {\n    /// Get the cache hit rate.\n    pub fn hit_rate(&self) -> f64 {\n        let total = self.hits + self.misses;\n        if total == 0 {\n            0.0\n        } else {\n            self.hits as f64 / total as f64\n        }\n    }\n}\n\n/// In-memory LLM cache.\n/// @implements FEAT0014\npub struct LLMCache {\n    config: CacheConfig,\n    completions: RwLock<HashMap<CacheKey, CacheEntry<LLMResponse>>>,\n    embeddings: RwLock<HashMap<CacheKey, CacheEntry<Vec<Vec<f32>>>>>,\n    stats: RwLock<CacheStats>,\n}\n\nimpl LLMCache {\n    /// Create a new LLM cache with the given configuration.\n    pub fn new(config: CacheConfig) -> Self {\n        Self {\n            config,\n            completions: RwLock::new(HashMap::new()),\n            embeddings: RwLock::new(HashMap::new()),\n            stats: RwLock::new(CacheStats::default()),\n        }\n    }\n\n    /// Get cache statistics.\n    pub async fn stats(&self) -> CacheStats {\n        let stats = self.stats.read().await;\n        let completions = self.completions.read().await;\n        let embeddings = self.embeddings.read().await;\n\n        CacheStats {\n            entries: completions.len() + embeddings.len(),\n            ..*stats\n        }\n    }\n\n    /// Clear all cache entries.\n    pub async fn clear(&self) {\n        let mut completions = self.completions.write().await;\n        let mut embeddings = self.embeddings.write().await;\n        let mut stats = self.stats.write().await;\n\n        let evicted = completions.len() + embeddings.len();\n        completions.clear();\n        embeddings.clear();\n        stats.evictions += evicted;\n    }\n\n    /// Get a cached completion response.\n    pub async fn get_completion(&self, prompt: &str) -> Option<LLMResponse> {\n        if !self.config.cache_completions {\n            return None;\n        }\n\n        let key = CacheKey::from_prompt(prompt);\n        let mut cache = self.completions.write().await;\n\n        if let Some(entry) = cache.get_mut(&key) {\n            if entry.is_expired(self.config.ttl) {\n                cache.remove(&key);\n                let mut stats = self.stats.write().await;\n                stats.misses += 1;\n                stats.evictions += 1;\n                return None;\n            }\n\n            let mut stats = self.stats.write().await;\n            stats.hits += 1;\n            return Some(entry.access());\n        }\n\n        let mut stats = self.stats.write().await;\n        stats.misses += 1;\n        None\n    }\n\n    /// Store a completion response in cache.\n    pub async fn put_completion(&self, prompt: &str, response: LLMResponse) {\n        if !self.config.cache_completions {\n            return;\n        }\n\n        let key = CacheKey::from_prompt(prompt);\n        let mut cache = self.completions.write().await;\n\n        // Evict if at capacity\n        if cache.len() >= self.config.max_entries {\n            self.evict_lru(&mut cache).await;\n        }\n\n        cache.insert(key, CacheEntry::new(response));\n    }\n\n    /// Get cached embeddings.\n    pub async fn get_embeddings(&self, texts: &[&str]) -> Option<Vec<Vec<f32>>> {\n        if !self.config.cache_embeddings {\n            return None;\n        }\n\n        let key = CacheKey::from_texts(texts);\n        let mut cache = self.embeddings.write().await;\n\n        if let Some(entry) = cache.get_mut(&key) {\n            if entry.is_expired(self.config.ttl) {\n                cache.remove(&key);\n                let mut stats = self.stats.write().await;\n                stats.misses += 1;\n                stats.evictions += 1;\n                return None;\n            }\n\n            let mut stats = self.stats.write().await;\n            stats.hits += 1;\n            return Some(entry.access());\n        }\n\n        let mut stats = self.stats.write().await;\n        stats.misses += 1;\n        None\n    }\n\n    /// Store embeddings in cache.\n    pub async fn put_embeddings(&self, texts: &[&str], embeddings: Vec<Vec<f32>>) {\n        if !self.config.cache_embeddings {\n            return;\n        }\n\n        let key = CacheKey::from_texts(texts);\n        let mut cache = self.embeddings.write().await;\n\n        // Evict if at capacity\n        if cache.len() >= self.config.max_entries {\n            self.evict_lru_embeddings(&mut cache).await;\n        }\n\n        cache.insert(key, CacheEntry::new(embeddings));\n    }\n\n    async fn evict_lru<T: Clone>(&self, cache: &mut HashMap<CacheKey, CacheEntry<T>>) {\n        // Find the least recently used entry (oldest with fewest accesses)\n        if let Some(key) = cache\n            .iter()\n            .min_by_key(|(_, entry)| (entry.access_count, entry.created_at))\n            .map(|(k, _)| k.clone())\n        {\n            cache.remove(&key);\n            let mut stats = self.stats.write().await;\n            stats.evictions += 1;\n        }\n    }\n\n    async fn evict_lru_embeddings(&self, cache: &mut HashMap<CacheKey, CacheEntry<Vec<Vec<f32>>>>) {\n        if let Some(key) = cache\n            .iter()\n            .min_by_key(|(_, entry)| (entry.access_count, entry.created_at))\n            .map(|(k, _)| k.clone())\n        {\n            cache.remove(&key);\n            let mut stats = self.stats.write().await;\n            stats.evictions += 1;\n        }\n    }\n}\n\n/// A cached LLM provider that wraps another provider with caching.\npub struct CachedProvider<P> {\n    inner: P,\n    cache: Arc<LLMCache>,\n}\n\nimpl<P> CachedProvider<P> {\n    /// Create a new cached provider.\n    pub fn new(inner: P, cache: Arc<LLMCache>) -> Self {\n        Self { inner, cache }\n    }\n\n    /// Create with default cache config.\n    pub fn with_default_cache(inner: P) -> Self {\n        Self {\n            inner,\n            cache: Arc::new(LLMCache::new(CacheConfig::default())),\n        }\n    }\n\n    /// Get cache statistics.\n    pub async fn cache_stats(&self) -> CacheStats {\n        self.cache.stats().await\n    }\n\n    /// Clear the cache.\n    pub async fn clear_cache(&self) {\n        self.cache.clear().await;\n    }\n}\n\n#[async_trait]\nimpl<P: LLMProvider> LLMProvider for CachedProvider<P> {\n    fn name(&self) -> &str {\n        self.inner.name()\n    }\n\n    fn model(&self) -> &str {\n        self.inner.model()\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.inner.max_context_length()\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        // Check cache first\n        if let Some(cached) = self.cache.get_completion(prompt).await {\n            tracing::debug!(\"Cache hit for completion\");\n            return Ok(cached);\n        }\n\n        // Call underlying provider\n        let response = self.inner.complete(prompt).await?;\n\n        // Store in cache\n        self.cache.put_completion(prompt, response.clone()).await;\n\n        Ok(response)\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        // For options-based completions, we skip caching since options affect output\n        self.inner.complete_with_options(prompt, options).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        // For chat completions, we skip caching since message history varies\n        self.inner.chat(messages, options).await\n    }\n}\n\n#[async_trait]\nimpl<P: EmbeddingProvider> EmbeddingProvider for CachedProvider<P> {\n    fn name(&self) -> &str {\n        self.inner.name()\n    }\n\n    fn model(&self) -> &str {\n        self.inner.model()\n    }\n\n    fn dimension(&self) -> usize {\n        self.inner.dimension()\n    }\n\n    fn max_tokens(&self) -> usize {\n        self.inner.max_tokens()\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // Convert to &str for cache lookup\n        let text_refs: Vec<&str> = texts.iter().map(|s| s.as_str()).collect();\n\n        // Check cache first\n        if let Some(cached) = self.cache.get_embeddings(&text_refs).await {\n            tracing::debug!(\"Cache hit for embeddings\");\n            return Ok(cached);\n        }\n\n        // Call underlying provider\n        let embeddings = self.inner.embed(texts).await?;\n\n        // Store in cache\n        self.cache\n            .put_embeddings(&text_refs, embeddings.clone())\n            .await;\n\n        Ok(embeddings)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_cache_key_from_prompt() {\n        let key1 = CacheKey::from_prompt(\"Hello world\");\n        let key2 = CacheKey::from_prompt(\"Hello world\");\n        let key3 = CacheKey::from_prompt(\"Different prompt\");\n\n        assert_eq!(key1, key2);\n        assert_ne!(key1, key3);\n    }\n\n    #[test]\n    fn test_cache_key_from_texts() {\n        let key1 = CacheKey::from_texts(&[\"a\", \"b\", \"c\"]);\n        let key2 = CacheKey::from_texts(&[\"a\", \"b\", \"c\"]);\n        let key3 = CacheKey::from_texts(&[\"x\", \"y\", \"z\"]);\n\n        assert_eq!(key1, key2);\n        assert_ne!(key1, key3);\n    }\n\n    #[test]\n    fn test_cache_config_default() {\n        let config = CacheConfig::default();\n        assert_eq!(config.max_entries, 1000);\n        assert!(config.cache_completions);\n        assert!(config.cache_embeddings);\n    }\n\n    #[test]\n    fn test_cache_config_builder() {\n        let config = CacheConfig::new(500)\n            .with_ttl(Duration::from_secs(600))\n            .with_completion_caching(false);\n\n        assert_eq!(config.max_entries, 500);\n        assert_eq!(config.ttl, Duration::from_secs(600));\n        assert!(!config.cache_completions);\n    }\n\n    #[tokio::test]\n    async fn test_cache_stats() {\n        let cache = LLMCache::new(CacheConfig::default());\n        let stats = cache.stats().await;\n\n        assert_eq!(stats.hits, 0);\n        assert_eq!(stats.misses, 0);\n        assert_eq!(stats.entries, 0);\n    }\n\n    #[tokio::test]\n    async fn test_cache_miss() {\n        let cache = LLMCache::new(CacheConfig::default());\n        let result = cache.get_completion(\"test prompt\").await;\n\n        assert!(result.is_none());\n\n        let stats = cache.stats().await;\n        assert_eq!(stats.misses, 1);\n    }\n\n    #[tokio::test]\n    async fn test_cache_hit() {\n        let cache = LLMCache::new(CacheConfig::default());\n\n        let response = LLMResponse::new(\"test response\", \"gpt-4\").with_usage(10, 5);\n\n        cache.put_completion(\"test prompt\", response.clone()).await;\n        let result = cache.get_completion(\"test prompt\").await;\n\n        assert!(result.is_some());\n        assert_eq!(result.unwrap().content, \"test response\");\n\n        let stats = cache.stats().await;\n        assert_eq!(stats.hits, 1);\n    }\n\n    #[tokio::test]\n    async fn test_cache_clear() {\n        let cache = LLMCache::new(CacheConfig::default());\n\n        let response = LLMResponse::new(\"test\", \"gpt-4\").with_usage(1, 1);\n\n        cache.put_completion(\"prompt\", response).await;\n        assert_eq!(cache.stats().await.entries, 1);\n\n        cache.clear().await;\n        assert_eq!(cache.stats().await.entries, 0);\n    }\n\n    #[test]\n    fn test_hit_rate() {\n        let mut stats = CacheStats::default();\n        assert_eq!(stats.hit_rate(), 0.0);\n\n        stats.hits = 3;\n        stats.misses = 1;\n        assert_eq!(stats.hit_rate(), 0.75);\n    }\n\n    #[tokio::test]\n    async fn test_embedding_cache() {\n        let cache = LLMCache::new(CacheConfig::default());\n\n        let embeddings = vec![vec![1.0, 2.0, 3.0], vec![4.0, 5.0, 6.0]];\n        let texts = [\"text1\", \"text2\"];\n\n        cache.put_embeddings(&texts, embeddings.clone()).await;\n        let result = cache.get_embeddings(&texts).await;\n\n        assert!(result.is_some());\n        assert_eq!(result.unwrap(), embeddings);\n    }\n\n    #[tokio::test]\n    async fn test_disabled_caching() {\n        let config = CacheConfig::default()\n            .with_completion_caching(false)\n            .with_embedding_caching(false);\n        let cache = LLMCache::new(config);\n\n        let response = LLMResponse::new(\"test\", \"gpt-4\").with_usage(1, 1);\n\n        cache.put_completion(\"prompt\", response).await;\n        assert!(cache.get_completion(\"prompt\").await.is_none());\n\n        cache.put_embeddings(&[\"text\"], vec![vec![1.0]]).await;\n        assert!(cache.get_embeddings(&[\"text\"]).await.is_none());\n    }\n\n    #[tokio::test]\n    async fn test_ttl_expiration_completion() {\n        // TTL of 1ms ensures entries expire quickly\n        let config = CacheConfig::new(100).with_ttl(Duration::from_millis(1));\n        let cache = LLMCache::new(config);\n\n        let response = LLMResponse::new(\"expires\", \"gpt-4\").with_usage(5, 3);\n        cache.put_completion(\"ephemeral\", response).await;\n\n        // Wait for TTL to elapse\n        tokio::time::sleep(Duration::from_millis(10)).await;\n\n        let result = cache.get_completion(\"ephemeral\").await;\n        assert!(result.is_none(), \"Expired entry should return None\");\n\n        let stats = cache.stats().await;\n        assert_eq!(stats.evictions, 1, \"Expired entry should count as eviction\");\n        assert_eq!(stats.misses, 1);\n    }\n\n    #[tokio::test]\n    async fn test_ttl_expiration_embeddings() {\n        let config = CacheConfig::new(100).with_ttl(Duration::from_millis(1));\n        let cache = LLMCache::new(config);\n\n        cache\n            .put_embeddings(&[\"txt\"], vec![vec![1.0, 2.0]])\n            .await;\n\n        tokio::time::sleep(Duration::from_millis(10)).await;\n\n        assert!(cache.get_embeddings(&[\"txt\"]).await.is_none());\n        let stats = cache.stats().await;\n        assert_eq!(stats.evictions, 1);\n    }\n\n    #[tokio::test]\n    async fn test_lru_eviction_completions() {\n        // Cache with max 2 entries\n        let config = CacheConfig::new(2);\n        let cache = LLMCache::new(config);\n\n        let r1 = LLMResponse::new(\"first\", \"gpt-4\").with_usage(1, 1);\n        let r2 = LLMResponse::new(\"second\", \"gpt-4\").with_usage(1, 1);\n        let r3 = LLMResponse::new(\"third\", \"gpt-4\").with_usage(1, 1);\n\n        cache.put_completion(\"p1\", r1).await;\n        cache.put_completion(\"p2\", r2).await;\n\n        // Access p2 to bump its access count\n        let _ = cache.get_completion(\"p2\").await;\n\n        // Inserting p3 should evict p1 (least recently used)\n        cache.put_completion(\"p3\", r3).await;\n\n        assert!(\n            cache.get_completion(\"p1\").await.is_none(),\n            \"p1 should have been evicted\"\n        );\n        // p2 and p3 should still exist\n        assert!(cache.get_completion(\"p2\").await.is_some());\n        assert!(cache.get_completion(\"p3\").await.is_some());\n    }\n\n    #[tokio::test]\n    async fn test_lru_eviction_embeddings() {\n        let config = CacheConfig::new(1);\n        let cache = LLMCache::new(config);\n\n        cache\n            .put_embeddings(&[\"a\"], vec![vec![1.0]])\n            .await;\n        cache\n            .put_embeddings(&[\"b\"], vec![vec![2.0]])\n            .await;\n\n        // \"a\" should have been evicted\n        assert!(cache.get_embeddings(&[\"a\"]).await.is_none());\n        assert!(cache.get_embeddings(&[\"b\"]).await.is_some());\n    }\n\n    #[tokio::test]\n    async fn test_access_count_increments() {\n        let cache = LLMCache::new(CacheConfig::default());\n        let response = LLMResponse::new(\"counter\", \"gpt-4\").with_usage(1, 1);\n\n        cache.put_completion(\"cnt\", response).await;\n\n        // Access 3 times\n        for _ in 0..3 {\n            let _ = cache.get_completion(\"cnt\").await;\n        }\n\n        let stats = cache.stats().await;\n        assert_eq!(stats.hits, 3);\n    }\n\n    #[tokio::test]\n    async fn test_cache_entry_is_expired() {\n        let entry = CacheEntry::new(\"value\".to_string());\n        // Just-created entry with large TTL should not be expired\n        assert!(!entry.is_expired(Duration::from_secs(3600)));\n        // Entry with zero TTL should be expired\n        assert!(entry.is_expired(Duration::ZERO));\n    }\n\n    #[tokio::test]\n    async fn test_cached_provider_complete_delegates() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        mock.add_response(\"cached answer\").await;\n\n        let cache = Arc::new(LLMCache::new(CacheConfig::default()));\n        let provider = CachedProvider::new(mock, cache);\n\n        // First call: cache miss, delegates to inner\n        let r1 = provider.inner.complete(\"hello\").await.unwrap();\n        assert_eq!(r1.content, \"cached answer\");\n    }\n\n    #[tokio::test]\n    async fn test_cached_provider_name_model_delegates() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let cache = Arc::new(LLMCache::new(CacheConfig::default()));\n        let provider = CachedProvider::new(mock, cache);\n\n        assert_eq!(LLMProvider::name(&provider), \"mock\");\n        assert_eq!(LLMProvider::model(&provider), \"mock-model\");\n        assert_eq!(provider.max_context_length(), 4096);\n    }\n\n    #[tokio::test]\n    async fn test_cached_provider_with_default_cache() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let provider = CachedProvider::with_default_cache(mock);\n\n        let stats = provider.cache_stats().await;\n        assert_eq!(stats.entries, 0);\n        assert_eq!(stats.hits, 0);\n    }\n\n    #[tokio::test]\n    async fn test_cached_provider_clear_cache() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let cache = Arc::new(LLMCache::new(CacheConfig::default()));\n        let provider = CachedProvider::new(mock, cache);\n\n        // Put something in cache directly\n        provider\n            .cache\n            .put_completion(\"test\", LLMResponse::new(\"v\", \"m\").with_usage(1, 1))\n            .await;\n        assert_eq!(provider.cache_stats().await.entries, 1);\n\n        provider.clear_cache().await;\n        assert_eq!(provider.cache_stats().await.entries, 0);\n    }\n\n    #[test]\n    fn test_cache_key_empty_prompt() {\n        let k1 = CacheKey::from_prompt(\"\");\n        let k2 = CacheKey::from_prompt(\"\");\n        assert_eq!(k1, k2);\n    }\n\n    #[test]\n    fn test_cache_key_empty_texts() {\n        let k = CacheKey::from_texts(&[]);\n        let k2 = CacheKey::from_texts(&[]);\n        assert_eq!(k, k2);\n    }\n\n    #[tokio::test]\n    async fn test_multiple_put_same_key_overwrites() {\n        let cache = LLMCache::new(CacheConfig::default());\n        let r1 = LLMResponse::new(\"first\", \"m\").with_usage(1, 1);\n        let r2 = LLMResponse::new(\"second\", \"m\").with_usage(1, 1);\n\n        cache.put_completion(\"key\", r1).await;\n        cache.put_completion(\"key\", r2).await;\n\n        let result = cache.get_completion(\"key\").await.unwrap();\n        assert_eq!(result.content, \"second\");\n        // Only 1 entry despite 2 puts to same key\n        assert_eq!(cache.stats().await.entries, 1);\n    }\n\n    #[test]\n    fn test_cache_config_with_embedding_caching() {\n        let config = CacheConfig::default().with_embedding_caching(false);\n        assert!(!config.cache_embeddings);\n        assert!(config.cache_completions); // default true\n    }\n\n    #[tokio::test]\n    async fn test_clear_updates_eviction_count() {\n        let cache = LLMCache::new(CacheConfig::default());\n        let r = LLMResponse::new(\"a\", \"m\").with_usage(1, 1);\n        cache.put_completion(\"x\", r).await;\n        cache.put_embeddings(&[\"y\"], vec![vec![1.0]]).await;\n\n        cache.clear().await;\n        let stats = cache.stats().await;\n        assert_eq!(stats.evictions, 2, \"Clear should count 2 evictions\");\n        assert_eq!(stats.entries, 0);\n    }\n\n    #[tokio::test]\n    async fn test_cached_provider_embed_delegates() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let cache = Arc::new(LLMCache::new(CacheConfig::default()));\n        let provider = CachedProvider::new(mock, cache);\n\n        // First call goes to inner\n        let result = provider.embed(&[\"hello\".to_string()]).await.unwrap();\n        assert_eq!(result.len(), 1);\n        assert_eq!(result[0].len(), 1536);\n\n        // Second call should hit cache\n        let result2 = provider.embed(&[\"hello\".to_string()]).await.unwrap();\n        assert_eq!(result2, result);\n\n        let stats = provider.cache_stats().await;\n        assert_eq!(stats.hits, 1);\n    }\n}\n","traces":[{"line":42,"address":[],"length":0,"stats":{"Line":24}},{"line":45,"address":[],"length":0,"stats":{"Line":24}},{"line":54,"address":[],"length":0,"stats":{"Line":5}},{"line":62,"address":[],"length":0,"stats":{"Line":3}},{"line":63,"address":[],"length":0,"stats":{"Line":3}},{"line":64,"address":[],"length":0,"stats":{"Line":3}},{"line":68,"address":[],"length":0,"stats":{"Line":2}},{"line":69,"address":[],"length":0,"stats":{"Line":2}},{"line":70,"address":[],"length":0,"stats":{"Line":2}},{"line":74,"address":[],"length":0,"stats":{"Line":2}},{"line":75,"address":[],"length":0,"stats":{"Line":2}},{"line":76,"address":[],"length":0,"stats":{"Line":2}},{"line":89,"address":[],"length":0,"stats":{"Line":20}},{"line":92,"address":[],"length":0,"stats":{"Line":20}},{"line":97,"address":[],"length":0,"stats":{"Line":16}},{"line":98,"address":[],"length":0,"stats":{"Line":16}},{"line":101,"address":[],"length":0,"stats":{"Line":12}},{"line":102,"address":[],"length":0,"stats":{"Line":12}},{"line":103,"address":[],"length":0,"stats":{"Line":24}},{"line":114,"address":[],"length":0,"stats":{"Line":32}},{"line":115,"address":[],"length":0,"stats":{"Line":64}},{"line":116,"address":[],"length":0,"stats":{"Line":96}},{"line":118,"address":[],"length":0,"stats":{"Line":32}},{"line":122,"address":[],"length":0,"stats":{"Line":17}},{"line":123,"address":[],"length":0,"stats":{"Line":34}},{"line":124,"address":[],"length":0,"stats":{"Line":86}},{"line":125,"address":[],"length":0,"stats":{"Line":46}},{"line":128,"address":[],"length":0,"stats":{"Line":17}},{"line":148,"address":[],"length":0,"stats":{"Line":2}},{"line":149,"address":[],"length":0,"stats":{"Line":4}},{"line":150,"address":[],"length":0,"stats":{"Line":2}},{"line":151,"address":[],"length":0,"stats":{"Line":1}},{"line":153,"address":[],"length":0,"stats":{"Line":1}},{"line":169,"address":[],"length":0,"stats":{"Line":20}},{"line":172,"address":[],"length":0,"stats":{"Line":60}},{"line":173,"address":[],"length":0,"stats":{"Line":60}},{"line":174,"address":[],"length":0,"stats":{"Line":20}},{"line":179,"address":[],"length":0,"stats":{"Line":30}},{"line":180,"address":[],"length":0,"stats":{"Line":45}},{"line":181,"address":[],"length":0,"stats":{"Line":45}},{"line":182,"address":[],"length":0,"stats":{"Line":45}},{"line":185,"address":[],"length":0,"stats":{"Line":30}},{"line":191,"address":[],"length":0,"stats":{"Line":6}},{"line":192,"address":[],"length":0,"stats":{"Line":9}},{"line":193,"address":[],"length":0,"stats":{"Line":9}},{"line":194,"address":[],"length":0,"stats":{"Line":9}},{"line":196,"address":[],"length":0,"stats":{"Line":9}},{"line":197,"address":[],"length":0,"stats":{"Line":3}},{"line":198,"address":[],"length":0,"stats":{"Line":3}},{"line":199,"address":[],"length":0,"stats":{"Line":3}},{"line":203,"address":[],"length":0,"stats":{"Line":30}},{"line":204,"address":[],"length":0,"stats":{"Line":15}},{"line":205,"address":[],"length":0,"stats":{"Line":1}},{"line":208,"address":[],"length":0,"stats":{"Line":42}},{"line":209,"address":[],"length":0,"stats":{"Line":42}},{"line":211,"address":[],"length":0,"stats":{"Line":38}},{"line":212,"address":[],"length":0,"stats":{"Line":30}},{"line":213,"address":[],"length":0,"stats":{"Line":2}},{"line":214,"address":[],"length":0,"stats":{"Line":3}},{"line":215,"address":[],"length":0,"stats":{"Line":1}},{"line":216,"address":[],"length":0,"stats":{"Line":1}},{"line":217,"address":[],"length":0,"stats":{"Line":1}},{"line":220,"address":[],"length":0,"stats":{"Line":27}},{"line":221,"address":[],"length":0,"stats":{"Line":9}},{"line":222,"address":[],"length":0,"stats":{"Line":9}},{"line":225,"address":[],"length":0,"stats":{"Line":12}},{"line":226,"address":[],"length":0,"stats":{"Line":4}},{"line":227,"address":[],"length":0,"stats":{"Line":4}},{"line":231,"address":[],"length":0,"stats":{"Line":28}},{"line":232,"address":[],"length":0,"stats":{"Line":14}},{"line":233,"address":[],"length":0,"stats":{"Line":1}},{"line":236,"address":[],"length":0,"stats":{"Line":39}},{"line":237,"address":[],"length":0,"stats":{"Line":39}},{"line":240,"address":[],"length":0,"stats":{"Line":13}},{"line":241,"address":[],"length":0,"stats":{"Line":3}},{"line":244,"address":[],"length":0,"stats":{"Line":52}},{"line":248,"address":[],"length":0,"stats":{"Line":14}},{"line":249,"address":[],"length":0,"stats":{"Line":7}},{"line":250,"address":[],"length":0,"stats":{"Line":1}},{"line":253,"address":[],"length":0,"stats":{"Line":18}},{"line":254,"address":[],"length":0,"stats":{"Line":18}},{"line":256,"address":[],"length":0,"stats":{"Line":16}},{"line":257,"address":[],"length":0,"stats":{"Line":12}},{"line":258,"address":[],"length":0,"stats":{"Line":2}},{"line":259,"address":[],"length":0,"stats":{"Line":3}},{"line":260,"address":[],"length":0,"stats":{"Line":1}},{"line":261,"address":[],"length":0,"stats":{"Line":1}},{"line":262,"address":[],"length":0,"stats":{"Line":1}},{"line":265,"address":[],"length":0,"stats":{"Line":9}},{"line":266,"address":[],"length":0,"stats":{"Line":3}},{"line":267,"address":[],"length":0,"stats":{"Line":3}},{"line":270,"address":[],"length":0,"stats":{"Line":6}},{"line":271,"address":[],"length":0,"stats":{"Line":2}},{"line":272,"address":[],"length":0,"stats":{"Line":2}},{"line":276,"address":[],"length":0,"stats":{"Line":14}},{"line":277,"address":[],"length":0,"stats":{"Line":7}},{"line":278,"address":[],"length":0,"stats":{"Line":1}},{"line":281,"address":[],"length":0,"stats":{"Line":18}},{"line":282,"address":[],"length":0,"stats":{"Line":18}},{"line":285,"address":[],"length":0,"stats":{"Line":6}},{"line":286,"address":[],"length":0,"stats":{"Line":3}},{"line":289,"address":[],"length":0,"stats":{"Line":24}},{"line":292,"address":[],"length":0,"stats":{"Line":2}},{"line":294,"address":[],"length":0,"stats":{"Line":2}},{"line":296,"address":[],"length":0,"stats":{"Line":5}},{"line":297,"address":[],"length":0,"stats":{"Line":3}},{"line":299,"address":[],"length":0,"stats":{"Line":3}},{"line":300,"address":[],"length":0,"stats":{"Line":3}},{"line":301,"address":[],"length":0,"stats":{"Line":1}},{"line":305,"address":[],"length":0,"stats":{"Line":2}},{"line":306,"address":[],"length":0,"stats":{"Line":2}},{"line":308,"address":[],"length":0,"stats":{"Line":3}},{"line":309,"address":[],"length":0,"stats":{"Line":3}},{"line":311,"address":[],"length":0,"stats":{"Line":3}},{"line":312,"address":[],"length":0,"stats":{"Line":3}},{"line":313,"address":[],"length":0,"stats":{"Line":1}},{"line":326,"address":[],"length":0,"stats":{"Line":5}},{"line":331,"address":[],"length":0,"stats":{"Line":1}},{"line":334,"address":[],"length":0,"stats":{"Line":2}},{"line":339,"address":[],"length":0,"stats":{"Line":8}},{"line":340,"address":[],"length":0,"stats":{"Line":4}},{"line":344,"address":[],"length":0,"stats":{"Line":2}},{"line":345,"address":[],"length":0,"stats":{"Line":1}},{"line":351,"address":[],"length":0,"stats":{"Line":1}},{"line":352,"address":[],"length":0,"stats":{"Line":1}},{"line":355,"address":[],"length":0,"stats":{"Line":1}},{"line":356,"address":[],"length":0,"stats":{"Line":1}},{"line":359,"address":[],"length":0,"stats":{"Line":1}},{"line":360,"address":[],"length":0,"stats":{"Line":2}},{"line":363,"address":[],"length":0,"stats":{"Line":3}},{"line":365,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":385,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":400,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[],"length":0,"stats":{"Line":0}},{"line":404,"address":[],"length":0,"stats":{"Line":0}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":0}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[],"length":0,"stats":{"Line":0}},{"line":413,"address":[],"length":0,"stats":{"Line":0}},{"line":416,"address":[],"length":0,"stats":{"Line":2}},{"line":418,"address":[],"length":0,"stats":{"Line":4}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":422,"address":[],"length":0,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}}],"covered":132,"coverable":156},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","cache_prompt.rs"],"content":"//! Prompt caching utilities for Anthropic Claude models.\n//!\n//! # OODA-17: Anthropic Prompt Caching\n//!\n//! This module provides utilities for leveraging Anthropic's prompt caching feature\n//! to reduce costs by 85-90% on repeated context.\n//!\n//! # Overview\n//!\n//! Prompt caching allows marking parts of the conversation as cacheable:\n//! - System prompts (rarely change)\n//! - Large file contexts (repeated across calls)\n//! - Recent conversation history (conversation context)\n//!\n//! Cached tokens are served at 90% discount for subsequent requests.\n//!\n//! # Usage\n//!\n//! ```rust\n//! use edgequake_llm::cache_prompt::{CachePromptConfig, apply_cache_control};\n//! use edgequake_llm::traits::ChatMessage;\n//!\n//! let config = CachePromptConfig::default();\n//! let mut messages = vec![\n//!     ChatMessage::system(\"You are a helpful assistant\"),\n//!     ChatMessage::user(\"Large file content here...\"),\n//! ];\n//!\n//! apply_cache_control(&mut messages, &config);\n//! // Now messages have cache_control set where appropriate\n//! ```\n//!\n//! # See Also\n//!\n//! - [Anthropic Prompt Caching](https://docs.anthropic.com/claude/docs/prompt-caching)\n//! - Aider reference: `base_coder.py`, `sendchat.py`\n\nuse crate::traits::{CacheControl, ChatMessage, ChatRole};\nuse serde::{Deserialize, Serialize};\n\n/// Configuration for automatic prompt cache control marking.\n///\n/// # Fields\n///\n/// - `enabled`: Whether to apply cache control (default: true)\n/// - `min_content_length`: Minimum message length to auto-cache (default: 1000)\n/// - `cache_system_prompt`: Whether to cache system prompts (default: true)\n/// - `cache_last_n_messages`: Number of recent user messages to cache (default: 3)\n///\n/// # Example\n///\n/// ```rust\n/// use edgequake_llm::cache_prompt::CachePromptConfig;\n///\n/// // Use defaults\n/// let config = CachePromptConfig::default();\n///\n/// // Custom configuration\n/// let config = CachePromptConfig {\n///     enabled: true,\n///     min_content_length: 500,\n///     cache_system_prompt: true,\n///     cache_last_n_messages: 5,\n/// };\n/// ```\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CachePromptConfig {\n    /// Whether cache control marking is enabled.\n    pub enabled: bool,\n\n    /// Minimum content length (in characters) to auto-cache user messages.\n    ///\n    /// Messages shorter than this threshold are not automatically cached\n    /// unless they are system prompts or in the last N messages.\n    pub min_content_length: usize,\n\n    /// Whether to cache the system prompt.\n    ///\n    /// System prompts rarely change and are excellent cache candidates.\n    pub cache_system_prompt: bool,\n\n    /// Number of recent user messages to cache.\n    ///\n    /// Caching recent messages helps with conversation context retention.\n    pub cache_last_n_messages: usize,\n}\n\nimpl Default for CachePromptConfig {\n    fn default() -> Self {\n        Self {\n            enabled: true,\n            min_content_length: 1000,\n            cache_system_prompt: true,\n            cache_last_n_messages: 3,\n        }\n    }\n}\n\nimpl CachePromptConfig {\n    /// Create a config with caching disabled.\n    pub fn disabled() -> Self {\n        Self {\n            enabled: false,\n            ..Default::default()\n        }\n    }\n\n    /// Create a config that only caches system prompts.\n    pub fn system_only() -> Self {\n        Self {\n            enabled: true,\n            min_content_length: usize::MAX,\n            cache_system_prompt: true,\n            cache_last_n_messages: 0,\n        }\n    }\n\n    /// Create an aggressive caching config.\n    ///\n    /// Caches more content for maximum cost reduction.\n    pub fn aggressive() -> Self {\n        Self {\n            enabled: true,\n            min_content_length: 100,\n            cache_system_prompt: true,\n            cache_last_n_messages: 10,\n        }\n    }\n}\n\n/// Statistics about cache usage from an API response.\n///\n/// Anthropic returns cache statistics in the usage field of responses:\n/// - `cache_read_input_tokens`: Tokens served from cache (90% cheaper)\n/// - `cache_creation_input_tokens`: Tokens used to create the cache\n///\n/// # Cost Model\n///\n/// - Normal input tokens: $0.003 per 1K tokens\n/// - Cached input tokens: $0.0003 per 1K tokens (90% discount)\n/// - Cache creation has a small overhead but pays off after 2-3 uses\n///\n/// # Example\n///\n/// ```rust\n/// use edgequake_llm::cache_prompt::CacheStats;\n///\n/// let stats = CacheStats {\n///     input_tokens: 10000,\n///     output_tokens: 1000,\n///     cache_read_tokens: 8000,\n///     cache_creation_tokens: 0,\n/// };\n///\n/// println!(\"Cache hit rate: {:.0}%\", stats.cache_hit_rate() * 100.0);\n/// println!(\"Estimated savings: ${:.4}\", stats.savings());\n/// ```\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct CacheStats {\n    /// Total input tokens in the request.\n    pub input_tokens: u64,\n\n    /// Total output tokens in the response.\n    pub output_tokens: u64,\n\n    /// Input tokens served from cache.\n    pub cache_read_tokens: u64,\n\n    /// Tokens used to create new cache entries.\n    pub cache_creation_tokens: u64,\n}\n\nimpl CacheStats {\n    /// Create new cache stats.\n    pub fn new(\n        input_tokens: u64,\n        output_tokens: u64,\n        cache_read_tokens: u64,\n        cache_creation_tokens: u64,\n    ) -> Self {\n        Self {\n            input_tokens,\n            output_tokens,\n            cache_read_tokens,\n            cache_creation_tokens,\n        }\n    }\n\n    /// Calculate the cache hit rate as a fraction (0.0 to 1.0).\n    ///\n    /// Returns 0.0 if there are no input tokens.\n    pub fn cache_hit_rate(&self) -> f64 {\n        if self.input_tokens == 0 {\n            0.0\n        } else {\n            self.cache_read_tokens as f64 / self.input_tokens as f64\n        }\n    }\n\n    /// Estimate cost savings in dollars.\n    ///\n    /// Based on Anthropic Claude pricing (as of 2024):\n    /// - Normal input: $0.003 per 1K tokens\n    /// - Cached input: $0.0003 per 1K tokens\n    ///\n    /// Returns the difference between what would have been paid\n    /// without caching vs with caching.\n    pub fn savings(&self) -> f64 {\n        const NORMAL_COST_PER_1K: f64 = 0.003;\n        const CACHE_COST_PER_1K: f64 = 0.0003;\n\n        // Cost without caching\n        let normal_cost = self.input_tokens as f64 * NORMAL_COST_PER_1K / 1000.0;\n\n        // Cost with caching\n        let uncached_tokens = self.input_tokens.saturating_sub(self.cache_read_tokens);\n        let cache_cost = self.cache_read_tokens as f64 * CACHE_COST_PER_1K / 1000.0\n            + uncached_tokens as f64 * NORMAL_COST_PER_1K / 1000.0;\n\n        normal_cost - cache_cost\n    }\n\n    /// Calculate the cost per call with current cache stats.\n    pub fn cost_per_call(&self) -> f64 {\n        const NORMAL_COST_PER_1K: f64 = 0.003;\n        const CACHE_COST_PER_1K: f64 = 0.0003;\n        const OUTPUT_COST_PER_1K: f64 = 0.015; // Claude output tokens\n\n        let uncached_tokens = self.input_tokens.saturating_sub(self.cache_read_tokens);\n\n        self.cache_read_tokens as f64 * CACHE_COST_PER_1K / 1000.0\n            + uncached_tokens as f64 * NORMAL_COST_PER_1K / 1000.0\n            + self.output_tokens as f64 * OUTPUT_COST_PER_1K / 1000.0\n    }\n\n    /// Check if caching was effective (hit rate > 50%).\n    pub fn is_effective(&self) -> bool {\n        self.cache_hit_rate() > 0.5\n    }\n\n    /// Merge stats from another request.\n    pub fn merge(&mut self, other: &CacheStats) {\n        self.input_tokens += other.input_tokens;\n        self.output_tokens += other.output_tokens;\n        self.cache_read_tokens += other.cache_read_tokens;\n        self.cache_creation_tokens += other.cache_creation_tokens;\n    }\n}\n\n/// Apply cache control to messages based on configuration.\n///\n/// This function marks messages with `cache_control` hints that providers\n/// like Anthropic Claude can use to cache prompt prefixes.\n///\n/// # Cache Marking Strategy\n///\n/// 1. **System prompts**: Always cached (if `cache_system_prompt` is true)\n/// 2. **Large user messages**: Cached if length > `min_content_length`\n/// 3. **Recent user messages**: Last N user messages are cached\n///\n/// # Arguments\n///\n/// * `messages` - Mutable slice of messages to apply cache control to\n/// * `config` - Configuration controlling which messages to cache\n///\n/// # Example\n///\n/// ```rust\n/// use edgequake_llm::cache_prompt::{CachePromptConfig, apply_cache_control};\n/// use edgequake_llm::traits::ChatMessage;\n///\n/// let config = CachePromptConfig::default();\n/// let mut messages = vec![\n///     ChatMessage::system(\"You are a helpful assistant\"),\n///     ChatMessage::user(\"Please analyze this file: ...\"),\n/// ];\n///\n/// apply_cache_control(&mut messages, &config);\n///\n/// assert!(messages[0].cache_control.is_some()); // System prompt cached\n/// ```\npub fn apply_cache_control(messages: &mut [ChatMessage], config: &CachePromptConfig) {\n    if !config.enabled {\n        return;\n    }\n\n    // Track user message indices for last-N caching\n    let user_indices: Vec<usize> = messages\n        .iter()\n        .enumerate()\n        .filter(|(_, m)| matches!(m.role, ChatRole::User))\n        .map(|(i, _)| i)\n        .collect();\n\n    // Determine which indices should be cached as \"last N\"\n    let last_n_start = user_indices\n        .len()\n        .saturating_sub(config.cache_last_n_messages);\n    let last_n_indices: std::collections::HashSet<usize> =\n        user_indices.into_iter().skip(last_n_start).collect();\n\n    for (i, msg) in messages.iter_mut().enumerate() {\n        let should_cache = match msg.role {\n            ChatRole::System => config.cache_system_prompt,\n            ChatRole::User => {\n                // Cache if large content OR in last N user messages\n                msg.content.len() >= config.min_content_length || last_n_indices.contains(&i)\n            }\n            _ => false, // Don't cache assistant/tool messages\n        };\n\n        if should_cache && msg.cache_control.is_none() {\n            msg.cache_control = Some(CacheControl::ephemeral());\n        }\n    }\n}\n\n/// Parse cache statistics from an Anthropic API response.\n///\n/// Anthropic includes cache stats in the `usage` field:\n/// ```json\n/// {\n///   \"usage\": {\n///     \"input_tokens\": 10000,\n///     \"output_tokens\": 500,\n///     \"cache_read_input_tokens\": 8000,\n///     \"cache_creation_input_tokens\": 0\n///   }\n/// }\n/// ```\npub fn parse_cache_stats(usage: &serde_json::Value) -> CacheStats {\n    CacheStats {\n        input_tokens: usage[\"input_tokens\"].as_u64().unwrap_or(0),\n        output_tokens: usage[\"output_tokens\"].as_u64().unwrap_or(0),\n        cache_read_tokens: usage[\"cache_read_input_tokens\"].as_u64().unwrap_or(0),\n        cache_creation_tokens: usage[\"cache_creation_input_tokens\"].as_u64().unwrap_or(0),\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_default_config() {\n        let config = CachePromptConfig::default();\n        assert!(config.enabled);\n        assert_eq!(config.min_content_length, 1000);\n        assert!(config.cache_system_prompt);\n        assert_eq!(config.cache_last_n_messages, 3);\n    }\n\n    #[test]\n    fn test_disabled_config() {\n        let config = CachePromptConfig::disabled();\n        assert!(!config.enabled);\n    }\n\n    #[test]\n    fn test_system_only_config() {\n        let config = CachePromptConfig::system_only();\n        assert!(config.enabled);\n        assert!(config.cache_system_prompt);\n        assert_eq!(config.cache_last_n_messages, 0);\n        assert_eq!(config.min_content_length, usize::MAX);\n    }\n\n    #[test]\n    fn test_aggressive_config() {\n        let config = CachePromptConfig::aggressive();\n        assert!(config.enabled);\n        assert_eq!(config.min_content_length, 100);\n        assert_eq!(config.cache_last_n_messages, 10);\n    }\n\n    #[test]\n    fn test_cache_control_disabled() {\n        let config = CachePromptConfig::disabled();\n        let mut messages = vec![\n            ChatMessage::system(\"System prompt\"),\n            ChatMessage::user(\"User message\"),\n        ];\n\n        apply_cache_control(&mut messages, &config);\n\n        assert!(messages[0].cache_control.is_none());\n        assert!(messages[1].cache_control.is_none());\n    }\n\n    #[test]\n    fn test_cache_system_prompt() {\n        let config = CachePromptConfig::default();\n        let mut messages = vec![\n            ChatMessage::system(\"You are a helpful assistant\"),\n            ChatMessage::user(\"Hello\"),\n        ];\n\n        apply_cache_control(&mut messages, &config);\n\n        assert!(messages[0].cache_control.is_some());\n        assert_eq!(\n            messages[0].cache_control.as_ref().unwrap().cache_type,\n            \"ephemeral\"\n        );\n    }\n\n    #[test]\n    fn test_cache_large_messages() {\n        let config = CachePromptConfig {\n            min_content_length: 100,\n            cache_last_n_messages: 0,\n            ..Default::default()\n        };\n\n        let large_content = \"x\".repeat(150);\n        let small_content = \"y\".repeat(50);\n\n        let mut messages = vec![\n            ChatMessage::system(\"System\"),\n            ChatMessage::user(&large_content),\n            ChatMessage::user(&small_content),\n        ];\n\n        apply_cache_control(&mut messages, &config);\n\n        // System should be cached\n        assert!(messages[0].cache_control.is_some());\n        // Large message should be cached\n        assert!(messages[1].cache_control.is_some());\n        // Small message should NOT be cached (last_n is 0)\n        assert!(messages[2].cache_control.is_none());\n    }\n\n    #[test]\n    fn test_cache_last_n_messages() {\n        let config = CachePromptConfig {\n            min_content_length: usize::MAX, // Disable size-based caching\n            cache_last_n_messages: 2,\n            cache_system_prompt: false,\n            ..Default::default()\n        };\n\n        let mut messages = vec![\n            ChatMessage::system(\"System\"),\n            ChatMessage::user(\"First\"),\n            ChatMessage::assistant(\"Response\"),\n            ChatMessage::user(\"Second\"),\n            ChatMessage::assistant(\"Response\"),\n            ChatMessage::user(\"Third\"),\n            ChatMessage::user(\"Fourth\"),\n        ];\n\n        apply_cache_control(&mut messages, &config);\n\n        // System not cached (disabled)\n        assert!(messages[0].cache_control.is_none());\n        // First two user messages not cached\n        assert!(messages[1].cache_control.is_none());\n        assert!(messages[3].cache_control.is_none());\n        // Last two user messages cached\n        assert!(messages[5].cache_control.is_some()); // Third\n        assert!(messages[6].cache_control.is_some()); // Fourth\n    }\n\n    #[test]\n    fn test_preserves_existing_cache_control() {\n        let config = CachePromptConfig::default();\n        let mut messages = vec![ChatMessage::system(\"System\")];\n\n        // Pre-set cache control\n        messages[0].cache_control = Some(CacheControl::ephemeral());\n\n        apply_cache_control(&mut messages, &config);\n\n        // Should still have cache control\n        assert!(messages[0].cache_control.is_some());\n    }\n\n    #[test]\n    fn test_cache_hit_rate_zero_tokens() {\n        let stats = CacheStats::default();\n        assert_eq!(stats.cache_hit_rate(), 0.0);\n    }\n\n    #[test]\n    fn test_cache_hit_rate_full_cache() {\n        let stats = CacheStats {\n            input_tokens: 10000,\n            output_tokens: 500,\n            cache_read_tokens: 10000,\n            cache_creation_tokens: 0,\n        };\n        assert_eq!(stats.cache_hit_rate(), 1.0);\n    }\n\n    #[test]\n    fn test_cache_hit_rate_partial() {\n        let stats = CacheStats {\n            input_tokens: 10000,\n            output_tokens: 500,\n            cache_read_tokens: 8000,\n            cache_creation_tokens: 0,\n        };\n        assert_eq!(stats.cache_hit_rate(), 0.8);\n    }\n\n    #[test]\n    fn test_cache_savings() {\n        let stats = CacheStats {\n            input_tokens: 10000,\n            output_tokens: 500,\n            cache_read_tokens: 8000,\n            cache_creation_tokens: 0,\n        };\n\n        let savings = stats.savings();\n\n        // 8000 tokens saved at 90% discount = 8000 * 0.0027 / 1000 = $0.0216\n        assert!(savings > 0.02);\n        assert!(savings < 0.03);\n    }\n\n    #[test]\n    fn test_cache_savings_no_cache() {\n        let stats = CacheStats {\n            input_tokens: 10000,\n            output_tokens: 500,\n            cache_read_tokens: 0,\n            cache_creation_tokens: 0,\n        };\n\n        assert_eq!(stats.savings(), 0.0);\n    }\n\n    #[test]\n    fn test_is_effective() {\n        let effective = CacheStats {\n            input_tokens: 10000,\n            cache_read_tokens: 6000,\n            ..Default::default()\n        };\n        assert!(effective.is_effective());\n\n        let ineffective = CacheStats {\n            input_tokens: 10000,\n            cache_read_tokens: 4000,\n            ..Default::default()\n        };\n        assert!(!ineffective.is_effective());\n    }\n\n    #[test]\n    fn test_merge_stats() {\n        let mut stats1 = CacheStats {\n            input_tokens: 1000,\n            output_tokens: 100,\n            cache_read_tokens: 500,\n            cache_creation_tokens: 200,\n        };\n\n        let stats2 = CacheStats {\n            input_tokens: 2000,\n            output_tokens: 200,\n            cache_read_tokens: 1000,\n            cache_creation_tokens: 100,\n        };\n\n        stats1.merge(&stats2);\n\n        assert_eq!(stats1.input_tokens, 3000);\n        assert_eq!(stats1.output_tokens, 300);\n        assert_eq!(stats1.cache_read_tokens, 1500);\n        assert_eq!(stats1.cache_creation_tokens, 300);\n    }\n\n    #[test]\n    fn test_parse_cache_stats() {\n        let usage = serde_json::json!({\n            \"input_tokens\": 10000,\n            \"output_tokens\": 500,\n            \"cache_read_input_tokens\": 8000,\n            \"cache_creation_input_tokens\": 100\n        });\n\n        let stats = parse_cache_stats(&usage);\n\n        assert_eq!(stats.input_tokens, 10000);\n        assert_eq!(stats.output_tokens, 500);\n        assert_eq!(stats.cache_read_tokens, 8000);\n        assert_eq!(stats.cache_creation_tokens, 100);\n    }\n\n    #[test]\n    fn test_parse_cache_stats_missing_fields() {\n        let usage = serde_json::json!({\n            \"input_tokens\": 5000,\n            \"output_tokens\": 200\n        });\n\n        let stats = parse_cache_stats(&usage);\n\n        assert_eq!(stats.input_tokens, 5000);\n        assert_eq!(stats.output_tokens, 200);\n        assert_eq!(stats.cache_read_tokens, 0);\n        assert_eq!(stats.cache_creation_tokens, 0);\n    }\n\n    #[test]\n    fn test_cost_per_call() {\n        let stats = CacheStats {\n            input_tokens: 10000,\n            output_tokens: 1000,\n            cache_read_tokens: 8000,\n            cache_creation_tokens: 0,\n        };\n\n        let cost = stats.cost_per_call();\n\n        // 8000 cached at $0.0003/1K = $0.0024\n        // 2000 normal at $0.003/1K = $0.006\n        // 1000 output at $0.015/1K = $0.015\n        // Total: $0.0234\n        assert!(cost > 0.02);\n        assert!(cost < 0.03);\n    }\n\n    #[test]\n    fn test_cache_stats_serialization() {\n        let stats = CacheStats {\n            input_tokens: 1000,\n            output_tokens: 100,\n            cache_read_tokens: 800,\n            cache_creation_tokens: 50,\n        };\n\n        let json = serde_json::to_string(&stats).unwrap();\n        let deserialized: CacheStats = serde_json::from_str(&json).unwrap();\n\n        assert_eq!(stats.input_tokens, deserialized.input_tokens);\n        assert_eq!(stats.output_tokens, deserialized.output_tokens);\n        assert_eq!(stats.cache_read_tokens, deserialized.cache_read_tokens);\n        assert_eq!(\n            stats.cache_creation_tokens,\n            deserialized.cache_creation_tokens\n        );\n    }\n\n    #[test]\n    fn test_cache_stats_new_constructor() {\n        let stats = CacheStats::new(5000, 500, 3000, 200);\n        assert_eq!(stats.input_tokens, 5000);\n        assert_eq!(stats.output_tokens, 500);\n        assert_eq!(stats.cache_read_tokens, 3000);\n        assert_eq!(stats.cache_creation_tokens, 200);\n    }\n\n    #[test]\n    fn test_apply_cache_control_empty_messages() {\n        let config = CachePromptConfig::default();\n        let mut messages: Vec<ChatMessage> = vec![];\n        apply_cache_control(&mut messages, &config);\n        assert!(messages.is_empty());\n    }\n\n    #[test]\n    fn test_apply_cache_control_only_assistant_messages() {\n        let config = CachePromptConfig::default();\n        let mut messages = vec![\n            ChatMessage::assistant(\"I will help you\"),\n            ChatMessage::assistant(\"Here is the answer\"),\n        ];\n        apply_cache_control(&mut messages, &config);\n        // Assistant messages should never be cached\n        assert!(messages[0].cache_control.is_none());\n        assert!(messages[1].cache_control.is_none());\n    }\n\n    #[test]\n    fn test_parse_cache_stats_empty_json() {\n        let usage = serde_json::json!({});\n        let stats = parse_cache_stats(&usage);\n        assert_eq!(stats.input_tokens, 0);\n        assert_eq!(stats.output_tokens, 0);\n        assert_eq!(stats.cache_read_tokens, 0);\n        assert_eq!(stats.cache_creation_tokens, 0);\n    }\n\n    #[test]\n    fn test_is_effective_boundary_at_50_percent() {\n        // Exactly 50% should NOT be effective (> 0.5 required)\n        let stats = CacheStats {\n            input_tokens: 10000,\n            cache_read_tokens: 5000,\n            ..Default::default()\n        };\n        assert!(!stats.is_effective());\n    }\n\n    #[test]\n    fn test_cost_per_call_zero_tokens() {\n        let stats = CacheStats::default();\n        assert_eq!(stats.cost_per_call(), 0.0);\n    }\n\n    #[test]\n    fn test_cost_per_call_all_cached() {\n        let stats = CacheStats {\n            input_tokens: 10000,\n            output_tokens: 0,\n            cache_read_tokens: 10000,\n            cache_creation_tokens: 0,\n        };\n        let cost = stats.cost_per_call();\n        // 10000 * 0.0003 / 1000 = $0.003\n        assert!((cost - 0.003).abs() < 1e-10);\n    }\n\n    #[test]\n    fn test_config_serialization_roundtrip() {\n        let config = CachePromptConfig::aggressive();\n        let json = serde_json::to_string(&config).unwrap();\n        let deserialized: CachePromptConfig = serde_json::from_str(&json).unwrap();\n        assert_eq!(deserialized.enabled, config.enabled);\n        assert_eq!(deserialized.min_content_length, config.min_content_length);\n        assert_eq!(deserialized.cache_system_prompt, config.cache_system_prompt);\n        assert_eq!(\n            deserialized.cache_last_n_messages,\n            config.cache_last_n_messages\n        );\n    }\n\n    #[test]\n    fn test_savings_when_cache_read_exceeds_input() {\n        // Edge case: cache_read_tokens > input_tokens should not panic\n        let stats = CacheStats {\n            input_tokens: 5000,\n            output_tokens: 100,\n            cache_read_tokens: 8000,\n            cache_creation_tokens: 0,\n        };\n        // Should not panic due to saturating_sub\n        let _ = stats.savings();\n    }\n\n    #[test]\n    fn test_merge_into_default() {\n        let mut stats = CacheStats::default();\n        let other = CacheStats::new(100, 50, 80, 10);\n        stats.merge(&other);\n        assert_eq!(stats.input_tokens, 100);\n        assert_eq!(stats.output_tokens, 50);\n        assert_eq!(stats.cache_read_tokens, 80);\n        assert_eq!(stats.cache_creation_tokens, 10);\n    }\n\n    #[test]\n    fn test_apply_cache_control_single_user_with_last_n() {\n        // With last_n_messages = 3 and only 1 user message, it should be cached\n        let config = CachePromptConfig {\n            min_content_length: usize::MAX,\n            cache_last_n_messages: 3,\n            cache_system_prompt: false,\n            ..Default::default()\n        };\n        let mut messages = vec![ChatMessage::user(\"Short msg\")];\n        apply_cache_control(&mut messages, &config);\n        assert!(messages[0].cache_control.is_some());\n    }\n}\n","traces":[{"line":89,"address":[],"length":0,"stats":{"Line":10}},{"line":101,"address":[],"length":0,"stats":{"Line":2}},{"line":109,"address":[],"length":0,"stats":{"Line":1}},{"line":121,"address":[],"length":0,"stats":{"Line":2}},{"line":175,"address":[],"length":0,"stats":{"Line":2}},{"line":192,"address":[],"length":0,"stats":{"Line":6}},{"line":193,"address":[],"length":0,"stats":{"Line":6}},{"line":194,"address":[],"length":0,"stats":{"Line":1}},{"line":196,"address":[],"length":0,"stats":{"Line":5}},{"line":208,"address":[],"length":0,"stats":{"Line":3}},{"line":213,"address":[],"length":0,"stats":{"Line":6}},{"line":216,"address":[],"length":0,"stats":{"Line":12}},{"line":217,"address":[],"length":0,"stats":{"Line":6}},{"line":218,"address":[],"length":0,"stats":{"Line":3}},{"line":220,"address":[],"length":0,"stats":{"Line":3}},{"line":224,"address":[],"length":0,"stats":{"Line":3}},{"line":229,"address":[],"length":0,"stats":{"Line":12}},{"line":231,"address":[],"length":0,"stats":{"Line":3}},{"line":232,"address":[],"length":0,"stats":{"Line":3}},{"line":233,"address":[],"length":0,"stats":{"Line":3}},{"line":237,"address":[],"length":0,"stats":{"Line":3}},{"line":238,"address":[],"length":0,"stats":{"Line":3}},{"line":242,"address":[],"length":0,"stats":{"Line":2}},{"line":243,"address":[],"length":0,"stats":{"Line":2}},{"line":244,"address":[],"length":0,"stats":{"Line":2}},{"line":245,"address":[],"length":0,"stats":{"Line":2}},{"line":246,"address":[],"length":0,"stats":{"Line":2}},{"line":282,"address":[],"length":0,"stats":{"Line":8}},{"line":283,"address":[],"length":0,"stats":{"Line":8}},{"line":284,"address":[],"length":0,"stats":{"Line":1}},{"line":288,"address":[],"length":0,"stats":{"Line":21}},{"line":291,"address":[],"length":0,"stats":{"Line":23}},{"line":292,"address":[],"length":0,"stats":{"Line":7}},{"line":296,"address":[],"length":0,"stats":{"Line":14}},{"line":298,"address":[],"length":0,"stats":{"Line":14}},{"line":299,"address":[],"length":0,"stats":{"Line":14}},{"line":300,"address":[],"length":0,"stats":{"Line":35}},{"line":302,"address":[],"length":0,"stats":{"Line":53}},{"line":303,"address":[],"length":0,"stats":{"Line":32}},{"line":304,"address":[],"length":0,"stats":{"Line":4}},{"line":307,"address":[],"length":0,"stats":{"Line":37}},{"line":309,"address":[],"length":0,"stats":{"Line":4}},{"line":312,"address":[],"length":0,"stats":{"Line":39}},{"line":313,"address":[],"length":0,"stats":{"Line":7}},{"line":331,"address":[],"length":0,"stats":{"Line":3}},{"line":333,"address":[],"length":0,"stats":{"Line":12}},{"line":334,"address":[],"length":0,"stats":{"Line":12}},{"line":335,"address":[],"length":0,"stats":{"Line":12}},{"line":336,"address":[],"length":0,"stats":{"Line":6}}],"covered":49,"coverable":49},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","cost_tracker.rs"],"content":"//! Session Cost Tracker\n//!\n//! # OODA-21: Cost Tracking\n//!\n//! This module provides session-level cost tracking and aggregation for LLM API calls.\n//!\n//! # Overview\n//!\n//! The `SessionCostTracker` aggregates costs across multiple LLM calls within a session,\n//! providing breakdowns by model, provider, and operation type.\n//!\n//! # Usage\n//!\n//! ```rust\n//! use edgequake_llm::cost_tracker::{SessionCostTracker, ModelPricing};\n//!\n//! let mut tracker = SessionCostTracker::new();\n//!\n//! // Set pricing for a model\n//! tracker.set_pricing(\"claude-3-opus-20240229\", ModelPricing::new(15.0, 75.0));\n//!\n//! // Record a cost entry\n//! tracker.record_usage(\n//!     \"claude-3-opus-20240229\",\n//!     \"anthropic\",\n//!     1000,  // input tokens\n//!     500,   // output tokens\n//! );\n//!\n//! // Get summary\n//! let summary = tracker.summary();\n//! println!(\"Total cost: ${:.4}\", summary.total_cost);\n//! ```\n//!\n//! # See Also\n//!\n//! - `middleware.rs` for per-request cost estimation\n//! - `cache_prompt.rs` for cache-aware cost calculations\n\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::time::{Duration, SystemTime};\n\n/// Pricing information for a model.\n///\n/// Costs are specified in dollars per million tokens.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ModelPricing {\n    /// Cost per million input tokens (USD).\n    pub input_cost_per_million: f64,\n\n    /// Cost per million output tokens (USD).\n    pub output_cost_per_million: f64,\n\n    /// Optional: Cost per million cached input tokens (USD).\n    /// If None, caching is not supported or priced the same as regular input.\n    pub cached_input_cost_per_million: Option<f64>,\n}\n\nimpl ModelPricing {\n    /// Create new pricing with input and output costs.\n    pub fn new(input_cost_per_million: f64, output_cost_per_million: f64) -> Self {\n        Self {\n            input_cost_per_million,\n            output_cost_per_million,\n            cached_input_cost_per_million: None,\n        }\n    }\n\n    /// Create pricing with cache support.\n    pub fn with_cache(\n        input_cost_per_million: f64,\n        output_cost_per_million: f64,\n        cached_cost_per_million: f64,\n    ) -> Self {\n        Self {\n            input_cost_per_million,\n            output_cost_per_million,\n            cached_input_cost_per_million: Some(cached_cost_per_million),\n        }\n    }\n\n    /// Calculate cost for given token counts.\n    pub fn calculate_cost(&self, input_tokens: u64, output_tokens: u64) -> f64 {\n        let input_cost = (input_tokens as f64 / 1_000_000.0) * self.input_cost_per_million;\n        let output_cost = (output_tokens as f64 / 1_000_000.0) * self.output_cost_per_million;\n        input_cost + output_cost\n    }\n\n    /// Calculate cost with cached tokens.\n    pub fn calculate_cost_with_cache(\n        &self,\n        input_tokens: u64,\n        cached_tokens: u64,\n        output_tokens: u64,\n    ) -> f64 {\n        let uncached = input_tokens.saturating_sub(cached_tokens);\n        let input_cost = (uncached as f64 / 1_000_000.0) * self.input_cost_per_million;\n\n        let cached_cost = if let Some(cache_price) = self.cached_input_cost_per_million {\n            (cached_tokens as f64 / 1_000_000.0) * cache_price\n        } else {\n            (cached_tokens as f64 / 1_000_000.0) * self.input_cost_per_million\n        };\n\n        let output_cost = (output_tokens as f64 / 1_000_000.0) * self.output_cost_per_million;\n\n        input_cost + cached_cost + output_cost\n    }\n}\n\nimpl Default for ModelPricing {\n    fn default() -> Self {\n        // Default to GPT-4o pricing as a reasonable default\n        Self::new(2.5, 10.0)\n    }\n}\n\n/// A single cost entry for one LLM call.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CostEntry {\n    /// Model name.\n    pub model: String,\n\n    /// Provider name.\n    pub provider: String,\n\n    /// Input tokens used.\n    pub input_tokens: u64,\n\n    /// Output tokens generated.\n    pub output_tokens: u64,\n\n    /// Cached input tokens (if applicable).\n    pub cached_tokens: u64,\n\n    /// Calculated cost in USD.\n    pub cost: f64,\n\n    /// When the call was made.\n    pub timestamp: SystemTime,\n\n    /// Duration of the call.\n    pub duration: Option<Duration>,\n\n    /// Operation type (e.g., \"chat\", \"completion\", \"embedding\").\n    pub operation: String,\n}\n\nimpl CostEntry {\n    /// Create a new cost entry.\n    pub fn new(\n        model: impl Into<String>,\n        provider: impl Into<String>,\n        input_tokens: u64,\n        output_tokens: u64,\n        cost: f64,\n    ) -> Self {\n        Self {\n            model: model.into(),\n            provider: provider.into(),\n            input_tokens,\n            output_tokens,\n            cached_tokens: 0,\n            cost,\n            timestamp: SystemTime::now(),\n            duration: None,\n            operation: \"chat\".to_string(),\n        }\n    }\n\n    /// Set cached tokens.\n    pub fn with_cached_tokens(mut self, cached_tokens: u64) -> Self {\n        self.cached_tokens = cached_tokens;\n        self\n    }\n\n    /// Set duration.\n    pub fn with_duration(mut self, duration: Duration) -> Self {\n        self.duration = Some(duration);\n        self\n    }\n\n    /// Set operation type.\n    pub fn with_operation(mut self, operation: impl Into<String>) -> Self {\n        self.operation = operation.into();\n        self\n    }\n}\n\n/// Summary of costs across a session.\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct CostSummary {\n    /// Total cost in USD.\n    pub total_cost: f64,\n\n    /// Total input tokens.\n    pub total_input_tokens: u64,\n\n    /// Total output tokens.\n    pub total_output_tokens: u64,\n\n    /// Total cached tokens.\n    pub total_cached_tokens: u64,\n\n    /// Number of API calls.\n    pub call_count: usize,\n\n    /// Average cost per call.\n    pub avg_cost_per_call: f64,\n\n    /// Breakdown by model.\n    pub by_model: HashMap<String, f64>,\n\n    /// Breakdown by provider.\n    pub by_provider: HashMap<String, f64>,\n\n    /// Breakdown by operation.\n    pub by_operation: HashMap<String, f64>,\n}\n\nimpl CostSummary {\n    /// Calculate cache hit rate.\n    pub fn cache_hit_rate(&self) -> f64 {\n        if self.total_input_tokens == 0 {\n            0.0\n        } else {\n            self.total_cached_tokens as f64 / self.total_input_tokens as f64\n        }\n    }\n\n    /// Estimate savings from caching.\n    pub fn cache_savings(&self, normal_price_per_million: f64) -> f64 {\n        let would_have_cost =\n            (self.total_cached_tokens as f64 / 1_000_000.0) * normal_price_per_million;\n        let actual_cost =\n            (self.total_cached_tokens as f64 / 1_000_000.0) * normal_price_per_million * 0.1;\n        would_have_cost - actual_cost\n    }\n}\n\n/// Session-level cost tracker.\n///\n/// Tracks and aggregates costs across multiple LLM calls within a session.\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct SessionCostTracker {\n    /// All cost entries.\n    entries: Vec<CostEntry>,\n\n    /// Pricing by model.\n    pricing: HashMap<String, ModelPricing>,\n\n    /// Optional budget limit in USD.\n    budget_limit: Option<f64>,\n\n    /// Warning threshold (percentage of budget).\n    warning_threshold: f64,\n}\n\nimpl SessionCostTracker {\n    /// Create a new session cost tracker.\n    pub fn new() -> Self {\n        Self {\n            entries: Vec::new(),\n            pricing: Self::default_pricing(),\n            budget_limit: None,\n            warning_threshold: 0.8,\n        }\n    }\n\n    /// Create with a budget limit.\n    pub fn with_budget(limit: f64) -> Self {\n        let mut tracker = Self::new();\n        tracker.budget_limit = Some(limit);\n        tracker\n    }\n\n    /// Set budget limit.\n    pub fn set_budget(&mut self, limit: f64) {\n        self.budget_limit = Some(limit);\n    }\n\n    /// Set warning threshold (0.0 to 1.0).\n    pub fn set_warning_threshold(&mut self, threshold: f64) {\n        self.warning_threshold = threshold.clamp(0.0, 1.0);\n    }\n\n    /// Set pricing for a model.\n    pub fn set_pricing(&mut self, model: impl Into<String>, pricing: ModelPricing) {\n        self.pricing.insert(model.into(), pricing);\n    }\n\n    /// Get pricing for a model.\n    pub fn get_pricing(&self, model: &str) -> Option<&ModelPricing> {\n        // Try exact match first\n        if let Some(p) = self.pricing.get(model) {\n            return Some(p);\n        }\n\n        // Try prefix match\n        for (pattern, pricing) in &self.pricing {\n            if model.starts_with(pattern) || model.contains(pattern) {\n                return Some(pricing);\n            }\n        }\n\n        None\n    }\n\n    /// Record usage and calculate cost.\n    pub fn record_usage(\n        &mut self,\n        model: &str,\n        provider: &str,\n        input_tokens: u64,\n        output_tokens: u64,\n    ) -> f64 {\n        let cost = self\n            .get_pricing(model)\n            .map(|p| p.calculate_cost(input_tokens, output_tokens))\n            .unwrap_or_else(|| {\n                // Use default pricing\n                ModelPricing::default().calculate_cost(input_tokens, output_tokens)\n            });\n\n        let entry = CostEntry::new(model, provider, input_tokens, output_tokens, cost);\n        self.entries.push(entry);\n\n        cost\n    }\n\n    /// Record usage with cached tokens.\n    pub fn record_usage_with_cache(\n        &mut self,\n        model: &str,\n        provider: &str,\n        input_tokens: u64,\n        cached_tokens: u64,\n        output_tokens: u64,\n    ) -> f64 {\n        let cost = self\n            .get_pricing(model)\n            .map(|p| p.calculate_cost_with_cache(input_tokens, cached_tokens, output_tokens))\n            .unwrap_or_else(|| {\n                ModelPricing::default().calculate_cost_with_cache(\n                    input_tokens,\n                    cached_tokens,\n                    output_tokens,\n                )\n            });\n\n        let entry = CostEntry::new(model, provider, input_tokens, output_tokens, cost)\n            .with_cached_tokens(cached_tokens);\n        self.entries.push(entry);\n\n        cost\n    }\n\n    /// Add a pre-calculated cost entry.\n    pub fn add_entry(&mut self, entry: CostEntry) {\n        self.entries.push(entry);\n    }\n\n    /// Get total cost.\n    pub fn total_cost(&self) -> f64 {\n        self.entries.iter().map(|e| e.cost).sum()\n    }\n\n    /// Get number of API calls.\n    pub fn call_count(&self) -> usize {\n        self.entries.len()\n    }\n\n    /// Check if budget is exceeded.\n    pub fn is_over_budget(&self) -> bool {\n        self.budget_limit\n            .map(|b| self.total_cost() >= b)\n            .unwrap_or(false)\n    }\n\n    /// Check if approaching budget limit.\n    pub fn is_near_budget(&self) -> bool {\n        self.budget_limit\n            .map(|b| self.total_cost() >= b * self.warning_threshold)\n            .unwrap_or(false)\n    }\n\n    /// Get remaining budget.\n    pub fn remaining_budget(&self) -> Option<f64> {\n        self.budget_limit.map(|b| (b - self.total_cost()).max(0.0))\n    }\n\n    /// Get budget usage percentage.\n    pub fn budget_usage_percent(&self) -> Option<f64> {\n        self.budget_limit\n            .map(|b| (self.total_cost() / b * 100.0).min(100.0))\n    }\n\n    /// Get summary statistics.\n    pub fn summary(&self) -> CostSummary {\n        let mut summary = CostSummary::default();\n\n        for entry in &self.entries {\n            summary.total_cost += entry.cost;\n            summary.total_input_tokens += entry.input_tokens;\n            summary.total_output_tokens += entry.output_tokens;\n            summary.total_cached_tokens += entry.cached_tokens;\n            summary.call_count += 1;\n\n            *summary.by_model.entry(entry.model.clone()).or_default() += entry.cost;\n            *summary\n                .by_provider\n                .entry(entry.provider.clone())\n                .or_default() += entry.cost;\n            *summary\n                .by_operation\n                .entry(entry.operation.clone())\n                .or_default() += entry.cost;\n        }\n\n        if summary.call_count > 0 {\n            summary.avg_cost_per_call = summary.total_cost / summary.call_count as f64;\n        }\n\n        summary\n    }\n\n    /// Get all entries.\n    pub fn entries(&self) -> &[CostEntry] {\n        &self.entries\n    }\n\n    /// Clear all entries.\n    pub fn clear(&mut self) {\n        self.entries.clear();\n    }\n\n    /// Get entries since a timestamp.\n    pub fn entries_since(&self, since: SystemTime) -> Vec<&CostEntry> {\n        self.entries\n            .iter()\n            .filter(|e| e.timestamp >= since)\n            .collect()\n    }\n\n    /// Default pricing for common models.\n    fn default_pricing() -> HashMap<String, ModelPricing> {\n        let mut pricing = HashMap::new();\n\n        // Claude models\n        pricing.insert(\n            \"claude-3-opus\".to_string(),\n            ModelPricing::with_cache(15.0, 75.0, 1.5),\n        );\n        pricing.insert(\n            \"claude-3-5-sonnet\".to_string(),\n            ModelPricing::with_cache(3.0, 15.0, 0.3),\n        );\n        pricing.insert(\n            \"claude-3-5-haiku\".to_string(),\n            ModelPricing::with_cache(0.8, 4.0, 0.08),\n        );\n        pricing.insert(\n            \"claude-sonnet-4\".to_string(),\n            ModelPricing::with_cache(3.0, 15.0, 0.3),\n        );\n\n        // OpenAI models\n        pricing.insert(\"gpt-4o\".to_string(), ModelPricing::new(2.5, 10.0));\n        pricing.insert(\"gpt-4o-mini\".to_string(), ModelPricing::new(0.15, 0.6));\n        pricing.insert(\"gpt-4-turbo\".to_string(), ModelPricing::new(10.0, 30.0));\n        pricing.insert(\"o1\".to_string(), ModelPricing::new(15.0, 60.0));\n        pricing.insert(\"o1-mini\".to_string(), ModelPricing::new(3.0, 12.0));\n\n        // Gemini models\n        pricing.insert(\n            \"gemini-2.0-flash\".to_string(),\n            ModelPricing::new(0.075, 0.3),\n        );\n        pricing.insert(\"gemini-1.5-pro\".to_string(), ModelPricing::new(1.25, 5.0));\n\n        pricing\n    }\n}\n\n/// Format cost in a human-readable way.\npub fn format_cost(cost: f64) -> String {\n    if cost < 0.01 {\n        format!(\"${:.4}\", cost)\n    } else if cost < 1.0 {\n        format!(\"${:.3}\", cost)\n    } else {\n        format!(\"${:.2}\", cost)\n    }\n}\n\n/// Format token count with commas.\npub fn format_tokens(tokens: u64) -> String {\n    if tokens >= 1_000_000 {\n        format!(\"{:.1}M\", tokens as f64 / 1_000_000.0)\n    } else if tokens >= 1_000 {\n        format!(\"{:.1}K\", tokens as f64 / 1_000.0)\n    } else {\n        format!(\"{}\", tokens)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_model_pricing_new() {\n        let pricing = ModelPricing::new(15.0, 75.0);\n        assert_eq!(pricing.input_cost_per_million, 15.0);\n        assert_eq!(pricing.output_cost_per_million, 75.0);\n        assert!(pricing.cached_input_cost_per_million.is_none());\n    }\n\n    #[test]\n    fn test_model_pricing_with_cache() {\n        let pricing = ModelPricing::with_cache(15.0, 75.0, 1.5);\n        assert_eq!(pricing.cached_input_cost_per_million, Some(1.5));\n    }\n\n    #[test]\n    fn test_calculate_cost() {\n        let pricing = ModelPricing::new(15.0, 75.0);\n        let cost = pricing.calculate_cost(1_000_000, 1_000_000);\n        assert!((cost - 90.0).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_calculate_cost_small() {\n        let pricing = ModelPricing::new(3.0, 15.0); // claude-3.5-sonnet\n        let cost = pricing.calculate_cost(1000, 500);\n        // 1000/1M * 3.0 + 500/1M * 15.0 = 0.003 + 0.0075 = 0.0105\n        assert!((cost - 0.0105).abs() < 0.0001);\n    }\n\n    #[test]\n    fn test_calculate_cost_with_cache() {\n        let pricing = ModelPricing::with_cache(15.0, 75.0, 1.5);\n        let cost = pricing.calculate_cost_with_cache(1_000_000, 800_000, 100_000);\n        // Uncached: 200K * 15/1M = 3.0\n        // Cached: 800K * 1.5/1M = 1.2\n        // Output: 100K * 75/1M = 7.5\n        // Total = 11.7\n        assert!((cost - 11.7).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_cost_entry_new() {\n        let entry = CostEntry::new(\"gpt-4o\", \"openai\", 1000, 500, 0.05);\n        assert_eq!(entry.model, \"gpt-4o\");\n        assert_eq!(entry.provider, \"openai\");\n        assert_eq!(entry.input_tokens, 1000);\n        assert_eq!(entry.output_tokens, 500);\n        assert_eq!(entry.cost, 0.05);\n    }\n\n    #[test]\n    fn test_cost_entry_with_cache() {\n        let entry = CostEntry::new(\"claude-3-opus\", \"anthropic\", 10000, 1000, 0.10)\n            .with_cached_tokens(8000);\n        assert_eq!(entry.cached_tokens, 8000);\n    }\n\n    #[test]\n    fn test_session_tracker_new() {\n        let tracker = SessionCostTracker::new();\n        assert_eq!(tracker.call_count(), 0);\n        assert_eq!(tracker.total_cost(), 0.0);\n    }\n\n    #[test]\n    fn test_session_tracker_with_budget() {\n        let tracker = SessionCostTracker::with_budget(10.0);\n        assert_eq!(tracker.remaining_budget(), Some(10.0));\n    }\n\n    #[test]\n    fn test_record_usage() {\n        let mut tracker = SessionCostTracker::new();\n        let cost = tracker.record_usage(\"gpt-4o\", \"openai\", 1000, 500);\n        assert!(cost > 0.0);\n        assert_eq!(tracker.call_count(), 1);\n    }\n\n    #[test]\n    fn test_record_usage_with_cache() {\n        let mut tracker = SessionCostTracker::new();\n        let cost = tracker.record_usage_with_cache(\"claude-3-opus\", \"anthropic\", 10000, 8000, 1000);\n        assert!(cost > 0.0);\n        assert_eq!(tracker.entries()[0].cached_tokens, 8000);\n    }\n\n    #[test]\n    fn test_budget_tracking() {\n        let mut tracker = SessionCostTracker::with_budget(0.1);\n        tracker.record_usage(\"gpt-4o\", \"openai\", 1000000, 500000);\n        assert!(tracker.is_over_budget());\n    }\n\n    #[test]\n    fn test_near_budget() {\n        let mut tracker = SessionCostTracker::with_budget(1.0);\n        tracker.set_warning_threshold(0.5);\n        // Add enough to be at 50%+\n        tracker.record_usage(\"gpt-4o\", \"openai\", 100000, 50000);\n        // Check if near budget\n        assert!(tracker.total_cost() > 0.0);\n    }\n\n    #[test]\n    fn test_summary() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.record_usage(\"gpt-4o\", \"openai\", 1000, 500);\n        tracker.record_usage(\"claude-3-opus\", \"anthropic\", 2000, 1000);\n\n        let summary = tracker.summary();\n        assert_eq!(summary.call_count, 2);\n        assert_eq!(summary.total_input_tokens, 3000);\n        assert_eq!(summary.total_output_tokens, 1500);\n        assert!(summary.by_model.contains_key(\"gpt-4o\"));\n        assert!(summary.by_model.contains_key(\"claude-3-opus\"));\n    }\n\n    #[test]\n    fn test_summary_by_provider() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.record_usage(\"gpt-4o\", \"openai\", 1000, 500);\n        tracker.record_usage(\"gpt-4o-mini\", \"openai\", 2000, 1000);\n        tracker.record_usage(\"claude-3-opus\", \"anthropic\", 1000, 500);\n\n        let summary = tracker.summary();\n        assert!(summary.by_provider.contains_key(\"openai\"));\n        assert!(summary.by_provider.contains_key(\"anthropic\"));\n    }\n\n    #[test]\n    fn test_cache_hit_rate() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.record_usage_with_cache(\"claude-3-opus\", \"anthropic\", 10000, 8000, 1000);\n\n        let summary = tracker.summary();\n        assert!((summary.cache_hit_rate() - 0.8).abs() < 0.01);\n    }\n\n    #[test]\n    fn test_clear() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.record_usage(\"gpt-4o\", \"openai\", 1000, 500);\n        assert_eq!(tracker.call_count(), 1);\n\n        tracker.clear();\n        assert_eq!(tracker.call_count(), 0);\n    }\n\n    #[test]\n    fn test_format_cost() {\n        assert_eq!(format_cost(0.001), \"$0.0010\");\n        assert_eq!(format_cost(0.05), \"$0.050\");\n        assert_eq!(format_cost(1.50), \"$1.50\");\n        assert_eq!(format_cost(10.00), \"$10.00\");\n    }\n\n    #[test]\n    fn test_format_tokens() {\n        assert_eq!(format_tokens(500), \"500\");\n        assert_eq!(format_tokens(1500), \"1.5K\");\n        assert_eq!(format_tokens(1_500_000), \"1.5M\");\n    }\n\n    #[test]\n    fn test_default_pricing() {\n        let tracker = SessionCostTracker::new();\n        assert!(tracker.get_pricing(\"claude-3-opus\").is_some());\n        assert!(tracker.get_pricing(\"gpt-4o\").is_some());\n        assert!(tracker.get_pricing(\"gemini-2.0-flash\").is_some());\n    }\n\n    #[test]\n    fn test_pricing_prefix_match() {\n        let tracker = SessionCostTracker::new();\n        // Should match \"claude-3-opus\" pattern\n        assert!(tracker.get_pricing(\"claude-3-opus-20240229\").is_some());\n    }\n\n    #[test]\n    fn test_serialization() {\n        let mut tracker = SessionCostTracker::with_budget(10.0);\n        tracker.record_usage(\"gpt-4o\", \"openai\", 1000, 500);\n\n        let json = serde_json::to_string(&tracker).unwrap();\n        let restored: SessionCostTracker = serde_json::from_str(&json).unwrap();\n\n        assert_eq!(restored.call_count(), 1);\n        assert!((restored.total_cost() - tracker.total_cost()).abs() < 0.0001);\n    }\n\n    #[test]\n    fn test_budget_usage_percent() {\n        let mut tracker = SessionCostTracker::with_budget(10.0);\n        // Manually add an entry with known cost\n        let entry = CostEntry::new(\"test\", \"test\", 0, 0, 5.0);\n        tracker.add_entry(entry);\n\n        assert_eq!(tracker.budget_usage_percent(), Some(50.0));\n    }\n\n    #[test]\n    fn test_cost_entry_with_duration() {\n        let entry = CostEntry::new(\"m\", \"p\", 100, 50, 0.01)\n            .with_duration(Duration::from_millis(500));\n        assert_eq!(entry.duration, Some(Duration::from_millis(500)));\n    }\n\n    #[test]\n    fn test_cost_entry_with_operation() {\n        let entry = CostEntry::new(\"m\", \"p\", 100, 50, 0.01)\n            .with_operation(\"embedding\");\n        assert_eq!(entry.operation, \"embedding\");\n    }\n\n    #[test]\n    fn test_model_pricing_default() {\n        let pricing = ModelPricing::default();\n        assert_eq!(pricing.input_cost_per_million, 2.5);\n        assert_eq!(pricing.output_cost_per_million, 10.0);\n        assert!(pricing.cached_input_cost_per_million.is_none());\n    }\n\n    #[test]\n    fn test_cost_summary_cache_savings() {\n        let summary = CostSummary {\n            total_cached_tokens: 1_000_000,\n            ..Default::default()\n        };\n        // Savings should be positive for cached tokens\n        let savings = summary.cache_savings(15.0);\n        // 1M * 15/1M - 1M * 15 * 0.1/1M = 15.0 - 1.5 = 13.5\n        assert!((savings - 13.5).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_cost_summary_cache_savings_zero() {\n        let summary = CostSummary::default();\n        assert_eq!(summary.cache_savings(15.0), 0.0);\n    }\n\n    #[test]\n    fn test_cost_summary_cache_hit_rate_zero_input() {\n        let summary = CostSummary::default();\n        assert_eq!(summary.cache_hit_rate(), 0.0);\n    }\n\n    #[test]\n    fn test_set_budget() {\n        let mut tracker = SessionCostTracker::new();\n        assert!(tracker.remaining_budget().is_none());\n        tracker.set_budget(5.0);\n        assert_eq!(tracker.remaining_budget(), Some(5.0));\n    }\n\n    #[test]\n    fn test_set_pricing_and_use() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.set_pricing(\"custom-model\", ModelPricing::new(1.0, 2.0));\n        let cost = tracker.record_usage(\"custom-model\", \"custom\", 1_000_000, 1_000_000);\n        // 1M * 1/1M + 1M * 2/1M = 3.0\n        assert!((cost - 3.0).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_entries_since() {\n        let mut tracker = SessionCostTracker::new();\n        let before = SystemTime::now();\n        tracker.record_usage(\"gpt-4o\", \"openai\", 100, 50);\n\n        let entries = tracker.entries_since(before);\n        assert_eq!(entries.len(), 1);\n\n        // Future time should return nothing\n        let future = SystemTime::now() + Duration::from_secs(3600);\n        let entries_future = tracker.entries_since(future);\n        assert!(entries_future.is_empty());\n    }\n\n    #[test]\n    fn test_is_near_budget_no_budget() {\n        let tracker = SessionCostTracker::new();\n        assert!(!tracker.is_near_budget());\n    }\n\n    #[test]\n    fn test_is_over_budget_no_budget() {\n        let tracker = SessionCostTracker::new();\n        assert!(!tracker.is_over_budget());\n    }\n\n    #[test]\n    fn test_budget_usage_percent_no_budget() {\n        let tracker = SessionCostTracker::new();\n        assert!(tracker.budget_usage_percent().is_none());\n    }\n\n    #[test]\n    fn test_budget_usage_percent_capped_at_100() {\n        let mut tracker = SessionCostTracker::with_budget(0.001);\n        tracker.add_entry(CostEntry::new(\"m\", \"p\", 0, 0, 1.0));\n        assert_eq!(tracker.budget_usage_percent(), Some(100.0));\n    }\n\n    #[test]\n    fn test_remaining_budget_capped_at_zero() {\n        let mut tracker = SessionCostTracker::with_budget(0.001);\n        tracker.add_entry(CostEntry::new(\"m\", \"p\", 0, 0, 1.0));\n        assert_eq!(tracker.remaining_budget(), Some(0.0));\n    }\n\n    #[test]\n    fn test_set_warning_threshold_clamped() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.set_warning_threshold(2.0);\n        // Should clamp to 1.0\n        tracker.set_budget(10.0);\n        tracker.add_entry(CostEntry::new(\"m\", \"p\", 0, 0, 10.0));\n        assert!(tracker.is_near_budget());\n    }\n\n    #[test]\n    fn test_summary_avg_cost_per_call() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.add_entry(CostEntry::new(\"m\", \"p\", 0, 0, 2.0));\n        tracker.add_entry(CostEntry::new(\"m\", \"p\", 0, 0, 4.0));\n        let summary = tracker.summary();\n        assert!((summary.avg_cost_per_call - 3.0).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_summary_by_operation() {\n        let mut tracker = SessionCostTracker::new();\n        tracker.add_entry(\n            CostEntry::new(\"m\", \"p\", 100, 50, 1.0).with_operation(\"embedding\"),\n        );\n        tracker.add_entry(\n            CostEntry::new(\"m\", \"p\", 100, 50, 2.0).with_operation(\"chat\"),\n        );\n        let summary = tracker.summary();\n        assert!(summary.by_operation.contains_key(\"embedding\"));\n        assert!(summary.by_operation.contains_key(\"chat\"));\n    }\n\n    #[test]\n    fn test_record_usage_unknown_model_uses_default() {\n        let mut tracker = SessionCostTracker::new();\n        let cost = tracker.record_usage(\"unknown-model\", \"unknown\", 1_000_000, 1_000_000);\n        // Default pricing: 2.5 + 10.0 = 12.5\n        assert!((cost - 12.5).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_calculate_cost_with_cache_no_cache_price() {\n        let pricing = ModelPricing::new(15.0, 75.0); // No cached price\n        let cost = pricing.calculate_cost_with_cache(1_000_000, 800_000, 100_000);\n        // Uncached: 200K * 15/1M = 3.0\n        // Cached at regular price: 800K * 15/1M = 12.0\n        // Output: 100K * 75/1M = 7.5\n        // Total: 22.5\n        assert!((cost - 22.5).abs() < 0.001);\n    }\n}\n","traces":[{"line":62,"address":[],"length":0,"stats":{"Line":189}},{"line":71,"address":[],"length":0,"stats":{"Line":106}},{"line":79,"address":[],"length":0,"stats":{"Line":106}},{"line":84,"address":[],"length":0,"stats":{"Line":15}},{"line":85,"address":[],"length":0,"stats":{"Line":30}},{"line":86,"address":[],"length":0,"stats":{"Line":30}},{"line":87,"address":[],"length":0,"stats":{"Line":15}},{"line":91,"address":[],"length":0,"stats":{"Line":4}},{"line":97,"address":[],"length":0,"stats":{"Line":16}},{"line":98,"address":[],"length":0,"stats":{"Line":8}},{"line":100,"address":[],"length":0,"stats":{"Line":11}},{"line":101,"address":[],"length":0,"stats":{"Line":3}},{"line":103,"address":[],"length":0,"stats":{"Line":1}},{"line":106,"address":[],"length":0,"stats":{"Line":8}},{"line":108,"address":[],"length":0,"stats":{"Line":4}},{"line":113,"address":[],"length":0,"stats":{"Line":2}},{"line":115,"address":[],"length":0,"stats":{"Line":2}},{"line":152,"address":[],"length":0,"stats":{"Line":27}},{"line":160,"address":[],"length":0,"stats":{"Line":81}},{"line":161,"address":[],"length":0,"stats":{"Line":81}},{"line":166,"address":[],"length":0,"stats":{"Line":54}},{"line":168,"address":[],"length":0,"stats":{"Line":27}},{"line":173,"address":[],"length":0,"stats":{"Line":3}},{"line":174,"address":[],"length":0,"stats":{"Line":3}},{"line":175,"address":[],"length":0,"stats":{"Line":3}},{"line":179,"address":[],"length":0,"stats":{"Line":1}},{"line":180,"address":[],"length":0,"stats":{"Line":1}},{"line":181,"address":[],"length":0,"stats":{"Line":1}},{"line":185,"address":[],"length":0,"stats":{"Line":3}},{"line":186,"address":[],"length":0,"stats":{"Line":9}},{"line":187,"address":[],"length":0,"stats":{"Line":3}},{"line":224,"address":[],"length":0,"stats":{"Line":2}},{"line":225,"address":[],"length":0,"stats":{"Line":2}},{"line":226,"address":[],"length":0,"stats":{"Line":1}},{"line":228,"address":[],"length":0,"stats":{"Line":1}},{"line":233,"address":[],"length":0,"stats":{"Line":2}},{"line":234,"address":[],"length":0,"stats":{"Line":2}},{"line":235,"address":[],"length":0,"stats":{"Line":2}},{"line":236,"address":[],"length":0,"stats":{"Line":2}},{"line":237,"address":[],"length":0,"stats":{"Line":2}},{"line":238,"address":[],"length":0,"stats":{"Line":2}},{"line":262,"address":[],"length":0,"stats":{"Line":26}},{"line":264,"address":[],"length":0,"stats":{"Line":52}},{"line":265,"address":[],"length":0,"stats":{"Line":26}},{"line":272,"address":[],"length":0,"stats":{"Line":7}},{"line":273,"address":[],"length":0,"stats":{"Line":14}},{"line":274,"address":[],"length":0,"stats":{"Line":7}},{"line":275,"address":[],"length":0,"stats":{"Line":7}},{"line":279,"address":[],"length":0,"stats":{"Line":2}},{"line":280,"address":[],"length":0,"stats":{"Line":2}},{"line":284,"address":[],"length":0,"stats":{"Line":2}},{"line":285,"address":[],"length":0,"stats":{"Line":2}},{"line":289,"address":[],"length":0,"stats":{"Line":1}},{"line":290,"address":[],"length":0,"stats":{"Line":5}},{"line":294,"address":[],"length":0,"stats":{"Line":19}},{"line":296,"address":[],"length":0,"stats":{"Line":55}},{"line":297,"address":[],"length":0,"stats":{"Line":17}},{"line":301,"address":[],"length":0,"stats":{"Line":46}},{"line":302,"address":[],"length":0,"stats":{"Line":87}},{"line":303,"address":[],"length":0,"stats":{"Line":1}},{"line":307,"address":[],"length":0,"stats":{"Line":1}},{"line":311,"address":[],"length":0,"stats":{"Line":13}},{"line":318,"address":[],"length":0,"stats":{"Line":26}},{"line":319,"address":[],"length":0,"stats":{"Line":26}},{"line":320,"address":[],"length":0,"stats":{"Line":61}},{"line":321,"address":[],"length":0,"stats":{"Line":14}},{"line":323,"address":[],"length":0,"stats":{"Line":4}},{"line":326,"address":[],"length":0,"stats":{"Line":91}},{"line":327,"address":[],"length":0,"stats":{"Line":39}},{"line":329,"address":[],"length":0,"stats":{"Line":13}},{"line":333,"address":[],"length":0,"stats":{"Line":2}},{"line":341,"address":[],"length":0,"stats":{"Line":4}},{"line":342,"address":[],"length":0,"stats":{"Line":4}},{"line":343,"address":[],"length":0,"stats":{"Line":12}},{"line":344,"address":[],"length":0,"stats":{"Line":2}},{"line":345,"address":[],"length":0,"stats":{"Line":0}},{"line":346,"address":[],"length":0,"stats":{"Line":0}},{"line":347,"address":[],"length":0,"stats":{"Line":0}},{"line":348,"address":[],"length":0,"stats":{"Line":0}},{"line":352,"address":[],"length":0,"stats":{"Line":14}},{"line":353,"address":[],"length":0,"stats":{"Line":4}},{"line":354,"address":[],"length":0,"stats":{"Line":6}},{"line":356,"address":[],"length":0,"stats":{"Line":2}},{"line":360,"address":[],"length":0,"stats":{"Line":8}},{"line":361,"address":[],"length":0,"stats":{"Line":24}},{"line":365,"address":[],"length":0,"stats":{"Line":11}},{"line":366,"address":[],"length":0,"stats":{"Line":33}},{"line":370,"address":[],"length":0,"stats":{"Line":5}},{"line":371,"address":[],"length":0,"stats":{"Line":10}},{"line":375,"address":[],"length":0,"stats":{"Line":2}},{"line":376,"address":[],"length":0,"stats":{"Line":2}},{"line":377,"address":[],"length":0,"stats":{"Line":5}},{"line":382,"address":[],"length":0,"stats":{"Line":2}},{"line":383,"address":[],"length":0,"stats":{"Line":2}},{"line":384,"address":[],"length":0,"stats":{"Line":5}},{"line":389,"address":[],"length":0,"stats":{"Line":4}},{"line":390,"address":[],"length":0,"stats":{"Line":17}},{"line":394,"address":[],"length":0,"stats":{"Line":3}},{"line":395,"address":[],"length":0,"stats":{"Line":3}},{"line":396,"address":[],"length":0,"stats":{"Line":9}},{"line":400,"address":[],"length":0,"stats":{"Line":5}},{"line":401,"address":[],"length":0,"stats":{"Line":10}},{"line":403,"address":[],"length":0,"stats":{"Line":35}},{"line":404,"address":[],"length":0,"stats":{"Line":20}},{"line":405,"address":[],"length":0,"stats":{"Line":20}},{"line":406,"address":[],"length":0,"stats":{"Line":20}},{"line":407,"address":[],"length":0,"stats":{"Line":20}},{"line":408,"address":[],"length":0,"stats":{"Line":20}},{"line":410,"address":[],"length":0,"stats":{"Line":60}},{"line":411,"address":[],"length":0,"stats":{"Line":30}},{"line":412,"address":[],"length":0,"stats":{"Line":30}},{"line":413,"address":[],"length":0,"stats":{"Line":50}},{"line":414,"address":[],"length":0,"stats":{"Line":20}},{"line":415,"address":[],"length":0,"stats":{"Line":20}},{"line":416,"address":[],"length":0,"stats":{"Line":20}},{"line":417,"address":[],"length":0,"stats":{"Line":40}},{"line":418,"address":[],"length":0,"stats":{"Line":10}},{"line":421,"address":[],"length":0,"stats":{"Line":10}},{"line":422,"address":[],"length":0,"stats":{"Line":5}},{"line":425,"address":[],"length":0,"stats":{"Line":5}},{"line":429,"address":[],"length":0,"stats":{"Line":1}},{"line":430,"address":[],"length":0,"stats":{"Line":1}},{"line":434,"address":[],"length":0,"stats":{"Line":1}},{"line":435,"address":[],"length":0,"stats":{"Line":2}},{"line":439,"address":[],"length":0,"stats":{"Line":2}},{"line":440,"address":[],"length":0,"stats":{"Line":2}},{"line":442,"address":[],"length":0,"stats":{"Line":6}},{"line":447,"address":[],"length":0,"stats":{"Line":26}},{"line":448,"address":[],"length":0,"stats":{"Line":52}},{"line":451,"address":[],"length":0,"stats":{"Line":52}},{"line":452,"address":[],"length":0,"stats":{"Line":52}},{"line":453,"address":[],"length":0,"stats":{"Line":26}},{"line":455,"address":[],"length":0,"stats":{"Line":52}},{"line":456,"address":[],"length":0,"stats":{"Line":52}},{"line":457,"address":[],"length":0,"stats":{"Line":26}},{"line":459,"address":[],"length":0,"stats":{"Line":52}},{"line":460,"address":[],"length":0,"stats":{"Line":52}},{"line":461,"address":[],"length":0,"stats":{"Line":26}},{"line":463,"address":[],"length":0,"stats":{"Line":52}},{"line":464,"address":[],"length":0,"stats":{"Line":52}},{"line":465,"address":[],"length":0,"stats":{"Line":26}},{"line":469,"address":[],"length":0,"stats":{"Line":130}},{"line":470,"address":[],"length":0,"stats":{"Line":130}},{"line":471,"address":[],"length":0,"stats":{"Line":130}},{"line":472,"address":[],"length":0,"stats":{"Line":130}},{"line":473,"address":[],"length":0,"stats":{"Line":130}},{"line":476,"address":[],"length":0,"stats":{"Line":52}},{"line":477,"address":[],"length":0,"stats":{"Line":52}},{"line":478,"address":[],"length":0,"stats":{"Line":26}},{"line":480,"address":[],"length":0,"stats":{"Line":130}},{"line":482,"address":[],"length":0,"stats":{"Line":26}},{"line":487,"address":[],"length":0,"stats":{"Line":4}},{"line":488,"address":[],"length":0,"stats":{"Line":4}},{"line":489,"address":[],"length":0,"stats":{"Line":2}},{"line":490,"address":[],"length":0,"stats":{"Line":3}},{"line":491,"address":[],"length":0,"stats":{"Line":2}},{"line":493,"address":[],"length":0,"stats":{"Line":4}},{"line":498,"address":[],"length":0,"stats":{"Line":3}},{"line":499,"address":[],"length":0,"stats":{"Line":3}},{"line":500,"address":[],"length":0,"stats":{"Line":3}},{"line":501,"address":[],"length":0,"stats":{"Line":2}},{"line":502,"address":[],"length":0,"stats":{"Line":3}},{"line":504,"address":[],"length":0,"stats":{"Line":2}}],"covered":159,"coverable":163},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","error.rs"],"content":"//! LLM error types with retry strategies.\n//!\n//! # Error Handling Philosophy\n//!\n//! Errors should be:\n//! 1. **Actionable**: Tell the user what to do, not just what went wrong\n//! 2. **Specific**: Include relevant context (model name, token counts, etc.)\n//! 3. **Recoverable**: Distinguish transient errors (retry) from permanent ones\n//!\n//! # Common Errors and Solutions\n//!\n//! | Error | Cause | Solution |\n//! |-------|-------|----------|\n//! | `AuthError` | Invalid/expired API key | Check `OPENAI_API_KEY` env var |\n//! | `RateLimited` | Too many requests | Wait for `retry_after` seconds |\n//! | `TokenLimitExceeded` | Input too long | Reduce chunk size or context |\n//! | `ModelNotFound` | Invalid model name | Use `gpt-4o-mini` or `gpt-3.5-turbo` |\n//! | `Timeout` | Network slow | Increase timeout or retry |\n//!\n//! # Retry Strategies\n//!\n//! Each error type has an associated retry strategy:\n//! - `ExponentialBackoff`: For transient network/server errors\n//! - `WaitAndRetry`: For rate limiting (wait specified duration)\n//! - `ReduceContext`: For token limit errors (caller should reduce input)\n//! - `NoRetry`: For permanent errors (auth, invalid request)\n//!\n//! @implements specs/improve-tools/006-error-handling.md\n//! @iteration OODA-11\n\nuse std::time::Duration;\nuse thiserror::Error;\n\n/// Result type for LLM operations.\npub type Result<T> = std::result::Result<T, LlmError>;\n\n// ============================================================================\n// Retry Strategy\n// ============================================================================\n\n/// Strategy for retrying failed LLM operations.\n///\n/// Each error type maps to an appropriate retry strategy based on\n/// whether the error is transient (retry) or permanent (no retry).\n#[derive(Debug, Clone, PartialEq)]\npub enum RetryStrategy {\n    /// Retry with exponential backoff (for transient errors).\n    ExponentialBackoff {\n        /// Initial delay before first retry.\n        base_delay: Duration,\n        /// Maximum delay between retries.\n        max_delay: Duration,\n        /// Maximum number of retry attempts.\n        max_attempts: u32,\n    },\n\n    /// Wait for a specific duration then retry once (for rate limits).\n    WaitAndRetry {\n        /// Duration to wait before retrying.\n        wait: Duration,\n    },\n\n    /// Do not retry, but caller should reduce context size and try again.\n    ReduceContext,\n\n    /// Do not retry at all (permanent error).\n    NoRetry,\n}\n\nimpl RetryStrategy {\n    /// Create a standard exponential backoff strategy for network errors.\n    pub fn network_backoff() -> Self {\n        Self::ExponentialBackoff {\n            base_delay: Duration::from_millis(125),\n            max_delay: Duration::from_secs(30),\n            max_attempts: 5,\n        }\n    }\n\n    /// Create a standard exponential backoff strategy for server errors.\n    pub fn server_backoff() -> Self {\n        Self::ExponentialBackoff {\n            base_delay: Duration::from_secs(1),\n            max_delay: Duration::from_secs(60),\n            max_attempts: 3,\n        }\n    }\n\n    /// Check if this strategy allows retrying.\n    pub fn should_retry(&self) -> bool {\n        !matches!(self, Self::NoRetry)\n    }\n}\n\n// ============================================================================\n// LLM Error Types\n// ============================================================================\n\n/// Errors that can occur in LLM operations.\n#[derive(Debug, Error)]\npub enum LlmError {\n    /// API error from the provider.\n    #[error(\"API error: {0}\")]\n    ApiError(String),\n\n    /// Rate limit exceeded.\n    #[error(\"Rate limit exceeded: {0}\")]\n    RateLimited(String),\n\n    /// Invalid request parameters.\n    #[error(\"Invalid request: {0}\")]\n    InvalidRequest(String),\n\n    /// Authentication error.\n    #[error(\"Authentication error: {0}\")]\n    AuthError(String),\n\n    /// Token limit exceeded.\n    #[error(\"Token limit exceeded: max {max}, got {got}\")]\n    TokenLimitExceeded { max: usize, got: usize },\n\n    /// Model not found.\n    #[error(\"Model not found: {0}\")]\n    ModelNotFound(String),\n\n    /// Network error.\n    #[error(\"Network error: {0}\")]\n    NetworkError(String),\n\n    /// Serialization error.\n    #[error(\"Serialization error: {0}\")]\n    SerializationError(#[from] serde_json::Error),\n\n    /// Configuration error.\n    #[error(\"Configuration error: {0}\")]\n    ConfigError(String),\n\n    /// Provider-specific error.\n    #[error(\"Provider error: {0}\")]\n    ProviderError(String),\n\n    /// Timeout error.\n    #[error(\"Request timed out\")]\n    Timeout,\n\n    /// Feature not supported.\n    #[error(\"Not supported: {0}\")]\n    NotSupported(String),\n\n    /// Unknown error.\n    #[error(\"Unknown error: {0}\")]\n    Unknown(String),\n}\n\nimpl From<reqwest::Error> for LlmError {\n    fn from(err: reqwest::Error) -> Self {\n        if err.is_timeout() {\n            LlmError::Timeout\n        } else if err.is_connect() {\n            LlmError::NetworkError(format!(\"Connection failed: {}\", err))\n        } else {\n            LlmError::NetworkError(err.to_string())\n        }\n    }\n}\n\nimpl From<async_openai::error::OpenAIError> for LlmError {\n    fn from(err: async_openai::error::OpenAIError) -> Self {\n        match err {\n            async_openai::error::OpenAIError::ApiError(api_err) => {\n                let message = api_err.message.clone();\n                if message.contains(\"rate limit\") || message.contains(\"Rate limit\") {\n                    LlmError::RateLimited(message)\n                } else if message.contains(\"authentication\") || message.contains(\"invalid_api_key\")\n                {\n                    LlmError::AuthError(message)\n                } else if message.contains(\"model\") && message.contains(\"not found\") {\n                    LlmError::ModelNotFound(message)\n                } else {\n                    LlmError::ApiError(message)\n                }\n            }\n            async_openai::error::OpenAIError::Reqwest(req_err) => LlmError::from(req_err),\n            async_openai::error::OpenAIError::JSONDeserialize(json_err) => {\n                LlmError::SerializationError(json_err)\n            }\n            _ => LlmError::ProviderError(err.to_string()),\n        }\n    }\n}\n\n// ============================================================================\n// Retry Strategy Methods\n// ============================================================================\n\nimpl LlmError {\n    /// Get the appropriate retry strategy for this error.\n    ///\n    /// # Returns\n    ///\n    /// - `ExponentialBackoff` for transient network/server errors\n    /// - `WaitAndRetry` for rate limiting\n    /// - `ReduceContext` for token limit errors\n    /// - `NoRetry` for permanent errors (auth, invalid request, etc.)\n    ///\n    /// # Example\n    ///\n    /// ```\n    /// use edgequake_llm::{LlmError, RetryStrategy};\n    ///\n    /// let error = LlmError::NetworkError(\"connection failed\".to_string());\n    /// let strategy = error.retry_strategy();\n    /// assert!(strategy.should_retry());\n    /// ```\n    pub fn retry_strategy(&self) -> RetryStrategy {\n        match self {\n            // Transient network errors - aggressive retry\n            Self::NetworkError(_) | Self::Timeout => RetryStrategy::network_backoff(),\n\n            // Rate limiting - wait the specified duration\n            Self::RateLimited(_) => RetryStrategy::WaitAndRetry {\n                wait: Duration::from_secs(60),\n            },\n\n            // Server errors - moderate retry\n            Self::ApiError(msg)\n                if msg.contains(\"500\") || msg.contains(\"502\") || msg.contains(\"503\") =>\n            {\n                RetryStrategy::server_backoff()\n            }\n            Self::ProviderError(_) => RetryStrategy::server_backoff(),\n\n            // Token limit - caller should reduce context\n            Self::TokenLimitExceeded { .. } => RetryStrategy::ReduceContext,\n\n            // Permanent errors - no retry\n            Self::AuthError(_)\n            | Self::InvalidRequest(_)\n            | Self::ModelNotFound(_)\n            | Self::ConfigError(_)\n            | Self::NotSupported(_) => RetryStrategy::NoRetry,\n\n            // Default for other errors - conservative retry\n            Self::ApiError(_) | Self::SerializationError(_) | Self::Unknown(_) => {\n                RetryStrategy::ExponentialBackoff {\n                    base_delay: Duration::from_secs(1),\n                    max_delay: Duration::from_secs(30),\n                    max_attempts: 2,\n                }\n            }\n        }\n    }\n\n    /// Get a user-friendly description of the error with suggested action.\n    ///\n    /// # Example\n    ///\n    /// ```\n    /// use edgequake_llm::LlmError;\n    ///\n    /// let error = LlmError::AuthError(\"invalid key\".to_string());\n    /// assert!(error.user_description().contains(\"API key\"));\n    /// ```\n    pub fn user_description(&self) -> String {\n        match self {\n            Self::NetworkError(_) => {\n                \"Unable to connect to the API. Check your internet connection.\".to_string()\n            }\n            Self::Timeout => \"Request timed out. The server may be overloaded.\".to_string(),\n            Self::RateLimited(_) => \"Rate limited by the API. Waiting before retry...\".to_string(),\n            Self::TokenLimitExceeded { max, got } => {\n                format!(\n                    \"Context too large ({}/{} tokens). Reducing context and retrying...\",\n                    got, max\n                )\n            }\n            Self::AuthError(_) => {\n                \"Authentication failed. Please check your API key is valid and not expired.\"\n                    .to_string()\n            }\n            Self::ModelNotFound(model) => {\n                format!(\n                    \"Model '{}' not found. Use a supported model like 'gpt-4o-mini'.\",\n                    model\n                )\n            }\n            Self::InvalidRequest(msg) => {\n                format!(\"Invalid request: {}. Check your parameters.\", msg)\n            }\n            Self::ConfigError(msg) => format!(\"Configuration error: {}.\", msg),\n            Self::NotSupported(feature) => {\n                format!(\"Feature '{}' is not supported by this provider.\", feature)\n            }\n            Self::ApiError(_) | Self::ProviderError(_) => {\n                \"API server error. Retrying...\".to_string()\n            }\n            Self::SerializationError(_) => {\n                \"Failed to parse API response. This may be a temporary issue.\".to_string()\n            }\n            Self::Unknown(msg) => format!(\"An unexpected error occurred: {}\", msg),\n        }\n    }\n\n    /// Check if this error is recoverable (can be retried).\n    pub fn is_recoverable(&self) -> bool {\n        self.retry_strategy().should_retry()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_llm_error_display() {\n        let error = LlmError::ApiError(\"something went wrong\".to_string());\n        assert_eq!(error.to_string(), \"API error: something went wrong\");\n\n        let error = LlmError::RateLimited(\"too many requests\".to_string());\n        assert_eq!(error.to_string(), \"Rate limit exceeded: too many requests\");\n\n        let error = LlmError::InvalidRequest(\"bad params\".to_string());\n        assert_eq!(error.to_string(), \"Invalid request: bad params\");\n    }\n\n    #[test]\n    fn test_llm_error_auth() {\n        let error = LlmError::AuthError(\"invalid key\".to_string());\n        assert_eq!(error.to_string(), \"Authentication error: invalid key\");\n    }\n\n    #[test]\n    fn test_llm_error_token_limit() {\n        let error = LlmError::TokenLimitExceeded {\n            max: 4096,\n            got: 5000,\n        };\n        assert_eq!(\n            error.to_string(),\n            \"Token limit exceeded: max 4096, got 5000\"\n        );\n    }\n\n    #[test]\n    fn test_llm_error_model_not_found() {\n        let error = LlmError::ModelNotFound(\"gpt-5-turbo\".to_string());\n        assert_eq!(error.to_string(), \"Model not found: gpt-5-turbo\");\n    }\n\n    #[test]\n    fn test_llm_error_network() {\n        let error = LlmError::NetworkError(\"connection refused\".to_string());\n        assert_eq!(error.to_string(), \"Network error: connection refused\");\n    }\n\n    #[test]\n    fn test_llm_error_config() {\n        let error = LlmError::ConfigError(\"missing api key\".to_string());\n        assert_eq!(error.to_string(), \"Configuration error: missing api key\");\n    }\n\n    #[test]\n    fn test_llm_error_provider() {\n        let error = LlmError::ProviderError(\"openai specific error\".to_string());\n        assert_eq!(error.to_string(), \"Provider error: openai specific error\");\n    }\n\n    #[test]\n    fn test_llm_error_timeout() {\n        let error = LlmError::Timeout;\n        assert_eq!(error.to_string(), \"Request timed out\");\n    }\n\n    #[test]\n    fn test_llm_error_not_supported() {\n        let error = LlmError::NotSupported(\"function calling\".to_string());\n        assert_eq!(error.to_string(), \"Not supported: function calling\");\n    }\n\n    #[test]\n    fn test_llm_error_unknown() {\n        let error = LlmError::Unknown(\"mystery error\".to_string());\n        assert_eq!(error.to_string(), \"Unknown error: mystery error\");\n    }\n\n    #[test]\n    fn test_llm_error_debug() {\n        let error = LlmError::ApiError(\"test\".to_string());\n        let debug = format!(\"{:?}\", error);\n        assert!(debug.contains(\"ApiError\"));\n        assert!(debug.contains(\"test\"));\n    }\n\n    #[test]\n    fn test_llm_error_from_serde_json() {\n        let json_str = \"not json at all\";\n        let json_err: serde_json::Error =\n            serde_json::from_str::<serde_json::Value>(json_str).unwrap_err();\n        let llm_err: LlmError = json_err.into();\n        assert!(matches!(llm_err, LlmError::SerializationError(_)));\n    }\n\n    // ========================================================================\n    // Retry Strategy Tests\n    // ========================================================================\n\n    #[test]\n    fn test_network_error_retry_strategy() {\n        let error = LlmError::NetworkError(\"connection failed\".to_string());\n        let strategy = error.retry_strategy();\n\n        match strategy {\n            RetryStrategy::ExponentialBackoff { max_attempts, .. } => {\n                assert_eq!(max_attempts, 5);\n            }\n            _ => panic!(\"Expected ExponentialBackoff for network error\"),\n        }\n        assert!(strategy.should_retry());\n        assert!(error.is_recoverable());\n    }\n\n    #[test]\n    fn test_timeout_retry_strategy() {\n        let error = LlmError::Timeout;\n        let strategy = error.retry_strategy();\n\n        assert!(matches!(strategy, RetryStrategy::ExponentialBackoff { .. }));\n        assert!(strategy.should_retry());\n    }\n\n    #[test]\n    fn test_rate_limited_retry_strategy() {\n        let error = LlmError::RateLimited(\"too many requests\".to_string());\n        let strategy = error.retry_strategy();\n\n        match strategy {\n            RetryStrategy::WaitAndRetry { wait } => {\n                assert_eq!(wait, Duration::from_secs(60));\n            }\n            _ => panic!(\"Expected WaitAndRetry for rate limit\"),\n        }\n        assert!(strategy.should_retry());\n    }\n\n    #[test]\n    fn test_token_limit_reduce_context_strategy() {\n        let error = LlmError::TokenLimitExceeded {\n            max: 4096,\n            got: 5000,\n        };\n        let strategy = error.retry_strategy();\n\n        assert!(matches!(strategy, RetryStrategy::ReduceContext));\n        assert!(strategy.should_retry());\n    }\n\n    #[test]\n    fn test_auth_error_no_retry() {\n        let error = LlmError::AuthError(\"invalid key\".to_string());\n        let strategy = error.retry_strategy();\n\n        assert!(matches!(strategy, RetryStrategy::NoRetry));\n        assert!(!strategy.should_retry());\n        assert!(!error.is_recoverable());\n    }\n\n    #[test]\n    fn test_invalid_request_no_retry() {\n        let error = LlmError::InvalidRequest(\"bad params\".to_string());\n        assert!(matches!(error.retry_strategy(), RetryStrategy::NoRetry));\n    }\n\n    #[test]\n    fn test_model_not_found_no_retry() {\n        let error = LlmError::ModelNotFound(\"gpt-5\".to_string());\n        assert!(matches!(error.retry_strategy(), RetryStrategy::NoRetry));\n    }\n\n    #[test]\n    fn test_user_description_network() {\n        let error = LlmError::NetworkError(\"connection refused\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"internet connection\"));\n    }\n\n    #[test]\n    fn test_user_description_auth() {\n        let error = LlmError::AuthError(\"invalid\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"API key\"));\n    }\n\n    #[test]\n    fn test_user_description_token_limit() {\n        let error = LlmError::TokenLimitExceeded {\n            max: 4096,\n            got: 5000,\n        };\n        let desc = error.user_description();\n        assert!(desc.contains(\"5000/4096\"));\n        assert!(desc.contains(\"Reducing\"));\n    }\n\n    #[test]\n    fn test_retry_strategy_equality() {\n        let s1 = RetryStrategy::network_backoff();\n        let s2 = RetryStrategy::network_backoff();\n        assert_eq!(s1, s2);\n\n        let s3 = RetryStrategy::NoRetry;\n        assert_ne!(s1, s3);\n    }\n\n    // ========================================================================\n    // user_description() coverage for all variants\n    // ========================================================================\n\n    #[test]\n    fn test_user_description_timeout() {\n        let error = LlmError::Timeout;\n        let desc = error.user_description();\n        assert!(desc.contains(\"timed out\"));\n    }\n\n    #[test]\n    fn test_user_description_rate_limited() {\n        let error = LlmError::RateLimited(\"slow down\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"Rate limited\"));\n    }\n\n    #[test]\n    fn test_user_description_model_not_found() {\n        let error = LlmError::ModelNotFound(\"gpt-5\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"gpt-5\"));\n        assert!(desc.contains(\"not found\"));\n    }\n\n    #[test]\n    fn test_user_description_not_supported() {\n        let error = LlmError::NotSupported(\"streaming\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"streaming\"));\n        assert!(desc.contains(\"not supported\"));\n    }\n\n    #[test]\n    fn test_user_description_unknown() {\n        let error = LlmError::Unknown(\"mystery\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"mystery\"));\n    }\n\n    #[test]\n    fn test_user_description_api_error() {\n        let error = LlmError::ApiError(\"server crashed\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"Retrying\"));\n    }\n\n    #[test]\n    fn test_user_description_provider_error() {\n        let error = LlmError::ProviderError(\"internal failure\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"Retrying\"));\n    }\n\n    #[test]\n    fn test_user_description_serialization() {\n        let json_err = serde_json::from_str::<serde_json::Value>(\"bad\").unwrap_err();\n        let error = LlmError::SerializationError(json_err);\n        let desc = error.user_description();\n        assert!(desc.contains(\"parse\"));\n    }\n\n    #[test]\n    fn test_user_description_config() {\n        let error = LlmError::ConfigError(\"missing field\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"Configuration\"));\n    }\n\n    #[test]\n    fn test_user_description_invalid_request() {\n        let error = LlmError::InvalidRequest(\"empty prompt\".to_string());\n        let desc = error.user_description();\n        assert!(desc.contains(\"empty prompt\"));\n    }\n\n    // ========================================================================\n    // retry_strategy() remaining branches\n    // ========================================================================\n\n    #[test]\n    fn test_api_error_500_server_backoff() {\n        let error = LlmError::ApiError(\"HTTP 500 internal server error\".to_string());\n        let strategy = error.retry_strategy();\n        match strategy {\n            RetryStrategy::ExponentialBackoff { max_attempts, .. } => {\n                assert_eq!(max_attempts, 3); // server_backoff has 3 attempts\n            }\n            _ => panic!(\"Expected ExponentialBackoff for 500 error\"),\n        }\n    }\n\n    #[test]\n    fn test_api_error_502_server_backoff() {\n        let error = LlmError::ApiError(\"502 bad gateway\".to_string());\n        assert!(matches!(\n            error.retry_strategy(),\n            RetryStrategy::ExponentialBackoff { .. }\n        ));\n    }\n\n    #[test]\n    fn test_api_error_503_server_backoff() {\n        let error = LlmError::ApiError(\"503 service unavailable\".to_string());\n        assert!(matches!(\n            error.retry_strategy(),\n            RetryStrategy::ExponentialBackoff { .. }\n        ));\n    }\n\n    #[test]\n    fn test_provider_error_server_backoff() {\n        let error = LlmError::ProviderError(\"internal issue\".to_string());\n        let strategy = error.retry_strategy();\n        match strategy {\n            RetryStrategy::ExponentialBackoff {\n                base_delay,\n                max_delay,\n                max_attempts,\n            } => {\n                assert_eq!(base_delay, Duration::from_secs(1));\n                assert_eq!(max_delay, Duration::from_secs(60));\n                assert_eq!(max_attempts, 3);\n            }\n            _ => panic!(\"Expected server_backoff for ProviderError\"),\n        }\n    }\n\n    #[test]\n    fn test_unknown_error_retry_strategy() {\n        let error = LlmError::Unknown(\"something\".to_string());\n        let strategy = error.retry_strategy();\n        match strategy {\n            RetryStrategy::ExponentialBackoff { max_attempts, .. } => {\n                assert_eq!(max_attempts, 2);\n            }\n            _ => panic!(\"Expected ExponentialBackoff for Unknown\"),\n        }\n    }\n\n    #[test]\n    fn test_serialization_error_retry_strategy() {\n        let json_err = serde_json::from_str::<serde_json::Value>(\"bad\").unwrap_err();\n        let error = LlmError::SerializationError(json_err);\n        let strategy = error.retry_strategy();\n        assert!(matches!(\n            strategy,\n            RetryStrategy::ExponentialBackoff { .. }\n        ));\n    }\n\n    #[test]\n    fn test_api_error_non_5xx_retry_strategy() {\n        let error = LlmError::ApiError(\"generic error\".to_string());\n        let strategy = error.retry_strategy();\n        match strategy {\n            RetryStrategy::ExponentialBackoff { max_attempts, .. } => {\n                assert_eq!(max_attempts, 2);\n            }\n            _ => panic!(\"Expected ExponentialBackoff for generic ApiError\"),\n        }\n    }\n\n    #[test]\n    fn test_config_error_no_retry() {\n        let error = LlmError::ConfigError(\"bad config\".to_string());\n        assert!(matches!(error.retry_strategy(), RetryStrategy::NoRetry));\n        assert!(!error.is_recoverable());\n    }\n\n    #[test]\n    fn test_not_supported_no_retry() {\n        let error = LlmError::NotSupported(\"embeddings\".to_string());\n        assert!(matches!(error.retry_strategy(), RetryStrategy::NoRetry));\n        assert!(!error.is_recoverable());\n    }\n\n    // ========================================================================\n    // RetryStrategy constructor verification\n    // ========================================================================\n\n    #[test]\n    fn test_server_backoff_values() {\n        let strategy = RetryStrategy::server_backoff();\n        match strategy {\n            RetryStrategy::ExponentialBackoff {\n                base_delay,\n                max_delay,\n                max_attempts,\n            } => {\n                assert_eq!(base_delay, Duration::from_secs(1));\n                assert_eq!(max_delay, Duration::from_secs(60));\n                assert_eq!(max_attempts, 3);\n            }\n            _ => panic!(\"Expected ExponentialBackoff\"),\n        }\n    }\n\n    #[test]\n    fn test_network_backoff_values() {\n        let strategy = RetryStrategy::network_backoff();\n        match strategy {\n            RetryStrategy::ExponentialBackoff {\n                base_delay,\n                max_delay,\n                max_attempts,\n            } => {\n                assert_eq!(base_delay, Duration::from_millis(125));\n                assert_eq!(max_delay, Duration::from_secs(30));\n                assert_eq!(max_attempts, 5);\n            }\n            _ => panic!(\"Expected ExponentialBackoff\"),\n        }\n    }\n\n    #[test]\n    fn test_reduce_context_should_retry() {\n        let strategy = RetryStrategy::ReduceContext;\n        assert!(strategy.should_retry());\n    }\n\n    #[test]\n    fn test_wait_and_retry_should_retry() {\n        let strategy = RetryStrategy::WaitAndRetry {\n            wait: Duration::from_secs(1),\n        };\n        assert!(strategy.should_retry());\n    }\n\n    // ========================================================================\n    // is_recoverable coverage\n    // ========================================================================\n\n    #[test]\n    fn test_is_recoverable_network() {\n        assert!(LlmError::NetworkError(\"fail\".to_string()).is_recoverable());\n    }\n\n    #[test]\n    fn test_is_recoverable_timeout() {\n        assert!(LlmError::Timeout.is_recoverable());\n    }\n\n    #[test]\n    fn test_is_recoverable_rate_limited() {\n        assert!(LlmError::RateLimited(\"wait\".to_string()).is_recoverable());\n    }\n\n    #[test]\n    fn test_is_not_recoverable_invalid_request() {\n        assert!(!LlmError::InvalidRequest(\"bad\".to_string()).is_recoverable());\n    }\n\n    #[test]\n    fn test_is_not_recoverable_model_not_found() {\n        assert!(!LlmError::ModelNotFound(\"x\".to_string()).is_recoverable());\n    }\n}\n","traces":[{"line":72,"address":[],"length":0,"stats":{"Line":13}},{"line":74,"address":[],"length":0,"stats":{"Line":13}},{"line":75,"address":[],"length":0,"stats":{"Line":13}},{"line":81,"address":[],"length":0,"stats":{"Line":5}},{"line":83,"address":[],"length":0,"stats":{"Line":5}},{"line":84,"address":[],"length":0,"stats":{"Line":5}},{"line":90,"address":[],"length":0,"stats":{"Line":18}},{"line":91,"address":[],"length":0,"stats":{"Line":29}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[],"length":0,"stats":{"Line":32}},{"line":216,"address":[],"length":0,"stats":{"Line":4}},{"line":218,"address":[],"length":0,"stats":{"Line":10}},{"line":222,"address":[],"length":0,"stats":{"Line":2}},{"line":226,"address":[],"length":0,"stats":{"Line":4}},{"line":227,"address":[],"length":0,"stats":{"Line":14}},{"line":229,"address":[],"length":0,"stats":{"Line":3}},{"line":231,"address":[],"length":0,"stats":{"Line":1}},{"line":234,"address":[],"length":0,"stats":{"Line":1}},{"line":241,"address":[],"length":0,"stats":{"Line":12}},{"line":246,"address":[],"length":0,"stats":{"Line":3}},{"line":247,"address":[],"length":0,"stats":{"Line":3}},{"line":264,"address":[],"length":0,"stats":{"Line":13}},{"line":265,"address":[],"length":0,"stats":{"Line":13}},{"line":267,"address":[],"length":0,"stats":{"Line":2}},{"line":269,"address":[],"length":0,"stats":{"Line":2}},{"line":270,"address":[],"length":0,"stats":{"Line":2}},{"line":271,"address":[],"length":0,"stats":{"Line":2}},{"line":272,"address":[],"length":0,"stats":{"Line":1}},{"line":278,"address":[],"length":0,"stats":{"Line":1}},{"line":281,"address":[],"length":0,"stats":{"Line":1}},{"line":282,"address":[],"length":0,"stats":{"Line":1}},{"line":287,"address":[],"length":0,"stats":{"Line":1}},{"line":288,"address":[],"length":0,"stats":{"Line":2}},{"line":290,"address":[],"length":0,"stats":{"Line":3}},{"line":291,"address":[],"length":0,"stats":{"Line":1}},{"line":292,"address":[],"length":0,"stats":{"Line":2}},{"line":295,"address":[],"length":0,"stats":{"Line":4}},{"line":298,"address":[],"length":0,"stats":{"Line":2}},{"line":300,"address":[],"length":0,"stats":{"Line":3}},{"line":305,"address":[],"length":0,"stats":{"Line":9}},{"line":306,"address":[],"length":0,"stats":{"Line":18}}],"covered":40,"coverable":61},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","factory.rs"],"content":"//! LLM provider factory for environment-based selection.\n//!\n//! @implements SPEC-032: Ollama/LM Studio provider support\n//! @implements FEAT0017: Multi-provider LLM support\n//!\n//! # Environment Variables\n//!\n//! ## Provider Selection\n//!\n//! - `EDGEQUAKE_LLM_PROVIDER`: Override provider selection (openai|ollama|lmstudio|mock)\n//!\n//! ## Provider-Specific Configuration\n//!\n//! See individual provider documentation for configuration variables:\n//! - OpenAI: `OPENAI_API_KEY`, `OPENAI_BASE_URL`\n//! - Ollama: `OLLAMA_HOST`, `OLLAMA_MODEL`, `OLLAMA_EMBEDDING_MODEL`\n//! - LM Studio: `LMSTUDIO_HOST`, `LMSTUDIO_MODEL`, `LMSTUDIO_EMBEDDING_MODEL`\n//!\n//! # Auto-Detection Priority\n//!\n//! When `EDGEQUAKE_LLM_PROVIDER` is not set:\n//! 1. Check for OLLAMA_HOST or OLLAMA_MODEL â†’ Use Ollama\n//! 2. Check for OPENAI_API_KEY â†’ Use OpenAI\n//! 3. Fallback â†’ Use Mock provider\n//!\n//! # Example\n//!\n//! ```rust,ignore\n//! use edgequake_llm::ProviderFactory;\n//!\n//! // Auto-detect from environment\n//! let (llm, embedding) = ProviderFactory::from_env()?;\n//!\n//! // Explicit provider selection\n//! std::env::set_var(\"EDGEQUAKE_LLM_PROVIDER\", \"ollama\");\n//! let (llm, embedding) = ProviderFactory::from_env()?;\n//! ```\n\nuse std::sync::Arc;\n\nuse tracing::warn;\n\nuse crate::error::{LlmError, Result};\nuse crate::model_config::{ProviderConfig, ProviderType as ConfigProviderType};\nuse crate::providers::anthropic::AnthropicProvider;\nuse crate::providers::gemini::GeminiProvider;\nuse crate::providers::huggingface::HuggingFaceProvider;\nuse crate::providers::lmstudio::LMStudioProvider;\nuse crate::providers::openai_compatible::OpenAICompatibleProvider;\nuse crate::providers::openrouter::OpenRouterProvider;\nuse crate::providers::xai::XAIProvider;\nuse crate::traits::{EmbeddingProvider, LLMProvider};\nuse crate::{MockProvider, OllamaProvider, OpenAIProvider, VsCodeCopilotProvider};\n\n/// Supported provider types\n#[derive(Debug, Clone, Copy, PartialEq, Eq)]\npub enum ProviderType {\n    /// OpenAI provider (cloud API)\n    OpenAI,\n    /// Anthropic provider (Claude models)\n    Anthropic,\n    /// Gemini provider (Google AI / VertexAI)\n    Gemini,\n    /// OpenRouter provider (200+ models)\n    OpenRouter,\n    /// xAI provider (Grok models via api.x.ai)\n    XAI,\n    /// HuggingFace Hub provider (open-source models)\n    HuggingFace,\n    /// Ollama provider (local models)\n    Ollama,\n    /// LM Studio provider (OpenAI-compatible local API)\n    LMStudio,\n    /// VSCode Copilot provider (via proxy)\n    VsCodeCopilot,\n    /// Mock provider (testing only)\n    Mock,\n}\n\nimpl ProviderType {\n    /// Parse provider type from string (case-insensitive)\n    ///\n    /// # Examples\n    ///\n    /// ```\n    /// use edgequake_llm::ProviderType;\n    ///\n    /// assert_eq!(ProviderType::from_str(\"openai\"), Some(ProviderType::OpenAI));\n    /// assert_eq!(ProviderType::from_str(\"OLLAMA\"), Some(ProviderType::Ollama));\n    /// assert_eq!(ProviderType::from_str(\"lm-studio\"), Some(ProviderType::LMStudio));\n    /// ```\n    #[allow(clippy::should_implement_trait)]\n    pub fn from_str(s: &str) -> Option<Self> {\n        match s.to_lowercase().as_str() {\n            \"openai\" => Some(Self::OpenAI),\n            \"anthropic\" | \"claude\" => Some(Self::Anthropic),\n            \"gemini\" | \"google\" | \"vertex\" | \"vertexai\" => Some(Self::Gemini),\n            \"openrouter\" | \"open-router\" => Some(Self::OpenRouter),\n            \"xai\" | \"grok\" => Some(Self::XAI),\n            \"huggingface\" | \"hf\" | \"hugging-face\" | \"hugging_face\" => Some(Self::HuggingFace),\n            \"ollama\" => Some(Self::Ollama),\n            \"lmstudio\" | \"lm-studio\" | \"lm_studio\" => Some(Self::LMStudio),\n            \"vscode\" | \"vscode-copilot\" | \"copilot\" => Some(Self::VsCodeCopilot),\n            \"mock\" => Some(Self::Mock),\n            _ => None,\n        }\n    }\n}\n\n/// Provider factory for creating LLM and embedding providers.\n///\n/// Provides environment-based auto-detection and explicit provider selection.\npub struct ProviderFactory;\n\nimpl ProviderFactory {\n    /// Auto-detect and create providers from environment.\n    ///\n    /// # Priority\n    ///\n    /// 1. `EDGEQUAKE_LLM_PROVIDER` environment variable (explicit selection)\n    /// 2. Auto-detect: OLLAMA_HOST â†’ LMSTUDIO_HOST â†’ OPENAI_API_KEY â†’ Mock\n    ///\n    /// # Returns\n    ///\n    /// Returns a tuple of (LLMProvider, EmbeddingProvider). In most cases,\n    /// the same provider implementation is used for both.\n    ///\n    /// # Errors\n    ///\n    /// Returns error if required configuration for selected provider is missing.\n    ///\n    /// # Examples\n    ///\n    /// ```ignore\n    /// std::env::set_var(\"OLLAMA_HOST\", \"http://localhost:11434\");\n    /// let (llm, embedding) = ProviderFactory::from_env()?;\n    /// assert_eq!(llm.name(), \"ollama\");\n    /// ```\n    pub fn from_env() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // Check explicit provider selection\n        if let Ok(provider_str) = std::env::var(\"EDGEQUAKE_LLM_PROVIDER\") {\n            if let Some(provider_type) = ProviderType::from_str(&provider_str) {\n                return Self::create(provider_type);\n            }\n            return Err(LlmError::ConfigError(format!(\n                \"Unknown provider type: {}. Valid options: openai, anthropic, ollama, lmstudio, mock\",\n                provider_str\n            )));\n        }\n\n        // Auto-detect based on environment\n        // Priority: Ollama â†’ LM Studio â†’ Anthropic â†’ Gemini â†’ xAI â†’ OpenRouter â†’ OpenAI â†’ Mock\n        if std::env::var(\"OLLAMA_HOST\").is_ok() || std::env::var(\"OLLAMA_MODEL\").is_ok() {\n            return Self::create(ProviderType::Ollama);\n        }\n\n        // LM Studio detection (checks LMSTUDIO_HOST or LMSTUDIO_MODEL)\n        if std::env::var(\"LMSTUDIO_HOST\").is_ok() || std::env::var(\"LMSTUDIO_MODEL\").is_ok() {\n            return Self::create(ProviderType::LMStudio);\n        }\n\n        // Anthropic detection\n        if let Ok(api_key) = std::env::var(\"ANTHROPIC_API_KEY\") {\n            if !api_key.is_empty() {\n                return Self::create(ProviderType::Anthropic);\n            }\n        }\n\n        // OODA-73: Gemini detection (GEMINI_API_KEY or GOOGLE_API_KEY)\n        if let Ok(api_key) = std::env::var(\"GEMINI_API_KEY\")\n            .or_else(|_| std::env::var(\"GOOGLE_API_KEY\"))\n        {\n            if !api_key.is_empty() {\n                return Self::create(ProviderType::Gemini);\n            }\n        }\n\n        // OODA-71: xAI detection (Grok models via api.x.ai)\n        if let Ok(api_key) = std::env::var(\"XAI_API_KEY\") {\n            if !api_key.is_empty() {\n                return Self::create(ProviderType::XAI);\n            }\n        }\n\n        // OODA-80: HuggingFace Hub detection (HF_TOKEN or HUGGINGFACE_TOKEN)\n        if let Ok(api_key) = std::env::var(\"HF_TOKEN\")\n            .or_else(|_| std::env::var(\"HUGGINGFACE_TOKEN\"))\n        {\n            if !api_key.is_empty() {\n                return Self::create(ProviderType::HuggingFace);\n            }\n        }\n\n        // OODA-73: OpenRouter detection\n        if let Ok(api_key) = std::env::var(\"OPENROUTER_API_KEY\") {\n            if !api_key.is_empty() {\n                return Self::create(ProviderType::OpenRouter);\n            }\n        }\n\n        if let Ok(api_key) = std::env::var(\"OPENAI_API_KEY\") {\n            if !api_key.is_empty() && api_key != \"test-key\" {\n                return Self::create(ProviderType::OpenAI);\n            }\n        }\n\n        // Fallback to mock\n        Ok(Self::create_mock())\n    }\n\n    /// Create specific provider type.\n    ///\n    /// # Arguments\n    ///\n    /// * `provider_type` - The type of provider to create\n    ///\n    /// # Returns\n    ///\n    /// Returns a tuple of (LLMProvider, EmbeddingProvider).\n    ///\n    /// # Errors\n    ///\n    /// Returns error if required configuration for the provider is missing.\n    pub fn create(\n        provider_type: ProviderType,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        match provider_type {\n            ProviderType::OpenAI => Self::create_openai(),\n            ProviderType::Anthropic => Self::create_anthropic(),\n            ProviderType::Gemini => Self::create_gemini(),\n            ProviderType::OpenRouter => Self::create_openrouter(),\n            ProviderType::XAI => Self::create_xai(),\n            ProviderType::HuggingFace => Self::create_huggingface(),\n            ProviderType::Ollama => Self::create_ollama(),\n            ProviderType::LMStudio => Self::create_lmstudio(),\n            ProviderType::VsCodeCopilot => Self::create_vscode_copilot(),\n            ProviderType::Mock => Ok(Self::create_mock()),\n        }\n    }\n\n    /// OODA-04: Create specific provider type with a model override.\n    ///\n    /// Like `create()` but allows specifying a model name instead of using defaults.\n    /// Useful for CLI where user specifies `--provider openrouter --model mistral/model`.\n    ///\n    /// # Arguments\n    ///\n    /// * `provider_type` - The type of provider to create\n    /// * `model` - Optional model name to use instead of provider default\n    ///\n    /// # Returns\n    ///\n    /// Returns a tuple of (LLMProvider, EmbeddingProvider).\n    ///\n    /// # Errors\n    ///\n    /// Returns error if required configuration for the provider is missing.\n    pub fn create_with_model(\n        provider_type: ProviderType,\n        model: Option<&str>,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        match model {\n            Some(m) => match provider_type {\n                ProviderType::OpenRouter => Self::create_openrouter_with_model(m),\n                ProviderType::Anthropic => Self::create_anthropic_with_model(m),\n                ProviderType::Gemini => Self::create_gemini_with_model(m),\n                ProviderType::XAI => Self::create_xai_with_model(m),\n                ProviderType::OpenAI => Self::create_openai_with_model(m),\n                ProviderType::Ollama => Self::create_ollama_with_model(m),\n                ProviderType::LMStudio => Self::create_lmstudio_with_model(m),\n                // These don't need model override\n                ProviderType::HuggingFace => Self::create_huggingface(),\n                ProviderType::VsCodeCopilot => Self::create_vscode_copilot(),\n                ProviderType::Mock => Ok(Self::create_mock()),\n            },\n            None => Self::create(provider_type),\n        }\n    }\n\n    /// OODA-200: Create provider from TOML configuration.\n    ///\n    /// This is the primary entry point for custom providers defined in models.toml.\n    /// Supports both native providers (OpenAI, Ollama) and OpenAI-compatible APIs.\n    ///\n    /// # Arguments\n    ///\n    /// * `config` - Provider configuration from models.toml\n    ///\n    /// # Returns\n    ///\n    /// Returns a tuple of (LLMProvider, EmbeddingProvider).\n    ///\n    /// # Errors\n    ///\n    /// Returns error if:\n    /// - Required API key environment variable is not set\n    /// - Base URL is not configured for OpenAI-compatible providers\n    ///\n    /// # Examples\n    ///\n    /// ```rust,ignore\n    /// use edgequake_llm::{ModelsConfig, ProviderFactory};\n    ///\n    /// let config = ModelsConfig::load()?;\n    /// let provider_config = config.get_provider(\"zai\").unwrap();\n    /// let (llm, embedding) = ProviderFactory::from_config(provider_config)?;\n    /// ```\n    pub fn from_config(\n        config: &ProviderConfig,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        Self::from_config_with_model(config, None)\n    }\n\n    /// Create provider from configuration with a specific model override.\n    ///\n    /// This is used when the user selects a specific model via `/model` command\n    /// that differs from the provider's default model.\n    ///\n    /// # Arguments\n    ///\n    /// * `config` - Provider configuration from models.toml\n    /// * `model_name` - Optional model name to use instead of the default\n    ///\n    /// # Returns\n    ///\n    /// Tuple of (LLM provider, Embedding provider)\n    pub fn from_config_with_model(\n        config: &ProviderConfig,\n        model_name: Option<&str>,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        match config.provider_type {\n            ConfigProviderType::OpenAI => Self::create_openai(),\n            ConfigProviderType::Ollama => Self::create_ollama(),\n            ConfigProviderType::LMStudio => Self::create_lmstudio(),\n            ConfigProviderType::Mock => Ok(Self::create_mock()),\n            ConfigProviderType::OpenAICompatible => {\n                Self::create_openai_compatible_with_model(config, model_name)\n            }\n            ConfigProviderType::Azure => {\n                Err(LlmError::ConfigError(\n                    \"Azure OpenAI is not yet supported via from_config. \\\n                     Use AZURE_OPENAI_* environment variables instead.\"\n                        .to_string(),\n                ))\n            }\n            ConfigProviderType::Anthropic => {\n                Self::create_anthropic_from_config(config, model_name)\n            }\n            ConfigProviderType::OpenRouter => {\n                Self::create_openrouter_from_config(config, model_name)\n            }\n        }\n    }\n\n    /// Create OpenAI-compatible provider from TOML configuration.\n    ///\n    /// This creates a generic provider that can work with any OpenAI-compatible API:\n    /// - Z.ai (GLM models)\n    /// - DeepSeek\n    /// - Together AI\n    /// - Groq\n    /// - Any custom endpoint\n    #[allow(dead_code)]\n    fn create_openai_compatible(\n        config: &ProviderConfig,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        Self::create_openai_compatible_with_model(config, None)\n    }\n\n    /// Create OpenAI-compatible provider with optional model override.\n    fn create_openai_compatible_with_model(\n        config: &ProviderConfig,\n        model_name: Option<&str>,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let mut provider_instance = OpenAICompatibleProvider::from_config(config.clone())?;\n\n        // Apply model override if provided\n        if let Some(model) = model_name {\n            provider_instance = provider_instance.with_model(model);\n        }\n\n        let provider = Arc::new(provider_instance);\n\n        // For embedding, check if this provider has embedding models configured\n        let has_embedding = config\n            .default_embedding_model\n            .is_some();\n\n        if has_embedding {\n            // Use the same provider for both LLM and embedding\n            Ok((provider.clone(), provider))\n        } else {\n            // Fall back to OpenAI for embeddings if available\n            match Self::create_openai() {\n                Ok((_, embedding)) => Ok((provider, embedding)),\n                Err(_) => {\n                    // No OpenAI available, use this provider anyway\n                    // (embedding calls will fail but LLM will work)\n                    Ok((provider.clone(), provider))\n                }\n            }\n        }\n    }\n\n    /// Create OpenAI provider from environment.\n    ///\n    /// Reads `OPENAI_API_KEY` environment variable.\n    fn create_openai() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let api_key = std::env::var(\"OPENAI_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"OPENAI_API_KEY not set for OpenAI provider\".to_string())\n        })?;\n\n        if api_key.is_empty() || api_key == \"test-key\" {\n            return Err(LlmError::ConfigError(\n                \"OPENAI_API_KEY is empty or invalid\".to_string(),\n            ));\n        }\n\n        let provider = Arc::new(OpenAIProvider::new(api_key));\n        Ok((provider.clone(), provider))\n    }\n\n    /// Create Anthropic provider from environment.\n    ///\n    /// Reads `ANTHROPIC_API_KEY` environment variable.\n    /// Also reads `ANTHROPIC_BASE_URL` and `ANTHROPIC_MODEL` if set.\n    ///\n    /// Note: Anthropic does not provide embeddings API, so we fall back to\n    /// OpenAI for embeddings if available, otherwise use mock.\n    ///\n    /// # OODA-44: Fixed to use from_env() instead of new()\n    ///\n    /// WHY: The original implementation only read ANTHROPIC_API_KEY and ANTHROPIC_MODEL,\n    ///      but ignored ANTHROPIC_BASE_URL. This prevented using Ollama's Anthropic-\n    ///      compatible API. Now uses from_env() which properly handles all env vars:\n    ///\n    /// ```text\n    /// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    /// â”‚  Environment Variables â†’ AnthropicProvider                         â”‚\n    /// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    /// â”‚  ANTHROPIC_API_KEY    â†’ Required authentication key                â”‚\n    /// â”‚  ANTHROPIC_BASE_URL   â†’ Custom endpoint (e.g., Ollama localhost)   â”‚\n    /// â”‚  ANTHROPIC_MODEL      â†’ Model to use (default: claude-sonnet-4)    â”‚\n    /// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    /// ```\n    fn create_anthropic() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // OODA-44: Use from_env() which handles ANTHROPIC_BASE_URL for Ollama support\n        let provider = Arc::new(AnthropicProvider::from_env()?);\n\n        // Anthropic doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((provider, embedding))\n    }\n\n    /// Create Anthropic provider from TOML configuration.\n    ///\n    /// Reads API key from environment variable specified in config.\n    fn create_anthropic_from_config(\n        config: &ProviderConfig,\n        model_name: Option<&str>,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // Get API key from environment\n        let api_key_var = config.api_key_env.as_deref().unwrap_or(\"ANTHROPIC_API_KEY\");\n        let api_key = std::env::var(api_key_var).map_err(|_| {\n            LlmError::ConfigError(format!(\n                \"{} not set for Anthropic provider\",\n                api_key_var\n            ))\n        })?;\n\n        if api_key.is_empty() {\n            return Err(LlmError::ConfigError(format!(\n                \"{} is empty\",\n                api_key_var\n            )));\n        }\n\n        // Determine model\n        let model = model_name\n            .map(|s| s.to_string())\n            .or_else(|| config.default_llm_model.clone())\n            .unwrap_or_else(|| \"claude-sonnet-4-5-20250929\".to_string());\n\n        // Create provider with optional base URL\n        let mut provider = AnthropicProvider::new(api_key).with_model(model);\n\n        if let Some(base_url) = &config.base_url {\n            provider = provider.with_base_url(base_url);\n        }\n\n        let llm_provider = Arc::new(provider);\n\n        // Anthropic doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// OODA-73: Create Gemini provider from environment.\n    ///\n    /// Uses GeminiProvider::from_env() which reads:\n    /// - GEMINI_API_KEY or GOOGLE_API_KEY (required for Google AI)\n    /// - GOOGLE_APPLICATION_CREDENTIALS (for VertexAI)\n    /// - GOOGLE_CLOUD_PROJECT (for VertexAI)\n    /// - GOOGLE_CLOUD_REGION (optional, default: us-central1)\n    /// - GEMINI_MODEL (optional, default: gemini-2.5-flash)\n    fn create_gemini() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        use crate::GeminiProvider;\n\n        let provider = GeminiProvider::from_env()?;\n        let llm_provider: Arc<dyn LLMProvider> = Arc::new(provider);\n\n        // Gemini doesn't have native embeddings API, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// Create OpenRouter provider from environment.\n    ///\n    /// Uses OpenRouterProvider::from_env() which reads:\n    /// - OPENROUTER_API_KEY (required)\n    /// - OPENROUTER_MODEL (optional, default: anthropic/claude-3.5-sonnet)\n    /// - OPENROUTER_SITE_URL (optional, for dashboard tracking)\n    /// - OPENROUTER_SITE_NAME (optional, for dashboard tracking)\n    fn create_openrouter() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let api_key = std::env::var(\"OPENROUTER_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"OPENROUTER_API_KEY not set for OpenRouter provider\".to_string())\n        })?;\n\n        if api_key.is_empty() {\n            return Err(LlmError::ConfigError(\n                \"OPENROUTER_API_KEY is empty\".to_string(),\n            ));\n        }\n\n        let model = std::env::var(\"OPENROUTER_MODEL\")\n            .unwrap_or_else(|_| \"anthropic/claude-3.5-sonnet\".to_string());\n\n        let mut provider = OpenRouterProvider::new(api_key).with_model(model);\n\n        // Optional site URL and name\n        if let Ok(url) = std::env::var(\"OPENROUTER_SITE_URL\") {\n            provider = provider.with_site_url(url);\n        }\n        if let Ok(name) = std::env::var(\"OPENROUTER_SITE_NAME\") {\n            provider = provider.with_site_name(name);\n        }\n\n        let llm_provider = Arc::new(provider);\n\n        // OpenRouter doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// OODA-04: Create OpenRouter provider with specific model.\n    ///\n    /// Like `create_openrouter()` but uses the provided model instead of env/default.\n    ///\n    /// # Arguments\n    ///\n    /// * `model` - Model name (e.g., \"mistralai/ministral-14b-2512\")\n    fn create_openrouter_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let api_key = std::env::var(\"OPENROUTER_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"OPENROUTER_API_KEY not set for OpenRouter provider\".to_string())\n        })?;\n\n        if api_key.is_empty() {\n            return Err(LlmError::ConfigError(\n                \"OPENROUTER_API_KEY is empty\".to_string(),\n            ));\n        }\n\n        let mut provider = OpenRouterProvider::new(api_key).with_model(model);\n\n        // Optional site URL and name\n        if let Ok(url) = std::env::var(\"OPENROUTER_SITE_URL\") {\n            provider = provider.with_site_url(url);\n        }\n        if let Ok(name) = std::env::var(\"OPENROUTER_SITE_NAME\") {\n            provider = provider.with_site_name(name);\n        }\n\n        let llm_provider = Arc::new(provider);\n\n        // OpenRouter doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// OODA-04: Create OpenAI provider with specific model.\n    fn create_openai_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let api_key = std::env::var(\"OPENAI_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"OPENAI_API_KEY not set for OpenAI provider\".to_string())\n        })?;\n\n        if api_key.is_empty() || api_key == \"test-key\" {\n            return Err(LlmError::ConfigError(\n                \"OPENAI_API_KEY is empty or invalid\".to_string(),\n            ));\n        }\n\n        let provider = Arc::new(OpenAIProvider::new(api_key).with_model(model));\n        Ok((provider.clone(), provider))\n    }\n\n    /// OODA-04: Create Anthropic provider with specific model.\n    fn create_anthropic_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let api_key = std::env::var(\"ANTHROPIC_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"ANTHROPIC_API_KEY not set for Anthropic provider\".to_string())\n        })?;\n\n        let mut provider = AnthropicProvider::new(&api_key);\n        \n        // Support custom base URL for Ollama-style compatibility\n        if let Ok(base_url) = std::env::var(\"ANTHROPIC_BASE_URL\") {\n            provider = provider.with_base_url(&base_url);\n        }\n        provider = provider.with_model(model);\n\n        let llm_provider = Arc::new(provider);\n\n        // Anthropic doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// OODA-04: Create Gemini provider with specific model.\n    /// OODA-95: Supports vertexai: prefixed models for VertexAI endpoint.\n    fn create_gemini_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // OODA-95: Check if this is a VertexAI model (prefixed with \"vertexai:\")\n        if model.starts_with(\"vertexai:\") {\n            // Strip the \"vertexai:\" prefix to get the actual model name\n            let actual_model = model.strip_prefix(\"vertexai:\").unwrap_or(model);\n            \n            // Use VertexAI endpoint with gcloud auto-auth\n            let provider = Arc::new(\n                GeminiProvider::from_env_vertex_ai()?.with_model(actual_model)\n            );\n\n            // Gemini doesn't have native embeddings API, fall back to OpenAI or mock\n            let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n                Ok((_, embedding)) => embedding,\n                Err(_) => Arc::new(MockProvider::new()),\n            };\n\n            return Ok((provider, embedding));\n        }\n\n        // Regular GoogleAI endpoint (requires GEMINI_API_KEY or GOOGLE_API_KEY)\n        let api_key = std::env::var(\"GEMINI_API_KEY\")\n            .or_else(|_| std::env::var(\"GOOGLE_API_KEY\"))\n            .map_err(|_| {\n                LlmError::ConfigError(\n                    \"GEMINI_API_KEY or GOOGLE_API_KEY not set for Gemini provider\".to_string(),\n                )\n            })?;\n\n        let provider = Arc::new(GeminiProvider::new(&api_key).with_model(model));\n\n        // Gemini doesn't have native embeddings API, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((provider, embedding))\n    }\n\n    /// OODA-04: Create xAI provider with specific model.\n    fn create_xai_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // Verify XAI_API_KEY is set before proceeding\n        std::env::var(\"XAI_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"XAI_API_KEY not set for xAI provider\".to_string())\n        })?;\n\n        // Use XAI_MODEL to set model, then call from_env\n        std::env::set_var(\"XAI_MODEL\", model);\n        let provider = XAIProvider::from_env()?;\n\n        let llm_provider = Arc::new(provider);\n\n        // xAI doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// OODA-04: Create Ollama provider with specific model.\n    fn create_ollama_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // Use OLLAMA_MODEL to set model, then call from_env\n        std::env::set_var(\"OLLAMA_MODEL\", model);\n        let provider = Arc::new(OllamaProvider::from_env()?);\n        Ok((provider.clone(), provider))\n    }\n\n    /// OODA-04: Create LM Studio provider with specific model.\n    fn create_lmstudio_with_model(model: &str) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // Use LMSTUDIO_MODEL to set model, then call from_env\n        std::env::set_var(\"LMSTUDIO_MODEL\", model);\n        let provider = Arc::new(LMStudioProvider::from_env()?);\n        Ok((provider.clone(), provider))\n    }\n\n    /// Create OpenRouter provider from TOML configuration.\n    ///\n    /// Reads API key from environment variable specified in config.\n    fn create_openrouter_from_config(\n        config: &ProviderConfig,\n        model_name: Option<&str>,\n    ) -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        // Get API key from environment\n        let api_key_var = config.api_key_env.as_deref().unwrap_or(\"OPENROUTER_API_KEY\");\n        let api_key = std::env::var(api_key_var).map_err(|_| {\n            LlmError::ConfigError(format!(\n                \"{} not set for OpenRouter provider\",\n                api_key_var\n            ))\n        })?;\n\n        if api_key.is_empty() {\n            return Err(LlmError::ConfigError(format!(\n                \"{} is empty\",\n                api_key_var\n            )));\n        }\n\n        // Determine model\n        let model = model_name\n            .map(|s| s.to_string())\n            .or_else(|| config.default_llm_model.clone())\n            .unwrap_or_else(|| \"anthropic/claude-3.5-sonnet\".to_string());\n\n        // Create provider with optional base URL\n        let mut provider = OpenRouterProvider::new(api_key).with_model(model);\n\n        if let Some(base_url) = &config.base_url {\n            provider = provider.with_base_url(base_url);\n        }\n\n        let llm_provider = Arc::new(provider);\n\n        // OpenRouter doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((llm_provider, embedding))\n    }\n\n    /// Create Ollama provider from environment.\n    ///\n    /// Uses OllamaProvider::from_env() which reads:\n    /// - OLLAMA_HOST\n    /// - OLLAMA_MODEL\n    /// - OLLAMA_EMBEDDING_MODEL\n    fn create_ollama() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let provider = Arc::new(OllamaProvider::from_env()?);\n        Ok((provider.clone(), provider))\n    }\n\n    /// Create LM Studio provider from environment.\n    ///\n    /// Uses the dedicated LMStudioProvider which reads:\n    /// - `LMSTUDIO_HOST`: LM Studio server URL (default: http://localhost:1234)\n    /// - `LMSTUDIO_MODEL`: Chat model name (default: gemma2-9b-it)\n    /// - `LMSTUDIO_EMBEDDING_MODEL`: Embedding model (default: nomic-embed-text-v1.5)\n    /// - `LMSTUDIO_EMBEDDING_DIM`: Embedding dimension (default: 768)\n    fn create_lmstudio() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let provider = Arc::new(LMStudioProvider::from_env()?);\n        Ok((provider.clone(), provider))\n    }\n\n    /// OODA-71: Create xAI provider from environment.\n    ///\n    /// Reads `XAI_API_KEY` environment variable for authentication.\n    /// Optional `XAI_MODEL` for model selection (default: grok-4).\n    /// Optional `XAI_BASE_URL` for custom endpoint.\n    ///\n    /// xAI doesn't provide embeddings API, so we fall back to OpenAI or mock.\n    fn create_xai() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let provider = Arc::new(XAIProvider::from_env()?);\n\n        // xAI doesn't provide embeddings, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((provider, embedding))\n    }\n\n    /// OODA-80: Create HuggingFace Hub provider from environment.\n    ///\n    /// Reads `HF_TOKEN` or `HUGGINGFACE_TOKEN` environment variable for authentication.\n    /// Optional `HF_MODEL` for model selection (default: meta-llama/Meta-Llama-3.1-70B-Instruct).\n    /// Optional `HF_BASE_URL` for custom endpoint.\n    ///\n    /// HuggingFace requires a separate endpoint for embeddings (not yet supported),\n    /// so we fall back to OpenAI or mock.\n    fn create_huggingface() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let provider = Arc::new(HuggingFaceProvider::from_env()?);\n\n        // HuggingFace embeddings require a different endpoint, fall back to OpenAI or mock\n        let embedding: Arc<dyn EmbeddingProvider> = match Self::create_openai() {\n            Ok((_, embedding)) => embedding,\n            Err(_) => Arc::new(MockProvider::new()),\n        };\n\n        Ok((provider, embedding))\n    }\n\n    /// Create VSCode Copilot provider from environment.\n    ///\n    /// Uses the VsCodeCopilotProvider which reads:\n    /// - `VSCODE_COPILOT_DIRECT`: Enable direct API mode (default: true)\n    /// - `VSCODE_COPILOT_PROXY_URL`: Proxy URL if not using direct mode\n    /// - `VSCODE_COPILOT_MODEL`: Model name (default: gpt-4o-mini)\n    /// - `VSCODE_COPILOT_ACCOUNT_TYPE`: Account type (individual/business/enterprise)\n    /// - `VSCODE_COPILOT_EMBEDDING_MODEL`: Embedding model (default: text-embedding-3-small)\n    ///\n    /// Note: Direct mode is now the default. No external proxy required.\n    /// For legacy proxy mode, set `VSCODE_COPILOT_DIRECT=false`.\n    ///\n    /// VsCodeCopilotProvider now implements both LLMProvider and EmbeddingProvider,\n    /// using the Copilot API's /embeddings endpoint for text embeddings.\n    fn create_vscode_copilot() -> Result<(Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>)> {\n        let model =\n            std::env::var(\"VSCODE_COPILOT_MODEL\").unwrap_or_else(|_| \"gpt-4o-mini\".to_string());\n\n        // Builder reads VSCODE_COPILOT_DIRECT, VSCODE_COPILOT_ACCOUNT_TYPE,\n        // and VSCODE_COPILOT_EMBEDDING_MODEL automatically\n        let builder = VsCodeCopilotProvider::new().model(model);\n\n        let provider = Arc::new(builder.build().map_err(|e| {\n            LlmError::ConfigError(format!(\"Failed to create VSCode Copilot provider: {}\", e))\n        })?);\n\n        // VsCodeCopilotProvider implements both LLMProvider and EmbeddingProvider\n        // Use same provider instance for both (shares HTTP client and token manager)\n        Ok((provider.clone(), provider))\n    }\n\n    /// Create mock provider for testing.\n    ///\n    /// Always returns deterministic responses.\n    fn create_mock() -> (Arc<dyn LLMProvider>, Arc<dyn EmbeddingProvider>) {\n        let provider = Arc::new(MockProvider::new());\n        (provider.clone(), provider)\n    }\n\n    /// Get embedding dimension for current provider configuration.\n    ///\n    /// Useful for configuring vector storage with the correct dimension.\n    ///\n    /// # Examples\n    ///\n    /// ```ignore\n    /// std::env::set_var(\"EDGEQUAKE_LLM_PROVIDER\", \"ollama\");\n    /// let dim = ProviderFactory::embedding_dimension()?;\n    /// assert_eq!(dim, 768); // embeddinggemma dimension\n    /// ```\n    pub fn embedding_dimension() -> Result<usize> {\n        let (_, embedding_provider) = Self::from_env()?;\n        Ok(embedding_provider.dimension())\n    }\n\n    /// Create an embedding provider from workspace configuration.\n    ///\n    /// This is used to create workspace-specific embedding providers for query execution.\n    /// The provider is configured with the workspace's embedding model and dimension.\n    ///\n    /// @implements SPEC-032: Workspace-specific embedding in query process\n    ///\n    /// # Arguments\n    ///\n    /// * `provider_name` - Provider type (e.g., \"openai\", \"ollama\", \"lmstudio\", \"vscode-copilot\", \"mock\")\n    /// * `model` - Embedding model name (e.g., \"text-embedding-3-small\", \"embeddinggemma:latest\")\n    /// * `dimension` - Embedding dimension (e.g., 1536, 768)\n    ///\n    /// # Returns\n    ///\n    /// Returns an `Arc<dyn EmbeddingProvider>` configured for the workspace.\n    ///\n    /// # Errors\n    ///\n    /// Returns error if the provider type is unknown or required configuration is missing.\n    ///\n    /// # Examples\n    ///\n    /// ```ignore\n    /// let provider = ProviderFactory::create_embedding_provider(\n    ///     \"ollama\",\n    ///     \"embeddinggemma:latest\",\n    ///     768,\n    /// )?;\n    /// assert_eq!(provider.dimension(), 768);\n    /// ```\n    pub fn create_embedding_provider(\n        provider_name: &str,\n        model: &str,\n        _dimension: usize,\n    ) -> Result<Arc<dyn EmbeddingProvider>> {\n        let provider_type = ProviderType::from_str(provider_name).ok_or_else(|| {\n            LlmError::ConfigError(format!(\n                \"Unknown embedding provider: {}. Valid: openai, ollama, lmstudio, vscode-copilot, mock\",\n                provider_name\n            ))\n        })?;\n\n        match provider_type {\n            ProviderType::OpenAI => {\n                let api_key = std::env::var(\"OPENAI_API_KEY\").map_err(|_| {\n                    LlmError::ConfigError(\n                        \"OPENAI_API_KEY required for OpenAI embedding provider\".to_string(),\n                    )\n                })?;\n                // OpenAI provider with specific embedding model\n                // Note: OpenAI dimension is auto-detected from model name\n                let provider = OpenAIProvider::new(api_key).with_embedding_model(model);\n                Ok(Arc::new(provider))\n            }\n            ProviderType::Anthropic => {\n                // Anthropic doesn't have embeddings, fall back to mock\n                warn!(\"Anthropic doesn't support embeddings, using mock provider\");\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::OpenRouter => {\n                // OpenRouter doesn't have embeddings, fall back to mock\n                warn!(\"OpenRouter doesn't support embeddings, using mock provider\");\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::XAI => {\n                // xAI doesn't have embeddings API, fall back to mock\n                warn!(\"xAI doesn't support embeddings, using mock provider\");\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::HuggingFace => {\n                // HuggingFace requires separate endpoint for embeddings, fall back to mock\n                warn!(\"HuggingFace LLM provider doesn't support embeddings, using mock provider\");\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::Gemini => {\n                // OODA-73: Gemini doesn't have embeddings API, fall back to mock\n                warn!(\"Gemini doesn't support embeddings, using mock provider\");\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::Ollama => {\n                // Ollama provider with specific embedding model\n                let host = std::env::var(\"OLLAMA_HOST\")\n                    .unwrap_or_else(|_| \"http://localhost:11434\".to_string());\n                let provider = OllamaProvider::builder()\n                    .host(&host)\n                    .embedding_model(model)\n                    .build()?;\n                Ok(Arc::new(provider))\n            }\n            ProviderType::LMStudio => {\n                // LM Studio provider with specific embedding model\n                let host = std::env::var(\"LMSTUDIO_HOST\")\n                    .unwrap_or_else(|_| \"http://localhost:1234\".to_string());\n                let provider = LMStudioProvider::builder()\n                    .host(&host)\n                    .embedding_model(model)\n                    .build()?;\n                Ok(Arc::new(provider))\n            }\n            ProviderType::Mock => {\n                // Mock provider ignores model/dimension and uses defaults\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::VsCodeCopilot => {\n                // VsCodeCopilot supports embeddings via Copilot API /embeddings endpoint\n                // Uses text-embedding-3-small by default (1536 dimensions)\n                let provider = VsCodeCopilotProvider::new()\n                    .embedding_model(model)\n                    .build()\n                    .map_err(|e| LlmError::ApiError(e.to_string()))?;\n                Ok(Arc::new(provider))\n            }\n        }\n    }\n\n    /// Create an LLM provider from workspace configuration.\n    ///\n    /// This is used to create workspace-specific LLM providers for ingestion/extraction.\n    /// The provider is configured with the workspace's LLM model.\n    ///\n    /// @implements SPEC-032: Workspace-specific LLM in ingestion process\n    ///\n    /// # Arguments\n    ///\n    /// * `provider_name` - Provider type (e.g., \"openai\", \"ollama\", \"lmstudio\", \"mock\")\n    /// * `model` - LLM model name (e.g., \"gpt-4o-mini\", \"gemma3:12b\")\n    ///\n    /// # Returns\n    ///\n    /// Returns an `Arc<dyn LLMProvider>` configured for the workspace.\n    ///\n    /// # Errors\n    ///\n    /// Returns error if the provider type is unknown or required configuration is missing.\n    ///\n    /// # Examples\n    ///\n    /// ```ignore\n    /// let provider = ProviderFactory::create_llm_provider(\n    ///     \"ollama\",\n    ///     \"gemma3:12b\",\n    /// )?;\n    /// assert_eq!(provider.model(), \"gemma3:12b\");\n    /// ```\n    pub fn create_llm_provider(provider_name: &str, model: &str) -> Result<Arc<dyn LLMProvider>> {\n        let provider_type = ProviderType::from_str(provider_name).ok_or_else(|| {\n            LlmError::ConfigError(format!(\n                \"Unknown LLM provider: {}. Valid: openai, ollama, lmstudio, mock\",\n                provider_name\n            ))\n        })?;\n\n        match provider_type {\n            ProviderType::OpenAI => {\n                let api_key = std::env::var(\"OPENAI_API_KEY\").map_err(|_| {\n                    LlmError::ConfigError(\n                        \"OPENAI_API_KEY required for OpenAI LLM provider\".to_string(),\n                    )\n                })?;\n                // OpenAI provider with specific model\n                let provider = OpenAIProvider::new(api_key).with_model(model);\n                Ok(Arc::new(provider))\n            }\n            ProviderType::Anthropic => {\n                let api_key = std::env::var(\"ANTHROPIC_API_KEY\").map_err(|_| {\n                    LlmError::ConfigError(\n                        \"ANTHROPIC_API_KEY required for Anthropic LLM provider\".to_string(),\n                    )\n                })?;\n                // Anthropic provider with specific model\n                let provider = AnthropicProvider::new(api_key).with_model(model);\n                Ok(Arc::new(provider))\n            }\n            ProviderType::OpenRouter => {\n                let api_key = std::env::var(\"OPENROUTER_API_KEY\").map_err(|_| {\n                    LlmError::ConfigError(\n                        \"OPENROUTER_API_KEY required for OpenRouter LLM provider\".to_string(),\n                    )\n                })?;\n                // OpenRouter provider with specific model\n                let provider = OpenRouterProvider::new(api_key).with_model(model);\n                Ok(Arc::new(provider))\n            }\n            ProviderType::XAI => {\n                // OODA-71: xAI Grok provider with specific model\n                let provider = XAIProvider::from_env()?.with_model(model);\n                Ok(Arc::new(provider))\n            }\n            ProviderType::HuggingFace => {\n                // OODA-80: HuggingFace Hub provider with specific model\n                let provider = HuggingFaceProvider::from_env()?.with_model(model);\n                Ok(Arc::new(provider))\n            }\n            ProviderType::Gemini => {\n                // OODA-73/95: Gemini provider with specific model\n                // Handle vertexai: prefix for VertexAI endpoint\n                if model.starts_with(\"vertexai:\") {\n                    let actual_model = model.strip_prefix(\"vertexai:\").unwrap_or(model);\n                    let provider = GeminiProvider::from_env_vertex_ai()?.with_model(actual_model);\n                    Ok(Arc::new(provider))\n                } else {\n                    let provider = GeminiProvider::from_env()?.with_model(model);\n                    Ok(Arc::new(provider))\n                }\n            }\n            ProviderType::Ollama => {\n                // Ollama provider with specific model\n                let host = std::env::var(\"OLLAMA_HOST\")\n                    .unwrap_or_else(|_| \"http://localhost:11434\".to_string());\n                let provider = OllamaProvider::builder().host(&host).model(model).build()?;\n                Ok(Arc::new(provider))\n            }\n            ProviderType::LMStudio => {\n                // LM Studio provider with specific model\n                let host = std::env::var(\"LMSTUDIO_HOST\")\n                    .unwrap_or_else(|_| \"http://localhost:1234\".to_string());\n                let provider = LMStudioProvider::builder()\n                    .host(&host)\n                    .model(model)\n                    .build()?;\n                Ok(Arc::new(provider))\n            }\n            ProviderType::Mock => {\n                // Mock provider ignores model and uses defaults\n                Ok(Arc::new(MockProvider::new()))\n            }\n            ProviderType::VsCodeCopilot => {\n                // VsCodeCopilot provider with specific model\n                let proxy_url = std::env::var(\"VSCODE_COPILOT_PROXY_URL\")\n                    .unwrap_or_else(|_| \"http://localhost:4141\".to_string());\n                let provider = VsCodeCopilotProvider::new()\n                    .proxy_url(&proxy_url)\n                    .model(model)\n                    .build()?;\n                Ok(Arc::new(provider))\n            }\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use serial_test::serial;\n\n    #[test]\n    fn test_provider_type_parsing() {\n        assert_eq!(ProviderType::from_str(\"openai\"), Some(ProviderType::OpenAI));\n        assert_eq!(ProviderType::from_str(\"OLLAMA\"), Some(ProviderType::Ollama));\n        assert_eq!(\n            ProviderType::from_str(\"lmstudio\"),\n            Some(ProviderType::LMStudio)\n        );\n        assert_eq!(\n            ProviderType::from_str(\"lm-studio\"),\n            Some(ProviderType::LMStudio)\n        );\n        assert_eq!(\n            ProviderType::from_str(\"lm_studio\"),\n            Some(ProviderType::LMStudio)\n        );\n        assert_eq!(ProviderType::from_str(\"mock\"), Some(ProviderType::Mock));\n        \n        // OODA-73: Test Gemini parsing\n        assert_eq!(ProviderType::from_str(\"gemini\"), Some(ProviderType::Gemini));\n        assert_eq!(ProviderType::from_str(\"google\"), Some(ProviderType::Gemini));\n        assert_eq!(ProviderType::from_str(\"vertex\"), Some(ProviderType::Gemini));\n        assert_eq!(ProviderType::from_str(\"vertexai\"), Some(ProviderType::Gemini));\n        \n        // Test OpenRouter parsing\n        assert_eq!(ProviderType::from_str(\"openrouter\"), Some(ProviderType::OpenRouter));\n        \n        // OODA-71: Test xAI parsing\n        assert_eq!(ProviderType::from_str(\"xai\"), Some(ProviderType::XAI));\n        assert_eq!(ProviderType::from_str(\"grok\"), Some(ProviderType::XAI));\n        \n        // OODA-80: Test HuggingFace parsing\n        assert_eq!(ProviderType::from_str(\"huggingface\"), Some(ProviderType::HuggingFace));\n        assert_eq!(ProviderType::from_str(\"hf\"), Some(ProviderType::HuggingFace));\n        assert_eq!(ProviderType::from_str(\"hugging-face\"), Some(ProviderType::HuggingFace));\n        \n        assert_eq!(ProviderType::from_str(\"invalid\"), None);\n        assert_eq!(ProviderType::from_str(\"\"), None);\n    }\n\n    #[test]\n    fn test_mock_creation() {\n        let (llm, embedding) = ProviderFactory::create_mock();\n        assert_eq!(llm.name(), \"mock\");\n        assert_eq!(embedding.name(), \"mock\");\n        assert_eq!(embedding.dimension(), 1536);\n    }\n\n    #[test]\n    fn test_explicit_mock_creation() {\n        let (llm, embedding) = ProviderFactory::create(ProviderType::Mock).unwrap();\n        assert_eq!(llm.name(), \"mock\");\n        assert_eq!(embedding.dimension(), 1536);\n    }\n\n    #[test]\n    #[serial]\n    fn test_from_env_fallback_to_mock() {\n        // Clear all provider environment variables\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"XAI_API_KEY\");\n        std::env::remove_var(\"GOOGLE_API_KEY\");\n        std::env::remove_var(\"GEMINI_API_KEY\");\n        std::env::remove_var(\"OPENROUTER_API_KEY\");\n        std::env::remove_var(\"ANTHROPIC_API_KEY\");\n        std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n        std::env::remove_var(\"HUGGINGFACE_API_KEY\");  // OODA-04: Added missing env var\n        std::env::remove_var(\"HF_TOKEN\");             // OODA-04: Primary HuggingFace token\n        std::env::remove_var(\"HUGGINGFACE_TOKEN\");    // OODA-04: Alternative HuggingFace token\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OLLAMA_MODEL\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n        std::env::remove_var(\"LMSTUDIO_MODEL\");\n\n        let (llm, _) = ProviderFactory::from_env().unwrap();\n        assert_eq!(llm.name(), \"mock\");\n    }\n\n    #[test]\n    #[serial]\n    fn test_explicit_provider_env() {\n        // Clean up first to avoid interference from other tests\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"XAI_API_KEY\");\n        std::env::remove_var(\"GOOGLE_API_KEY\");\n        std::env::remove_var(\"GEMINI_API_KEY\");\n        std::env::remove_var(\"OPENROUTER_API_KEY\");\n        std::env::remove_var(\"ANTHROPIC_API_KEY\");\n        std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n\n        std::env::set_var(\"EDGEQUAKE_LLM_PROVIDER\", \"mock\");\n        let (llm, _) = ProviderFactory::from_env().unwrap();\n        assert_eq!(llm.name(), \"mock\");\n\n        // Clean up after\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    }\n\n    #[test]\n    #[serial]\n    fn test_lmstudio_auto_detection() {\n        // Clean up first to avoid interference from other tests\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OLLAMA_MODEL\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"XAI_API_KEY\");\n        std::env::remove_var(\"GOOGLE_API_KEY\");\n        std::env::remove_var(\"GEMINI_API_KEY\");\n        std::env::remove_var(\"OPENROUTER_API_KEY\");\n        std::env::remove_var(\"ANTHROPIC_API_KEY\");\n        std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n\n        // Set LM Studio environment\n        std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n        let (llm, embedding) = ProviderFactory::from_env().unwrap();\n        assert_eq!(llm.name(), \"lmstudio\");\n        assert_eq!(embedding.name(), \"lmstudio\");\n\n        // Clean up after\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n    }\n\n    #[test]\n    #[serial]\n    fn test_lmstudio_model_detection() {\n        // Clean up first to avoid interference from other tests\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OLLAMA_MODEL\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"XAI_API_KEY\");\n        std::env::remove_var(\"GOOGLE_API_KEY\");\n        std::env::remove_var(\"GEMINI_API_KEY\");\n        std::env::remove_var(\"OPENROUTER_API_KEY\");\n        std::env::remove_var(\"ANTHROPIC_API_KEY\");\n        std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n\n        // Set LM Studio model only\n        std::env::set_var(\"LMSTUDIO_MODEL\", \"mistral-7b\");\n        let (llm, _) = ProviderFactory::from_env().unwrap();\n        assert_eq!(llm.name(), \"lmstudio\");\n\n        // Clean up after\n        std::env::remove_var(\"LMSTUDIO_MODEL\");\n    }\n\n    #[test]\n    fn test_explicit_lmstudio_creation() {\n        let (llm, embedding) = ProviderFactory::create(ProviderType::LMStudio).unwrap();\n        assert_eq!(llm.name(), \"lmstudio\");\n        assert_eq!(embedding.name(), \"lmstudio\");\n        // Default LM Studio embedding dimension is 768 (nomic-embed-text-v1.5)\n        assert_eq!(embedding.dimension(), 768);\n    }\n\n    #[test]\n    #[serial]\n    fn test_invalid_provider_env() {\n        // Clean up first to avoid interference from other tests\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"XAI_API_KEY\");\n        std::env::remove_var(\"GOOGLE_API_KEY\");\n        std::env::remove_var(\"GEMINI_API_KEY\");\n        std::env::remove_var(\"OPENROUTER_API_KEY\");\n        std::env::remove_var(\"ANTHROPIC_API_KEY\");\n        std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n\n        std::env::set_var(\"EDGEQUAKE_LLM_PROVIDER\", \"invalid_provider\");\n        let result = ProviderFactory::from_env();\n        assert!(result.is_err());\n        if let Err(e) = result {\n            assert!(e.to_string().contains(\"Unknown provider type\"));\n        }\n\n        // Clean up after\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    }\n\n    #[test]\n    #[serial]\n    fn test_openai_creation_requires_api_key() {\n        // Clean up first to avoid interference from other tests\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n\n        let result = ProviderFactory::create(ProviderType::OpenAI);\n        assert!(result.is_err());\n        if let Err(e) = result {\n            assert!(e.to_string().contains(\"OPENAI_API_KEY not set\"));\n        }\n    }\n\n    #[test]\n    #[serial]\n    fn test_embedding_dimension_detection() {\n        // Clean up first to avoid interference from other tests\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n\n        std::env::set_var(\"EDGEQUAKE_LLM_PROVIDER\", \"mock\");\n        let dim = ProviderFactory::embedding_dimension().unwrap();\n        assert_eq!(dim, 1536);\n\n        // Clean up after\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    }\n\n    #[test]\n    #[serial]\n    fn test_provider_priority_ollama_over_lmstudio() {\n        // Clean up first\n        std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n        std::env::remove_var(\"OPENAI_API_KEY\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n        std::env::remove_var(\"LMSTUDIO_MODEL\");\n\n        // Set both Ollama and LM Studio - Ollama should win\n        std::env::set_var(\"OLLAMA_HOST\", \"http://localhost:11434\");\n        std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n\n        let (llm, _) = ProviderFactory::from_env().unwrap();\n        assert_eq!(llm.name(), \"ollama\");\n\n        // Clean up after\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n    }\n\n    #[test]\n    fn test_create_with_model_mock_none() {\n        // create_with_model with Mock and None model â†’ delegates to create\n        let (llm, emb) = ProviderFactory::create_with_model(ProviderType::Mock, None).unwrap();\n        assert_eq!(llm.name(), \"mock\");\n        assert_eq!(emb.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_create_with_model_mock_some() {\n        // create_with_model with Mock and Some model â†’ still creates mock\n        let (llm, _) =\n            ProviderFactory::create_with_model(ProviderType::Mock, Some(\"any-model\")).unwrap();\n        assert_eq!(llm.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_create_embedding_provider_mock() {\n        let provider =\n            ProviderFactory::create_embedding_provider(\"mock\", \"mock-model\", 1536).unwrap();\n        assert_eq!(provider.name(), \"mock\");\n        assert_eq!(provider.dimension(), 1536);\n    }\n\n    #[test]\n    fn test_create_embedding_provider_unknown() {\n        let result = ProviderFactory::create_embedding_provider(\"unknown\", \"model\", 1536);\n        match result {\n            Err(e) => assert!(e.to_string().contains(\"Unknown embedding provider\")),\n            Ok(_) => panic!(\"Expected error for unknown provider\"),\n        }\n    }\n\n    #[test]\n    fn test_create_llm_provider_mock() {\n        let provider = ProviderFactory::create_llm_provider(\"mock\", \"mock-model\").unwrap();\n        assert_eq!(provider.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_create_llm_provider_unknown() {\n        let result = ProviderFactory::create_llm_provider(\"unknown\", \"model\");\n        match result {\n            Err(e) => assert!(e.to_string().contains(\"Unknown LLM provider\")),\n            Ok(_) => panic!(\"Expected error for unknown provider\"),\n        }\n    }\n\n    #[test]\n    fn test_provider_type_debug() {\n        // Verify Debug trait is derived\n        let pt = ProviderType::Mock;\n        let debug = format!(\"{:?}\", pt);\n        assert_eq!(debug, \"Mock\");\n    }\n\n    #[test]\n    fn test_provider_type_clone_eq() {\n        let pt1 = ProviderType::OpenAI;\n        let pt2 = pt1;\n        assert_eq!(pt1, pt2);\n        assert_ne!(pt1, ProviderType::Ollama);\n    }\n\n    #[test]\n    fn test_from_config_azure_error() {\n        use crate::model_config::{ProviderConfig, ProviderType as ConfigProviderType};\n        let config = ProviderConfig {\n            provider_type: ConfigProviderType::Azure,\n            ..ProviderConfig::default()\n        };\n        let result = ProviderFactory::from_config(&config);\n        match result {\n            Err(e) => assert!(e.to_string().contains(\"Azure OpenAI\")),\n            Ok(_) => panic!(\"Expected error for Azure config\"),\n        }\n    }\n\n    #[test]\n    fn test_from_config_mock() {\n        use crate::model_config::{ProviderConfig, ProviderType as ConfigProviderType};\n        let config = ProviderConfig {\n            provider_type: ConfigProviderType::Mock,\n            ..ProviderConfig::default()\n        };\n        let (llm, emb) = ProviderFactory::from_config(&config).unwrap();\n        assert_eq!(llm.name(), \"mock\");\n        assert_eq!(emb.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_vscode_copilot_parsing() {\n        assert_eq!(\n            ProviderType::from_str(\"vscode\"),\n            Some(ProviderType::VsCodeCopilot)\n        );\n        assert_eq!(\n            ProviderType::from_str(\"copilot\"),\n            Some(ProviderType::VsCodeCopilot)\n        );\n        assert_eq!(\n            ProviderType::from_str(\"vscode-copilot\"),\n            Some(ProviderType::VsCodeCopilot)\n        );\n    }\n}\n","traces":[{"line":93,"address":[],"length":0,"stats":{"Line":31}},{"line":94,"address":[],"length":0,"stats":{"Line":31}},{"line":95,"address":[],"length":0,"stats":{"Line":32}},{"line":96,"address":[],"length":0,"stats":{"Line":60}},{"line":97,"address":[],"length":0,"stats":{"Line":118}},{"line":98,"address":[],"length":0,"stats":{"Line":52}},{"line":99,"address":[],"length":0,"stats":{"Line":51}},{"line":100,"address":[],"length":0,"stats":{"Line":89}},{"line":101,"address":[],"length":0,"stats":{"Line":21}},{"line":102,"address":[],"length":0,"stats":{"Line":57}},{"line":103,"address":[],"length":0,"stats":{"Line":49}},{"line":104,"address":[],"length":0,"stats":{"Line":17}},{"line":105,"address":[],"length":0,"stats":{"Line":5}},{"line":139,"address":[],"length":0,"stats":{"Line":19}},{"line":141,"address":[],"length":0,"stats":{"Line":23}},{"line":142,"address":[],"length":0,"stats":{"Line":7}},{"line":143,"address":[],"length":0,"stats":{"Line":6}},{"line":145,"address":[],"length":0,"stats":{"Line":1}},{"line":146,"address":[],"length":0,"stats":{"Line":1}},{"line":147,"address":[],"length":0,"stats":{"Line":1}},{"line":153,"address":[],"length":0,"stats":{"Line":52}},{"line":154,"address":[],"length":0,"stats":{"Line":8}},{"line":158,"address":[],"length":0,"stats":{"Line":36}},{"line":159,"address":[],"length":0,"stats":{"Line":10}},{"line":163,"address":[],"length":0,"stats":{"Line":6}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":6}},{"line":171,"address":[],"length":0,"stats":{"Line":12}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":179,"address":[],"length":0,"stats":{"Line":6}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":7}},{"line":187,"address":[],"length":0,"stats":{"Line":11}},{"line":189,"address":[],"length":0,"stats":{"Line":1}},{"line":190,"address":[],"length":0,"stats":{"Line":2}},{"line":195,"address":[],"length":0,"stats":{"Line":5}},{"line":196,"address":[],"length":0,"stats":{"Line":0}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":7}},{"line":202,"address":[],"length":0,"stats":{"Line":4}},{"line":203,"address":[],"length":0,"stats":{"Line":4}},{"line":208,"address":[],"length":0,"stats":{"Line":3}},{"line":224,"address":[],"length":0,"stats":{"Line":60}},{"line":227,"address":[],"length":0,"stats":{"Line":60}},{"line":228,"address":[],"length":0,"stats":{"Line":13}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":1}},{"line":234,"address":[],"length":0,"stats":{"Line":14}},{"line":235,"address":[],"length":0,"stats":{"Line":16}},{"line":236,"address":[],"length":0,"stats":{"Line":10}},{"line":237,"address":[],"length":0,"stats":{"Line":6}},{"line":258,"address":[],"length":0,"stats":{"Line":2}},{"line":262,"address":[],"length":0,"stats":{"Line":2}},{"line":263,"address":[],"length":0,"stats":{"Line":2}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":273,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":1}},{"line":276,"address":[],"length":0,"stats":{"Line":2}},{"line":308,"address":[],"length":0,"stats":{"Line":2}},{"line":311,"address":[],"length":0,"stats":{"Line":6}},{"line":327,"address":[],"length":0,"stats":{"Line":2}},{"line":331,"address":[],"length":0,"stats":{"Line":2}},{"line":332,"address":[],"length":0,"stats":{"Line":0}},{"line":333,"address":[],"length":0,"stats":{"Line":0}},{"line":334,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":1}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":340,"address":[],"length":0,"stats":{"Line":1}},{"line":341,"address":[],"length":0,"stats":{"Line":1}},{"line":342,"address":[],"length":0,"stats":{"Line":1}},{"line":343,"address":[],"length":0,"stats":{"Line":1}},{"line":347,"address":[],"length":0,"stats":{"Line":0}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":378,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":0}},{"line":385,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[],"length":0,"stats":{"Line":0}},{"line":389,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":399,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":14}},{"line":409,"address":[],"length":0,"stats":{"Line":42}},{"line":410,"address":[],"length":0,"stats":{"Line":12}},{"line":413,"address":[],"length":0,"stats":{"Line":6}},{"line":414,"address":[],"length":0,"stats":{"Line":0}},{"line":415,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":8}},{"line":420,"address":[],"length":0,"stats":{"Line":2}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":451,"address":[],"length":0,"stats":{"Line":0}},{"line":452,"address":[],"length":0,"stats":{"Line":0}},{"line":453,"address":[],"length":0,"stats":{"Line":0}},{"line":456,"address":[],"length":0,"stats":{"Line":0}},{"line":462,"address":[],"length":0,"stats":{"Line":0}},{"line":467,"address":[],"length":0,"stats":{"Line":0}},{"line":468,"address":[],"length":0,"stats":{"Line":0}},{"line":469,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":471,"address":[],"length":0,"stats":{"Line":0}},{"line":475,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":483,"address":[],"length":0,"stats":{"Line":0}},{"line":484,"address":[],"length":0,"stats":{"Line":0}},{"line":485,"address":[],"length":0,"stats":{"Line":0}},{"line":486,"address":[],"length":0,"stats":{"Line":0}},{"line":489,"address":[],"length":0,"stats":{"Line":0}},{"line":491,"address":[],"length":0,"stats":{"Line":0}},{"line":492,"address":[],"length":0,"stats":{"Line":0}},{"line":495,"address":[],"length":0,"stats":{"Line":0}},{"line":498,"address":[],"length":0,"stats":{"Line":0}},{"line":499,"address":[],"length":0,"stats":{"Line":0}},{"line":500,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":514,"address":[],"length":0,"stats":{"Line":0}},{"line":517,"address":[],"length":0,"stats":{"Line":0}},{"line":518,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":522,"address":[],"length":0,"stats":{"Line":0}},{"line":523,"address":[],"length":0,"stats":{"Line":0}},{"line":526,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":537,"address":[],"length":0,"stats":{"Line":0}},{"line":538,"address":[],"length":0,"stats":{"Line":0}},{"line":541,"address":[],"length":0,"stats":{"Line":0}},{"line":542,"address":[],"length":0,"stats":{"Line":0}},{"line":543,"address":[],"length":0,"stats":{"Line":0}},{"line":547,"address":[],"length":0,"stats":{"Line":0}},{"line":548,"address":[],"length":0,"stats":{"Line":0}},{"line":550,"address":[],"length":0,"stats":{"Line":0}},{"line":553,"address":[],"length":0,"stats":{"Line":0}},{"line":554,"address":[],"length":0,"stats":{"Line":0}},{"line":556,"address":[],"length":0,"stats":{"Line":0}},{"line":557,"address":[],"length":0,"stats":{"Line":0}},{"line":560,"address":[],"length":0,"stats":{"Line":0}},{"line":563,"address":[],"length":0,"stats":{"Line":0}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":565,"address":[],"length":0,"stats":{"Line":0}},{"line":568,"address":[],"length":0,"stats":{"Line":0}},{"line":578,"address":[],"length":0,"stats":{"Line":0}},{"line":579,"address":[],"length":0,"stats":{"Line":0}},{"line":580,"address":[],"length":0,"stats":{"Line":0}},{"line":583,"address":[],"length":0,"stats":{"Line":0}},{"line":584,"address":[],"length":0,"stats":{"Line":0}},{"line":585,"address":[],"length":0,"stats":{"Line":0}},{"line":589,"address":[],"length":0,"stats":{"Line":0}},{"line":592,"address":[],"length":0,"stats":{"Line":0}},{"line":593,"address":[],"length":0,"stats":{"Line":0}},{"line":595,"address":[],"length":0,"stats":{"Line":0}},{"line":596,"address":[],"length":0,"stats":{"Line":0}},{"line":599,"address":[],"length":0,"stats":{"Line":0}},{"line":602,"address":[],"length":0,"stats":{"Line":0}},{"line":603,"address":[],"length":0,"stats":{"Line":0}},{"line":604,"address":[],"length":0,"stats":{"Line":0}},{"line":607,"address":[],"length":0,"stats":{"Line":0}},{"line":611,"address":[],"length":0,"stats":{"Line":0}},{"line":612,"address":[],"length":0,"stats":{"Line":0}},{"line":613,"address":[],"length":0,"stats":{"Line":0}},{"line":616,"address":[],"length":0,"stats":{"Line":0}},{"line":617,"address":[],"length":0,"stats":{"Line":0}},{"line":618,"address":[],"length":0,"stats":{"Line":0}},{"line":622,"address":[],"length":0,"stats":{"Line":0}},{"line":623,"address":[],"length":0,"stats":{"Line":0}},{"line":627,"address":[],"length":0,"stats":{"Line":0}},{"line":628,"address":[],"length":0,"stats":{"Line":0}},{"line":629,"address":[],"length":0,"stats":{"Line":0}},{"line":632,"address":[],"length":0,"stats":{"Line":0}},{"line":635,"address":[],"length":0,"stats":{"Line":0}},{"line":636,"address":[],"length":0,"stats":{"Line":0}},{"line":638,"address":[],"length":0,"stats":{"Line":0}},{"line":640,"address":[],"length":0,"stats":{"Line":0}},{"line":643,"address":[],"length":0,"stats":{"Line":0}},{"line":644,"address":[],"length":0,"stats":{"Line":0}},{"line":645,"address":[],"length":0,"stats":{"Line":0}},{"line":648,"address":[],"length":0,"stats":{"Line":0}},{"line":653,"address":[],"length":0,"stats":{"Line":0}},{"line":655,"address":[],"length":0,"stats":{"Line":0}},{"line":657,"address":[],"length":0,"stats":{"Line":0}},{"line":661,"address":[],"length":0,"stats":{"Line":0}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":666,"address":[],"length":0,"stats":{"Line":0}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":670,"address":[],"length":0,"stats":{"Line":0}},{"line":674,"address":[],"length":0,"stats":{"Line":0}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":676,"address":[],"length":0,"stats":{"Line":0}},{"line":677,"address":[],"length":0,"stats":{"Line":0}},{"line":678,"address":[],"length":0,"stats":{"Line":0}},{"line":682,"address":[],"length":0,"stats":{"Line":0}},{"line":685,"address":[],"length":0,"stats":{"Line":0}},{"line":686,"address":[],"length":0,"stats":{"Line":0}},{"line":687,"address":[],"length":0,"stats":{"Line":0}},{"line":690,"address":[],"length":0,"stats":{"Line":0}},{"line":694,"address":[],"length":0,"stats":{"Line":0}},{"line":696,"address":[],"length":0,"stats":{"Line":0}},{"line":697,"address":[],"length":0,"stats":{"Line":0}},{"line":701,"address":[],"length":0,"stats":{"Line":0}},{"line":702,"address":[],"length":0,"stats":{"Line":0}},{"line":704,"address":[],"length":0,"stats":{"Line":0}},{"line":707,"address":[],"length":0,"stats":{"Line":0}},{"line":708,"address":[],"length":0,"stats":{"Line":0}},{"line":709,"address":[],"length":0,"stats":{"Line":0}},{"line":712,"address":[],"length":0,"stats":{"Line":0}},{"line":716,"address":[],"length":0,"stats":{"Line":0}},{"line":718,"address":[],"length":0,"stats":{"Line":0}},{"line":719,"address":[],"length":0,"stats":{"Line":0}},{"line":720,"address":[],"length":0,"stats":{"Line":0}},{"line":724,"address":[],"length":0,"stats":{"Line":0}},{"line":726,"address":[],"length":0,"stats":{"Line":0}},{"line":727,"address":[],"length":0,"stats":{"Line":0}},{"line":728,"address":[],"length":0,"stats":{"Line":0}},{"line":734,"address":[],"length":0,"stats":{"Line":0}},{"line":739,"address":[],"length":0,"stats":{"Line":0}},{"line":740,"address":[],"length":0,"stats":{"Line":0}},{"line":741,"address":[],"length":0,"stats":{"Line":0}},{"line":742,"address":[],"length":0,"stats":{"Line":0}},{"line":743,"address":[],"length":0,"stats":{"Line":0}},{"line":747,"address":[],"length":0,"stats":{"Line":0}},{"line":748,"address":[],"length":0,"stats":{"Line":0}},{"line":749,"address":[],"length":0,"stats":{"Line":0}},{"line":750,"address":[],"length":0,"stats":{"Line":0}},{"line":755,"address":[],"length":0,"stats":{"Line":0}},{"line":756,"address":[],"length":0,"stats":{"Line":0}},{"line":757,"address":[],"length":0,"stats":{"Line":0}},{"line":758,"address":[],"length":0,"stats":{"Line":0}},{"line":761,"address":[],"length":0,"stats":{"Line":0}},{"line":763,"address":[],"length":0,"stats":{"Line":0}},{"line":764,"address":[],"length":0,"stats":{"Line":0}},{"line":767,"address":[],"length":0,"stats":{"Line":0}},{"line":770,"address":[],"length":0,"stats":{"Line":0}},{"line":771,"address":[],"length":0,"stats":{"Line":0}},{"line":772,"address":[],"length":0,"stats":{"Line":0}},{"line":775,"address":[],"length":0,"stats":{"Line":0}},{"line":784,"address":[],"length":0,"stats":{"Line":14}},{"line":785,"address":[],"length":0,"stats":{"Line":42}},{"line":786,"address":[],"length":0,"stats":{"Line":14}},{"line":796,"address":[],"length":0,"stats":{"Line":16}},{"line":797,"address":[],"length":0,"stats":{"Line":48}},{"line":798,"address":[],"length":0,"stats":{"Line":16}},{"line":808,"address":[],"length":0,"stats":{"Line":0}},{"line":809,"address":[],"length":0,"stats":{"Line":0}},{"line":812,"address":[],"length":0,"stats":{"Line":0}},{"line":813,"address":[],"length":0,"stats":{"Line":0}},{"line":814,"address":[],"length":0,"stats":{"Line":0}},{"line":817,"address":[],"length":0,"stats":{"Line":0}},{"line":828,"address":[],"length":0,"stats":{"Line":1}},{"line":829,"address":[],"length":0,"stats":{"Line":3}},{"line":832,"address":[],"length":0,"stats":{"Line":3}},{"line":833,"address":[],"length":0,"stats":{"Line":0}},{"line":834,"address":[],"length":0,"stats":{"Line":1}},{"line":837,"address":[],"length":0,"stats":{"Line":1}},{"line":854,"address":[],"length":0,"stats":{"Line":10}},{"line":855,"address":[],"length":0,"stats":{"Line":10}},{"line":856,"address":[],"length":0,"stats":{"Line":40}},{"line":860,"address":[],"length":0,"stats":{"Line":40}},{"line":862,"address":[],"length":0,"stats":{"Line":50}},{"line":863,"address":[],"length":0,"stats":{"Line":0}},{"line":868,"address":[],"length":0,"stats":{"Line":10}},{"line":874,"address":[],"length":0,"stats":{"Line":12}},{"line":875,"address":[],"length":0,"stats":{"Line":36}},{"line":876,"address":[],"length":0,"stats":{"Line":12}},{"line":890,"address":[],"length":0,"stats":{"Line":4}},{"line":891,"address":[],"length":0,"stats":{"Line":8}},{"line":892,"address":[],"length":0,"stats":{"Line":4}},{"line":926,"address":[],"length":0,"stats":{"Line":4}},{"line":931,"address":[],"length":0,"stats":{"Line":16}},{"line":932,"address":[],"length":0,"stats":{"Line":1}},{"line":933,"address":[],"length":0,"stats":{"Line":1}},{"line":934,"address":[],"length":0,"stats":{"Line":1}},{"line":938,"address":[],"length":0,"stats":{"Line":3}},{"line":940,"address":[],"length":0,"stats":{"Line":0}},{"line":941,"address":[],"length":0,"stats":{"Line":0}},{"line":942,"address":[],"length":0,"stats":{"Line":0}},{"line":947,"address":[],"length":0,"stats":{"Line":0}},{"line":948,"address":[],"length":0,"stats":{"Line":0}},{"line":952,"address":[],"length":0,"stats":{"Line":0}},{"line":953,"address":[],"length":0,"stats":{"Line":0}},{"line":957,"address":[],"length":0,"stats":{"Line":0}},{"line":958,"address":[],"length":0,"stats":{"Line":0}},{"line":962,"address":[],"length":0,"stats":{"Line":0}},{"line":963,"address":[],"length":0,"stats":{"Line":0}},{"line":967,"address":[],"length":0,"stats":{"Line":0}},{"line":968,"address":[],"length":0,"stats":{"Line":0}},{"line":972,"address":[],"length":0,"stats":{"Line":0}},{"line":973,"address":[],"length":0,"stats":{"Line":0}},{"line":977,"address":[],"length":0,"stats":{"Line":0}},{"line":978,"address":[],"length":0,"stats":{"Line":0}},{"line":979,"address":[],"length":0,"stats":{"Line":0}},{"line":980,"address":[],"length":0,"stats":{"Line":0}},{"line":981,"address":[],"length":0,"stats":{"Line":0}},{"line":983,"address":[],"length":0,"stats":{"Line":0}},{"line":987,"address":[],"length":0,"stats":{"Line":0}},{"line":988,"address":[],"length":0,"stats":{"Line":0}},{"line":989,"address":[],"length":0,"stats":{"Line":0}},{"line":990,"address":[],"length":0,"stats":{"Line":0}},{"line":991,"address":[],"length":0,"stats":{"Line":0}},{"line":993,"address":[],"length":0,"stats":{"Line":0}},{"line":997,"address":[],"length":0,"stats":{"Line":1}},{"line":1002,"address":[],"length":0,"stats":{"Line":4}},{"line":1003,"address":[],"length":0,"stats":{"Line":4}},{"line":1005,"address":[],"length":0,"stats":{"Line":2}},{"line":1006,"address":[],"length":0,"stats":{"Line":2}},{"line":1040,"address":[],"length":0,"stats":{"Line":2}},{"line":1041,"address":[],"length":0,"stats":{"Line":8}},{"line":1042,"address":[],"length":0,"stats":{"Line":1}},{"line":1043,"address":[],"length":0,"stats":{"Line":1}},{"line":1044,"address":[],"length":0,"stats":{"Line":1}},{"line":1048,"address":[],"length":0,"stats":{"Line":1}},{"line":1050,"address":[],"length":0,"stats":{"Line":0}},{"line":1051,"address":[],"length":0,"stats":{"Line":0}},{"line":1052,"address":[],"length":0,"stats":{"Line":0}},{"line":1056,"address":[],"length":0,"stats":{"Line":0}},{"line":1057,"address":[],"length":0,"stats":{"Line":0}},{"line":1060,"address":[],"length":0,"stats":{"Line":0}},{"line":1061,"address":[],"length":0,"stats":{"Line":0}},{"line":1062,"address":[],"length":0,"stats":{"Line":0}},{"line":1066,"address":[],"length":0,"stats":{"Line":0}},{"line":1067,"address":[],"length":0,"stats":{"Line":0}},{"line":1070,"address":[],"length":0,"stats":{"Line":0}},{"line":1071,"address":[],"length":0,"stats":{"Line":0}},{"line":1072,"address":[],"length":0,"stats":{"Line":0}},{"line":1076,"address":[],"length":0,"stats":{"Line":0}},{"line":1077,"address":[],"length":0,"stats":{"Line":0}},{"line":1081,"address":[],"length":0,"stats":{"Line":0}},{"line":1082,"address":[],"length":0,"stats":{"Line":0}},{"line":1086,"address":[],"length":0,"stats":{"Line":0}},{"line":1087,"address":[],"length":0,"stats":{"Line":0}},{"line":1092,"address":[],"length":0,"stats":{"Line":0}},{"line":1093,"address":[],"length":0,"stats":{"Line":0}},{"line":1094,"address":[],"length":0,"stats":{"Line":0}},{"line":1095,"address":[],"length":0,"stats":{"Line":0}},{"line":1097,"address":[],"length":0,"stats":{"Line":0}},{"line":1098,"address":[],"length":0,"stats":{"Line":0}},{"line":1103,"address":[],"length":0,"stats":{"Line":0}},{"line":1104,"address":[],"length":0,"stats":{"Line":0}},{"line":1105,"address":[],"length":0,"stats":{"Line":0}},{"line":1106,"address":[],"length":0,"stats":{"Line":0}},{"line":1110,"address":[],"length":0,"stats":{"Line":0}},{"line":1111,"address":[],"length":0,"stats":{"Line":0}},{"line":1112,"address":[],"length":0,"stats":{"Line":0}},{"line":1113,"address":[],"length":0,"stats":{"Line":0}},{"line":1114,"address":[],"length":0,"stats":{"Line":0}},{"line":1116,"address":[],"length":0,"stats":{"Line":0}},{"line":1120,"address":[],"length":0,"stats":{"Line":1}},{"line":1124,"address":[],"length":0,"stats":{"Line":0}},{"line":1125,"address":[],"length":0,"stats":{"Line":0}},{"line":1126,"address":[],"length":0,"stats":{"Line":0}},{"line":1127,"address":[],"length":0,"stats":{"Line":0}},{"line":1128,"address":[],"length":0,"stats":{"Line":0}},{"line":1130,"address":[],"length":0,"stats":{"Line":0}}],"covered":106,"coverable":372},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","inference_metrics.rs"],"content":"//! Inference Metrics for Real-Time Streaming Display\n//!\n//! OODA-33: Unified metrics collection for LLM streaming operations.\n//!\n//! ## Purpose\n//!\n//! Provides a single source of truth for all metrics collected during\n//! a single LLM inference stream. Used by the display layer to show:\n//! - Time to first token (TTFT)\n//! - Token generation rate (tokens/second)\n//! - Thinking/reasoning progress\n//! - Total tokens generated\n//!\n//! ## Architecture\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    InferenceMetrics Flow                                â”‚\n//! â”‚                                                                         â”‚\n//! â”‚  Provider Stream â”€â”€â–º InferenceMetrics â”€â”€â–º Display Layer                 â”‚\n//! â”‚        â”‚                    â”‚                   â”‚                       â”‚\n//! â”‚        â–¼                    â–¼                   â–¼                       â”‚\n//! â”‚  - StreamChunk        - record_first_token()  - ttft_ms()              â”‚\n//! â”‚  - ThinkingContent    - add_output_tokens()   - tokens_per_second()    â”‚\n//! â”‚  - Finished           - add_thinking_tokens() - thinking_tokens()      â”‚\n//! â”‚                       - set_provider_ttft()                            â”‚\n//! â”‚                                                                         â”‚\n//! â”‚  Provider Preference:                                                   â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n//! â”‚  â”‚ ttft_ms() returns:                                              â”‚   â”‚\n//! â”‚  â”‚   1. provider_ttft_ms if set (native, most accurate)           â”‚   â”‚\n//! â”‚  â”‚   2. measured TTFT from first_token_time (client-side)         â”‚   â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! ## Usage\n//!\n//! ```rust,ignore\n//! use edgequake_llm::InferenceMetrics;\n//!\n//! let mut metrics = InferenceMetrics::new();\n//!\n//! // On first token received\n//! metrics.record_first_token();\n//!\n//! // On each content chunk\n//! metrics.add_output_tokens(5);\n//!\n//! // On thinking chunk\n//! metrics.add_thinking_tokens(10);\n//!\n//! // Display metrics\n//! println!(\"TTFT: {:?}ms\", metrics.ttft_ms());\n//! println!(\"Rate: {:.1} t/s\", metrics.tokens_per_second());\n//! ```\n\nuse std::time::{Duration, Instant};\n\n/// Characters per token estimation (average across models).\nconst CHARS_PER_TOKEN: usize = 4;\n\n/// Real-time metrics for a single LLM inference stream.\n///\n/// OODA-33: Unified metrics collection for streaming display.\n///\n/// Captures timing, token counts, and thinking metrics in one place.\n/// Provides both measured and provider-reported values where available.\n#[derive(Debug, Clone)]\npub struct InferenceMetrics {\n    /// When the request was sent\n    request_start: Instant,\n\n    /// When the first token was received (for TTFT calculation)\n    first_token_time: Option<Instant>,\n\n    /// Total output tokens generated\n    output_tokens: usize,\n\n    /// Thinking/reasoning tokens (from ThinkingContent)\n    thinking_tokens: usize,\n\n    /// Input tokens (from request or provider response)\n    input_tokens: Option<usize>,\n\n    /// Provider-reported TTFT in milliseconds (if available)\n    provider_ttft_ms: Option<f64>,\n\n    /// Thinking budget (if applicable, for budget display)\n    thinking_budget: Option<usize>,\n\n    /// Total characters received (for token estimation)\n    chars_received: usize,\n\n    /// Last token time for rate calculation\n    last_token_time: Option<Instant>,\n}\n\nimpl Default for InferenceMetrics {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl InferenceMetrics {\n    /// Create new metrics instance with current time as start.\n    pub fn new() -> Self {\n        let now = Instant::now();\n        Self {\n            request_start: now,\n            first_token_time: None,\n            output_tokens: 0,\n            thinking_tokens: 0,\n            input_tokens: None,\n            provider_ttft_ms: None,\n            thinking_budget: None,\n            chars_received: 0,\n            last_token_time: None,\n        }\n    }\n\n    /// Create metrics with a specific start time (for testing).\n    pub fn with_start_time(start: Instant) -> Self {\n        Self {\n            request_start: start,\n            first_token_time: None,\n            output_tokens: 0,\n            thinking_tokens: 0,\n            input_tokens: None,\n            provider_ttft_ms: None,\n            thinking_budget: None,\n            chars_received: 0,\n            last_token_time: None,\n        }\n    }\n\n    /// Record arrival of first token (for TTFT calculation).\n    ///\n    /// Should be called once when the first content/thinking token arrives.\n    /// If called multiple times, only the first call is recorded.\n    pub fn record_first_token(&mut self) {\n        if self.first_token_time.is_none() {\n            let now = Instant::now();\n            self.first_token_time = Some(now);\n            self.last_token_time = Some(now);\n        }\n    }\n\n    /// Add output tokens to the count.\n    ///\n    /// # Arguments\n    /// * `count` - Number of tokens to add (can be 0)\n    pub fn add_output_tokens(&mut self, count: usize) {\n        self.output_tokens += count;\n        self.last_token_time = Some(Instant::now());\n    }\n\n    /// Add thinking/reasoning tokens to the count.\n    ///\n    /// # Arguments\n    /// * `count` - Number of thinking tokens to add\n    pub fn add_thinking_tokens(&mut self, count: usize) {\n        self.thinking_tokens += count;\n        self.last_token_time = Some(Instant::now());\n    }\n\n    /// Add characters received (for token estimation).\n    ///\n    /// # Arguments\n    /// * `count` - Number of characters to add\n    pub fn add_chars(&mut self, count: usize) {\n        self.chars_received += count;\n    }\n\n    /// Set provider-reported TTFT in milliseconds.\n    ///\n    /// When available, this takes precedence over measured TTFT.\n    ///\n    /// # Arguments\n    /// * `ms` - Time to first token in milliseconds\n    pub fn set_provider_ttft(&mut self, ms: f64) {\n        self.provider_ttft_ms = Some(ms);\n    }\n\n    /// Set input token count from provider response.\n    ///\n    /// # Arguments\n    /// * `count` - Number of input/prompt tokens\n    pub fn set_input_tokens(&mut self, count: usize) {\n        self.input_tokens = Some(count);\n    }\n\n    /// Set thinking budget (for budget display like \"1.2k/10k\").\n    ///\n    /// # Arguments\n    /// * `budget` - Total thinking token budget\n    pub fn set_thinking_budget(&mut self, budget: usize) {\n        self.thinking_budget = Some(budget);\n    }\n\n    /// Get time to first token in milliseconds.\n    ///\n    /// Prefers provider-reported TTFT if available, otherwise\n    /// calculates from measured first_token_time.\n    ///\n    /// # Returns\n    /// TTFT in milliseconds, or None if no token received yet\n    pub fn ttft_ms(&self) -> Option<f64> {\n        // Prefer provider-reported TTFT\n        if let Some(ttft) = self.provider_ttft_ms {\n            return Some(ttft);\n        }\n\n        // Fall back to measured TTFT\n        self.first_token_time\n            .map(|ft| ft.duration_since(self.request_start).as_secs_f64() * 1000.0)\n    }\n\n    /// Get output tokens per second.\n    ///\n    /// Calculates generation rate based on elapsed time since first token.\n    /// Returns 0.0 if no tokens generated or no time elapsed.\n    ///\n    /// # Returns\n    /// Tokens per second as f64\n    pub fn tokens_per_second(&self) -> f64 {\n        let Some(first) = self.first_token_time else {\n            return 0.0;\n        };\n\n        let elapsed = first.elapsed().as_secs_f64();\n        if elapsed <= 0.0 || self.output_tokens == 0 {\n            return 0.0;\n        }\n\n        self.output_tokens as f64 / elapsed\n    }\n\n    /// Get total tokens per second (output + thinking).\n    ///\n    /// # Returns\n    /// Total tokens per second as f64\n    pub fn total_tokens_per_second(&self) -> f64 {\n        let Some(first) = self.first_token_time else {\n            return 0.0;\n        };\n\n        let elapsed = first.elapsed().as_secs_f64();\n        if elapsed <= 0.0 {\n            return 0.0;\n        }\n\n        (self.output_tokens + self.thinking_tokens) as f64 / elapsed\n    }\n\n    /// Get elapsed time since request start.\n    ///\n    /// # Returns\n    /// Duration since metrics were created\n    pub fn elapsed(&self) -> Duration {\n        self.request_start.elapsed()\n    }\n\n    /// Get time since first token.\n    ///\n    /// # Returns\n    /// Duration since first token, or Duration::ZERO if no token yet\n    pub fn time_since_first_token(&self) -> Duration {\n        self.first_token_time\n            .map(|ft| ft.elapsed())\n            .unwrap_or(Duration::ZERO)\n    }\n\n    /// Get output token count.\n    pub fn output_tokens(&self) -> usize {\n        self.output_tokens\n    }\n\n    /// Get thinking token count.\n    pub fn thinking_tokens(&self) -> usize {\n        self.thinking_tokens\n    }\n\n    /// Get total tokens (output + thinking).\n    pub fn total_tokens(&self) -> usize {\n        self.output_tokens + self.thinking_tokens\n    }\n\n    /// Get input token count (if set).\n    pub fn input_tokens(&self) -> Option<usize> {\n        self.input_tokens\n    }\n\n    /// Get thinking budget (if set).\n    pub fn thinking_budget(&self) -> Option<usize> {\n        self.thinking_budget\n    }\n\n    /// Estimate tokens from accumulated characters.\n    ///\n    /// Uses the standard heuristic of ~4 characters per token.\n    ///\n    /// # Returns\n    /// Estimated token count (minimum 1 for non-zero chars)\n    pub fn estimated_tokens(&self) -> usize {\n        if self.chars_received == 0 {\n            return 0;\n        }\n        std::cmp::max(1, self.chars_received / CHARS_PER_TOKEN)\n    }\n\n    /// Get characters received count.\n    pub fn chars_received(&self) -> usize {\n        self.chars_received\n    }\n\n    /// Check if first token has been received.\n    pub fn has_first_token(&self) -> bool {\n        self.first_token_time.is_some()\n    }\n\n    /// Format thinking progress (e.g., \"1.2k/10k\").\n    ///\n    /// # Returns\n    /// Formatted string, or None if no thinking tokens\n    pub fn format_thinking_progress(&self) -> Option<String> {\n        if self.thinking_tokens == 0 {\n            return None;\n        }\n\n        let tokens = format_tokens(self.thinking_tokens);\n        match self.thinking_budget {\n            Some(budget) => Some(format!(\"{}/{}\", tokens, format_tokens(budget))),\n            None => Some(tokens),\n        }\n    }\n\n    /// Format TTFT for display (e.g., \"1.2s\" or \"850ms\").\n    ///\n    /// # Returns\n    /// Formatted string, or None if no TTFT available\n    pub fn format_ttft(&self) -> Option<String> {\n        self.ttft_ms().map(|ms| {\n            if ms >= 1000.0 {\n                format!(\"{:.1}s\", ms / 1000.0)\n            } else {\n                format!(\"{:.0}ms\", ms)\n            }\n        })\n    }\n\n    /// Format token rate for display (e.g., \"42 t/s\").\n    ///\n    /// # Returns\n    /// Formatted string (always returns, shows \"0 t/s\" if no rate)\n    pub fn format_rate(&self) -> String {\n        let rate = self.tokens_per_second();\n        if rate >= 10.0 {\n            format!(\"{:.0} t/s\", rate)\n        } else {\n            format!(\"{:.1} t/s\", rate)\n        }\n    }\n}\n\n/// Format token count for display (e.g., \"1.2k\", \"12\", \"1.5M\").\nfn format_tokens(count: usize) -> String {\n    if count >= 1_000_000 {\n        format!(\"{:.1}M\", count as f64 / 1_000_000.0)\n    } else if count >= 1_000 {\n        format!(\"{:.1}k\", count as f64 / 1_000.0)\n    } else {\n        format!(\"{}\", count)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::thread::sleep;\n    use std::time::Duration;\n\n    #[test]\n    fn test_new_initializes_correctly() {\n        let metrics = InferenceMetrics::new();\n        assert_eq!(metrics.output_tokens(), 0);\n        assert_eq!(metrics.thinking_tokens(), 0);\n        assert!(!metrics.has_first_token());\n        assert!(metrics.ttft_ms().is_none());\n    }\n\n    #[test]\n    fn test_first_token_ttft() {\n        let start = Instant::now();\n        let mut metrics = InferenceMetrics::with_start_time(start);\n\n        // Small delay to simulate TTFT\n        sleep(Duration::from_millis(10));\n        metrics.record_first_token();\n\n        let ttft = metrics.ttft_ms();\n        assert!(ttft.is_some());\n        assert!(ttft.unwrap() >= 10.0);\n    }\n\n    #[test]\n    fn test_tokens_per_second() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.record_first_token();\n\n        // Add tokens with small delay\n        sleep(Duration::from_millis(50));\n        metrics.add_output_tokens(50);\n\n        let rate = metrics.tokens_per_second();\n        // Should be approximately 50 tokens / 0.05 seconds = 1000 t/s\n        // But timing can vary, so just check it's reasonable\n        assert!(rate > 0.0);\n    }\n\n    #[test]\n    fn test_provider_ttft_takes_precedence() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.record_first_token();\n\n        // Set provider TTFT (should override measured)\n        metrics.set_provider_ttft(123.45);\n\n        let ttft = metrics.ttft_ms();\n        assert!(ttft.is_some());\n        assert!((ttft.unwrap() - 123.45).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_thinking_tokens_tracked() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.add_thinking_tokens(100);\n        metrics.add_thinking_tokens(50);\n\n        assert_eq!(metrics.thinking_tokens(), 150);\n        assert_eq!(metrics.total_tokens(), 150);\n    }\n\n    #[test]\n    fn test_total_tokens() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.add_output_tokens(100);\n        metrics.add_thinking_tokens(50);\n\n        assert_eq!(metrics.output_tokens(), 100);\n        assert_eq!(metrics.thinking_tokens(), 50);\n        assert_eq!(metrics.total_tokens(), 150);\n    }\n\n    #[test]\n    fn test_estimated_tokens() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.add_chars(100);\n\n        assert_eq!(metrics.estimated_tokens(), 25); // 100 / 4\n    }\n\n    #[test]\n    fn test_estimated_tokens_minimum() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.add_chars(3);\n\n        // 3 / 4 = 0, but minimum is 1\n        assert_eq!(metrics.estimated_tokens(), 1);\n    }\n\n    #[test]\n    fn test_format_tokens() {\n        assert_eq!(format_tokens(0), \"0\");\n        assert_eq!(format_tokens(123), \"123\");\n        assert_eq!(format_tokens(1234), \"1.2k\");\n        assert_eq!(format_tokens(12345), \"12.3k\");\n        assert_eq!(format_tokens(1234567), \"1.2M\");\n    }\n\n    #[test]\n    fn test_format_thinking_progress() {\n        let mut metrics = InferenceMetrics::new();\n        \n        // No thinking tokens\n        assert!(metrics.format_thinking_progress().is_none());\n\n        // With thinking tokens, no budget\n        metrics.add_thinking_tokens(1500);\n        assert_eq!(metrics.format_thinking_progress(), Some(\"1.5k\".to_string()));\n\n        // With budget\n        metrics.set_thinking_budget(10000);\n        assert_eq!(\n            metrics.format_thinking_progress(),\n            Some(\"1.5k/10.0k\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_format_ttft() {\n        let mut metrics = InferenceMetrics::new();\n\n        // No TTFT yet\n        assert!(metrics.format_ttft().is_none());\n\n        // Milliseconds\n        metrics.set_provider_ttft(850.0);\n        assert_eq!(metrics.format_ttft(), Some(\"850ms\".to_string()));\n\n        // Seconds\n        metrics.set_provider_ttft(1250.0);\n        assert_eq!(metrics.format_ttft(), Some(\"1.2s\".to_string()));\n    }\n\n    #[test]\n    fn test_first_token_only_recorded_once() {\n        let start = Instant::now();\n        let mut metrics = InferenceMetrics::with_start_time(start);\n\n        sleep(Duration::from_millis(10));\n        metrics.record_first_token();\n        let ttft1 = metrics.ttft_ms().unwrap();\n\n        sleep(Duration::from_millis(10));\n        metrics.record_first_token(); // Should not update\n        let ttft2 = metrics.ttft_ms().unwrap();\n\n        // TTFTs should be the same (only first call matters)\n        assert!((ttft1 - ttft2).abs() < 1.0);\n    }\n\n    #[test]\n    fn test_default_impl() {\n        let metrics = InferenceMetrics::default();\n        assert_eq!(metrics.output_tokens(), 0);\n        assert_eq!(metrics.thinking_tokens(), 0);\n        assert!(!metrics.has_first_token());\n    }\n\n    #[test]\n    fn test_total_tokens_per_second_no_first_token() {\n        let metrics = InferenceMetrics::new();\n        assert_eq!(metrics.total_tokens_per_second(), 0.0);\n    }\n\n    #[test]\n    fn test_total_tokens_per_second_with_tokens() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.record_first_token();\n        sleep(Duration::from_millis(50));\n        metrics.add_output_tokens(30);\n        metrics.add_thinking_tokens(20);\n\n        let rate = metrics.total_tokens_per_second();\n        assert!(rate > 0.0);\n    }\n\n    #[test]\n    fn test_tokens_per_second_no_first_token() {\n        let metrics = InferenceMetrics::new();\n        assert_eq!(metrics.tokens_per_second(), 0.0);\n    }\n\n    #[test]\n    fn test_tokens_per_second_zero_output_tokens() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.record_first_token();\n        sleep(Duration::from_millis(10));\n        // No output tokens added\n        assert_eq!(metrics.tokens_per_second(), 0.0);\n    }\n\n    #[test]\n    fn test_elapsed() {\n        let metrics = InferenceMetrics::new();\n        sleep(Duration::from_millis(10));\n        let elapsed = metrics.elapsed();\n        assert!(elapsed >= Duration::from_millis(10));\n    }\n\n    #[test]\n    fn test_time_since_first_token_none() {\n        let metrics = InferenceMetrics::new();\n        assert_eq!(metrics.time_since_first_token(), Duration::ZERO);\n    }\n\n    #[test]\n    fn test_time_since_first_token_some() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.record_first_token();\n        sleep(Duration::from_millis(10));\n        let since = metrics.time_since_first_token();\n        assert!(since >= Duration::from_millis(10));\n    }\n\n    #[test]\n    fn test_input_tokens() {\n        let mut metrics = InferenceMetrics::new();\n        assert!(metrics.input_tokens().is_none());\n        metrics.set_input_tokens(500);\n        assert_eq!(metrics.input_tokens(), Some(500));\n    }\n\n    #[test]\n    fn test_thinking_budget() {\n        let mut metrics = InferenceMetrics::new();\n        assert!(metrics.thinking_budget().is_none());\n        metrics.set_thinking_budget(10000);\n        assert_eq!(metrics.thinking_budget(), Some(10000));\n    }\n\n    #[test]\n    fn test_chars_received() {\n        let mut metrics = InferenceMetrics::new();\n        assert_eq!(metrics.chars_received(), 0);\n        metrics.add_chars(100);\n        assert_eq!(metrics.chars_received(), 100);\n        metrics.add_chars(50);\n        assert_eq!(metrics.chars_received(), 150);\n    }\n\n    #[test]\n    fn test_estimated_tokens_zero_chars() {\n        let metrics = InferenceMetrics::new();\n        assert_eq!(metrics.estimated_tokens(), 0);\n    }\n\n    #[test]\n    fn test_format_rate_high() {\n        let mut metrics = InferenceMetrics::new();\n        metrics.record_first_token();\n        sleep(Duration::from_millis(10));\n        metrics.add_output_tokens(500);\n        let rate = metrics.format_rate();\n        // High rate, should use {:.0} format\n        assert!(rate.contains(\"t/s\"));\n    }\n\n    #[test]\n    fn test_format_rate_low() {\n        let metrics = InferenceMetrics::new();\n        // No first token - rate = 0\n        let rate = metrics.format_rate();\n        assert_eq!(rate, \"0.0 t/s\");\n    }\n\n    #[test]\n    fn test_has_first_token() {\n        let mut metrics = InferenceMetrics::new();\n        assert!(!metrics.has_first_token());\n        metrics.record_first_token();\n        assert!(metrics.has_first_token());\n    }\n\n    #[test]\n    fn test_debug_impl() {\n        let metrics = InferenceMetrics::new();\n        let debug = format!(\"{:?}\", metrics);\n        assert!(debug.contains(\"InferenceMetrics\"));\n    }\n}\n","traces":[{"line":100,"address":[],"length":0,"stats":{"Line":1}},{"line":101,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":25}},{"line":108,"address":[],"length":0,"stats":{"Line":50}},{"line":123,"address":[],"length":0,"stats":{"Line":2}},{"line":141,"address":[],"length":0,"stats":{"Line":10}},{"line":142,"address":[],"length":0,"stats":{"Line":29}},{"line":143,"address":[],"length":0,"stats":{"Line":27}},{"line":144,"address":[],"length":0,"stats":{"Line":9}},{"line":145,"address":[],"length":0,"stats":{"Line":9}},{"line":153,"address":[],"length":0,"stats":{"Line":4}},{"line":154,"address":[],"length":0,"stats":{"Line":4}},{"line":155,"address":[],"length":0,"stats":{"Line":4}},{"line":162,"address":[],"length":0,"stats":{"Line":5}},{"line":163,"address":[],"length":0,"stats":{"Line":5}},{"line":164,"address":[],"length":0,"stats":{"Line":5}},{"line":171,"address":[],"length":0,"stats":{"Line":4}},{"line":172,"address":[],"length":0,"stats":{"Line":4}},{"line":181,"address":[],"length":0,"stats":{"Line":3}},{"line":182,"address":[],"length":0,"stats":{"Line":3}},{"line":189,"address":[],"length":0,"stats":{"Line":1}},{"line":190,"address":[],"length":0,"stats":{"Line":1}},{"line":197,"address":[],"length":0,"stats":{"Line":2}},{"line":198,"address":[],"length":0,"stats":{"Line":2}},{"line":208,"address":[],"length":0,"stats":{"Line":8}},{"line":210,"address":[],"length":0,"stats":{"Line":11}},{"line":211,"address":[],"length":0,"stats":{"Line":3}},{"line":215,"address":[],"length":0,"stats":{"Line":5}},{"line":216,"address":[],"length":0,"stats":{"Line":14}},{"line":226,"address":[],"length":0,"stats":{"Line":5}},{"line":227,"address":[],"length":0,"stats":{"Line":8}},{"line":228,"address":[],"length":0,"stats":{"Line":2}},{"line":231,"address":[],"length":0,"stats":{"Line":9}},{"line":232,"address":[],"length":0,"stats":{"Line":6}},{"line":233,"address":[],"length":0,"stats":{"Line":1}},{"line":236,"address":[],"length":0,"stats":{"Line":2}},{"line":243,"address":[],"length":0,"stats":{"Line":2}},{"line":244,"address":[],"length":0,"stats":{"Line":3}},{"line":245,"address":[],"length":0,"stats":{"Line":1}},{"line":248,"address":[],"length":0,"stats":{"Line":3}},{"line":249,"address":[],"length":0,"stats":{"Line":1}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":1}},{"line":260,"address":[],"length":0,"stats":{"Line":1}},{"line":261,"address":[],"length":0,"stats":{"Line":2}},{"line":268,"address":[],"length":0,"stats":{"Line":2}},{"line":269,"address":[],"length":0,"stats":{"Line":2}},{"line":270,"address":[],"length":0,"stats":{"Line":4}},{"line":271,"address":[],"length":0,"stats":{"Line":2}},{"line":275,"address":[],"length":0,"stats":{"Line":3}},{"line":276,"address":[],"length":0,"stats":{"Line":3}},{"line":280,"address":[],"length":0,"stats":{"Line":4}},{"line":281,"address":[],"length":0,"stats":{"Line":4}},{"line":285,"address":[],"length":0,"stats":{"Line":2}},{"line":286,"address":[],"length":0,"stats":{"Line":2}},{"line":290,"address":[],"length":0,"stats":{"Line":2}},{"line":291,"address":[],"length":0,"stats":{"Line":2}},{"line":295,"address":[],"length":0,"stats":{"Line":2}},{"line":296,"address":[],"length":0,"stats":{"Line":2}},{"line":305,"address":[],"length":0,"stats":{"Line":3}},{"line":306,"address":[],"length":0,"stats":{"Line":3}},{"line":307,"address":[],"length":0,"stats":{"Line":1}},{"line":309,"address":[],"length":0,"stats":{"Line":4}},{"line":313,"address":[],"length":0,"stats":{"Line":3}},{"line":314,"address":[],"length":0,"stats":{"Line":3}},{"line":318,"address":[],"length":0,"stats":{"Line":4}},{"line":319,"address":[],"length":0,"stats":{"Line":8}},{"line":326,"address":[],"length":0,"stats":{"Line":3}},{"line":327,"address":[],"length":0,"stats":{"Line":3}},{"line":328,"address":[],"length":0,"stats":{"Line":1}},{"line":331,"address":[],"length":0,"stats":{"Line":6}},{"line":332,"address":[],"length":0,"stats":{"Line":2}},{"line":333,"address":[],"length":0,"stats":{"Line":4}},{"line":334,"address":[],"length":0,"stats":{"Line":1}},{"line":342,"address":[],"length":0,"stats":{"Line":3}},{"line":343,"address":[],"length":0,"stats":{"Line":11}},{"line":344,"address":[],"length":0,"stats":{"Line":2}},{"line":345,"address":[],"length":0,"stats":{"Line":3}},{"line":347,"address":[],"length":0,"stats":{"Line":2}},{"line":356,"address":[],"length":0,"stats":{"Line":2}},{"line":357,"address":[],"length":0,"stats":{"Line":6}},{"line":358,"address":[],"length":0,"stats":{"Line":2}},{"line":359,"address":[],"length":0,"stats":{"Line":2}},{"line":361,"address":[],"length":0,"stats":{"Line":2}},{"line":367,"address":[],"length":0,"stats":{"Line":8}},{"line":368,"address":[],"length":0,"stats":{"Line":8}},{"line":369,"address":[],"length":0,"stats":{"Line":3}},{"line":370,"address":[],"length":0,"stats":{"Line":7}},{"line":371,"address":[],"length":0,"stats":{"Line":15}},{"line":373,"address":[],"length":0,"stats":{"Line":4}}],"covered":89,"coverable":90},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","lib.rs"],"content":"//! EdgeQuake LLM - LLM and Embedding Provider Abstraction\n//!\n//! # Implements\n//!\n//! - **FEAT0017**: Multi-Provider LLM Support\n//! - **FEAT0018**: Embedding Provider Abstraction\n//! - **FEAT0019**: LLM Response Caching\n//! - **FEAT0020**: API Rate Limiting\n//! - **FEAT0005**: Embedding Generation (via providers)\n//!\n//! # Enforces\n//!\n//! - **BR0301**: LLM API rate limits (configurable per provider)\n//! - **BR0302**: Document size limits (context window awareness)\n//! - **BR0303**: Cost tracking per request\n//! - **BR0010**: Embedding dimension validated (1536 default)\n//!\n//! This crate provides traits and implementations for:\n//! - Text completion (LLM providers)\n//! - Text embedding (embedding providers)\n//! - Token counting and management\n//! - Rate limiting for API calls\n//! - Response caching for cost reduction\n//!\n//! # Providers\n//!\n//! | Provider | FEAT0017 | Chat | Embeddings | Notes |\n//! |----------|----------|------|------------|-------|\n//! | OpenAI | âœ“ | âœ“ | âœ“ | Primary production provider |\n//! | Azure OpenAI | âœ“ | âœ“ | âœ“ | Enterprise deployments |\n//! | Ollama | âœ“ | âœ“ | âœ“ | Local/on-prem models |\n//! | LM Studio | âœ“ | âœ“ | âœ“ | Local OpenAI-compatible API |\n//! | Gemini | âœ“ | âœ“ | âœ“ | Google AI |\n//! | Mock | âœ“ | âœ“ | âœ“ | Testing (no API calls) |\n//!\n//! # Architecture\n//!\n//! The crate uses trait-based abstraction to support multiple LLM backends:\n//! - OpenAI (GPT-4, GPT-3.5)\n//! - OpenAI-compatible APIs (Ollama, LM Studio, etc.)\n//! - Anthropic (Claude 3.5, Claude 3)\n//! - Future: Mistral, local models\n//!\n//! # Example\n//!\n//! ```ignore\n//! use edgequake_llm::{LLMProvider, OpenAIProvider};\n//!\n//! let provider = OpenAIProvider::new(\"your-api-key\");\n//! let response = provider.complete(\"Hello, world!\").await?;\n//! ```\n//!\n//! # See Also\n//!\n//! - [`crate::traits`] for provider trait definitions\n//! - [`crate::providers`] for concrete implementations\n//! - [`crate::cache`] for response caching\n\npub mod cache;\npub mod cache_prompt;\npub mod cost_tracker; // OODA-21: Session-level cost tracking\npub mod error;\npub mod factory;\npub mod inference_metrics; // OODA-33: Unified streaming metrics\npub mod middleware;\npub mod model_config;\npub mod providers;\npub mod rate_limiter;\npub mod registry;\npub mod reranker;\npub mod retry;\npub mod tokenizer;\npub mod traits;\n\npub use cache::{CacheConfig, CacheStats, CachedProvider, LLMCache};\npub use cache_prompt::{\n    apply_cache_control, parse_cache_stats, CachePromptConfig, CacheStats as PromptCacheStats,\n};\npub use cost_tracker::{\n    format_cost, format_tokens, CostEntry, CostSummary, ModelPricing, SessionCostTracker,\n};\npub use error::{LlmError, Result, RetryStrategy};\npub use factory::{ProviderFactory, ProviderType};\npub use inference_metrics::InferenceMetrics; // OODA-33\npub use middleware::{\n    LLMMiddleware, LLMMiddlewareStack, LLMRequest, LogLevel, LoggingLLMMiddleware,\n    MetricsLLMMiddleware, MetricsSummary,\n};\npub use model_config::{\n    DefaultsConfig, ModelCapabilities, ModelCard, ModelConfigError, ModelCost, ModelType,\n    ModelsConfig, ProviderConfig, ProviderType as ConfigProviderType,\n};\npub use providers::azure_openai::AzureOpenAIProvider;\npub use providers::gemini::GeminiProvider;\npub use providers::jina::JinaProvider;\npub use providers::lmstudio::LMStudioProvider;\npub use providers::mock::MockProvider;\npub use providers::ollama::{\n    OllamaModelDetails, OllamaModelInfo, OllamaModelsResponse, OllamaProvider,\n};\npub use providers::openai::OpenAIProvider;\n// OODA-01: Anthropic (Claude) provider\npub use providers::anthropic::AnthropicProvider;\n// OODA-02: OpenRouter provider (200+ models)\n// OODA-72: Dynamic model discovery with caching\npub use providers::openrouter::{\n    ModelArchitecture as OpenRouterModelArchitecture, ModelInfo as OpenRouterModelInfo,\n    ModelPricing as OpenRouterModelPricing, ModelsResponse as OpenRouterModelsResponse,\n    OpenRouterProvider,\n};\n// OODA-200: Configurable OpenAI-compatible provider\npub use providers::openai_compatible::OpenAICompatibleProvider;\n// OODA-71: xAI Grok provider (api.x.ai)\npub use providers::xai::XAIProvider;\npub use providers::vscode::{\n    Model as CopilotModel, ModelsResponse as CopilotModelsResponse, VsCodeCopilotProvider,\n};\npub use rate_limiter::{RateLimitedProvider, RateLimiter, RateLimiterConfig};\npub use registry::ProviderRegistry;\npub use reranker::{\n    BM25Reranker, HttpReranker, HybridReranker, MockReranker, RRFReranker, RerankConfig,\n    RerankResult, Reranker, ScoreAggregation, TermOverlapReranker,\n};\npub use retry::RetryExecutor;\npub use tokenizer::Tokenizer;\npub use traits::{\n    CacheControl, ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, FunctionCall,\n    FunctionDefinition, ImageData, LLMProvider, LLMResponse, ToolCall, ToolChoice, ToolDefinition, ToolResult,\n};\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","middleware.rs"],"content":"//! LLM Provider Middleware System (OODA-125)\n//!\n//! Provides middleware for LLM provider calls, enabling cross-cutting\n//! concerns like logging, metrics, cost tracking, and auditing.\n//!\n//! # Architecture\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    LLM Request Pipeline                          â”‚\n//! â”‚                                                                  â”‚\n//! â”‚   Messages  â”€â”€â–º  [Middleware 1]  â”€â”€â–º  [Middleware 2]  â”€â”€â–º  ...  â”‚\n//! â”‚                       â”‚                    â”‚                     â”‚\n//! â”‚                    before()             before()                 â”‚\n//! â”‚                       â”‚                    â”‚                     â”‚\n//! â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n//! â”‚                                â–¼                                 â”‚\n//! â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n//! â”‚                   â”‚  LLMProvider.chat()    â”‚                     â”‚\n//! â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n//! â”‚                                â”‚                                 â”‚\n//! â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n//! â”‚                       â”‚                    â”‚                     â”‚\n//! â”‚                    after()              after()                  â”‚\n//! â”‚                       â”‚                    â”‚                     â”‚\n//! â”‚   LLMResponse    â—„â”€â”€  [Middleware 1]  â—„â”€â”€  [Middleware 2]  â—„â”€â”€  â”‚\n//! â”‚                                                                  â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Example\n//! ```ignore\n//! use edgequake_llm::middleware::{LLMMiddleware, LoggingLLMMiddleware, MetricsLLMMiddleware};\n//!\n//! let mut stack = LLMMiddlewareStack::new();\n//! stack.add(Arc::new(LoggingLLMMiddleware::new()));\n//! stack.add(Arc::new(MetricsLLMMiddleware::new()));\n//!\n//! // Execute with middleware\n//! stack.before(&request).await?;\n//! let response = provider.chat(&messages, options).await?;\n//! stack.after(&request, &response, duration_ms).await?;\n//! ```\n\nuse async_trait::async_trait;\nuse std::fmt;\nuse std::sync::atomic::{AtomicU64, Ordering};\nuse std::sync::Arc;\nuse tracing::{debug, info, trace};\n\nuse crate::error::Result;\nuse crate::model_config::ModelCost;\nuse crate::traits::{ChatMessage, CompletionOptions, LLMResponse, ToolDefinition};\n\n// ============================================================================\n// Request Type\n// ============================================================================\n\n/// Wrapper for LLM request data passed to middleware.\n#[derive(Debug, Clone)]\npub struct LLMRequest {\n    /// Chat messages for the request.\n    pub messages: Vec<ChatMessage>,\n\n    /// Optional tools for function calling.\n    pub tools: Option<Vec<ToolDefinition>>,\n\n    /// Optional completion options.\n    pub options: Option<CompletionOptions>,\n\n    /// Provider name.\n    pub provider: String,\n\n    /// Model name.\n    pub model: String,\n}\n\nimpl LLMRequest {\n    /// Create a new LLM request.\n    pub fn new(\n        messages: Vec<ChatMessage>,\n        provider: impl Into<String>,\n        model: impl Into<String>,\n    ) -> Self {\n        Self {\n            messages,\n            tools: None,\n            options: None,\n            provider: provider.into(),\n            model: model.into(),\n        }\n    }\n\n    /// Add tools to the request.\n    pub fn with_tools(mut self, tools: Vec<ToolDefinition>) -> Self {\n        self.tools = Some(tools);\n        self\n    }\n\n    /// Add options to the request.\n    pub fn with_options(mut self, options: CompletionOptions) -> Self {\n        self.options = Some(options);\n        self\n    }\n\n    /// Get the number of messages.\n    pub fn message_count(&self) -> usize {\n        self.messages.len()\n    }\n\n    /// Get the number of tools.\n    pub fn tool_count(&self) -> usize {\n        self.tools.as_ref().map(|t| t.len()).unwrap_or(0)\n    }\n}\n\n// ============================================================================\n// Middleware Trait\n// ============================================================================\n\n/// Middleware for intercepting LLM provider calls.\n///\n/// Implement this trait to add cross-cutting concerns to all LLM calls.\n/// Middlewares are executed in registration order for `before()` and\n/// reverse order for `after()`.\n#[async_trait]\npub trait LLMMiddleware: Send + Sync {\n    /// Middleware name for debugging and logging.\n    fn name(&self) -> &str;\n\n    /// Called before LLM request is sent.\n    ///\n    /// Use for:\n    /// - Logging the request\n    /// - Validating input\n    /// - Recording start time\n    ///\n    /// Return `Ok(())` to continue, or `Err` to abort the request.\n    async fn before(&self, request: &LLMRequest) -> Result<()> {\n        let _ = request;\n        Ok(())\n    }\n\n    /// Called after LLM response is received.\n    ///\n    /// Use for:\n    /// - Logging the response\n    /// - Recording metrics\n    /// - Cost tracking\n    async fn after(\n        &self,\n        request: &LLMRequest,\n        response: &LLMResponse,\n        duration_ms: u64,\n    ) -> Result<()> {\n        let _ = (request, response, duration_ms);\n        Ok(())\n    }\n}\n\n// ============================================================================\n// Middleware Stack\n// ============================================================================\n\n/// Stack of middlewares to execute in order.\n#[derive(Default)]\npub struct LLMMiddlewareStack {\n    middlewares: Vec<Arc<dyn LLMMiddleware>>,\n}\n\nimpl LLMMiddlewareStack {\n    /// Create a new empty middleware stack.\n    pub fn new() -> Self {\n        Self {\n            middlewares: Vec::new(),\n        }\n    }\n\n    /// Add a middleware to the stack.\n    pub fn add(&mut self, middleware: Arc<dyn LLMMiddleware>) {\n        self.middlewares.push(middleware);\n    }\n\n    /// Get the number of middlewares.\n    pub fn len(&self) -> usize {\n        self.middlewares.len()\n    }\n\n    /// Check if the stack is empty.\n    pub fn is_empty(&self) -> bool {\n        self.middlewares.is_empty()\n    }\n\n    /// Execute all before hooks in registration order.\n    pub async fn before(&self, request: &LLMRequest) -> Result<()> {\n        for middleware in &self.middlewares {\n            middleware.before(request).await?;\n        }\n        Ok(())\n    }\n\n    /// Execute all after hooks in reverse order.\n    pub async fn after(\n        &self,\n        request: &LLMRequest,\n        response: &LLMResponse,\n        duration_ms: u64,\n    ) -> Result<()> {\n        for middleware in self.middlewares.iter().rev() {\n            middleware.after(request, response, duration_ms).await?;\n        }\n        Ok(())\n    }\n}\n\n// ============================================================================\n// Built-in Middleware Implementations\n// ============================================================================\n\n/// Logging middleware that logs LLM requests and responses.\npub struct LoggingLLMMiddleware {\n    /// Log level for before hooks.\n    log_level: LogLevel,\n}\n\n/// Log level for logging middleware.\n#[derive(Debug, Clone, Copy, Default)]\npub enum LogLevel {\n    /// Minimal logging (request/response summary).\n    #[default]\n    Info,\n    /// Detailed logging (includes message previews).\n    Debug,\n    /// Full logging (complete messages and responses).\n    Trace,\n}\n\nimpl LoggingLLMMiddleware {\n    /// Create a new logging middleware with default settings.\n    pub fn new() -> Self {\n        Self {\n            log_level: LogLevel::Info,\n        }\n    }\n\n    /// Create a logging middleware with specified log level.\n    pub fn with_level(level: LogLevel) -> Self {\n        Self { log_level: level }\n    }\n}\n\nimpl Default for LoggingLLMMiddleware {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[async_trait]\nimpl LLMMiddleware for LoggingLLMMiddleware {\n    fn name(&self) -> &str {\n        \"logging\"\n    }\n\n    async fn before(&self, request: &LLMRequest) -> Result<()> {\n        match self.log_level {\n            LogLevel::Info => {\n                info!(\n                    provider = %request.provider,\n                    model = %request.model,\n                    messages = request.message_count(),\n                    tools = request.tool_count(),\n                    \"[LLM] Request\"\n                );\n            }\n            LogLevel::Debug => {\n                let last_msg = request.messages.last().map(|m| {\n                    let preview = if m.content.chars().count() > 100 {\n                        let truncated: String = m.content.chars().take(97).collect();\n                        format!(\"{}...\", truncated)\n                    } else {\n                        m.content.clone()\n                    };\n                    format!(\"[{:?}] {}\", m.role, preview)\n                });\n                debug!(\n                    provider = %request.provider,\n                    model = %request.model,\n                    messages = request.message_count(),\n                    tools = request.tool_count(),\n                    last_message = ?last_msg,\n                    \"[LLM] Request\"\n                );\n            }\n            LogLevel::Trace => {\n                trace!(\n                    provider = %request.provider,\n                    model = %request.model,\n                    messages = ?request.messages,\n                    \"[LLM] Full request\"\n                );\n            }\n        }\n        Ok(())\n    }\n\n    async fn after(\n        &self,\n        request: &LLMRequest,\n        response: &LLMResponse,\n        duration_ms: u64,\n    ) -> Result<()> {\n        match self.log_level {\n            LogLevel::Info => {\n                info!(\n                    model = %request.model,\n                    tokens = response.total_tokens,\n                    duration_ms = duration_ms,\n                    finish_reason = ?response.finish_reason,\n                    \"[LLM] Response\"\n                );\n            }\n            LogLevel::Debug => {\n                let preview = if response.content.chars().count() > 200 {\n                    let truncated: String = response.content.chars().take(197).collect();\n                    format!(\"{}...\", truncated)\n                } else {\n                    response.content.clone()\n                };\n                debug!(\n                    model = %request.model,\n                    tokens = response.total_tokens,\n                    duration_ms = duration_ms,\n                    tool_calls = response.tool_calls.len(),\n                    content_preview = %preview,\n                    \"[LLM] Response\"\n                );\n            }\n            LogLevel::Trace => {\n                trace!(\n                    model = %request.model,\n                    response = ?response,\n                    \"[LLM] Full response\"\n                );\n            }\n        }\n        Ok(())\n    }\n}\n\n/// Metrics middleware that tracks LLM usage statistics.\n///\n/// OODA-22: Extended with cache metrics for context engineering visibility.\n/// Tracks cache hit tokens to measure effectiveness of append-only context\n/// and deterministic serialization patterns.\npub struct MetricsLLMMiddleware {\n    /// Total requests made.\n    pub total_requests: AtomicU64,\n    /// Total tokens used (prompt + completion).\n    pub total_tokens: AtomicU64,\n    /// Total prompt tokens.\n    pub prompt_tokens: AtomicU64,\n    /// Total completion tokens.\n    pub completion_tokens: AtomicU64,\n    /// Total time spent in milliseconds.\n    pub total_time_ms: AtomicU64,\n    /// Number of requests with tool calls.\n    pub tool_call_requests: AtomicU64,\n    /// Total cache hit tokens across all requests (OODA-22).\n    /// WHY: Track effectiveness of context engineering patterns.\n    /// High values indicate good KV-cache utilization (10x cost savings).\n    pub cache_hit_tokens: AtomicU64,\n    /// Number of requests that had cache hits (OODA-22).\n    /// WHY: Distinguish between \"no cache support\" vs \"cache miss\".\n    pub requests_with_cache: AtomicU64,\n}\n\nimpl MetricsLLMMiddleware {\n    /// Create a new metrics middleware.\n    pub fn new() -> Self {\n        Self {\n            total_requests: AtomicU64::new(0),\n            total_tokens: AtomicU64::new(0),\n            prompt_tokens: AtomicU64::new(0),\n            completion_tokens: AtomicU64::new(0),\n            total_time_ms: AtomicU64::new(0),\n            tool_call_requests: AtomicU64::new(0),\n            cache_hit_tokens: AtomicU64::new(0),\n            requests_with_cache: AtomicU64::new(0),\n        }\n    }\n\n    /// Get the total number of requests.\n    pub fn get_total_requests(&self) -> u64 {\n        self.total_requests.load(Ordering::Relaxed)\n    }\n\n    /// Get the total tokens used.\n    pub fn get_total_tokens(&self) -> u64 {\n        self.total_tokens.load(Ordering::Relaxed)\n    }\n\n    /// Get the average latency in milliseconds.\n    pub fn get_average_latency_ms(&self) -> f64 {\n        let requests = self.total_requests.load(Ordering::Relaxed);\n        if requests == 0 {\n            0.0\n        } else {\n            self.total_time_ms.load(Ordering::Relaxed) as f64 / requests as f64\n        }\n    }\n\n    /// Get the total cache hit tokens (OODA-22).\n    pub fn get_cache_hit_tokens(&self) -> u64 {\n        self.cache_hit_tokens.load(Ordering::Relaxed)\n    }\n\n    /// Get the cache hit rate as a percentage (OODA-22).\n    ///\n    /// # Returns\n    /// Percentage (0.0 - 100.0) of prompt tokens that were cache hits.\n    /// Returns 0.0 if no prompt tokens were used.\n    pub fn get_cache_hit_rate(&self) -> f64 {\n        let prompt = self.prompt_tokens.load(Ordering::Relaxed);\n        if prompt == 0 {\n            0.0\n        } else {\n            (self.cache_hit_tokens.load(Ordering::Relaxed) as f64 / prompt as f64) * 100.0\n        }\n    }\n\n    /// Get all metrics as a summary.\n    pub fn get_summary(&self) -> MetricsSummary {\n        MetricsSummary {\n            total_requests: self.total_requests.load(Ordering::Relaxed),\n            total_tokens: self.total_tokens.load(Ordering::Relaxed),\n            prompt_tokens: self.prompt_tokens.load(Ordering::Relaxed),\n            completion_tokens: self.completion_tokens.load(Ordering::Relaxed),\n            total_time_ms: self.total_time_ms.load(Ordering::Relaxed),\n            tool_call_requests: self.tool_call_requests.load(Ordering::Relaxed),\n            cache_hit_tokens: self.cache_hit_tokens.load(Ordering::Relaxed),\n            requests_with_cache: self.requests_with_cache.load(Ordering::Relaxed),\n        }\n    }\n}\n\nimpl Default for MetricsLLMMiddleware {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Summary of LLM metrics.\n///\n/// OODA-22: Extended with cache metrics for context engineering visibility.\n#[derive(Debug, Clone, Default)]\npub struct MetricsSummary {\n    /// Total requests made.\n    pub total_requests: u64,\n    /// Total tokens used.\n    pub total_tokens: u64,\n    /// Total prompt tokens.\n    pub prompt_tokens: u64,\n    /// Total completion tokens.\n    pub completion_tokens: u64,\n    /// Total time in milliseconds.\n    pub total_time_ms: u64,\n    /// Requests with tool calls.\n    pub tool_call_requests: u64,\n    /// Total cache hit tokens (OODA-22).\n    pub cache_hit_tokens: u64,\n    /// Requests with cache hits (OODA-22).\n    pub requests_with_cache: u64,\n}\n\n/// Builder for MetricsSummary (OODA-36).\n///\n/// Provides fluent API for constructing summaries with sensible defaults.\n/// Particularly useful for tests where only specific fields need values.\n///\n/// # Example\n/// ```ignore\n/// let summary = MetricsSummaryBuilder::new()\n///     .requests(10)\n///     .prompt_tokens(4000)\n///     .completion_tokens(1000)\n///     .cache_hit_tokens(3200)\n///     .time_ms(1500)\n///     .build();\n/// assert_eq!(summary.cache_hit_rate(), 80.0);\n/// ```\n#[derive(Debug, Clone, Default)]\npub struct MetricsSummaryBuilder {\n    inner: MetricsSummary,\n}\n\nimpl MetricsSummaryBuilder {\n    /// Create a new builder with default values.\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Set total requests.\n    pub fn requests(mut self, n: u64) -> Self {\n        self.inner.total_requests = n;\n        self\n    }\n\n    /// Set prompt (input) tokens.\n    pub fn prompt_tokens(mut self, n: u64) -> Self {\n        self.inner.prompt_tokens = n;\n        self.inner.total_tokens = self.inner.prompt_tokens + self.inner.completion_tokens;\n        self\n    }\n\n    /// Set completion (output) tokens.\n    pub fn completion_tokens(mut self, n: u64) -> Self {\n        self.inner.completion_tokens = n;\n        self.inner.total_tokens = self.inner.prompt_tokens + self.inner.completion_tokens;\n        self\n    }\n\n    /// Set cache hit tokens.\n    pub fn cache_hit_tokens(mut self, n: u64) -> Self {\n        self.inner.cache_hit_tokens = n;\n        self.inner.requests_with_cache = if n > 0 { 1 } else { 0 };\n        self\n    }\n\n    /// Set total time in milliseconds.\n    pub fn time_ms(mut self, ms: u64) -> Self {\n        self.inner.total_time_ms = ms;\n        self\n    }\n\n    /// Set tool call requests count.\n    pub fn tool_calls(mut self, n: u64) -> Self {\n        self.inner.tool_call_requests = n;\n        self\n    }\n\n    /// Set requests with cache hits.\n    pub fn requests_with_cache(mut self, n: u64) -> Self {\n        self.inner.requests_with_cache = n;\n        self\n    }\n\n    /// Build the MetricsSummary.\n    pub fn build(self) -> MetricsSummary {\n        self.inner\n    }\n}\n\nimpl MetricsSummary {\n    /// Get average latency in milliseconds.\n    pub fn average_latency_ms(&self) -> f64 {\n        if self.total_requests == 0 {\n            0.0\n        } else {\n            self.total_time_ms as f64 / self.total_requests as f64\n        }\n    }\n\n    /// Get average tokens per request.\n    pub fn average_tokens_per_request(&self) -> f64 {\n        if self.total_requests == 0 {\n            0.0\n        } else {\n            self.total_tokens as f64 / self.total_requests as f64\n        }\n    }\n\n    /// Total tokens processed per second (OODA-34).\n    ///\n    /// Calculates overall throughput including both input and output tokens.\n    /// Useful for comparing total processing capacity across sessions.\n    ///\n    /// WHY: Standard metric for LLM throughput monitoring and benchmarking.\n    ///\n    /// # Returns\n    /// Tokens per second (0.0 if no time elapsed).\n    pub fn tokens_per_second(&self) -> f64 {\n        if self.total_time_ms == 0 {\n            return 0.0;\n        }\n        (self.total_tokens as f64) / (self.total_time_ms as f64 / 1000.0)\n    }\n\n    /// Output tokens generated per second (OODA-34).\n    ///\n    /// Calculates completion throughput, which is the key metric for\n    /// generation speed comparisons between models and providers.\n    ///\n    /// WHY: Output tokens/sec is the industry standard for LLM speed benchmarks.\n    ///\n    /// # Returns\n    /// Completion tokens per second (0.0 if no time elapsed).\n    pub fn output_tokens_per_second(&self) -> f64 {\n        if self.total_time_ms == 0 {\n            return 0.0;\n        }\n        (self.completion_tokens as f64) / (self.total_time_ms as f64 / 1000.0)\n    }\n\n    /// Token efficiency ratio (OODA-35).\n    ///\n    /// Measures productive output relative to input context.\n    /// Higher values indicate more efficient prompting.\n    ///\n    /// WHY: Low efficiency may indicate context bloat or\n    /// overly verbose system prompts.\n    ///\n    /// # Returns\n    /// Percentage: (completion_tokens / prompt_tokens) * 100\n    ///\n    /// # Interpretation\n    /// - <5%: Heavy context, minimal output\n    /// - 5-15%: Normal for complex coding tasks\n    /// - 15-30%: Efficient prompting\n    /// - >30%: Very efficient (simple tasks)\n    pub fn token_efficiency(&self) -> f64 {\n        if self.prompt_tokens == 0 {\n            return 0.0;\n        }\n        (self.completion_tokens as f64 / self.prompt_tokens as f64) * 100.0\n    }\n\n    /// Cache hit rate as percentage of prompt tokens (OODA-22).\n    ///\n    /// # Returns\n    /// Percentage (0.0 - 100.0) of prompt tokens that were cache hits.\n    /// Returns 0.0 if no prompt tokens were used.\n    ///\n    /// # Example\n    /// ```ignore\n    /// let summary = metrics.get_summary();\n    /// println!(\"Cache hit rate: {:.1}%\", summary.cache_hit_rate());\n    /// ```\n    pub fn cache_hit_rate(&self) -> f64 {\n        if self.prompt_tokens == 0 {\n            0.0\n        } else {\n            (self.cache_hit_tokens as f64 / self.prompt_tokens as f64) * 100.0\n        }\n    }\n\n    /// Percentage of requests that utilized cache (OODA-22).\n    ///\n    /// # Returns\n    /// Percentage (0.0 - 100.0) of requests that had cache hits.\n    pub fn cache_utilization(&self) -> f64 {\n        if self.total_requests == 0 {\n            0.0\n        } else {\n            (self.requests_with_cache as f64 / self.total_requests as f64) * 100.0\n        }\n    }\n\n    /// Estimated cost savings from cache hits (OODA-30).\n    ///\n    /// Calculates the savings based on the difference between cached and\n    /// uncached token costs. Anthropic Claude cached tokens cost ~0.1x of\n    /// uncached tokens, so savings = cache_hit_tokens * 0.9 * cost_per_1k / 1000.\n    ///\n    /// # Arguments\n    /// * `cost_per_1k_prompt` - Cost per 1000 uncached prompt tokens (e.g., 0.003 for $0.003/1k)\n    ///\n    /// # Returns\n    /// Estimated savings in the same currency as cost_per_1k_prompt.\n    ///\n    /// # Example\n    /// ```ignore\n    /// let summary = metrics.get_summary();\n    /// let savings = summary.estimated_savings(0.003); // $0.003/1k tokens\n    /// println!(\"Estimated savings: ${:.4}\", savings);\n    /// ```\n    pub fn estimated_savings(&self, cost_per_1k_prompt: f64) -> f64 {\n        // Cached tokens cost ~10% of uncached tokens (90% savings)\n        let savings_rate = 0.9;\n        (self.cache_hit_tokens as f64 / 1000.0) * cost_per_1k_prompt * savings_rate\n    }\n\n    /// Estimated total cost (OODA-30).\n    ///\n    /// Calculates total cost based on cached and uncached token usage.\n    ///\n    /// # Arguments\n    /// * `cost_per_1k_prompt` - Cost per 1000 prompt tokens\n    /// * `cost_per_1k_completion` - Cost per 1000 completion tokens\n    ///\n    /// # Returns\n    /// Estimated total cost.\n    pub fn estimated_cost(&self, cost_per_1k_prompt: f64, cost_per_1k_completion: f64) -> f64 {\n        // Uncached prompt tokens = prompt_tokens - cache_hit_tokens\n        let uncached_prompt = self.prompt_tokens.saturating_sub(self.cache_hit_tokens);\n\n        // Cached tokens cost 10% of uncached\n        let cached_cost = (self.cache_hit_tokens as f64 / 1000.0) * cost_per_1k_prompt * 0.1;\n        let uncached_cost = (uncached_prompt as f64 / 1000.0) * cost_per_1k_prompt;\n        let completion_cost = (self.completion_tokens as f64 / 1000.0) * cost_per_1k_completion;\n\n        cached_cost + uncached_cost + completion_cost\n    }\n\n    /// Estimated savings using ModelCost pricing (OODA-31).\n    ///\n    /// Convenience method that extracts pricing from a ModelCost struct.\n    /// WHY: Single source of truth for model pricing improves accuracy.\n    ///\n    /// # Arguments\n    /// * `cost` - ModelCost containing input_per_1k pricing\n    ///\n    /// # Returns\n    /// Estimated savings from cache hits in USD.\n    pub fn estimated_savings_for_model(&self, cost: &ModelCost) -> f64 {\n        self.estimated_savings(cost.input_per_1k)\n    }\n\n    /// Estimated cost using ModelCost pricing (OODA-31).\n    ///\n    /// Convenience method that extracts pricing from a ModelCost struct.\n    /// WHY: Single source of truth for model pricing improves accuracy.\n    ///\n    /// # Arguments\n    /// * `cost` - ModelCost containing input_per_1k and output_per_1k pricing\n    ///\n    /// # Returns\n    /// Estimated total cost in USD.\n    pub fn estimated_cost_for_model(&self, cost: &ModelCost) -> f64 {\n        self.estimated_cost(cost.input_per_1k, cost.output_per_1k)\n    }\n}\n\n/// Display implementation for MetricsSummary (OODA-33).\n///\n/// Provides compact single-line format for logging:\n/// `reqs=10 tokens=5000/1000/4000 cache=80.0% latency=150ms tps=66.7`\n///\n/// WHY: Standard Display trait enables easy integration with\n/// logging frameworks, debugging output, and tracing.\nimpl fmt::Display for MetricsSummary {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        write!(\n            f,\n            \"reqs={} tokens={}/{}/{} cache={:.1}% latency={:.0}ms tps={:.1}\",\n            self.total_requests,\n            self.prompt_tokens,\n            self.completion_tokens,\n            self.cache_hit_tokens,\n            self.cache_hit_rate(),\n            self.average_latency_ms(),\n            self.output_tokens_per_second()\n        )\n    }\n}\n\n#[async_trait]\nimpl LLMMiddleware for MetricsLLMMiddleware {\n    fn name(&self) -> &str {\n        \"metrics\"\n    }\n\n    async fn before(&self, _request: &LLMRequest) -> Result<()> {\n        self.total_requests.fetch_add(1, Ordering::Relaxed);\n        Ok(())\n    }\n\n    async fn after(\n        &self,\n        _request: &LLMRequest,\n        response: &LLMResponse,\n        duration_ms: u64,\n    ) -> Result<()> {\n        self.total_tokens\n            .fetch_add(response.total_tokens as u64, Ordering::Relaxed);\n        self.prompt_tokens\n            .fetch_add(response.prompt_tokens as u64, Ordering::Relaxed);\n        self.completion_tokens\n            .fetch_add(response.completion_tokens as u64, Ordering::Relaxed);\n        self.total_time_ms.fetch_add(duration_ms, Ordering::Relaxed);\n\n        if !response.tool_calls.is_empty() {\n            self.tool_call_requests.fetch_add(1, Ordering::Relaxed);\n        }\n\n        // OODA-22: Track cache hit tokens for context engineering visibility\n        // WHY: Enables measurement of KV-cache effectiveness.\n        // High cache hit rates (>80%) indicate successful context engineering.\n        if let Some(cache_hits) = response.cache_hit_tokens {\n            self.cache_hit_tokens\n                .fetch_add(cache_hits as u64, Ordering::Relaxed);\n            if cache_hits > 0 {\n                self.requests_with_cache.fetch_add(1, Ordering::Relaxed);\n            }\n        }\n\n        Ok(())\n    }\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn create_test_request() -> LLMRequest {\n        LLMRequest::new(\n            vec![ChatMessage::user(\"Hello\")],\n            \"test-provider\",\n            \"test-model\",\n        )\n    }\n\n    fn create_test_response() -> LLMResponse {\n        LLMResponse::new(\"Hello back!\", \"test-model\").with_usage(10, 5)\n    }\n\n    #[tokio::test]\n    async fn test_empty_middleware_stack() {\n        let stack = LLMMiddlewareStack::new();\n        assert!(stack.is_empty());\n\n        let request = create_test_request();\n        let response = create_test_response();\n\n        // Should not fail with empty stack\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 100).await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_middleware_stack_with_logging() {\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(Arc::new(LoggingLLMMiddleware::new()));\n\n        assert_eq!(stack.len(), 1);\n\n        let request = create_test_request();\n        let response = create_test_response();\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 100).await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_metrics_middleware() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(metrics.clone());\n\n        let request = create_test_request();\n        let response = create_test_response();\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 150).await.unwrap();\n\n        assert_eq!(metrics.get_total_requests(), 1);\n        assert_eq!(metrics.get_total_tokens(), 15);\n        assert_eq!(metrics.get_average_latency_ms(), 150.0);\n    }\n\n    #[tokio::test]\n    async fn test_multiple_middlewares() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(Arc::new(LoggingLLMMiddleware::new()));\n        stack.add(metrics.clone());\n\n        assert_eq!(stack.len(), 2);\n\n        let request = create_test_request();\n        let response = create_test_response();\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 200).await.unwrap();\n\n        assert_eq!(metrics.get_total_requests(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_metrics_summary() {\n        let metrics = MetricsLLMMiddleware::new();\n\n        let request = create_test_request();\n        let response = create_test_response();\n\n        metrics.before(&request).await.unwrap();\n        metrics.after(&request, &response, 100).await.unwrap();\n\n        metrics.before(&request).await.unwrap();\n        metrics.after(&request, &response, 200).await.unwrap();\n\n        let summary = metrics.get_summary();\n        assert_eq!(summary.total_requests, 2);\n        assert_eq!(summary.total_tokens, 30);\n        assert_eq!(summary.average_latency_ms(), 150.0);\n        assert_eq!(summary.average_tokens_per_request(), 15.0);\n    }\n\n    #[test]\n    fn test_llm_request_builder() {\n        let request = LLMRequest::new(vec![ChatMessage::user(\"Test\")], \"openai\", \"gpt-4\")\n            .with_options(CompletionOptions::with_temperature(0.7));\n\n        assert_eq!(request.provider, \"openai\");\n        assert_eq!(request.model, \"gpt-4\");\n        assert_eq!(request.message_count(), 1);\n        assert_eq!(request.tool_count(), 0);\n        assert!(request.options.is_some());\n    }\n\n    // ========================================================================\n    // OODA-22: Cache Metrics Tests\n    // ========================================================================\n\n    #[tokio::test]\n    async fn test_cache_metrics_tracking() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(metrics.clone());\n\n        let request = create_test_request();\n\n        // Response with cache hits\n        let response = LLMResponse::new(\"Hello\", \"test-model\")\n            .with_usage(100, 20)\n            .with_cache_hit_tokens(80);\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 100).await.unwrap();\n\n        assert_eq!(metrics.get_cache_hit_tokens(), 80);\n        assert_eq!(metrics.get_summary().requests_with_cache, 1);\n\n        // Verify cache hit rate: 80/100 = 80%\n        let rate = metrics.get_cache_hit_rate();\n        assert!((rate - 80.0).abs() < 0.01);\n    }\n\n    #[tokio::test]\n    async fn test_cache_metrics_none() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(metrics.clone());\n\n        let request = create_test_request();\n\n        // Response without cache hits (None)\n        let response = LLMResponse::new(\"Hello\", \"test-model\").with_usage(100, 20);\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 100).await.unwrap();\n\n        assert_eq!(metrics.get_cache_hit_tokens(), 0);\n        assert_eq!(metrics.get_summary().requests_with_cache, 0);\n        assert_eq!(metrics.get_cache_hit_rate(), 0.0);\n    }\n\n    #[tokio::test]\n    async fn test_cache_metrics_zero_hits() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(metrics.clone());\n\n        let request = create_test_request();\n\n        // Response with explicit 0 cache hits\n        let response = LLMResponse::new(\"Hello\", \"test-model\")\n            .with_usage(100, 20)\n            .with_cache_hit_tokens(0);\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 100).await.unwrap();\n\n        assert_eq!(metrics.get_cache_hit_tokens(), 0);\n        // Zero cache hits should not count as \"with cache\"\n        assert_eq!(metrics.get_summary().requests_with_cache, 0);\n    }\n\n    #[tokio::test]\n    async fn test_cache_metrics_aggregation() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(metrics.clone());\n\n        let request = create_test_request();\n\n        // First request: 80 cache hits / 100 prompt tokens\n        let response1 = LLMResponse::new(\"Hello\", \"test-model\")\n            .with_usage(100, 20)\n            .with_cache_hit_tokens(80);\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response1, 100).await.unwrap();\n\n        // Second request: 150 cache hits / 200 prompt tokens\n        let response2 = LLMResponse::new(\"World\", \"test-model\")\n            .with_usage(200, 50)\n            .with_cache_hit_tokens(150);\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response2, 100).await.unwrap();\n\n        // Third request: no cache support\n        let response3 = LLMResponse::new(\"Test\", \"test-model\").with_usage(100, 30);\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response3, 100).await.unwrap();\n\n        let summary = metrics.get_summary();\n        assert_eq!(summary.total_requests, 3);\n        assert_eq!(summary.prompt_tokens, 400); // 100 + 200 + 100\n        assert_eq!(summary.cache_hit_tokens, 230); // 80 + 150\n        assert_eq!(summary.requests_with_cache, 2);\n\n        // Cache hit rate: 230/400 = 57.5%\n        assert!((summary.cache_hit_rate() - 57.5).abs() < 0.01);\n\n        // Cache utilization: 2/3 = 66.67%\n        assert!((summary.cache_utilization() - 66.67).abs() < 0.1);\n    }\n\n    #[test]\n    fn test_cache_hit_rate_calculation() {\n        let summary = MetricsSummary {\n            total_requests: 10,\n            total_tokens: 1500,\n            prompt_tokens: 1000,\n            completion_tokens: 500,\n            total_time_ms: 5000,\n            tool_call_requests: 5,\n            cache_hit_tokens: 800,\n            requests_with_cache: 8,\n        };\n\n        // 800 / 1000 = 80%\n        assert!((summary.cache_hit_rate() - 80.0).abs() < 0.01);\n\n        // 8 / 10 = 80%\n        assert!((summary.cache_utilization() - 80.0).abs() < 0.01);\n    }\n\n    #[test]\n    fn test_cache_hit_rate_zero_prompts() {\n        let summary = MetricsSummary {\n            total_requests: 0,\n            total_tokens: 0,\n            prompt_tokens: 0,\n            completion_tokens: 0,\n            total_time_ms: 0,\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // Should return 0.0 instead of NaN/infinity\n        assert_eq!(summary.cache_hit_rate(), 0.0);\n        assert_eq!(summary.cache_utilization(), 0.0);\n    }\n\n    // ========================================================================\n    // OODA-30: Cost Estimation Tests\n    // ========================================================================\n\n    #[test]\n    fn test_estimated_savings_calculation() {\n        let summary = MetricsSummary {\n            total_requests: 5,\n            total_tokens: 1500,\n            prompt_tokens: 1000,\n            completion_tokens: 500,\n            total_time_ms: 5000,\n            tool_call_requests: 2,\n            cache_hit_tokens: 800,\n            requests_with_cache: 4,\n        };\n\n        // Cost per 1k tokens = $0.003\n        // Savings = (800 / 1000) * 0.003 * 0.9 = 0.00216\n        let savings = summary.estimated_savings(0.003);\n        assert!((savings - 0.00216).abs() < 0.00001);\n    }\n\n    #[test]\n    fn test_estimated_savings_zero_cache() {\n        let summary = MetricsSummary {\n            total_requests: 5,\n            total_tokens: 1500,\n            prompt_tokens: 1000,\n            completion_tokens: 500,\n            total_time_ms: 5000,\n            tool_call_requests: 2,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // No cache hits = no savings\n        assert_eq!(summary.estimated_savings(0.003), 0.0);\n    }\n\n    #[test]\n    fn test_estimated_cost_all_cached() {\n        let summary = MetricsSummary {\n            total_requests: 1,\n            total_tokens: 2000,\n            prompt_tokens: 1000,\n            completion_tokens: 1000,\n            total_time_ms: 100,\n            tool_call_requests: 0,\n            cache_hit_tokens: 1000, // All prompt tokens cached\n            requests_with_cache: 1,\n        };\n\n        // Prompt: $0.003/1k, Completion: $0.015/1k\n        // All 1000 prompt tokens cached = (1000/1000) * 0.003 * 0.1 = 0.0003\n        // Uncached prompt = 0\n        // Completion = (1000/1000) * 0.015 = 0.015\n        // Total = 0.0003 + 0 + 0.015 = 0.0153\n        let cost = summary.estimated_cost(0.003, 0.015);\n        assert!((cost - 0.0153).abs() < 0.00001);\n    }\n\n    #[test]\n    fn test_estimated_cost_no_cache() {\n        let summary = MetricsSummary {\n            total_requests: 1,\n            total_tokens: 2000,\n            prompt_tokens: 1000,\n            completion_tokens: 1000,\n            total_time_ms: 100,\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // Prompt: $0.003/1k, Completion: $0.015/1k\n        // Cached = 0\n        // Uncached prompt = (1000/1000) * 0.003 = 0.003\n        // Completion = (1000/1000) * 0.015 = 0.015\n        // Total = 0 + 0.003 + 0.015 = 0.018\n        let cost = summary.estimated_cost(0.003, 0.015);\n        assert!((cost - 0.018).abs() < 0.00001);\n    }\n\n    #[test]\n    fn test_estimated_cost_partial_cache() {\n        let summary = MetricsSummary {\n            total_requests: 1,\n            total_tokens: 2000,\n            prompt_tokens: 1000,\n            completion_tokens: 1000,\n            total_time_ms: 100,\n            tool_call_requests: 0,\n            cache_hit_tokens: 500, // 50% cached\n            requests_with_cache: 1,\n        };\n\n        // Prompt: $0.003/1k, Completion: $0.015/1k\n        // Cached = (500/1000) * 0.003 * 0.1 = 0.00015\n        // Uncached prompt = (500/1000) * 0.003 = 0.0015\n        // Completion = (1000/1000) * 0.015 = 0.015\n        // Total = 0.00015 + 0.0015 + 0.015 = 0.01665\n        let cost = summary.estimated_cost(0.003, 0.015);\n        assert!((cost - 0.01665).abs() < 0.00001);\n    }\n\n    // ========================================================================\n    // OODA-31: ModelCost Integration Tests\n    // ========================================================================\n\n    #[test]\n    fn test_estimated_savings_for_model() {\n        use crate::model_config::ModelCost;\n\n        let summary = MetricsSummary {\n            total_requests: 5,\n            total_tokens: 1500,\n            prompt_tokens: 1000,\n            completion_tokens: 500,\n            total_time_ms: 5000,\n            tool_call_requests: 2,\n            cache_hit_tokens: 800,\n            requests_with_cache: 4,\n        };\n\n        let model_cost = ModelCost {\n            input_per_1k: 0.003,\n            output_per_1k: 0.015,\n            embedding_per_1k: 0.0,\n            image_per_unit: 0.0,\n            currency: \"USD\".to_string(),\n        };\n\n        // Should delegate to estimated_savings correctly\n        let savings_direct = summary.estimated_savings(0.003);\n        let savings_model = summary.estimated_savings_for_model(&model_cost);\n        assert!((savings_direct - savings_model).abs() < 0.00001);\n    }\n\n    #[test]\n    fn test_estimated_cost_for_model() {\n        use crate::model_config::ModelCost;\n\n        let summary = MetricsSummary {\n            total_requests: 1,\n            total_tokens: 2000,\n            prompt_tokens: 1000,\n            completion_tokens: 1000,\n            total_time_ms: 100,\n            tool_call_requests: 0,\n            cache_hit_tokens: 500,\n            requests_with_cache: 1,\n        };\n\n        let model_cost = ModelCost {\n            input_per_1k: 0.003,\n            output_per_1k: 0.015,\n            embedding_per_1k: 0.0,\n            image_per_unit: 0.0,\n            currency: \"USD\".to_string(),\n        };\n\n        // Should delegate to estimated_cost correctly\n        let cost_direct = summary.estimated_cost(0.003, 0.015);\n        let cost_model = summary.estimated_cost_for_model(&model_cost);\n        assert!((cost_direct - cost_model).abs() < 0.00001);\n    }\n\n    #[test]\n    fn test_model_cost_gpt4_pricing() {\n        use crate::model_config::ModelCost;\n\n        let summary = MetricsSummary {\n            total_requests: 10,\n            total_tokens: 50000,\n            prompt_tokens: 40000,\n            completion_tokens: 10000,\n            total_time_ms: 30000,\n            tool_call_requests: 5,\n            cache_hit_tokens: 35000, // 87.5% cache hit\n            requests_with_cache: 9,\n        };\n\n        // GPT-4o pricing (approximate as of 2025)\n        let gpt4_cost = ModelCost {\n            input_per_1k: 0.0025, // $2.50/1M input\n            output_per_1k: 0.01,  // $10/1M output\n            embedding_per_1k: 0.0,\n            image_per_unit: 0.0,\n            currency: \"USD\".to_string(),\n        };\n\n        // Savings from cache: 35K * 0.9 * $0.0025/1k = $0.07875\n        let savings = summary.estimated_savings_for_model(&gpt4_cost);\n        assert!((savings - 0.07875).abs() < 0.0001);\n\n        // Cost calculation:\n        // Cached: 35K * 0.1 * $0.0025/1k = $0.00875\n        // Uncached prompt: 5K * $0.0025/1k = $0.0125\n        // Completion: 10K * $0.01/1k = $0.10\n        // Total: $0.00875 + $0.0125 + $0.10 = $0.12125\n        let cost = summary.estimated_cost_for_model(&gpt4_cost);\n        assert!((cost - 0.12125).abs() < 0.0001);\n    }\n\n    // ========================================================================\n    // OODA-33: Display Trait Tests\n    // ========================================================================\n\n    #[test]\n    fn test_metrics_summary_display() {\n        let summary = MetricsSummary {\n            total_requests: 10,\n            total_tokens: 5000,\n            prompt_tokens: 4000,\n            completion_tokens: 1000,\n            total_time_ms: 1500,\n            tool_call_requests: 3,\n            cache_hit_tokens: 3200,\n            requests_with_cache: 8,\n        };\n\n        let display = format!(\"{}\", summary);\n\n        // Verify format: reqs=10 tokens=4000/1000/3200 cache=80.0% latency=150ms tps=666.7\n        assert!(display.contains(\"reqs=10\"));\n        assert!(display.contains(\"tokens=4000/1000/3200\"));\n        assert!(display.contains(\"cache=80.0%\")); // 3200/4000 = 80%\n        assert!(display.contains(\"latency=150ms\")); // 1500ms / 10 reqs = 150ms\n        assert!(display.contains(\"tps=\")); // Output tps: 1000 / 1.5s = 666.7\n    }\n\n    #[test]\n    fn test_metrics_summary_display_zero_values() {\n        let summary = MetricsSummary {\n            total_requests: 0,\n            total_tokens: 0,\n            prompt_tokens: 0,\n            completion_tokens: 0,\n            total_time_ms: 0,\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        let display = format!(\"{}\", summary);\n\n        // Should handle zero values gracefully\n        assert!(display.contains(\"reqs=0\"));\n        assert!(display.contains(\"cache=0.0%\"));\n        assert!(display.contains(\"tps=0.0\"));\n    }\n\n    // ========================================================================\n    // OODA-34: Throughput Metrics Tests\n    // ========================================================================\n\n    #[test]\n    fn test_tokens_per_second() {\n        let summary = MetricsSummary {\n            total_requests: 5,\n            total_tokens: 10000, // 10K total tokens\n            prompt_tokens: 8000,\n            completion_tokens: 2000,\n            total_time_ms: 2000, // 2 seconds\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // 10000 tokens / 2 seconds = 5000 tokens/sec\n        assert!((summary.tokens_per_second() - 5000.0).abs() < 0.1);\n    }\n\n    #[test]\n    fn test_output_tokens_per_second() {\n        let summary = MetricsSummary {\n            total_requests: 5,\n            total_tokens: 10000,\n            prompt_tokens: 8000,\n            completion_tokens: 2000, // 2K output tokens\n            total_time_ms: 2000,     // 2 seconds\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // 2000 completion tokens / 2 seconds = 1000 tokens/sec\n        assert!((summary.output_tokens_per_second() - 1000.0).abs() < 0.1);\n    }\n\n    #[test]\n    fn test_throughput_zero_time() {\n        let summary = MetricsSummary {\n            total_requests: 1,\n            total_tokens: 1000,\n            prompt_tokens: 800,\n            completion_tokens: 200,\n            total_time_ms: 0, // Zero time\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // Should return 0.0 for zero time (not NaN or infinity)\n        assert_eq!(summary.tokens_per_second(), 0.0);\n        assert_eq!(summary.output_tokens_per_second(), 0.0);\n    }\n\n    #[test]\n    fn test_throughput_realistic_session() {\n        // Simulate a realistic 30-second agent session\n        let summary = MetricsSummary {\n            total_requests: 20,\n            total_tokens: 50000,      // 50K total\n            prompt_tokens: 40000,     // 40K prompt\n            completion_tokens: 10000, // 10K completion\n            total_time_ms: 30000,     // 30 seconds\n            tool_call_requests: 15,\n            cache_hit_tokens: 35000, // 87.5% cache\n            requests_with_cache: 18,\n        };\n\n        // Total: 50K / 30s = 1666.7 tps\n        assert!((summary.tokens_per_second() - 1666.67).abs() < 1.0);\n\n        // Output: 10K / 30s = 333.3 tps\n        assert!((summary.output_tokens_per_second() - 333.33).abs() < 1.0);\n    }\n\n    // ========================================================================\n    // OODA-35: Token Efficiency Tests\n    // ========================================================================\n\n    #[test]\n    fn test_token_efficiency() {\n        let summary = MetricsSummary {\n            total_requests: 5,\n            total_tokens: 5000,\n            prompt_tokens: 4000,\n            completion_tokens: 1000, // 25% efficiency\n            total_time_ms: 5000,\n            tool_call_requests: 2,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // 1000 / 4000 * 100 = 25%\n        assert!((summary.token_efficiency() - 25.0).abs() < 0.01);\n    }\n\n    #[test]\n    fn test_token_efficiency_low() {\n        let summary = MetricsSummary {\n            total_requests: 1,\n            total_tokens: 10100,\n            prompt_tokens: 10000,   // 10K input\n            completion_tokens: 100, // Only 100 output = 1%\n            total_time_ms: 1000,\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // Low efficiency: 100 / 10000 * 100 = 1%\n        assert!((summary.token_efficiency() - 1.0).abs() < 0.01);\n    }\n\n    #[test]\n    fn test_token_efficiency_zero_prompt() {\n        let summary = MetricsSummary {\n            total_requests: 0,\n            total_tokens: 0,\n            prompt_tokens: 0, // Edge case\n            completion_tokens: 0,\n            total_time_ms: 0,\n            tool_call_requests: 0,\n            cache_hit_tokens: 0,\n            requests_with_cache: 0,\n        };\n\n        // Should return 0.0, not NaN\n        assert_eq!(summary.token_efficiency(), 0.0);\n    }\n\n    // ========================================================================\n    // OODA-36: Builder Pattern Tests\n    // ========================================================================\n\n    #[test]\n    fn test_builder_basic() {\n        let summary = MetricsSummaryBuilder::new()\n            .requests(10)\n            .prompt_tokens(4000)\n            .completion_tokens(1000)\n            .time_ms(1500)\n            .build();\n\n        assert_eq!(summary.total_requests, 10);\n        assert_eq!(summary.prompt_tokens, 4000);\n        assert_eq!(summary.completion_tokens, 1000);\n        assert_eq!(summary.total_tokens, 5000); // Auto-calculated\n        assert_eq!(summary.total_time_ms, 1500);\n    }\n\n    #[test]\n    fn test_builder_with_cache() {\n        let summary = MetricsSummaryBuilder::new()\n            .requests(5)\n            .prompt_tokens(10000)\n            .completion_tokens(2000)\n            .cache_hit_tokens(8000)\n            .build();\n\n        // Cache hit rate should be 80%\n        assert!((summary.cache_hit_rate() - 80.0).abs() < 0.01);\n        // Auto-set requests_with_cache\n        assert_eq!(summary.requests_with_cache, 1);\n    }\n\n    #[test]\n    fn test_builder_default() {\n        let summary = MetricsSummaryBuilder::new().build();\n\n        assert_eq!(summary.total_requests, 0);\n        assert_eq!(summary.prompt_tokens, 0);\n        assert_eq!(summary.completion_tokens, 0);\n        assert_eq!(summary.cache_hit_tokens, 0);\n    }\n\n    #[test]\n    fn test_builder_metrics_calculation() {\n        let summary = MetricsSummaryBuilder::new()\n            .requests(10)\n            .prompt_tokens(5000)\n            .completion_tokens(1000)\n            .cache_hit_tokens(4000)\n            .time_ms(2000)\n            .build();\n\n        // Latency: 2000ms / 10 = 200ms\n        assert!((summary.average_latency_ms() - 200.0).abs() < 0.01);\n\n        // TPS: 1000 / 2s = 500\n        assert!((summary.output_tokens_per_second() - 500.0).abs() < 0.01);\n\n        // Efficiency: 1000 / 5000 * 100 = 20%\n        assert!((summary.token_efficiency() - 20.0).abs() < 0.01);\n    }\n\n    #[test]\n    fn test_llm_request_with_tools() {\n        let tools = vec![ToolDefinition::function(\n            \"get_weather\",\n            \"Get weather data\",\n            serde_json::json!({}),\n        )];\n        let request = LLMRequest::new(vec![ChatMessage::user(\"Hi\")], \"p\", \"m\")\n            .with_tools(tools);\n        assert_eq!(request.tool_count(), 1);\n        assert!(request.tools.is_some());\n    }\n\n    #[tokio::test]\n    async fn test_metrics_tool_call_tracking() {\n        let metrics = Arc::new(MetricsLLMMiddleware::new());\n        let mut stack = LLMMiddlewareStack::new();\n        stack.add(metrics.clone());\n\n        let request = create_test_request();\n\n        // Response with tool calls\n        let mut response = LLMResponse::new(\"\", \"m\").with_usage(10, 5);\n        response.tool_calls = vec![crate::traits::ToolCall {\n            id: \"call_1\".to_string(),\n            call_type: \"function\".to_string(),\n            function: crate::traits::FunctionCall {\n                name: \"test\".to_string(),\n                arguments: \"{}\".to_string(),\n            },\n        }];\n\n        stack.before(&request).await.unwrap();\n        stack.after(&request, &response, 100).await.unwrap();\n\n        let summary = metrics.get_summary();\n        assert_eq!(summary.tool_call_requests, 1);\n    }\n\n    #[tokio::test]\n    async fn test_logging_middleware_debug_level() {\n        let logging = LoggingLLMMiddleware::with_level(LogLevel::Debug);\n        assert_eq!(logging.name(), \"logging\");\n\n        let request = create_test_request();\n        let response = create_test_response();\n\n        // Should not panic at Debug level\n        logging.before(&request).await.unwrap();\n        logging.after(&request, &response, 100).await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_logging_middleware_trace_level() {\n        let logging = LoggingLLMMiddleware::with_level(LogLevel::Trace);\n        let request = create_test_request();\n        let response = create_test_response();\n\n        logging.before(&request).await.unwrap();\n        logging.after(&request, &response, 100).await.unwrap();\n    }\n\n    #[test]\n    fn test_logging_middleware_default() {\n        let logging = LoggingLLMMiddleware::default();\n        assert_eq!(logging.name(), \"logging\");\n    }\n\n    #[test]\n    fn test_metrics_middleware_default() {\n        let metrics = MetricsLLMMiddleware::default();\n        assert_eq!(metrics.get_total_requests(), 0);\n        assert_eq!(metrics.get_total_tokens(), 0);\n    }\n\n    #[test]\n    fn test_middleware_stack_default() {\n        let stack = LLMMiddlewareStack::default();\n        assert!(stack.is_empty());\n        assert_eq!(stack.len(), 0);\n    }\n\n    #[test]\n    fn test_builder_tool_calls() {\n        let summary = MetricsSummaryBuilder::new()\n            .requests(5)\n            .tool_calls(3)\n            .build();\n        assert_eq!(summary.tool_call_requests, 3);\n    }\n\n    #[test]\n    fn test_builder_requests_with_cache_override() {\n        let summary = MetricsSummaryBuilder::new()\n            .requests(10)\n            .requests_with_cache(7)\n            .build();\n        assert_eq!(summary.requests_with_cache, 7);\n    }\n\n    #[tokio::test]\n    async fn test_logging_debug_long_message() {\n        let logging = LoggingLLMMiddleware::with_level(LogLevel::Debug);\n        // Message longer than 100 chars\n        let long_msg = \"x\".repeat(200);\n        let request = LLMRequest::new(\n            vec![ChatMessage::user(&long_msg)],\n            \"p\",\n            \"m\",\n        );\n        // Should truncate without panic\n        logging.before(&request).await.unwrap();\n    }\n\n    #[tokio::test]\n    async fn test_logging_debug_long_response() {\n        let logging = LoggingLLMMiddleware::with_level(LogLevel::Debug);\n        let request = create_test_request();\n        let long_content = \"y\".repeat(300);\n        let response = LLMResponse::new(&long_content, \"m\").with_usage(10, 5);\n        // Should truncate without panic\n        logging.after(&request, &response, 100).await.unwrap();\n    }\n\n    #[test]\n    fn test_metrics_cache_hit_rate_no_prompts() {\n        let metrics = MetricsLLMMiddleware::new();\n        // No requests made\n        assert_eq!(metrics.get_cache_hit_rate(), 0.0);\n    }\n}\n","traces":[{"line":79,"address":[],"length":0,"stats":{"Line":16}},{"line":88,"address":[],"length":0,"stats":{"Line":48}},{"line":89,"address":[],"length":0,"stats":{"Line":16}},{"line":94,"address":[],"length":0,"stats":{"Line":1}},{"line":95,"address":[],"length":0,"stats":{"Line":2}},{"line":96,"address":[],"length":0,"stats":{"Line":1}},{"line":100,"address":[],"length":0,"stats":{"Line":1}},{"line":101,"address":[],"length":0,"stats":{"Line":2}},{"line":102,"address":[],"length":0,"stats":{"Line":1}},{"line":106,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":2}},{"line":111,"address":[],"length":0,"stats":{"Line":2}},{"line":112,"address":[],"length":0,"stats":{"Line":10}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":9}},{"line":174,"address":[],"length":0,"stats":{"Line":9}},{"line":179,"address":[],"length":0,"stats":{"Line":9}},{"line":180,"address":[],"length":0,"stats":{"Line":27}},{"line":184,"address":[],"length":0,"stats":{"Line":3}},{"line":185,"address":[],"length":0,"stats":{"Line":6}},{"line":189,"address":[],"length":0,"stats":{"Line":2}},{"line":190,"address":[],"length":0,"stats":{"Line":4}},{"line":194,"address":[],"length":0,"stats":{"Line":22}},{"line":195,"address":[],"length":0,"stats":{"Line":33}},{"line":196,"address":[],"length":0,"stats":{"Line":22}},{"line":198,"address":[],"length":0,"stats":{"Line":11}},{"line":202,"address":[],"length":0,"stats":{"Line":11}},{"line":208,"address":[],"length":0,"stats":{"Line":33}},{"line":209,"address":[],"length":0,"stats":{"Line":44}},{"line":211,"address":[],"length":0,"stats":{"Line":11}},{"line":239,"address":[],"length":0,"stats":{"Line":3}},{"line":246,"address":[],"length":0,"stats":{"Line":4}},{"line":252,"address":[],"length":0,"stats":{"Line":1}},{"line":253,"address":[],"length":0,"stats":{"Line":1}},{"line":259,"address":[],"length":0,"stats":{"Line":2}},{"line":260,"address":[],"length":0,"stats":{"Line":2}},{"line":263,"address":[],"length":0,"stats":{"Line":5}},{"line":275,"address":[],"length":0,"stats":{"Line":2}},{"line":276,"address":[],"length":0,"stats":{"Line":4}},{"line":277,"address":[],"length":0,"stats":{"Line":5}},{"line":278,"address":[],"length":0,"stats":{"Line":2}},{"line":280,"address":[],"length":0,"stats":{"Line":2}},{"line":282,"address":[],"length":0,"stats":{"Line":4}},{"line":378,"address":[],"length":0,"stats":{"Line":10}},{"line":380,"address":[],"length":0,"stats":{"Line":20}},{"line":381,"address":[],"length":0,"stats":{"Line":20}},{"line":382,"address":[],"length":0,"stats":{"Line":20}},{"line":383,"address":[],"length":0,"stats":{"Line":20}},{"line":384,"address":[],"length":0,"stats":{"Line":20}},{"line":385,"address":[],"length":0,"stats":{"Line":20}},{"line":386,"address":[],"length":0,"stats":{"Line":10}},{"line":387,"address":[],"length":0,"stats":{"Line":10}},{"line":392,"address":[],"length":0,"stats":{"Line":3}},{"line":393,"address":[],"length":0,"stats":{"Line":9}},{"line":397,"address":[],"length":0,"stats":{"Line":2}},{"line":398,"address":[],"length":0,"stats":{"Line":6}},{"line":402,"address":[],"length":0,"stats":{"Line":1}},{"line":403,"address":[],"length":0,"stats":{"Line":4}},{"line":404,"address":[],"length":0,"stats":{"Line":1}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":407,"address":[],"length":0,"stats":{"Line":2}},{"line":412,"address":[],"length":0,"stats":{"Line":3}},{"line":413,"address":[],"length":0,"stats":{"Line":9}},{"line":421,"address":[],"length":0,"stats":{"Line":3}},{"line":422,"address":[],"length":0,"stats":{"Line":12}},{"line":423,"address":[],"length":0,"stats":{"Line":3}},{"line":424,"address":[],"length":0,"stats":{"Line":1}},{"line":426,"address":[],"length":0,"stats":{"Line":4}},{"line":431,"address":[],"length":0,"stats":{"Line":6}},{"line":433,"address":[],"length":0,"stats":{"Line":24}},{"line":434,"address":[],"length":0,"stats":{"Line":24}},{"line":435,"address":[],"length":0,"stats":{"Line":24}},{"line":436,"address":[],"length":0,"stats":{"Line":24}},{"line":437,"address":[],"length":0,"stats":{"Line":24}},{"line":438,"address":[],"length":0,"stats":{"Line":24}},{"line":439,"address":[],"length":0,"stats":{"Line":24}},{"line":440,"address":[],"length":0,"stats":{"Line":12}},{"line":446,"address":[],"length":0,"stats":{"Line":1}},{"line":447,"address":[],"length":0,"stats":{"Line":1}},{"line":497,"address":[],"length":0,"stats":{"Line":6}},{"line":498,"address":[],"length":0,"stats":{"Line":6}},{"line":502,"address":[],"length":0,"stats":{"Line":5}},{"line":503,"address":[],"length":0,"stats":{"Line":5}},{"line":504,"address":[],"length":0,"stats":{"Line":5}},{"line":508,"address":[],"length":0,"stats":{"Line":3}},{"line":509,"address":[],"length":0,"stats":{"Line":3}},{"line":510,"address":[],"length":0,"stats":{"Line":3}},{"line":511,"address":[],"length":0,"stats":{"Line":3}},{"line":515,"address":[],"length":0,"stats":{"Line":3}},{"line":516,"address":[],"length":0,"stats":{"Line":3}},{"line":517,"address":[],"length":0,"stats":{"Line":3}},{"line":518,"address":[],"length":0,"stats":{"Line":3}},{"line":522,"address":[],"length":0,"stats":{"Line":2}},{"line":523,"address":[],"length":0,"stats":{"Line":2}},{"line":524,"address":[],"length":0,"stats":{"Line":4}},{"line":525,"address":[],"length":0,"stats":{"Line":2}},{"line":529,"address":[],"length":0,"stats":{"Line":2}},{"line":530,"address":[],"length":0,"stats":{"Line":2}},{"line":531,"address":[],"length":0,"stats":{"Line":2}},{"line":535,"address":[],"length":0,"stats":{"Line":1}},{"line":536,"address":[],"length":0,"stats":{"Line":1}},{"line":537,"address":[],"length":0,"stats":{"Line":1}},{"line":541,"address":[],"length":0,"stats":{"Line":1}},{"line":542,"address":[],"length":0,"stats":{"Line":1}},{"line":543,"address":[],"length":0,"stats":{"Line":1}},{"line":547,"address":[],"length":0,"stats":{"Line":6}},{"line":548,"address":[],"length":0,"stats":{"Line":6}},{"line":554,"address":[],"length":0,"stats":{"Line":4}},{"line":555,"address":[],"length":0,"stats":{"Line":4}},{"line":556,"address":[],"length":0,"stats":{"Line":1}},{"line":558,"address":[],"length":0,"stats":{"Line":3}},{"line":563,"address":[],"length":0,"stats":{"Line":1}},{"line":564,"address":[],"length":0,"stats":{"Line":1}},{"line":565,"address":[],"length":0,"stats":{"Line":0}},{"line":567,"address":[],"length":0,"stats":{"Line":1}},{"line":580,"address":[],"length":0,"stats":{"Line":3}},{"line":581,"address":[],"length":0,"stats":{"Line":3}},{"line":582,"address":[],"length":0,"stats":{"Line":1}},{"line":584,"address":[],"length":0,"stats":{"Line":2}},{"line":596,"address":[],"length":0,"stats":{"Line":6}},{"line":597,"address":[],"length":0,"stats":{"Line":6}},{"line":598,"address":[],"length":0,"stats":{"Line":2}},{"line":600,"address":[],"length":0,"stats":{"Line":4}},{"line":619,"address":[],"length":0,"stats":{"Line":4}},{"line":620,"address":[],"length":0,"stats":{"Line":4}},{"line":621,"address":[],"length":0,"stats":{"Line":1}},{"line":623,"address":[],"length":0,"stats":{"Line":3}},{"line":637,"address":[],"length":0,"stats":{"Line":6}},{"line":638,"address":[],"length":0,"stats":{"Line":6}},{"line":639,"address":[],"length":0,"stats":{"Line":2}},{"line":641,"address":[],"length":0,"stats":{"Line":4}},{"line":649,"address":[],"length":0,"stats":{"Line":3}},{"line":650,"address":[],"length":0,"stats":{"Line":3}},{"line":651,"address":[],"length":0,"stats":{"Line":1}},{"line":653,"address":[],"length":0,"stats":{"Line":2}},{"line":675,"address":[],"length":0,"stats":{"Line":5}},{"line":677,"address":[],"length":0,"stats":{"Line":10}},{"line":678,"address":[],"length":0,"stats":{"Line":5}},{"line":691,"address":[],"length":0,"stats":{"Line":6}},{"line":693,"address":[],"length":0,"stats":{"Line":24}},{"line":696,"address":[],"length":0,"stats":{"Line":12}},{"line":697,"address":[],"length":0,"stats":{"Line":12}},{"line":698,"address":[],"length":0,"stats":{"Line":12}},{"line":700,"address":[],"length":0,"stats":{"Line":6}},{"line":713,"address":[],"length":0,"stats":{"Line":2}},{"line":714,"address":[],"length":0,"stats":{"Line":6}},{"line":727,"address":[],"length":0,"stats":{"Line":2}},{"line":728,"address":[],"length":0,"stats":{"Line":8}},{"line":740,"address":[],"length":0,"stats":{"Line":2}},{"line":741,"address":[],"length":0,"stats":{"Line":2}},{"line":742,"address":[],"length":0,"stats":{"Line":2}},{"line":748,"address":[],"length":0,"stats":{"Line":4}},{"line":749,"address":[],"length":0,"stats":{"Line":4}},{"line":750,"address":[],"length":0,"stats":{"Line":4}},{"line":757,"address":[],"length":0,"stats":{"Line":0}},{"line":758,"address":[],"length":0,"stats":{"Line":0}},{"line":761,"address":[],"length":0,"stats":{"Line":11}},{"line":787,"address":[],"length":0,"stats":{"Line":11}}],"covered":153,"coverable":162},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","model_config.rs"],"content":"//! Model Configuration Module\n//!\n//! This module provides TOML-based configuration for LLM and embedding models,\n//! including model cards with capabilities (vision, context length, costs, etc.).\n//!\n//! @implements SPEC-032: Ollama/LM Studio provider support - Model cards configuration\n//! @iteration OODA Loop #51-55 - TOML Config Schema Design\n//!\n//! # Overview\n//!\n//! The configuration file (`models.toml`) defines:\n//! - Available LLM providers and models\n//! - Embedding providers and models  \n//! - Model capabilities (vision, max tokens, context length)\n//! - Cost information (per 1K tokens)\n//! - Default selections for LLM and embedding\n//!\n//! # Configuration File Location\n//!\n//! The config file is loaded from (in order of priority):\n//! 1. `EDGEQUAKE_MODELS_CONFIG` environment variable\n//! 2. `./models.toml` (current working directory)\n//! 3. `~/.edgequake/models.toml` (user config)\n//! 4. Built-in default configuration\n//!\n//! # Example Configuration\n//!\n//! ```toml\n//! [defaults]\n//! llm_provider = \"openai\"\n//! llm_model = \"gpt-4o-mini\"\n//! embedding_provider = \"openai\"\n//! embedding_model = \"text-embedding-3-small\"\n//!\n//! [[providers]]\n//! name = \"openai\"\n//! display_name = \"OpenAI\"\n//! type = \"openai\"\n//! api_key_env = \"OPENAI_API_KEY\"\n//!\n//! [[providers.models]]\n//! name = \"gpt-4o\"\n//! display_name = \"GPT-4 Omni\"\n//! type = \"llm\"\n//! context_length = 128000\n//! max_output_tokens = 16384\n//! supports_vision = true\n//! supports_function_calling = true\n//! cost_per_1k_input = 0.0025\n//! cost_per_1k_output = 0.01\n//! ```\n\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse std::path::Path;\nuse thiserror::Error;\n\n// ============================================================================\n// Error Types\n// ============================================================================\n\n/// Errors that can occur during model configuration loading.\n#[derive(Error, Debug)]\npub enum ModelConfigError {\n    /// Failed to read configuration file.\n    #[error(\"Failed to read config file: {0}\")]\n    IoError(#[from] std::io::Error),\n\n    /// Failed to parse TOML configuration.\n    #[error(\"Failed to parse TOML config: {0}\")]\n    ParseError(String),\n\n    /// Invalid configuration (missing required fields, invalid values).\n    #[error(\"Invalid configuration: {0}\")]\n    ValidationError(String),\n\n    /// Provider not found in configuration.\n    #[error(\"Provider not found: {0}\")]\n    ProviderNotFound(String),\n\n    /// Model not found in configuration.\n    #[error(\"Model not found: {0}\")]\n    ModelNotFound(String),\n}\n\n// ============================================================================\n// Model Types\n// ============================================================================\n\n/// Type of model (LLM for chat/completion, Embedding for vectors).\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, Default)]\n#[serde(rename_all = \"lowercase\")]\npub enum ModelType {\n    /// Language model for chat/completion.\n    #[default]\n    Llm,\n    /// Embedding model for vector generation.\n    Embedding,\n    /// Multi-modal model supporting both.\n    Multimodal,\n}\n\nimpl std::fmt::Display for ModelType {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        match self {\n            ModelType::Llm => write!(f, \"llm\"),\n            ModelType::Embedding => write!(f, \"embedding\"),\n            ModelType::Multimodal => write!(f, \"multimodal\"),\n        }\n    }\n}\n\n/// Provider type for API compatibility.\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize, Default)]\n#[serde(rename_all = \"lowercase\")]\npub enum ProviderType {\n    /// OpenAI API.\n    #[default]\n    OpenAI,\n    /// Ollama local server.\n    Ollama,\n    /// LM Studio local server.\n    LMStudio,\n    /// Azure OpenAI.\n    Azure,\n    /// Anthropic Claude.\n    Anthropic,\n    /// OpenRouter (200+ models).\n    OpenRouter,\n    /// Generic OpenAI-compatible API.\n    OpenAICompatible,\n    /// Mock provider for testing.\n    Mock,\n}\n\nimpl std::fmt::Display for ProviderType {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        match self {\n            ProviderType::OpenAI => write!(f, \"openai\"),\n            ProviderType::Ollama => write!(f, \"ollama\"),\n            ProviderType::LMStudio => write!(f, \"lmstudio\"),\n            ProviderType::Azure => write!(f, \"azure\"),\n            ProviderType::Anthropic => write!(f, \"anthropic\"),\n            ProviderType::OpenRouter => write!(f, \"openrouter\"),\n            ProviderType::OpenAICompatible => write!(f, \"openai_compatible\"),\n            ProviderType::Mock => write!(f, \"mock\"),\n        }\n    }\n}\n\n// ============================================================================\n// Model Capabilities\n// ============================================================================\n\n/// Capabilities of a specific model.\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct ModelCapabilities {\n    /// Maximum context length (input + output tokens).\n    #[serde(default)]\n    pub context_length: usize,\n\n    /// Maximum output tokens the model can generate.\n    #[serde(default)]\n    pub max_output_tokens: usize,\n\n    /// Whether the model supports vision/image input.\n    #[serde(default)]\n    pub supports_vision: bool,\n\n    /// Whether the model supports function/tool calling.\n    #[serde(default)]\n    pub supports_function_calling: bool,\n\n    /// Whether the model supports structured JSON output.\n    #[serde(default)]\n    pub supports_json_mode: bool,\n\n    /// Whether the model supports streaming responses.\n    #[serde(default = \"default_true\")]\n    pub supports_streaming: bool,\n\n    /// Whether the model supports system messages.\n    #[serde(default = \"default_true\")]\n    pub supports_system_message: bool,\n\n    /// Embedding dimension (only for embedding models).\n    #[serde(default)]\n    pub embedding_dimension: usize,\n\n    /// Maximum tokens for embedding input.\n    #[serde(default)]\n    pub max_embedding_tokens: usize,\n\n    /// OODA-200: Whether the model supports thinking/chain-of-thought mode.\n    #[serde(default)]\n    pub supports_thinking: bool,\n\n    /// OODA-200: Whether the model supports web search tool.\n    #[serde(default)]\n    pub supports_web_search: bool,\n\n    /// OODA-200: Recommended temperature for this model (0.0-1.0).\n    #[serde(default = \"default_temperature\")]\n    pub default_temperature: f32,\n}\n\nfn default_temperature() -> f32 {\n    1.0\n}\n\nfn default_true() -> bool {\n    true\n}\n\n// ============================================================================\n// Cost Information\n// ============================================================================\n\n/// Cost information for a model (per 1000 tokens).\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct ModelCost {\n    /// Cost per 1000 input tokens (USD).\n    #[serde(default)]\n    pub input_per_1k: f64,\n\n    /// Cost per 1000 output tokens (USD).\n    #[serde(default)]\n    pub output_per_1k: f64,\n\n    /// Cost per 1000 embedding tokens (USD, for embedding models).\n    #[serde(default)]\n    pub embedding_per_1k: f64,\n\n    /// Cost per image processed (USD, for vision models).\n    #[serde(default)]\n    pub image_per_unit: f64,\n\n    /// Currency code (default: USD).\n    #[serde(default = \"default_currency\")]\n    pub currency: String,\n}\n\nfn default_currency() -> String {\n    \"USD\".to_string()\n}\n\n// ============================================================================\n// Model Card\n// ============================================================================\n\n/// Complete model card with all metadata.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ModelCard {\n    /// Unique model identifier (e.g., \"gpt-4o\", \"nomic-embed-text\").\n    pub name: String,\n\n    /// Human-readable display name.\n    pub display_name: String,\n\n    /// Model type (LLM, Embedding, Multimodal).\n    #[serde(default)]\n    pub model_type: ModelType,\n\n    /// Model capabilities.\n    #[serde(default)]\n    pub capabilities: ModelCapabilities,\n\n    /// Cost information.\n    #[serde(default)]\n    pub cost: ModelCost,\n\n    /// Optional description of the model.\n    #[serde(default)]\n    pub description: String,\n\n    /// Release date or version.\n    #[serde(default)]\n    pub version: String,\n\n    /// Whether the model is deprecated.\n    #[serde(default)]\n    pub deprecated: bool,\n\n    /// Recommended replacement if deprecated.\n    #[serde(default)]\n    pub replacement: Option<String>,\n\n    /// Tags for categorization (e.g., \"recommended\", \"fast\", \"vision\").\n    #[serde(default)]\n    pub tags: Vec<String>,\n\n    /// Additional metadata as key-value pairs.\n    #[serde(default)]\n    pub metadata: HashMap<String, String>,\n}\n\nimpl Default for ModelCard {\n    fn default() -> Self {\n        Self {\n            name: \"unknown\".to_string(),\n            display_name: \"Unknown Model\".to_string(),\n            model_type: ModelType::Llm,\n            capabilities: ModelCapabilities::default(),\n            cost: ModelCost::default(),\n            description: String::new(),\n            version: String::new(),\n            deprecated: false,\n            replacement: None,\n            tags: Vec::new(),\n            metadata: HashMap::new(),\n        }\n    }\n}\n\n// ============================================================================\n// Provider Configuration\n// ============================================================================\n\n/// Configuration for a provider (OpenAI, Ollama, LM Studio, etc.).\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ProviderConfig {\n    /// Unique provider identifier (e.g., \"openai\", \"ollama\").\n    pub name: String,\n\n    /// Human-readable display name.\n    pub display_name: String,\n\n    /// Provider type for API compatibility.\n    #[serde(rename = \"type\")]\n    pub provider_type: ProviderType,\n\n    /// Environment variable name for API key (if required).\n    #[serde(default)]\n    pub api_key_env: Option<String>,\n\n    /// Base URL for the provider API.\n    #[serde(default)]\n    pub base_url: Option<String>,\n\n    /// Environment variable for base URL override.\n    #[serde(default)]\n    pub base_url_env: Option<String>,\n\n    /// Default model for LLM operations.\n    #[serde(default)]\n    pub default_llm_model: Option<String>,\n\n    /// Default model for embedding operations.\n    #[serde(default)]\n    pub default_embedding_model: Option<String>,\n\n    /// List of available models for this provider.\n    #[serde(default)]\n    pub models: Vec<ModelCard>,\n\n    /// Whether this provider is enabled.\n    #[serde(default = \"default_true\")]\n    pub enabled: bool,\n\n    /// Priority for auto-selection (lower = higher priority).\n    #[serde(default = \"default_priority\")]\n    pub priority: u32,\n\n    /// Description of the provider.\n    #[serde(default)]\n    pub description: String,\n\n    /// Additional provider-specific settings.\n    #[serde(default)]\n    pub settings: HashMap<String, String>,\n\n    /// OODA-200: Custom HTTP headers for API requests.\n    /// Useful for providers that require additional headers like Accept-Language.\n    #[serde(default)]\n    pub headers: HashMap<String, String>,\n\n    /// OODA-200: Request timeout in seconds (default: 120).\n    #[serde(default = \"default_timeout\")]\n    pub timeout_seconds: u64,\n\n    /// OODA-200: Whether this provider supports thinking/reasoning mode (e.g., Z.ai GLM-4.5).\n    #[serde(default)]\n    pub supports_thinking: bool,\n}\n\nfn default_timeout() -> u64 {\n    120\n}\n\nfn default_priority() -> u32 {\n    100\n}\n\nimpl Default for ProviderConfig {\n    fn default() -> Self {\n        Self {\n            name: \"unknown\".to_string(),\n            display_name: \"Unknown Provider\".to_string(),\n            provider_type: ProviderType::OpenAI,\n            api_key_env: None,\n            base_url: None,\n            base_url_env: None,\n            default_llm_model: None,\n            default_embedding_model: None,\n            models: Vec::new(),\n            enabled: true,\n            priority: 100,\n            description: String::new(),\n            settings: HashMap::new(),\n            headers: HashMap::new(),\n            timeout_seconds: default_timeout(),\n            supports_thinking: false,\n        }\n    }\n}\n\n// ============================================================================\n// Default Configuration\n// ============================================================================\n\n/// Default provider and model selections.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct DefaultsConfig {\n    /// Default LLM provider name.\n    #[serde(default = \"default_llm_provider\")]\n    pub llm_provider: String,\n\n    /// Default LLM model name.\n    #[serde(default = \"default_llm_model\")]\n    pub llm_model: String,\n\n    /// Default embedding provider name.\n    #[serde(default = \"default_embedding_provider\")]\n    pub embedding_provider: String,\n\n    /// Default embedding model name.\n    #[serde(default = \"default_embedding_model\")]\n    pub embedding_model: String,\n}\n\nfn default_llm_provider() -> String {\n    \"openai\".to_string()\n}\n\nfn default_llm_model() -> String {\n    \"gpt-4o-mini\".to_string()\n}\n\nfn default_embedding_provider() -> String {\n    \"openai\".to_string()\n}\n\nfn default_embedding_model() -> String {\n    \"text-embedding-3-small\".to_string()\n}\n\nimpl Default for DefaultsConfig {\n    fn default() -> Self {\n        Self {\n            llm_provider: default_llm_provider(),\n            llm_model: default_llm_model(),\n            embedding_provider: default_embedding_provider(),\n            embedding_model: default_embedding_model(),\n        }\n    }\n}\n\n// ============================================================================\n// Root Configuration\n// ============================================================================\n\n/// Root configuration structure for models.toml.\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\npub struct ModelsConfig {\n    /// Default selections.\n    #[serde(default)]\n    pub defaults: DefaultsConfig,\n\n    /// List of configured providers.\n    #[serde(default)]\n    pub providers: Vec<ProviderConfig>,\n}\n\nimpl ModelsConfig {\n    /// Load configuration from the default location.\n    ///\n    /// Searches in order:\n    /// 1. `EDGEQUAKE_MODELS_CONFIG` environment variable\n    /// 2. `./models.toml`\n    /// 3. `~/.edgequake/models.toml`\n    /// 4. Built-in defaults\n    pub fn load() -> Result<Self, ModelConfigError> {\n        // Check environment variable first\n        if let Ok(path) = std::env::var(\"EDGEQUAKE_MODELS_CONFIG\") {\n            if Path::new(&path).exists() {\n                return Self::from_file(&path);\n            }\n        }\n\n        // Check current directory\n        let local_path = Path::new(\"models.toml\");\n        if local_path.exists() {\n            return Self::from_file(local_path);\n        }\n\n        // Check user config directory\n        if let Some(home) = dirs::home_dir() {\n            let user_path = home.join(\".edgequake\").join(\"models.toml\");\n            if user_path.exists() {\n                return Self::from_file(&user_path);\n            }\n        }\n\n        // Fall back to built-in defaults\n        Ok(Self::builtin_defaults())\n    }\n\n    /// Load configuration from a specific file path.\n    pub fn from_file(path: impl AsRef<Path>) -> Result<Self, ModelConfigError> {\n        let content = std::fs::read_to_string(path.as_ref())?;\n        Self::from_toml(&content)\n    }\n\n    /// Parse configuration from TOML string.\n    pub fn from_toml(toml_str: &str) -> Result<Self, ModelConfigError> {\n        toml::from_str(toml_str).map_err(|e| ModelConfigError::ParseError(e.to_string()))\n    }\n\n    /// Serialize configuration to TOML string.\n    pub fn to_toml(&self) -> Result<String, ModelConfigError> {\n        toml::to_string_pretty(self).map_err(|e| ModelConfigError::ParseError(e.to_string()))\n    }\n\n    /// Save configuration to a file.\n    pub fn save(&self, path: impl AsRef<Path>) -> Result<(), ModelConfigError> {\n        let toml_str = self.to_toml()?;\n        std::fs::write(path.as_ref(), toml_str)?;\n        Ok(())\n    }\n\n    /// Get built-in default configuration with common providers.\n    pub fn builtin_defaults() -> Self {\n        Self {\n            defaults: DefaultsConfig::default(),\n            providers: vec![\n                // OpenAI provider\n                ProviderConfig {\n                    name: \"openai\".to_string(),\n                    display_name: \"OpenAI\".to_string(),\n                    provider_type: ProviderType::OpenAI,\n                    api_key_env: Some(\"OPENAI_API_KEY\".to_string()),\n                    base_url: Some(\"https://api.openai.com/v1\".to_string()),\n                    base_url_env: Some(\"OPENAI_API_BASE\".to_string()),\n                    default_llm_model: Some(\"gpt-4o-mini\".to_string()),\n                    default_embedding_model: Some(\"text-embedding-3-small\".to_string()),\n                    priority: 10,\n                    models: vec![\n                        ModelCard {\n                            name: \"gpt-4o\".to_string(),\n                            display_name: \"GPT-4 Omni\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_json_mode: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.0025,\n                                output_per_1k: 0.01,\n                                ..Default::default()\n                            },\n                            description: \"Most capable GPT-4 model with vision support\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"gpt-4o-mini\".to_string(),\n                            display_name: \"GPT-4 Omni Mini\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_json_mode: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.00015,\n                                output_per_1k: 0.0006,\n                                ..Default::default()\n                            },\n                            description: \"Cost-effective GPT-4 variant\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"text-embedding-3-small\".to_string(),\n                            display_name: \"Embedding 3 Small\".to_string(),\n                            model_type: ModelType::Embedding,\n                            capabilities: ModelCapabilities {\n                                embedding_dimension: 1536,\n                                max_embedding_tokens: 8191,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                embedding_per_1k: 0.00002,\n                                ..Default::default()\n                            },\n                            description: \"Efficient embedding model\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"text-embedding-3-large\".to_string(),\n                            display_name: \"Embedding 3 Large\".to_string(),\n                            model_type: ModelType::Embedding,\n                            capabilities: ModelCapabilities {\n                                embedding_dimension: 3072,\n                                max_embedding_tokens: 8191,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                embedding_per_1k: 0.00013,\n                                ..Default::default()\n                            },\n                            description: \"High-quality embedding model\".to_string(),\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n                // OODA-32: Anthropic provider (Claude models)\n                // WHY: Direct Anthropic API access for Claude models\n                // Supports: claude-sonnet-4.5, claude-3.5-sonnet, claude-3.5-haiku\n                ProviderConfig {\n                    name: \"anthropic\".to_string(),\n                    display_name: \"Anthropic (Claude)\".to_string(),\n                    provider_type: ProviderType::Anthropic,\n                    api_key_env: Some(\"ANTHROPIC_API_KEY\".to_string()),\n                    base_url: Some(\"https://api.anthropic.com\".to_string()),\n                    base_url_env: Some(\"ANTHROPIC_API_BASE\".to_string()),\n                    default_llm_model: Some(\"claude-sonnet-4-5-20250929\".to_string()),\n                    default_embedding_model: None, // Anthropic doesn't support embeddings\n                    priority: 15, // Higher than OpenAI (10), prefer Claude when available\n                    models: vec![\n                        ModelCard {\n                            name: \"claude-sonnet-4-5-20250929\".to_string(),\n                            display_name: \"Claude Sonnet 4.5\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 200000,\n                                max_output_tokens: 8192,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.003,\n                                output_per_1k: 0.015,\n                                ..Default::default()\n                            },\n                            description: \"Anthropic's most capable model with excellent coding\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"claude-3-5-sonnet-20241022\".to_string(),\n                            display_name: \"Claude 3.5 Sonnet\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 200000,\n                                max_output_tokens: 8192,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.003,\n                                output_per_1k: 0.015,\n                                ..Default::default()\n                            },\n                            description: \"Previous generation Sonnet, stable and reliable\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"claude-3-5-haiku-20241022\".to_string(),\n                            display_name: \"Claude 3.5 Haiku\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 200000,\n                                max_output_tokens: 8192,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.0008,\n                                output_per_1k: 0.004,\n                                ..Default::default()\n                            },\n                            description: \"Fast and cost-effective Claude model\".to_string(),\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n                // Ollama provider\n                ProviderConfig {\n                    name: \"ollama\".to_string(),\n                    display_name: \"Ollama (Local)\".to_string(),\n                    provider_type: ProviderType::Ollama,\n                    base_url: Some(\"http://localhost:11434\".to_string()),\n                    base_url_env: Some(\"OLLAMA_HOST\".to_string()),\n                    default_llm_model: Some(\"gemma3:12b\".to_string()),\n                    default_embedding_model: Some(\"nomic-embed-text\".to_string()),\n                    priority: 20,\n                    models: vec![\n                        ModelCard {\n                            name: \"gemma3:12b\".to_string(),\n                            display_name: \"Gemma 3 12B\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 8192,\n                                max_output_tokens: 4096,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(), // Free for local\n                            description: \"Google's Gemma 3 12B parameter model\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"llama3.3:70b\".to_string(),\n                            display_name: \"Llama 3.3 70B\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 131072,\n                                max_output_tokens: 8192,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Meta's Llama 3.3 70B with extended context\".to_string(),\n                            ..Default::default()\n                        },\n                        // OODA-32: Add qwen3-coder and gpt-oss:20b for coding tasks\n                        ModelCard {\n                            name: \"qwen3-coder\".to_string(),\n                            display_name: \"Qwen3 Coder\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 32768,\n                                max_output_tokens: 8192,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Qwen3 optimized for coding tasks\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"gpt-oss:20b\".to_string(),\n                            display_name: \"GPT-OSS 20B\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 32768,\n                                max_output_tokens: 8192,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Open-source GPT model, 20B parameters\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"nomic-embed-text\".to_string(),\n                            display_name: \"Nomic Embed Text\".to_string(),\n                            model_type: ModelType::Embedding,\n                            capabilities: ModelCapabilities {\n                                embedding_dimension: 768,\n                                max_embedding_tokens: 8192,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"High-quality local embedding model\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"mxbai-embed-large\".to_string(),\n                            display_name: \"MxBai Embed Large\".to_string(),\n                            model_type: ModelType::Embedding,\n                            capabilities: ModelCapabilities {\n                                embedding_dimension: 1024,\n                                max_embedding_tokens: 512,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Large embedding model with 1024 dimensions\".to_string(),\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n                // LM Studio provider\n                ProviderConfig {\n                    name: \"lmstudio\".to_string(),\n                    display_name: \"LM Studio (Local)\".to_string(),\n                    provider_type: ProviderType::LMStudio,\n                    base_url: Some(\"http://localhost:1234/v1\".to_string()),\n                    base_url_env: Some(\"LMSTUDIO_HOST\".to_string()),\n                    default_llm_model: Some(\"local-model\".to_string()),\n                    default_embedding_model: Some(\"nomic-embed-text-v1.5\".to_string()),\n                    priority: 30,\n                    models: vec![\n                        ModelCard {\n                            name: \"local-model\".to_string(),\n                            display_name: \"Local LM Studio Model\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 4096,\n                                max_output_tokens: 2048,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Currently loaded model in LM Studio\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"nomic-embed-text-v1.5\".to_string(),\n                            display_name: \"Nomic Embed Text v1.5\".to_string(),\n                            model_type: ModelType::Embedding,\n                            capabilities: ModelCapabilities {\n                                embedding_dimension: 768,\n                                max_embedding_tokens: 8192,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Nomic embedding model for LM Studio\".to_string(),\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n                // Z.ai provider (OpenAI-compatible)\n                // OODA-200: Configurable OpenAI-compatible providers\n                ProviderConfig {\n                    name: \"zai\".to_string(),\n                    display_name: \"Z.AI Platform\".to_string(),\n                    provider_type: ProviderType::OpenAICompatible,\n                    api_key_env: Some(\"ZAI_API_KEY\".to_string()),\n                    base_url: Some(\"https://api.z.ai/api/paas/v4\".to_string()),\n                    default_llm_model: Some(\"glm-4.7-flash\".to_string()),\n                    priority: 15,\n                    headers: {\n                        let mut h = std::collections::HashMap::new();\n                        h.insert(\"Accept-Language\".to_string(), \"en-US,en\".to_string());\n                        h\n                    },\n                    supports_thinking: true,\n                    models: vec![\n                        ModelCard {\n                            name: \"glm-4.7\".to_string(),\n                            display_name: \"GLM-4.7 (Premium)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_json_mode: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.2,\n                                output_per_1k: 1.1,\n                                ..Default::default()\n                            },\n                            description: \"Z.ai's flagship model with thinking mode\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"coding\".to_string(), \"agent\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"glm-4.7-flash\".to_string(),\n                            display_name: \"GLM-4.7 Flash (Fast)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 8192,\n                                supports_function_calling: true,\n                                supports_json_mode: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.0,\n                                output_per_1k: 0.0,\n                                ..Default::default()\n                            },\n                            description: \"Free, fast Z.ai model\".to_string(),\n                            tags: vec![\"fast\".to_string(), \"free\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"glm-4.5\".to_string(),\n                            display_name: \"GLM-4.5 (Reasoning)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 96000,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost {\n                                input_per_1k: 0.2,\n                                output_per_1k: 1.1,\n                                ..Default::default()\n                            },\n                            description: \"Z.ai reasoning model for complex tasks\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"coding\".to_string()],\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n                // POE provider (OpenAI-compatible)\n                // OODA-200: Configurable OpenAI-compatible providers\n                // Updated 2026-01-24: Use correct POE API model names (PascalCase)\n                // Reference: https://creator.poe.com/api-reference/listModels\n                ProviderConfig {\n                    name: \"poe\".to_string(),\n                    display_name: \"POE Platform\".to_string(),\n                    provider_type: ProviderType::OpenAICompatible,\n                    api_key_env: Some(\"POE_API_KEY\".to_string()),\n                    base_url: Some(\"https://api.poe.com/v1\".to_string()),\n                    default_llm_model: Some(\"Claude-Haiku-4.5\".to_string()),\n                    priority: 16,\n                    models: vec![\n                        // Claude models via POE (Anthropic's latest)\n                        ModelCard {\n                            name: \"Claude-Sonnet-4.5\".to_string(),\n                            display_name: \"Claude Sonnet 4.5 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 200000,\n                                max_output_tokens: 16384,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Claude Sonnet 4.5 - Anthropic's most advanced model via POE\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"coding\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"Claude-Haiku-4.5\".to_string(),\n                            display_name: \"Claude Haiku 4.5 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 200000,\n                                max_output_tokens: 8192,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Claude Haiku 4.5 - Fast and efficient with frontier intelligence via POE\".to_string(),\n                            tags: vec![\"fast\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"Claude-Opus-4.1\".to_string(),\n                            display_name: \"Claude Opus 4.1 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 200000,\n                                max_output_tokens: 16384,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Claude Opus 4.1 - Anthropic's premium model for complex tasks via POE\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"pro\".to_string()],\n                            ..Default::default()\n                        },\n                        // GPT models via POE (OpenAI's latest)\n                        ModelCard {\n                            name: \"GPT-5-Pro\".to_string(),\n                            display_name: \"GPT-5 Pro (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 32768,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"GPT-5 Pro - OpenAI's flagship model with extended reasoning via POE\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"pro\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"GPT-5\".to_string(),\n                            display_name: \"GPT-5 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"GPT-5 - OpenAI's next-generation model via POE\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"GPT-5-Codex\".to_string(),\n                            display_name: \"GPT-5 Codex (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"GPT-5 Codex - Specialized for software engineering tasks via POE\".to_string(),\n                            tags: vec![\"coding\".to_string()],\n                            ..Default::default()\n                        },\n                        // Grok models via POE (xAI)\n                        ModelCard {\n                            name: \"Grok-4\".to_string(),\n                            display_name: \"Grok-4 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 131072,\n                                max_output_tokens: 32768,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Grok-4 - xAI's most intelligent language model via POE\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"coding\".to_string()],\n                            ..Default::default()\n                        },\n                        // DeepSeek models via POE\n                        ModelCard {\n                            name: \"DeepSeek-R1\".to_string(),\n                            display_name: \"DeepSeek R1 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                supports_thinking: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"DeepSeek R1 - Top open-source reasoning model via POE\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"open-source\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"DeepSeek-V3\".to_string(),\n                            display_name: \"DeepSeek V3 (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 128000,\n                                max_output_tokens: 16384,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"DeepSeek V3 - Advanced open-source model via POE\".to_string(),\n                            tags: vec![\"open-source\".to_string()],\n                            ..Default::default()\n                        },\n                        // Gemini models via POE (Google)\n                        ModelCard {\n                            name: \"Gemini-2.5-Pro\".to_string(),\n                            display_name: \"Gemini 2.5 Pro (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 1000000,\n                                max_output_tokens: 65536,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Gemini 2.5 Pro - Google's advanced model with web search via POE\".to_string(),\n                            tags: vec![\"reasoning\".to_string(), \"web-search\".to_string()],\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"Gemini-2.5-Flash\".to_string(),\n                            display_name: \"Gemini 2.5 Flash (POE)\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 1000000,\n                                max_output_tokens: 65536,\n                                supports_vision: true,\n                                supports_function_calling: true,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Gemini 2.5 Flash - Fast variant with large context via POE\".to_string(),\n                            tags: vec![\"fast\".to_string()],\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n                // Mock provider for testing\n                ProviderConfig {\n                    name: \"mock\".to_string(),\n                    display_name: \"Mock (Testing)\".to_string(),\n                    provider_type: ProviderType::Mock,\n                    default_llm_model: Some(\"mock-model\".to_string()),\n                    default_embedding_model: Some(\"mock-embedding\".to_string()),\n                    priority: 1000,\n                    models: vec![\n                        ModelCard {\n                            name: \"mock-model\".to_string(),\n                            display_name: \"Mock LLM\".to_string(),\n                            model_type: ModelType::Llm,\n                            capabilities: ModelCapabilities {\n                                context_length: 4096,\n                                max_output_tokens: 2048,\n                                supports_streaming: true,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Mock model for testing\".to_string(),\n                            ..Default::default()\n                        },\n                        ModelCard {\n                            name: \"mock-embedding\".to_string(),\n                            display_name: \"Mock Embedding\".to_string(),\n                            model_type: ModelType::Embedding,\n                            capabilities: ModelCapabilities {\n                                embedding_dimension: 1536,\n                                max_embedding_tokens: 512,\n                                ..Default::default()\n                            },\n                            cost: ModelCost::default(),\n                            description: \"Mock embedding for testing\".to_string(),\n                            ..Default::default()\n                        },\n                    ],\n                    ..Default::default()\n                },\n            ],\n        }\n    }\n\n    /// Get a provider by name.\n    pub fn get_provider(&self, name: &str) -> Option<&ProviderConfig> {\n        self.providers.iter().find(|p| p.name == name)\n    }\n\n    /// Get a model by provider and model name.\n    pub fn get_model(&self, provider: &str, model: &str) -> Option<&ModelCard> {\n        self.get_provider(provider)\n            .and_then(|p| p.models.iter().find(|m| m.name == model))\n    }\n\n    /// OODA-200: Find a provider by model name.\n    ///\n    /// Searches all enabled providers for a model with the given name.\n    /// Returns the provider config if found, None otherwise.\n    ///\n    /// # Arguments\n    ///\n    /// * `model_name` - The model identifier to search for (e.g., \"glm-4.7\")\n    ///\n    /// # Returns\n    ///\n    /// The provider configuration containing this model, or None.\n    pub fn find_provider_for_model(&self, model_name: &str) -> Option<&ProviderConfig> {\n        self.providers.iter().find(|p| {\n            p.enabled && p.models.iter().any(|m| m.name == model_name)\n        })\n    }\n\n    /// OODA-200: Find a provider and model by model name.\n    ///\n    /// Searches all enabled providers for a model with the given name.\n    /// Returns both the provider config and model card if found.\n    ///\n    /// # Arguments\n    ///\n    /// * `model_name` - The model identifier to search for (e.g., \"glm-4.7\")\n    ///\n    /// # Returns\n    ///\n    /// A tuple of (ProviderConfig, ModelCard) if found, None otherwise.\n    pub fn find_provider_and_model(&self, model_name: &str) -> Option<(&ProviderConfig, &ModelCard)> {\n        for provider in &self.providers {\n            if !provider.enabled {\n                continue;\n            }\n            for model in &provider.models {\n                if model.name == model_name {\n                    return Some((provider, model));\n                }\n            }\n        }\n        None\n    }\n\n    /// Get all LLM models across all providers.\n    pub fn all_llm_models(&self) -> Vec<(&ProviderConfig, &ModelCard)> {\n        self.providers\n            .iter()\n            .filter(|p| p.enabled)\n            .flat_map(|p| {\n                p.models\n                    .iter()\n                    .filter(|m| matches!(m.model_type, ModelType::Llm | ModelType::Multimodal))\n                    .map(move |m| (p, m))\n            })\n            .collect()\n    }\n\n    /// Get all embedding models across all providers.\n    pub fn all_embedding_models(&self) -> Vec<(&ProviderConfig, &ModelCard)> {\n        self.providers\n            .iter()\n            .filter(|p| p.enabled)\n            .flat_map(|p| {\n                p.models\n                    .iter()\n                    .filter(|m| {\n                        matches!(m.model_type, ModelType::Embedding | ModelType::Multimodal)\n                    })\n                    .map(move |m| (p, m))\n            })\n            .collect()\n    }\n\n    /// Get the default LLM provider and model.\n    pub fn default_llm(&self) -> Option<(&ProviderConfig, &ModelCard)> {\n        self.get_model(&self.defaults.llm_provider, &self.defaults.llm_model)\n            .and_then(|m| {\n                self.get_provider(&self.defaults.llm_provider)\n                    .map(|p| (p, m))\n            })\n    }\n\n    /// Get the default embedding provider and model.\n    pub fn default_embedding(&self) -> Option<(&ProviderConfig, &ModelCard)> {\n        self.get_model(\n            &self.defaults.embedding_provider,\n            &self.defaults.embedding_model,\n        )\n        .and_then(|m| {\n            self.get_provider(&self.defaults.embedding_provider)\n                .map(|p| (p, m))\n        })\n    }\n\n    /// Validate the configuration.\n    pub fn validate(&self) -> Result<(), ModelConfigError> {\n        // Check that default providers exist\n        if self.get_provider(&self.defaults.llm_provider).is_none() {\n            return Err(ModelConfigError::ValidationError(format!(\n                \"Default LLM provider '{}' not found in providers list\",\n                self.defaults.llm_provider\n            )));\n        }\n\n        if self\n            .get_provider(&self.defaults.embedding_provider)\n            .is_none()\n        {\n            return Err(ModelConfigError::ValidationError(format!(\n                \"Default embedding provider '{}' not found in providers list\",\n                self.defaults.embedding_provider\n            )));\n        }\n\n        // Check that default models exist\n        if self\n            .get_model(&self.defaults.llm_provider, &self.defaults.llm_model)\n            .is_none()\n        {\n            return Err(ModelConfigError::ValidationError(format!(\n                \"Default LLM model '{}' not found in provider '{}'\",\n                self.defaults.llm_model, self.defaults.llm_provider\n            )));\n        }\n\n        if self\n            .get_model(\n                &self.defaults.embedding_provider,\n                &self.defaults.embedding_model,\n            )\n            .is_none()\n        {\n            return Err(ModelConfigError::ValidationError(format!(\n                \"Default embedding model '{}' not found in provider '{}'\",\n                self.defaults.embedding_model, self.defaults.embedding_provider\n            )));\n        }\n\n        // Check for duplicate provider names\n        let mut seen_providers = std::collections::HashSet::new();\n        for provider in &self.providers {\n            if !seen_providers.insert(&provider.name) {\n                return Err(ModelConfigError::ValidationError(format!(\n                    \"Duplicate provider name: '{}'\",\n                    provider.name\n                )));\n            }\n\n            // Check for duplicate model names within a provider\n            let mut seen_models = std::collections::HashSet::new();\n            for model in &provider.models {\n                if !seen_models.insert(&model.name) {\n                    return Err(ModelConfigError::ValidationError(format!(\n                        \"Duplicate model name '{}' in provider '{}'\",\n                        model.name, provider.name\n                    )));\n                }\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_builtin_defaults() {\n        let config = ModelsConfig::builtin_defaults();\n        assert!(config.validate().is_ok());\n        assert!(!config.providers.is_empty());\n    }\n\n    #[test]\n    fn test_get_provider() {\n        let config = ModelsConfig::builtin_defaults();\n        assert!(config.get_provider(\"openai\").is_some());\n        assert!(config.get_provider(\"ollama\").is_some());\n        assert!(config.get_provider(\"nonexistent\").is_none());\n    }\n\n    #[test]\n    fn test_get_model() {\n        let config = ModelsConfig::builtin_defaults();\n        assert!(config.get_model(\"openai\", \"gpt-4o\").is_some());\n        assert!(config.get_model(\"ollama\", \"nomic-embed-text\").is_some());\n        assert!(config.get_model(\"openai\", \"nonexistent\").is_none());\n    }\n\n    #[test]\n    fn test_all_llm_models() {\n        let config = ModelsConfig::builtin_defaults();\n        let llm_models = config.all_llm_models();\n        assert!(!llm_models.is_empty());\n        assert!(llm_models.iter().any(|(_, m)| m.name == \"gpt-4o\"));\n    }\n\n    #[test]\n    fn test_all_embedding_models() {\n        let config = ModelsConfig::builtin_defaults();\n        let embedding_models = config.all_embedding_models();\n        assert!(!embedding_models.is_empty());\n        assert!(embedding_models\n            .iter()\n            .any(|(_, m)| m.name == \"text-embedding-3-small\"));\n    }\n\n    #[test]\n    fn test_toml_roundtrip() {\n        let config = ModelsConfig::builtin_defaults();\n        let toml_str = config.to_toml().expect(\"Failed to serialize\");\n        let parsed: ModelsConfig = ModelsConfig::from_toml(&toml_str).expect(\"Failed to parse\");\n        assert_eq!(config.providers.len(), parsed.providers.len());\n    }\n\n    #[test]\n    fn test_model_capabilities() {\n        let config = ModelsConfig::builtin_defaults();\n        let gpt4o = config\n            .get_model(\"openai\", \"gpt-4o\")\n            .expect(\"gpt-4o should exist\");\n        assert!(gpt4o.capabilities.supports_vision);\n        assert!(gpt4o.capabilities.supports_function_calling);\n        assert_eq!(gpt4o.capabilities.context_length, 128000);\n    }\n\n    #[test]\n    fn test_embedding_dimensions() {\n        let config = ModelsConfig::builtin_defaults();\n\n        let openai_embed = config\n            .get_model(\"openai\", \"text-embedding-3-small\")\n            .unwrap();\n        assert_eq!(openai_embed.capabilities.embedding_dimension, 1536);\n\n        let ollama_embed = config.get_model(\"ollama\", \"nomic-embed-text\").unwrap();\n        assert_eq!(ollama_embed.capabilities.embedding_dimension, 768);\n    }\n\n    #[test]\n    fn test_validation_duplicate_provider() {\n        let mut config = ModelsConfig::builtin_defaults();\n        config.providers.push(config.providers[0].clone());\n        assert!(config.validate().is_err());\n    }\n\n    #[test]\n    fn test_parse_models_toml_file() {\n        // Read the actual models.toml file from the project root\n        let manifest_dir = std::env::var(\"CARGO_MANIFEST_DIR\").unwrap();\n        let toml_path = std::path::Path::new(&manifest_dir)\n            .parent() // crates/\n            .unwrap()\n            .parent() // edgequake/\n            .unwrap()\n            .join(\"models.toml\");\n\n        if toml_path.exists() {\n            let content = std::fs::read_to_string(&toml_path).expect(\"Failed to read models.toml\");\n            let config = ModelsConfig::from_toml(&content).expect(\"Failed to parse models.toml\");\n\n            // Validate the parsed config\n            assert!(config.validate().is_ok(), \"models.toml failed validation\");\n\n            // Check we have expected providers\n            assert!(\n                config.get_provider(\"openai\").is_some(),\n                \"OpenAI provider should exist\"\n            );\n            assert!(\n                config.get_provider(\"ollama\").is_some(),\n                \"Ollama provider should exist\"\n            );\n            assert!(\n                config.get_provider(\"lmstudio\").is_some(),\n                \"LM Studio provider should exist\"\n            );\n            assert!(\n                config.get_provider(\"mock\").is_some(),\n                \"Mock provider should exist\"\n            );\n\n            // Check default selections are set\n            assert!(!config.defaults.llm_provider.is_empty());\n            assert!(!config.defaults.llm_model.is_empty());\n            assert!(!config.defaults.embedding_provider.is_empty());\n            assert!(!config.defaults.embedding_model.is_empty());\n\n            // Check we have LLM and embedding models\n            let llm_models = config.all_llm_models();\n            let embedding_models = config.all_embedding_models();\n            assert!(!llm_models.is_empty(), \"Should have LLM models\");\n            assert!(!embedding_models.is_empty(), \"Should have embedding models\");\n        }\n    }\n\n    #[test]\n    fn test_provider_priorities() {\n        let config = ModelsConfig::builtin_defaults();\n        let mut priorities: Vec<(String, u32)> = config\n            .providers\n            .iter()\n            .map(|p| (p.name.clone(), p.priority))\n            .collect();\n        priorities.sort_by_key(|(_, p)| *p);\n\n        // Lower priority means higher preference\n        // OpenAI should have lower priority number than Mock\n        let openai_prio = config.get_provider(\"openai\").unwrap().priority;\n        let mock_prio = config.get_provider(\"mock\").unwrap().priority;\n        assert!(\n            openai_prio < mock_prio,\n            \"OpenAI should have higher priority than mock\"\n        );\n    }\n\n    // ====================================================================\n    // Display impl tests\n    // ====================================================================\n\n    #[test]\n    fn test_model_type_display() {\n        assert_eq!(ModelType::Llm.to_string(), \"llm\");\n        assert_eq!(ModelType::Embedding.to_string(), \"embedding\");\n        assert_eq!(ModelType::Multimodal.to_string(), \"multimodal\");\n    }\n\n    #[test]\n    fn test_provider_type_display() {\n        assert_eq!(ProviderType::OpenAI.to_string(), \"openai\");\n        assert_eq!(ProviderType::Ollama.to_string(), \"ollama\");\n        assert_eq!(ProviderType::LMStudio.to_string(), \"lmstudio\");\n        assert_eq!(ProviderType::Azure.to_string(), \"azure\");\n        assert_eq!(ProviderType::Anthropic.to_string(), \"anthropic\");\n        assert_eq!(ProviderType::OpenRouter.to_string(), \"openrouter\");\n        assert_eq!(ProviderType::OpenAICompatible.to_string(), \"openai_compatible\");\n        assert_eq!(ProviderType::Mock.to_string(), \"mock\");\n    }\n\n    // ====================================================================\n    // Default impl tests\n    // ====================================================================\n\n    #[test]\n    fn test_model_type_default() {\n        assert_eq!(ModelType::default(), ModelType::Llm);\n    }\n\n    #[test]\n    fn test_provider_type_default() {\n        assert_eq!(ProviderType::default(), ProviderType::OpenAI);\n    }\n\n    #[test]\n    fn test_model_card_default() {\n        let card = ModelCard::default();\n        assert_eq!(card.name, \"unknown\");\n        assert_eq!(card.display_name, \"Unknown Model\");\n        assert_eq!(card.model_type, ModelType::Llm);\n        assert!(!card.deprecated);\n        assert!(card.replacement.is_none());\n        assert!(card.tags.is_empty());\n    }\n\n    #[test]\n    fn test_provider_config_default() {\n        let config = ProviderConfig::default();\n        assert_eq!(config.name, \"unknown\");\n        assert!(config.enabled);\n        assert_eq!(config.priority, 100);\n        assert_eq!(config.timeout_seconds, 120);\n        assert!(config.api_key_env.is_none());\n    }\n\n    #[test]\n    fn test_defaults_config_default() {\n        let defaults = DefaultsConfig::default();\n        assert_eq!(defaults.llm_provider, \"openai\");\n        assert_eq!(defaults.llm_model, \"gpt-4o-mini\");\n        assert_eq!(defaults.embedding_provider, \"openai\");\n        assert_eq!(defaults.embedding_model, \"text-embedding-3-small\");\n    }\n\n    #[test]\n    fn test_model_capabilities_default() {\n        let caps = ModelCapabilities::default();\n        assert_eq!(caps.context_length, 0);\n        assert!(!caps.supports_vision);\n        assert!(!caps.supports_function_calling);\n    }\n\n    #[test]\n    fn test_model_cost_default() {\n        let cost = ModelCost::default();\n        assert_eq!(cost.input_per_1k, 0.0);\n        assert_eq!(cost.output_per_1k, 0.0);\n    }\n\n    // ====================================================================\n    // Find methods\n    // ====================================================================\n\n    #[test]\n    fn test_find_provider_for_model() {\n        let config = ModelsConfig::builtin_defaults();\n        let provider = config.find_provider_for_model(\"gpt-4o\");\n        assert!(provider.is_some());\n        assert_eq!(provider.unwrap().name, \"openai\");\n    }\n\n    #[test]\n    fn test_find_provider_for_model_not_found() {\n        let config = ModelsConfig::builtin_defaults();\n        assert!(config.find_provider_for_model(\"nonexistent-model-xyz\").is_none());\n    }\n\n    #[test]\n    fn test_find_provider_and_model() {\n        let config = ModelsConfig::builtin_defaults();\n        let result = config.find_provider_and_model(\"gpt-4o\");\n        assert!(result.is_some());\n        let (provider, model) = result.unwrap();\n        assert_eq!(provider.name, \"openai\");\n        assert_eq!(model.name, \"gpt-4o\");\n    }\n\n    #[test]\n    fn test_find_provider_and_model_not_found() {\n        let config = ModelsConfig::builtin_defaults();\n        assert!(config.find_provider_and_model(\"nonexistent-xyz\").is_none());\n    }\n\n    // ====================================================================\n    // Default model selection\n    // ====================================================================\n\n    #[test]\n    fn test_default_llm() {\n        let config = ModelsConfig::builtin_defaults();\n        let result = config.default_llm();\n        assert!(result.is_some());\n        let (provider, model) = result.unwrap();\n        assert_eq!(provider.name, \"openai\");\n        assert_eq!(model.name, \"gpt-4o-mini\");\n    }\n\n    #[test]\n    fn test_default_embedding() {\n        let config = ModelsConfig::builtin_defaults();\n        let result = config.default_embedding();\n        assert!(result.is_some());\n        let (provider, model) = result.unwrap();\n        assert_eq!(provider.name, \"openai\");\n        assert_eq!(model.name, \"text-embedding-3-small\");\n    }\n\n    // ====================================================================\n    // Error type tests\n    // ====================================================================\n\n    #[test]\n    fn test_model_config_error_display() {\n        let err = ModelConfigError::ProviderNotFound(\"test\".to_string());\n        assert!(err.to_string().contains(\"test\"));\n\n        let err = ModelConfigError::ModelNotFound(\"gpt-5\".to_string());\n        assert!(err.to_string().contains(\"gpt-5\"));\n\n        let err = ModelConfigError::ValidationError(\"missing field\".to_string());\n        assert!(err.to_string().contains(\"missing field\"));\n\n        let err = ModelConfigError::ParseError(\"bad toml\".to_string());\n        assert!(err.to_string().contains(\"bad toml\"));\n    }\n\n    // ====================================================================\n    // TOML parsing edge cases\n    // ====================================================================\n\n    #[test]\n    fn test_from_toml_invalid() {\n        let result = ModelsConfig::from_toml(\"this is not valid toml {{{\");\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_from_toml_empty() {\n        let config = ModelsConfig::from_toml(\"\").unwrap();\n        assert!(config.providers.is_empty());\n    }\n\n    #[test]\n    fn test_models_config_default() {\n        let config = ModelsConfig::default();\n        assert!(config.providers.is_empty());\n    }\n\n    #[test]\n    fn test_validation_empty_config() {\n        let config = ModelsConfig::default();\n        // Empty config should fail validation because default providers are missing\n        assert!(config.validate().is_err());\n    }\n}\n","traces":[{"line":104,"address":[],"length":0,"stats":{"Line":3}},{"line":105,"address":[],"length":0,"stats":{"Line":3}},{"line":106,"address":[],"length":0,"stats":{"Line":3}},{"line":107,"address":[],"length":0,"stats":{"Line":3}},{"line":108,"address":[],"length":0,"stats":{"Line":3}},{"line":137,"address":[],"length":0,"stats":{"Line":8}},{"line":138,"address":[],"length":0,"stats":{"Line":8}},{"line":139,"address":[],"length":0,"stats":{"Line":3}},{"line":140,"address":[],"length":0,"stats":{"Line":3}},{"line":141,"address":[],"length":0,"stats":{"Line":3}},{"line":142,"address":[],"length":0,"stats":{"Line":3}},{"line":143,"address":[],"length":0,"stats":{"Line":3}},{"line":144,"address":[],"length":0,"stats":{"Line":3}},{"line":145,"address":[],"length":0,"stats":{"Line":3}},{"line":146,"address":[],"length":0,"stats":{"Line":3}},{"line":207,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":709}},{"line":300,"address":[],"length":0,"stats":{"Line":2127}},{"line":301,"address":[],"length":0,"stats":{"Line":2127}},{"line":303,"address":[],"length":0,"stats":{"Line":1418}},{"line":304,"address":[],"length":0,"stats":{"Line":1418}},{"line":305,"address":[],"length":0,"stats":{"Line":1418}},{"line":306,"address":[],"length":0,"stats":{"Line":1418}},{"line":309,"address":[],"length":0,"stats":{"Line":709}},{"line":310,"address":[],"length":0,"stats":{"Line":709}},{"line":386,"address":[],"length":0,"stats":{"Line":140}},{"line":387,"address":[],"length":0,"stats":{"Line":140}},{"line":390,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":140}},{"line":397,"address":[],"length":0,"stats":{"Line":420}},{"line":398,"address":[],"length":0,"stats":{"Line":420}},{"line":405,"address":[],"length":0,"stats":{"Line":280}},{"line":408,"address":[],"length":0,"stats":{"Line":280}},{"line":409,"address":[],"length":0,"stats":{"Line":280}},{"line":410,"address":[],"length":0,"stats":{"Line":140}},{"line":411,"address":[],"length":0,"stats":{"Line":140}},{"line":441,"address":[],"length":0,"stats":{"Line":20}},{"line":442,"address":[],"length":0,"stats":{"Line":40}},{"line":445,"address":[],"length":0,"stats":{"Line":20}},{"line":446,"address":[],"length":0,"stats":{"Line":40}},{"line":449,"address":[],"length":0,"stats":{"Line":20}},{"line":450,"address":[],"length":0,"stats":{"Line":40}},{"line":453,"address":[],"length":0,"stats":{"Line":20}},{"line":454,"address":[],"length":0,"stats":{"Line":40}},{"line":458,"address":[],"length":0,"stats":{"Line":20}},{"line":460,"address":[],"length":0,"stats":{"Line":40}},{"line":461,"address":[],"length":0,"stats":{"Line":40}},{"line":462,"address":[],"length":0,"stats":{"Line":20}},{"line":463,"address":[],"length":0,"stats":{"Line":20}},{"line":492,"address":[],"length":0,"stats":{"Line":0}},{"line":494,"address":[],"length":0,"stats":{"Line":0}},{"line":495,"address":[],"length":0,"stats":{"Line":0}},{"line":496,"address":[],"length":0,"stats":{"Line":0}},{"line":501,"address":[],"length":0,"stats":{"Line":0}},{"line":502,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":508,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":515,"address":[],"length":0,"stats":{"Line":0}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":520,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":525,"address":[],"length":0,"stats":{"Line":3}},{"line":526,"address":[],"length":0,"stats":{"Line":11}},{"line":530,"address":[],"length":0,"stats":{"Line":1}},{"line":531,"address":[],"length":0,"stats":{"Line":3}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":537,"address":[],"length":0,"stats":{"Line":0}},{"line":538,"address":[],"length":0,"stats":{"Line":0}},{"line":542,"address":[],"length":0,"stats":{"Line":16}},{"line":544,"address":[],"length":0,"stats":{"Line":32}},{"line":545,"address":[],"length":0,"stats":{"Line":32}},{"line":1190,"address":[],"length":0,"stats":{"Line":24}},{"line":1191,"address":[],"length":0,"stats":{"Line":130}},{"line":1195,"address":[],"length":0,"stats":{"Line":12}},{"line":1196,"address":[],"length":0,"stats":{"Line":36}},{"line":1197,"address":[],"length":0,"stats":{"Line":104}},{"line":1212,"address":[],"length":0,"stats":{"Line":2}},{"line":1213,"address":[],"length":0,"stats":{"Line":12}},{"line":1214,"address":[],"length":0,"stats":{"Line":88}},{"line":1230,"address":[],"length":0,"stats":{"Line":2}},{"line":1231,"address":[],"length":0,"stats":{"Line":17}},{"line":1232,"address":[],"length":0,"stats":{"Line":8}},{"line":1233,"address":[],"length":0,"stats":{"Line":0}},{"line":1235,"address":[],"length":0,"stats":{"Line":71}},{"line":1236,"address":[],"length":0,"stats":{"Line":32}},{"line":1237,"address":[],"length":0,"stats":{"Line":1}},{"line":1241,"address":[],"length":0,"stats":{"Line":1}},{"line":1245,"address":[],"length":0,"stats":{"Line":1}},{"line":1246,"address":[],"length":0,"stats":{"Line":1}},{"line":1248,"address":[],"length":0,"stats":{"Line":1}},{"line":1249,"address":[],"length":0,"stats":{"Line":8}},{"line":1250,"address":[],"length":0,"stats":{"Line":7}},{"line":1251,"address":[],"length":0,"stats":{"Line":7}},{"line":1252,"address":[],"length":0,"stats":{"Line":38}},{"line":1253,"address":[],"length":0,"stats":{"Line":57}},{"line":1259,"address":[],"length":0,"stats":{"Line":1}},{"line":1260,"address":[],"length":0,"stats":{"Line":1}},{"line":1262,"address":[],"length":0,"stats":{"Line":1}},{"line":1263,"address":[],"length":0,"stats":{"Line":8}},{"line":1264,"address":[],"length":0,"stats":{"Line":7}},{"line":1265,"address":[],"length":0,"stats":{"Line":7}},{"line":1266,"address":[],"length":0,"stats":{"Line":38}},{"line":1267,"address":[],"length":0,"stats":{"Line":56}},{"line":1269,"address":[],"length":0,"stats":{"Line":19}},{"line":1275,"address":[],"length":0,"stats":{"Line":1}},{"line":1276,"address":[],"length":0,"stats":{"Line":4}},{"line":1277,"address":[],"length":0,"stats":{"Line":2}},{"line":1278,"address":[],"length":0,"stats":{"Line":3}},{"line":1279,"address":[],"length":0,"stats":{"Line":3}},{"line":1284,"address":[],"length":0,"stats":{"Line":1}},{"line":1285,"address":[],"length":0,"stats":{"Line":2}},{"line":1286,"address":[],"length":0,"stats":{"Line":1}},{"line":1287,"address":[],"length":0,"stats":{"Line":1}},{"line":1289,"address":[],"length":0,"stats":{"Line":2}},{"line":1290,"address":[],"length":0,"stats":{"Line":3}},{"line":1291,"address":[],"length":0,"stats":{"Line":3}},{"line":1296,"address":[],"length":0,"stats":{"Line":3}},{"line":1298,"address":[],"length":0,"stats":{"Line":9}},{"line":1299,"address":[],"length":0,"stats":{"Line":1}},{"line":1300,"address":[],"length":0,"stats":{"Line":1}},{"line":1301,"address":[],"length":0,"stats":{"Line":1}},{"line":1305,"address":[],"length":0,"stats":{"Line":4}},{"line":1306,"address":[],"length":0,"stats":{"Line":2}},{"line":1309,"address":[],"length":0,"stats":{"Line":0}},{"line":1310,"address":[],"length":0,"stats":{"Line":0}},{"line":1311,"address":[],"length":0,"stats":{"Line":0}},{"line":1316,"address":[],"length":0,"stats":{"Line":4}},{"line":1317,"address":[],"length":0,"stats":{"Line":4}},{"line":1320,"address":[],"length":0,"stats":{"Line":0}},{"line":1321,"address":[],"length":0,"stats":{"Line":0}},{"line":1322,"address":[],"length":0,"stats":{"Line":0}},{"line":1326,"address":[],"length":0,"stats":{"Line":4}},{"line":1328,"address":[],"length":0,"stats":{"Line":2}},{"line":1329,"address":[],"length":0,"stats":{"Line":2}},{"line":1333,"address":[],"length":0,"stats":{"Line":0}},{"line":1334,"address":[],"length":0,"stats":{"Line":0}},{"line":1335,"address":[],"length":0,"stats":{"Line":0}},{"line":1340,"address":[],"length":0,"stats":{"Line":4}},{"line":1341,"address":[],"length":0,"stats":{"Line":31}},{"line":1342,"address":[],"length":0,"stats":{"Line":30}},{"line":1343,"address":[],"length":0,"stats":{"Line":1}},{"line":1344,"address":[],"length":0,"stats":{"Line":1}},{"line":1345,"address":[],"length":0,"stats":{"Line":1}},{"line":1350,"address":[],"length":0,"stats":{"Line":28}},{"line":1351,"address":[],"length":0,"stats":{"Line":138}},{"line":1352,"address":[],"length":0,"stats":{"Line":124}},{"line":1353,"address":[],"length":0,"stats":{"Line":0}},{"line":1354,"address":[],"length":0,"stats":{"Line":0}},{"line":1355,"address":[],"length":0,"stats":{"Line":0}},{"line":1361,"address":[],"length":0,"stats":{"Line":1}}],"covered":120,"coverable":160},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","anthropic.rs"],"content":"//! Anthropic (Claude) LLM provider implementation.\n//!\n//! Supports Anthropic's Messages API for Claude models.\n//!\n//! # Environment Variables\n//! - `ANTHROPIC_API_KEY`: API key for Anthropic API\n//!\n//! # Models Supported\n//! - Claude 4.5 Opus: `claude-opus-4-5-20250929`\n//! - Claude 4 Sonnet: `claude-sonnet-4-5-20250929`\n//! - Claude 3.5 Sonnet: `claude-3-5-sonnet-20241022`\n//! - Claude 3.5 Haiku: `claude-3-5-haiku-20241022`\n//! - Claude 3 Opus: `claude-3-opus-20240229`\n//!\n//! # Example\n//! ```ignore\n//! use edgequake_llm::AnthropicProvider;\n//!\n//! let provider = AnthropicProvider::new(\"your-api-key\");\n//! let response = provider.chat(&[ChatMessage::user(\"Hello!\")], None).await?;\n//! ```\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse futures::StreamExt;\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse tracing::{debug, instrument, warn};\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, FunctionCall, LLMProvider, LLMResponse, StreamChunk, ToolCall,\n    ToolChoice, ToolDefinition,\n};\n\n/// Anthropic API base URL\nconst ANTHROPIC_API_BASE: &str = \"https://api.anthropic.com\";\n\n/// Anthropic API version (required header)\nconst ANTHROPIC_API_VERSION: &str = \"2023-06-01\";\n\n/// Default model\nconst DEFAULT_MODEL: &str = \"claude-sonnet-4-5-20250929\";\n\n// ============================================================================\n// Anthropic API Request/Response Types\n// ============================================================================\n\n/// Message for Anthropic API\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct AnthropicMessage {\n    role: String,\n    content: AnthropicContent,\n}\n\n/// Content can be a string or an array of content blocks\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(untagged)]\nenum AnthropicContent {\n    Text(String),\n    Blocks(Vec<ContentBlock>),\n}\n\n// ============================================================================\n// Image Support Types (OODA-53)\n// ============================================================================\n//\n// Anthropic uses a different image format than OpenAI:\n//\n// OpenAI:                            Anthropic:\n// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n// â”‚ type: \"image_url\"       â”‚        â”‚ type: \"image\"           â”‚\n// â”‚ image_url:              â”‚        â”‚ source:                 â”‚\n// â”‚   url: \"data:...\"       â”‚        â”‚   type: \"base64\"        â”‚\n// â”‚   detail: \"high\"        â”‚        â”‚   media_type: \"image/x\" â”‚\n// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   data: \"base64...\"     â”‚\n//                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//\n// WHY: Separate struct for clarity and correct serde serialization\n// ============================================================================\n\n/// Image source for Anthropic API (base64 encoded images)\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct ImageSource {\n    #[serde(rename = \"type\")]\n    source_type: String,  // Always \"base64\"\n    media_type: String,   // MIME type, e.g., \"image/png\"\n    data: String,         // Base64-encoded image data\n}\n\n/// Content block for structured content\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct ContentBlock {\n    #[serde(rename = \"type\")]\n    content_type: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    text: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    id: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    name: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    input: Option<serde_json::Value>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_use_id: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    content: Option<String>,\n    // OODA-53: Image source for multimodal messages\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    source: Option<ImageSource>,\n}\n\n/// Tool definition for Anthropic API\n#[derive(Debug, Clone, Serialize)]\nstruct AnthropicTool {\n    name: String,\n    description: String,\n    input_schema: serde_json::Value,\n}\n\n/// Request body for messages endpoint\n#[derive(Debug, Clone, Serialize)]\nstruct MessagesRequest {\n    model: String,\n    max_tokens: u32,\n    messages: Vec<AnthropicMessage>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    system: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stream: Option<bool>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<AnthropicTool>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_choice: Option<serde_json::Value>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    top_p: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stop_sequences: Option<Vec<String>>,\n}\n\n/// Response from messages endpoint\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]  // Fields used for deserialization only\nstruct MessagesResponse {\n    id: String,\n    #[serde(rename = \"type\")]\n    response_type: String,\n    role: String,\n    content: Vec<ContentBlock>,\n    model: String,\n    stop_reason: Option<String>,\n    usage: AnthropicUsage,\n}\n\n/// Usage statistics from Anthropic API\n#[derive(Debug, Clone, Deserialize, Default)]\n#[allow(dead_code)]  // Fields used for deserialization only\nstruct AnthropicUsage {\n    input_tokens: u32,\n    output_tokens: u32,\n    #[serde(default)]\n    cache_creation_input_tokens: Option<u32>,\n    #[serde(default)]\n    cache_read_input_tokens: Option<u32>,\n}\n\n/// Error response from Anthropic API\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]  // Fields used for deserialization only\nstruct AnthropicErrorResponse {\n    #[serde(rename = \"type\")]\n    error_type: String,\n    error: AnthropicError,\n}\n\n#[derive(Debug, Clone, Deserialize)]\nstruct AnthropicError {\n    #[serde(rename = \"type\")]\n    error_type: String,\n    message: String,\n}\n\n/// SSE event for streaming responses\n#[derive(Debug, Clone, Deserialize)]\n#[serde(tag = \"type\")]\n#[allow(dead_code)]  // Variant fields used for deserialization only\nenum StreamEvent {\n    #[serde(rename = \"message_start\")]\n    MessageStart { message: MessagesResponse },\n    #[serde(rename = \"content_block_start\")]\n    ContentBlockStart {\n        index: usize,\n        content_block: ContentBlock,\n    },\n    #[serde(rename = \"content_block_delta\")]\n    ContentBlockDelta { index: usize, delta: DeltaBlock },\n    #[serde(rename = \"content_block_stop\")]\n    ContentBlockStop { index: usize },\n    #[serde(rename = \"message_delta\")]\n    MessageDelta {\n        delta: MessageDeltaData,\n        usage: Option<DeltaUsage>,\n    },\n    #[serde(rename = \"message_stop\")]\n    MessageStop,\n    #[serde(rename = \"ping\")]\n    Ping,\n    #[serde(rename = \"error\")]\n    Error { error: AnthropicError },\n}\n\n#[derive(Debug, Clone, Deserialize)]\nstruct DeltaBlock {\n    #[serde(rename = \"type\")]\n    delta_type: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    text: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    partial_json: Option<String>,\n    /// OODA-03: Extended thinking content from thinking_delta events\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    thinking: Option<String>,\n}\n\n#[derive(Debug, Clone, Deserialize)]\nstruct MessageDeltaData {\n    stop_reason: Option<String>,\n}\n\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]  // Fields used for deserialization only\nstruct DeltaUsage {\n    output_tokens: u32,\n}\n\n// ============================================================================\n// AnthropicProvider Implementation\n// ============================================================================\n\n/// Anthropic (Claude) LLM provider.\n///\n/// Supports Claude 3.5, Claude 4, and Claude 4.5 models via the Messages API.\n#[derive(Debug, Clone)]\npub struct AnthropicProvider {\n    client: Client,\n    api_key: String,\n    model: String,\n    base_url: String,\n    max_context_length: usize,\n    api_version: String,\n}\n\nimpl AnthropicProvider {\n    /// Create a new Anthropic provider with the given API key.\n    ///\n    /// # Arguments\n    /// * `api_key` - Anthropic API key (from <https://console.anthropic.com/>)\n    pub fn new(api_key: impl Into<String>) -> Self {\n        let model = DEFAULT_MODEL.to_string();\n        Self {\n            client: Client::new(),\n            api_key: api_key.into(),\n            max_context_length: Self::context_length_for_model(&model),\n            model,\n            base_url: ANTHROPIC_API_BASE.to_string(),\n            api_version: ANTHROPIC_API_VERSION.to_string(),\n        }\n    }\n\n    /// Create a provider from environment variables.\n    ///\n    /// # Environment Variables\n    ///\n    /// - `ANTHROPIC_API_KEY` or `ANTHROPIC_AUTH_TOKEN`: API key (required)\n    /// - `ANTHROPIC_BASE_URL`: Custom endpoint, e.g., `http://localhost:11434` for Ollama\n    /// - `ANTHROPIC_MODEL`: Default model to use\n    ///\n    /// # Ollama Compatibility\n    ///\n    /// To use with Ollama's Anthropic-compatible API (v0.14.0+):\n    /// ```bash\n    /// export ANTHROPIC_BASE_URL=http://localhost:11434\n    /// export ANTHROPIC_API_KEY=ollama  # or ANTHROPIC_AUTH_TOKEN=ollama\n    /// ```\n    ///\n    /// See: <https://docs.ollama.com/api/anthropic-compatibility>\n    pub fn from_env() -> Result<Self> {\n        // WHY: Support both ANTHROPIC_API_KEY and ANTHROPIC_AUTH_TOKEN\n        // Ollama documentation uses ANTHROPIC_AUTH_TOKEN, but ANTHROPIC_API_KEY\n        // is more common. Support both for maximum compatibility.\n        let api_key = std::env::var(\"ANTHROPIC_API_KEY\")\n            .or_else(|_| std::env::var(\"ANTHROPIC_AUTH_TOKEN\"))\n            .map_err(|_| {\n                LlmError::ConfigError(\n                    \"ANTHROPIC_API_KEY or ANTHROPIC_AUTH_TOKEN environment variable not set\"\n                        .to_string(),\n                )\n            })?;\n\n        let mut provider = Self::new(api_key);\n\n        // WHY: Allow custom base URL for Ollama or proxy servers\n        // This enables using the same provider implementation with Ollama's\n        // Anthropic-compatible endpoint at http://localhost:11434/v1/messages\n        if let Ok(base_url) = std::env::var(\"ANTHROPIC_BASE_URL\") {\n            provider = provider.with_base_url(base_url);\n        }\n\n        // WHY: Allow model override via environment\n        // Useful for testing different models without code changes\n        if let Ok(model) = std::env::var(\"ANTHROPIC_MODEL\") {\n            provider = provider.with_model(model);\n        }\n\n        Ok(provider)\n    }\n\n    /// Configure provider for use with Ollama's Anthropic-compatible API.\n    ///\n    /// Ollama v0.14.0+ provides a `/v1/messages` endpoint that is compatible\n    /// with the Anthropic Messages API, enabling use of open-source models\n    /// with tools designed for Claude.\n    ///\n    /// This method sets:\n    /// - Base URL to `http://localhost:11434`\n    /// - API key to `\"ollama\"` (required by API but ignored by Ollama)\n    /// - Default model to `qwen3-coder`\n    ///\n    /// # Requirements\n    ///\n    /// - Ollama v0.14.0 or later\n    /// - Model must be pulled: `ollama pull qwen3-coder`\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// use edgequake_llm::AnthropicProvider;\n    ///\n    /// let provider = AnthropicProvider::for_ollama();\n    /// let response = provider.chat(&messages, None).await?;\n    /// ```\n    ///\n    /// # Recommended Models\n    ///\n    /// - `qwen3-coder` - Excellent for coding tasks\n    /// - `gpt-oss:20b` - Strong general-purpose model\n    /// - `glm-4.7:cloud` - Cloud model (no local GPU needed)\n    ///\n    /// See: <https://docs.ollama.com/api/anthropic-compatibility>\n    pub fn for_ollama() -> Self {\n        Self::new(\"ollama\")\n            .with_base_url(\"http://localhost:11434\")\n            .with_model(\"qwen3-coder\")\n    }\n\n    /// Configure provider for Ollama with a specific model.\n    ///\n    /// # Arguments\n    ///\n    /// * `model` - Ollama model name (e.g., `gpt-oss:20b`, `qwen3-coder`)\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// use edgequake_llm::AnthropicProvider;\n    ///\n    /// // Use gpt-oss:20b model\n    /// let provider = AnthropicProvider::for_ollama_with_model(\"gpt-oss:20b\");\n    ///\n    /// // Use cloud model (no local GPU needed)\n    /// let provider = AnthropicProvider::for_ollama_with_model(\"glm-4.7:cloud\");\n    /// ```\n    pub fn for_ollama_with_model(model: impl Into<String>) -> Self {\n        Self::new(\"ollama\")\n            .with_base_url(\"http://localhost:11434\")\n            .with_model(model)\n    }\n\n    /// Configure provider for Ollama at a custom host.\n    ///\n    /// Use this when Ollama is running on a different host or port,\n    /// such as a remote server or Docker container.\n    ///\n    /// # Arguments\n    ///\n    /// * `host` - Ollama server URL (e.g., `http://192.168.1.100:11434`)\n    /// * `model` - Model name to use\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// use edgequake_llm::AnthropicProvider;\n    ///\n    /// // Connect to remote Ollama instance\n    /// let provider = AnthropicProvider::for_ollama_at(\n    ///     \"http://192.168.1.100:11434\",\n    ///     \"qwen3-coder\"\n    /// );\n    /// ```\n    pub fn for_ollama_at(host: impl Into<String>, model: impl Into<String>) -> Self {\n        Self::new(\"ollama\")\n            .with_base_url(host)\n            .with_model(model)\n    }\n\n    /// Set the model to use.\n    pub fn with_model(mut self, model: impl Into<String>) -> Self {\n        let model_name = model.into();\n        self.max_context_length = Self::context_length_for_model(&model_name);\n        self.model = model_name;\n        self\n    }\n\n    /// Set a custom base URL (for proxies or alternative endpoints).\n    pub fn with_base_url(mut self, url: impl Into<String>) -> Self {\n        self.base_url = url.into();\n        self\n    }\n\n    /// Set a custom API version.\n    pub fn with_api_version(mut self, version: impl Into<String>) -> Self {\n        self.api_version = version.into();\n        self\n    }\n\n    /// Get context length for a given model.\n    pub fn context_length_for_model(model: &str) -> usize {\n        match model {\n            // Claude 4.5 series (latest 2025)\n            m if m.contains(\"claude-opus-4\") || m.contains(\"opus-4.5\") => 200_000,\n            m if m.contains(\"claude-sonnet-4\") => 200_000,\n\n            // Claude 3.5 series\n            m if m.contains(\"claude-3-5-sonnet\") => 200_000,\n            m if m.contains(\"claude-3-5-haiku\") => 200_000,\n\n            // Claude 3 series\n            m if m.contains(\"claude-3-opus\") => 200_000,\n            m if m.contains(\"claude-3-sonnet\") => 200_000,\n            m if m.contains(\"claude-3-haiku\") => 200_000,\n\n            // Legacy models\n            m if m.contains(\"claude-2\") => 100_000,\n            m if m.contains(\"claude-instant\") => 100_000,\n\n            _ => 200_000, // Default for new models\n        }\n    }\n\n    /// Build the messages endpoint URL.\n    fn endpoint(&self) -> String {\n        format!(\"{}/v1/messages\", self.base_url)\n    }\n\n    /// Build headers for API requests.\n    fn headers(&self) -> reqwest::header::HeaderMap {\n        let mut headers = reqwest::header::HeaderMap::new();\n        headers.insert(\n            \"x-api-key\",\n            self.api_key.parse().expect(\"Invalid API key\"),\n        );\n        headers.insert(\n            \"anthropic-version\",\n            self.api_version.parse().expect(\"Invalid API version\"),\n        );\n        headers.insert(\n            reqwest::header::CONTENT_TYPE,\n            \"application/json\".parse().unwrap(),\n        );\n        headers\n    }\n\n    /// Convert EdgeCode ChatMessage to Anthropic format.\n    ///\n    /// Anthropic uses a separate `system` field, so system messages are extracted.\n    /// Returns (system_prompt, messages).\n    fn convert_messages(messages: &[ChatMessage]) -> (Option<String>, Vec<AnthropicMessage>) {\n        let mut system_prompt = None;\n        let mut anthropic_messages = Vec::new();\n\n        for msg in messages {\n            match msg.role {\n                ChatRole::System => {\n                    // Anthropic uses a separate system field\n                    system_prompt = Some(msg.content.clone());\n                }\n                ChatRole::User => {\n                    // OODA-53: Check if message has images for multipart content\n                    if msg.has_images() {\n                        let mut blocks = Vec::new();\n                        \n                        // Add text block first (if non-empty)\n                        if !msg.content.is_empty() {\n                            blocks.push(ContentBlock {\n                                content_type: \"text\".to_string(),\n                                text: Some(msg.content.clone()),\n                                id: None,\n                                name: None,\n                                input: None,\n                                tool_use_id: None,\n                                content: None,\n                                source: None,\n                            });\n                        }\n                        \n                        // Add image blocks\n                        if let Some(ref images) = msg.images {\n                            for img in images {\n                                blocks.push(ContentBlock {\n                                    content_type: \"image\".to_string(),\n                                    text: None,\n                                    id: None,\n                                    name: None,\n                                    input: None,\n                                    tool_use_id: None,\n                                    content: None,\n                                    source: Some(ImageSource {\n                                        source_type: \"base64\".to_string(),\n                                        media_type: img.mime_type.clone(),\n                                        data: img.data.clone(),\n                                    }),\n                                });\n                            }\n                        }\n                        \n                        anthropic_messages.push(AnthropicMessage {\n                            role: \"user\".to_string(),\n                            content: AnthropicContent::Blocks(blocks),\n                        });\n                    } else {\n                        anthropic_messages.push(AnthropicMessage {\n                            role: \"user\".to_string(),\n                            content: AnthropicContent::Text(msg.content.clone()),\n                        });\n                    }\n                }\n                ChatRole::Assistant => {\n                    anthropic_messages.push(AnthropicMessage {\n                        role: \"assistant\".to_string(),\n                        content: AnthropicContent::Text(msg.content.clone()),\n                    });\n                }\n                ChatRole::Tool => {\n                    // Tool results need special handling\n                    if let Some(tool_call_id) = &msg.tool_call_id {\n                        anthropic_messages.push(AnthropicMessage {\n                            role: \"user\".to_string(),\n                            content: AnthropicContent::Blocks(vec![ContentBlock {\n                                content_type: \"tool_result\".to_string(),\n                                tool_use_id: Some(tool_call_id.clone()),\n                                content: Some(msg.content.clone()),\n                                text: None,\n                                id: None,\n                                name: None,\n                                input: None,\n                                source: None,\n                            }]),\n                        });\n                    } else {\n                        // Fallback to user message\n                        anthropic_messages.push(AnthropicMessage {\n                            role: \"user\".to_string(),\n                            content: AnthropicContent::Text(msg.content.clone()),\n                        });\n                    }\n                }\n                ChatRole::Function => {\n                    // Legacy function role, treat as user message\n                    anthropic_messages.push(AnthropicMessage {\n                        role: \"user\".to_string(),\n                        content: AnthropicContent::Text(msg.content.clone()),\n                    });\n                }\n            }\n        }\n\n        (system_prompt, anthropic_messages)\n    }\n\n    /// Convert EdgeCode ToolDefinition to Anthropic format.\n    fn convert_tools(tools: &[ToolDefinition]) -> Vec<AnthropicTool> {\n        tools\n            .iter()\n            .map(|tool| AnthropicTool {\n                name: tool.function.name.clone(),\n                description: tool.function.description.clone(),\n                input_schema: tool.function.parameters.clone(),\n            })\n            .collect()\n    }\n\n    /// Convert Anthropic tool_choice to JSON value.\n    fn convert_tool_choice(choice: &ToolChoice) -> serde_json::Value {\n        match choice {\n            ToolChoice::Auto(s) if s == \"none\" => serde_json::json!({\"type\": \"none\"}),\n            ToolChoice::Auto(_) => serde_json::json!({\"type\": \"auto\"}),\n            ToolChoice::Required(_) => serde_json::json!({\"type\": \"any\"}),\n            ToolChoice::Function { function, .. } => {\n                serde_json::json!({\"type\": \"tool\", \"name\": function.name})\n            }\n        }\n    }\n\n    /// Parse Anthropic response to LLMResponse.\n    fn parse_response(response: MessagesResponse) -> LLMResponse {\n        let mut content = String::new();\n        let mut tool_calls = Vec::new();\n        let mut metadata = HashMap::new();\n\n        for block in &response.content {\n            match block.content_type.as_str() {\n                \"text\" => {\n                    if let Some(text) = &block.text {\n                        content.push_str(text);\n                    }\n                }\n                \"tool_use\" => {\n                    if let (Some(id), Some(name), Some(input)) =\n                        (&block.id, &block.name, &block.input)\n                    {\n                        tool_calls.push(ToolCall {\n                            id: id.clone(),\n                            call_type: \"function\".to_string(),\n                            function: FunctionCall {\n                                name: name.clone(),\n                                arguments: input.to_string(),\n                            },\n                        });\n                    }\n                }\n                _ => {\n                    debug!(\"Unknown content block type: {}\", block.content_type);\n                }\n            }\n        }\n\n        metadata.insert(\"response_id\".to_string(), serde_json::json!(response.id));\n\n        // Calculate cache hit tokens if available\n        let cache_hit_tokens = response\n            .usage\n            .cache_read_input_tokens\n            .map(|t| t as usize);\n\n        LLMResponse {\n            content,\n            prompt_tokens: response.usage.input_tokens as usize,\n            completion_tokens: response.usage.output_tokens as usize,\n            total_tokens: (response.usage.input_tokens + response.usage.output_tokens) as usize,\n            model: response.model,\n            finish_reason: response.stop_reason,\n            tool_calls,\n            metadata,\n            cache_hit_tokens,\n            thinking_tokens: None,\n            thinking_content: None,\n        }\n    }\n\n    /// Send a request and handle errors.\n    #[instrument(skip(self, request))]\n    async fn send_request(&self, request: &MessagesRequest) -> Result<MessagesResponse> {\n        debug!(\"Sending request to Anthropic API: model={}\", request.model);\n\n        let response = self\n            .client\n            .post(self.endpoint())\n            .headers(self.headers())\n            .json(request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            // Try to parse as Anthropic error\n            if let Ok(error_response) = serde_json::from_str::<AnthropicErrorResponse>(&error_text)\n            {\n                return Err(match status.as_u16() {\n                    401 => LlmError::AuthError(error_response.error.message),\n                    429 => LlmError::RateLimited(error_response.error.message),\n                    400 => LlmError::InvalidRequest(error_response.error.message),\n                    _ => LlmError::ApiError(format!(\n                        \"{}: {}\",\n                        error_response.error.error_type, error_response.error.message\n                    )),\n                });\n            }\n\n            return Err(LlmError::ApiError(format!(\n                \"HTTP {}: {}\",\n                status, error_text\n            )));\n        }\n\n        let response_text = response\n            .text()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        debug!(\"Anthropic response received: {} bytes\", response_text.len());\n\n        serde_json::from_str(&response_text)\n            .map_err(|e| LlmError::NetworkError(format!(\"Failed to parse response: {}\", e)))\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for AnthropicProvider {\n    fn name(&self) -> &str {\n        \"anthropic\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    #[instrument(skip(self, prompt))]\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    #[instrument(skip(self, prompt, options))]\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    #[instrument(skip(self, messages, options))]\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let (system, anthropic_messages) = Self::convert_messages(messages);\n        let options = options.cloned().unwrap_or_default();\n\n        let request = MessagesRequest {\n            model: self.model.clone(),\n            max_tokens: options.max_tokens.unwrap_or(4096) as u32,\n            messages: anthropic_messages,\n            system,\n            stream: None,\n            tools: None,\n            tool_choice: None,\n            temperature: options.temperature,\n            top_p: options.top_p,\n            stop_sequences: options.stop.clone(),\n        };\n\n        let response = self.send_request(&request).await?;\n        Ok(Self::parse_response(response))\n    }\n\n    #[instrument(skip(self, prompt))]\n    async fn stream(\n        &self,\n        prompt: &str,\n    ) -> Result<BoxStream<'static, Result<String>>> {\n        let messages = vec![ChatMessage::user(prompt)];\n        let (system, anthropic_messages) = Self::convert_messages(&messages);\n\n        let request = MessagesRequest {\n            model: self.model.clone(),\n            max_tokens: 4096,\n            messages: anthropic_messages,\n            system,\n            stream: Some(true),\n            tools: None,\n            tool_choice: None,\n            temperature: None,\n            top_p: None,\n            stop_sequences: None,\n        };\n\n        let response = self\n            .client\n            .post(self.endpoint())\n            .headers(self.headers())\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response.text().await.unwrap_or_else(|_| \"Unknown error\".to_string());\n            return Err(LlmError::ApiError(format!(\"HTTP {}: {}\", status, error_text)));\n        }\n\n        let stream = response\n            .bytes_stream()\n            .map(move |chunk| {\n                let chunk = chunk.map_err(|e| LlmError::NetworkError(e.to_string()))?;\n                let text = String::from_utf8_lossy(&chunk);\n\n                let mut result = String::new();\n                for line in text.lines() {\n                    if let Some(data) = line.strip_prefix(\"data: \") {\n                        if data.trim() == \"[DONE]\" {\n                            continue;\n                        }\n                        if let Ok(event) = serde_json::from_str::<StreamEvent>(data) {\n                            match event {\n                                StreamEvent::ContentBlockDelta { delta, .. } => {\n                                    if delta.delta_type == \"text_delta\" {\n                                        if let Some(text) = delta.text {\n                                            result.push_str(&text);\n                                        }\n                                    }\n                                }\n                                StreamEvent::Error { error } => {\n                                    warn!(\"Stream error: {}\", error.message);\n                                }\n                                _ => {}\n                            }\n                        }\n                    }\n                }\n                Ok(result)\n            })\n            .filter(|r| {\n                let keep = match r {\n                    Ok(s) => !s.is_empty(),\n                    Err(_) => true,\n                };\n                futures::future::ready(keep)\n            });\n\n        Ok(stream.boxed())\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    // OODA-44: Enable tool streaming for React agent\n    // WHY: AnthropicProvider has a fully working `chat_with_tools_stream()` implementation\n    //      but the default `supports_tool_streaming()` returns false.\n    //      React agent checks this flag to decide between:\n    //        - true  â†’ uses chat_with_tools_stream() (streaming with tools)\n    //        - false â†’ uses chat_with_tools() (non-streaming, default ignores tools!)\n    //      By returning true, we enable the React agent to use the working stream method.\n    //\n    //      Flow diagram:\n    //      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    //      â”‚ React Agent     â”‚\n    //      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    //               â”‚ supports_tool_streaming()?\n    //               â–¼\n    //         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    //         â”‚   true    â”‚â”€â”€â”€â”€â”€â–º chat_with_tools_stream() âœ“\n    //         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    //         â”‚   false   â”‚â”€â”€â”€â”€â”€â–º chat_with_tools() â†’ chat() (tools ignored!)\n    //         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n\n    #[instrument(skip(self, messages, tools, options))]\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        let (system, anthropic_messages) = Self::convert_messages(messages);\n        let anthropic_tools = Self::convert_tools(tools);\n        let options = options.cloned().unwrap_or_default();\n\n        let request = MessagesRequest {\n            model: self.model.clone(),\n            max_tokens: options.max_tokens.unwrap_or(4096) as u32,\n            messages: anthropic_messages,\n            system,\n            stream: Some(true),\n            tools: Some(anthropic_tools),\n            tool_choice: tool_choice.map(|tc| Self::convert_tool_choice(&tc)),\n            temperature: options.temperature,\n            top_p: options.top_p,\n            stop_sequences: options.stop.clone(),\n        };\n\n        let response = self\n            .client\n            .post(self.endpoint())\n            .headers(self.headers())\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response.text().await.unwrap_or_else(|_| \"Unknown error\".to_string());\n            return Err(LlmError::ApiError(format!(\"HTTP {}: {}\", status, error_text)));\n        }\n\n        // Track current tool call being built\n        let stream = response\n            .bytes_stream()\n            .map(move |chunk| {\n                let chunk = chunk.map_err(|e| LlmError::NetworkError(e.to_string()))?;\n                let text = String::from_utf8_lossy(&chunk);\n\n                let mut chunks: Vec<StreamChunk> = Vec::new();\n                for line in text.lines() {\n                    if let Some(data) = line.strip_prefix(\"data: \") {\n                        if data.trim() == \"[DONE]\" {\n                            continue;\n                        }\n                        if let Ok(event) = serde_json::from_str::<StreamEvent>(data) {\n                            match event {\n                                StreamEvent::ContentBlockStart {\n                                    index,\n                                    content_block,\n                                } => {\n                                    if content_block.content_type == \"tool_use\" {\n                                        if let (Some(id), Some(name)) =\n                                            (content_block.id, content_block.name)\n                                        {\n                                            // Send tool call start as a delta with id and name\n                                            chunks.push(StreamChunk::ToolCallDelta {\n                                                index,\n                                                id: Some(id),\n                                                function_name: Some(name),\n                                                function_arguments: None,\n                                            });\n                                        }\n                                    }\n                                }\n                                StreamEvent::ContentBlockDelta { index, delta } => {\n                                    match delta.delta_type.as_str() {\n                                        \"text_delta\" => {\n                                            if let Some(text) = delta.text {\n                                                chunks.push(StreamChunk::Content(text));\n                                            }\n                                        }\n                                        \"input_json_delta\" => {\n                                            if let Some(json) = delta.partial_json {\n                                                chunks.push(StreamChunk::ToolCallDelta {\n                                                    index,\n                                                    id: None,\n                                                    function_name: None,\n                                                    function_arguments: Some(json),\n                                                });\n                                            }\n                                        }\n                                        // OODA-03: Extended thinking streaming support\n                                        \"thinking_delta\" => {\n                                            if let Some(thinking) = delta.thinking {\n                                                chunks.push(StreamChunk::ThinkingContent {\n                                                    text: thinking,\n                                                    tokens_used: None,\n                                                    budget_total: None,\n                                                });\n                                            }\n                                        }\n                                        _ => {}\n                                    }\n                                }\n                                StreamEvent::ContentBlockStop { .. } => {\n                                    // Block completed, no special StreamChunk variant needed\n                                }\n                                StreamEvent::MessageStop => {\n                                    chunks.push(StreamChunk::Finished {\n                                        reason: \"stop\".to_string(),\n                                        ttft_ms: None,\n                                    });\n                                }\n                                StreamEvent::MessageDelta { delta, .. } => {\n                                    if let Some(reason) = delta.stop_reason {\n                                        chunks.push(StreamChunk::Finished { reason, ttft_ms: None });\n                                    }\n                                }\n                                StreamEvent::Error { error } => {\n                                    return Err(LlmError::ApiError(error.message));\n                                }\n                                _ => {}\n                            }\n                        }\n                    }\n                }\n                Ok(chunks)\n            })\n            .flat_map(|result: Result<Vec<StreamChunk>>| {\n                futures::stream::iter(match result {\n                    Ok(chunks) => chunks.into_iter().map(Ok).collect::<Vec<_>>(),\n                    Err(e) => vec![Err(e)],\n                })\n            });\n\n        Ok(stream.boxed())\n    }\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_new_provider() {\n        let provider = AnthropicProvider::new(\"test-api-key\");\n        assert_eq!(provider.name(), \"anthropic\");\n        assert_eq!(provider.model(), DEFAULT_MODEL);\n    }\n\n    #[test]\n    fn test_with_model() {\n        let provider = AnthropicProvider::new(\"test-api-key\")\n            .with_model(\"claude-3-5-sonnet-20241022\");\n        assert_eq!(provider.model(), \"claude-3-5-sonnet-20241022\");\n        assert_eq!(provider.max_context_length(), 200_000);\n    }\n\n    #[test]\n    fn test_context_length_for_model() {\n        assert_eq!(\n            AnthropicProvider::context_length_for_model(\"claude-opus-4-5-20250929\"),\n            200_000\n        );\n        assert_eq!(\n            AnthropicProvider::context_length_for_model(\"claude-sonnet-4-5-20250929\"),\n            200_000\n        );\n        assert_eq!(\n            AnthropicProvider::context_length_for_model(\"claude-3-5-sonnet-20241022\"),\n            200_000\n        );\n        assert_eq!(\n            AnthropicProvider::context_length_for_model(\"claude-2.1\"),\n            100_000\n        );\n    }\n\n    #[test]\n    fn test_convert_messages_with_system() {\n        let messages = vec![\n            ChatMessage::system(\"You are a helpful assistant.\"),\n            ChatMessage::user(\"Hello!\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let (system, anthropic_messages) = AnthropicProvider::convert_messages(&messages);\n\n        assert_eq!(system, Some(\"You are a helpful assistant.\".to_string()));\n        assert_eq!(anthropic_messages.len(), 2);\n        assert_eq!(anthropic_messages[0].role, \"user\");\n        assert_eq!(anthropic_messages[1].role, \"assistant\");\n    }\n\n    #[test]\n    fn test_convert_messages_without_system() {\n        let messages = vec![\n            ChatMessage::user(\"Hello!\"),\n        ];\n\n        let (system, anthropic_messages) = AnthropicProvider::convert_messages(&messages);\n\n        assert_eq!(system, None);\n        assert_eq!(anthropic_messages.len(), 1);\n    }\n\n    #[test]\n    fn test_convert_tools() {\n        use crate::traits::FunctionDefinition;\n\n        let tools = vec![ToolDefinition {\n            tool_type: \"function\".to_string(),\n            function: FunctionDefinition {\n                name: \"get_weather\".to_string(),\n                description: \"Get the weather for a location\".to_string(),\n                parameters: serde_json::json!({\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\"type\": \"string\"}\n                    }\n                }),\n                strict: None,\n            },\n        }];\n\n        let anthropic_tools = AnthropicProvider::convert_tools(&tools);\n\n        assert_eq!(anthropic_tools.len(), 1);\n        assert_eq!(anthropic_tools[0].name, \"get_weather\");\n        assert_eq!(anthropic_tools[0].description, \"Get the weather for a location\");\n    }\n\n    #[test]\n    fn test_convert_tool_choice() {\n        let auto = AnthropicProvider::convert_tool_choice(&ToolChoice::auto());\n        assert_eq!(auto, serde_json::json!({\"type\": \"auto\"}));\n\n        let required = AnthropicProvider::convert_tool_choice(&ToolChoice::required());\n        assert_eq!(required, serde_json::json!({\"type\": \"any\"}));\n\n        let none = AnthropicProvider::convert_tool_choice(&ToolChoice::none());\n        assert_eq!(none, serde_json::json!({\"type\": \"none\"}));\n\n        let specific = AnthropicProvider::convert_tool_choice(&ToolChoice::function(\"my_tool\"));\n        assert_eq!(specific, serde_json::json!({\"type\": \"tool\", \"name\": \"my_tool\"}));\n    }\n\n    #[test]\n    fn test_headers() {\n        let provider = AnthropicProvider::new(\"test-key\");\n        let headers = provider.headers();\n\n        assert!(headers.contains_key(\"x-api-key\"));\n        assert!(headers.contains_key(\"anthropic-version\"));\n        assert!(headers.contains_key(reqwest::header::CONTENT_TYPE));\n    }\n\n    #[test]\n    fn test_endpoint() {\n        let provider = AnthropicProvider::new(\"test-key\");\n        assert_eq!(provider.endpoint(), \"https://api.anthropic.com/v1/messages\");\n\n        let custom = provider.with_base_url(\"https://custom.api.com\");\n        assert_eq!(custom.endpoint(), \"https://custom.api.com/v1/messages\");\n    }\n\n    #[test]\n    fn test_parse_response() {\n        let response = MessagesResponse {\n            id: \"msg_123\".to_string(),\n            response_type: \"message\".to_string(),\n            role: \"assistant\".to_string(),\n            content: vec![ContentBlock {\n                content_type: \"text\".to_string(),\n                text: Some(\"Hello, world!\".to_string()),\n                id: None,\n                name: None,\n                input: None,\n                tool_use_id: None,\n                content: None,\n                source: None,\n            }],\n            model: \"claude-3-5-sonnet-20241022\".to_string(),\n            stop_reason: Some(\"end_turn\".to_string()),\n            usage: AnthropicUsage {\n                input_tokens: 10,\n                output_tokens: 5,\n                cache_creation_input_tokens: None,\n                cache_read_input_tokens: None,\n            },\n        };\n\n        let llm_response = AnthropicProvider::parse_response(response);\n\n        assert_eq!(llm_response.content, \"Hello, world!\");\n        assert_eq!(llm_response.prompt_tokens, 10);\n        assert_eq!(llm_response.completion_tokens, 5);\n        assert_eq!(llm_response.total_tokens, 15);\n        assert_eq!(llm_response.model, \"claude-3-5-sonnet-20241022\");\n        assert_eq!(llm_response.finish_reason, Some(\"end_turn\".to_string()));\n        assert!(llm_response.tool_calls.is_empty());\n    }\n\n    #[test]\n    fn test_parse_response_with_tool_calls() {\n        let response = MessagesResponse {\n            id: \"msg_456\".to_string(),\n            response_type: \"message\".to_string(),\n            role: \"assistant\".to_string(),\n            content: vec![ContentBlock {\n                content_type: \"tool_use\".to_string(),\n                text: None,\n                id: Some(\"tool_1\".to_string()),\n                name: Some(\"get_weather\".to_string()),\n                input: Some(serde_json::json!({\"location\": \"Paris\"})),\n                tool_use_id: None,\n                content: None,\n                source: None,\n            }],\n            model: \"claude-3-5-sonnet-20241022\".to_string(),\n            stop_reason: Some(\"tool_use\".to_string()),\n            usage: AnthropicUsage {\n                input_tokens: 20,\n                output_tokens: 10,\n                cache_creation_input_tokens: None,\n                cache_read_input_tokens: None,\n            },\n        };\n\n        let llm_response = AnthropicProvider::parse_response(response);\n\n        assert_eq!(llm_response.tool_calls.len(), 1);\n        assert_eq!(llm_response.tool_calls[0].id, \"tool_1\");\n        assert_eq!(llm_response.tool_calls[0].name(), \"get_weather\");\n        assert!(llm_response.tool_calls[0].arguments().contains(\"Paris\"));\n    }\n\n    // Integration test - requires API key\n    #[tokio::test]\n    #[ignore]\n    async fn test_chat_completion_live() {\n        let provider = AnthropicProvider::from_env().expect(\"ANTHROPIC_API_KEY not set\");\n        let messages = vec![ChatMessage::user(\"Say 'hello' and nothing else.\")];\n\n        let response = provider.chat(&messages, None).await;\n        assert!(response.is_ok());\n\n        let response = response.unwrap();\n        assert!(!response.content.is_empty());\n        assert!(response.prompt_tokens > 0);\n        assert!(response.completion_tokens > 0);\n    }\n\n    // =========================================================================\n    // Ollama Compatibility Tests\n    // =========================================================================\n\n    #[test]\n    fn test_for_ollama_defaults() {\n        // WHY: Verify that for_ollama() sets correct defaults for Ollama usage\n        // These defaults match Ollama's Anthropic compatibility requirements\n        let provider = AnthropicProvider::for_ollama();\n\n        assert_eq!(provider.base_url, \"http://localhost:11434\");\n        assert_eq!(provider.api_key, \"ollama\");\n        assert_eq!(provider.model, \"qwen3-coder\");\n    }\n\n    #[test]\n    fn test_for_ollama_with_model() {\n        // WHY: Users may want to use different Ollama models\n        let provider = AnthropicProvider::for_ollama_with_model(\"gpt-oss:20b\");\n\n        assert_eq!(provider.model, \"gpt-oss:20b\");\n        assert_eq!(provider.base_url, \"http://localhost:11434\");\n        assert_eq!(provider.api_key, \"ollama\");\n    }\n\n    #[test]\n    fn test_for_ollama_at_custom_host() {\n        // WHY: Ollama may run on remote servers or Docker containers\n        let provider = AnthropicProvider::for_ollama_at(\n            \"http://192.168.1.100:11434\",\n            \"llama3\"\n        );\n\n        assert_eq!(provider.base_url, \"http://192.168.1.100:11434\");\n        assert_eq!(provider.model, \"llama3\");\n        assert_eq!(provider.api_key, \"ollama\");\n    }\n\n    #[test]\n    fn test_for_ollama_endpoint() {\n        // WHY: Ollama's Anthropic-compatible endpoint is at /v1/messages\n        let provider = AnthropicProvider::for_ollama();\n\n        assert_eq!(provider.endpoint(), \"http://localhost:11434/v1/messages\");\n    }\n\n    #[test]\n    fn test_with_base_url_chain() {\n        // WHY: Ensure builder pattern works correctly for custom endpoints\n        let provider = AnthropicProvider::new(\"test-key\")\n            .with_base_url(\"http://localhost:11434\")\n            .with_model(\"qwen3-coder\");\n\n        assert_eq!(provider.base_url, \"http://localhost:11434\");\n        assert_eq!(provider.model, \"qwen3-coder\");\n        assert_eq!(provider.api_key, \"test-key\");\n    }\n\n    // Integration test - requires Ollama running locally\n    #[tokio::test]\n    #[ignore]\n    async fn test_ollama_chat_completion_live() {\n        // WHY: E2E test to verify Ollama integration works\n        // Run with: cargo test test_ollama_chat_completion_live -- --ignored\n        // Requires: ollama pull qwen3-coder\n        let provider = AnthropicProvider::for_ollama();\n        let messages = vec![ChatMessage::user(\"Say 'hello' and nothing else.\")];\n\n        let response = provider.chat(&messages, None).await;\n        assert!(response.is_ok(), \"Ollama chat failed: {:?}\", response.err());\n\n        let response = response.unwrap();\n        assert!(!response.content.is_empty());\n    }\n\n    // =========================================================================\n    // Image Support Tests (OODA-53)\n    // =========================================================================\n\n    #[test]\n    fn test_convert_messages_text_only() {\n        // WHY: Verify text-only messages still serialize as simple strings\n        let messages = vec![ChatMessage::user(\"Hello, world!\")];\n        let (_, anthropic_messages) = AnthropicProvider::convert_messages(&messages);\n\n        assert_eq!(anthropic_messages.len(), 1);\n        \n        // Text-only should serialize as string\n        let json = serde_json::to_value(&anthropic_messages[0]).unwrap();\n        assert_eq!(json[\"content\"], \"Hello, world!\");\n    }\n\n    #[test]\n    fn test_convert_messages_with_images() {\n        use crate::traits::ImageData;\n        \n        // WHY: Verify images use Anthropic's base64 source format\n        let images = vec![ImageData::new(\"base64data\", \"image/png\")];\n        let messages = vec![ChatMessage::user_with_images(\"What's this?\", images)];\n        let (_, anthropic_messages) = AnthropicProvider::convert_messages(&messages);\n\n        assert_eq!(anthropic_messages.len(), 1);\n        \n        // With images should serialize as array of blocks\n        let json = serde_json::to_value(&anthropic_messages[0]).unwrap();\n        let content = &json[\"content\"];\n        \n        assert!(content.is_array(), \"Content should be an array for images\");\n        assert_eq!(content.as_array().unwrap().len(), 2);\n        \n        // First block: text\n        assert_eq!(content[0][\"type\"], \"text\");\n        assert_eq!(content[0][\"text\"], \"What's this?\");\n        \n        // Second block: image with base64 source (Anthropic format)\n        assert_eq!(content[1][\"type\"], \"image\");\n        assert!(content[1][\"source\"].is_object(), \"Image should have source object\");\n        assert_eq!(content[1][\"source\"][\"type\"], \"base64\");\n        assert_eq!(content[1][\"source\"][\"media_type\"], \"image/png\");\n        assert_eq!(content[1][\"source\"][\"data\"], \"base64data\");\n    }\n\n    #[test]\n    fn test_convert_messages_multiple_images() {\n        use crate::traits::ImageData;\n        \n        // WHY: Verify multiple images are handled correctly\n        let images = vec![\n            ImageData::new(\"img1data\", \"image/png\"),\n            ImageData::new(\"img2data\", \"image/jpeg\"),\n        ];\n        let messages = vec![ChatMessage::user_with_images(\"Compare these\", images)];\n        let (_, anthropic_messages) = AnthropicProvider::convert_messages(&messages);\n\n        let json = serde_json::to_value(&anthropic_messages[0]).unwrap();\n        let content = &json[\"content\"];\n        \n        assert_eq!(content.as_array().unwrap().len(), 3); // 1 text + 2 images\n        \n        // Verify both images\n        assert_eq!(content[1][\"source\"][\"media_type\"], \"image/png\");\n        assert_eq!(content[2][\"source\"][\"media_type\"], \"image/jpeg\");\n    }\n\n    // =========================================================================\n    // Extended Thinking Tests (OODA-04)\n    // =========================================================================\n\n    #[test]\n    fn test_delta_block_parses_thinking_delta() {\n        // OODA-04: Verify DeltaBlock can parse thinking_delta events\n        // WHY: OODA-03 added thinking field to support extended thinking streaming\n        let json = r#\"{\"type\":\"thinking_delta\",\"thinking\":\"Let me analyze step by step...\"}\"#;\n        \n        let delta: DeltaBlock = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(delta.delta_type, \"thinking_delta\");\n        assert_eq!(delta.thinking, Some(\"Let me analyze step by step...\".to_string()));\n        assert!(delta.text.is_none());\n        assert!(delta.partial_json.is_none());\n    }\n\n    #[test]\n    fn test_delta_block_parses_text_delta() {\n        // OODA-04: Verify text_delta still works (regression test)\n        let json = r#\"{\"type\":\"text_delta\",\"text\":\"Hello world\"}\"#;\n        \n        let delta: DeltaBlock = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(delta.delta_type, \"text_delta\");\n        assert_eq!(delta.text, Some(\"Hello world\".to_string()));\n        assert!(delta.thinking.is_none());\n    }\n\n    #[test]\n    fn test_delta_block_parses_input_json_delta() {\n        // OODA-04: Verify input_json_delta still works (regression test)\n        let json = r#\"{\"type\":\"input_json_delta\",\"partial_json\":\"{\\\"name\\\":\"}\"#;\n        \n        let delta: DeltaBlock = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(delta.delta_type, \"input_json_delta\");\n        assert_eq!(delta.partial_json, Some(\"{\\\"name\\\":\".to_string()));\n        assert!(delta.thinking.is_none());\n        assert!(delta.text.is_none());\n    }\n\n    // =========================================================================\n    // OODA-35: Additional Unit Tests\n    // =========================================================================\n\n    #[test]\n    fn test_constants() {\n        // WHY: Verify constants are as expected for API compatibility\n        assert_eq!(ANTHROPIC_API_BASE, \"https://api.anthropic.com\");\n        assert_eq!(ANTHROPIC_API_VERSION, \"2023-06-01\");\n        assert_eq!(DEFAULT_MODEL, \"claude-sonnet-4-5-20250929\");\n    }\n\n    #[test]\n    fn test_supports_streaming() {\n        let provider = AnthropicProvider::new(\"key\");\n        assert!(provider.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_tool_streaming() {\n        // WHY: OODA-44 enabled tool streaming for React agent\n        let provider = AnthropicProvider::new(\"key\");\n        assert!(provider.supports_tool_streaming());\n    }\n\n    #[test]\n    fn test_anthropic_usage_with_cache_tokens() {\n        // WHY: Anthropic prompt caching adds cache_creation_input_tokens and cache_read_input_tokens\n        let json = r#\"{\n            \"input_tokens\": 100,\n            \"output_tokens\": 50,\n            \"cache_creation_input_tokens\": 25,\n            \"cache_read_input_tokens\": 10\n        }\"#;\n        let usage: AnthropicUsage = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(usage.input_tokens, 100);\n        assert_eq!(usage.output_tokens, 50);\n        assert_eq!(usage.cache_creation_input_tokens, Some(25));\n        assert_eq!(usage.cache_read_input_tokens, Some(10));\n    }\n\n    #[test]\n    fn test_anthropic_error_response_deserialization() {\n        let json = r#\"{\n            \"type\": \"error\",\n            \"error\": {\n                \"type\": \"invalid_request_error\",\n                \"message\": \"messages: Required field missing\"\n            }\n        }\"#;\n        let error: AnthropicErrorResponse = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(error.error_type, \"error\");\n        assert_eq!(error.error.error_type, \"invalid_request_error\");\n        assert_eq!(error.error.message, \"messages: Required field missing\");\n    }\n\n    #[test]\n    fn test_stream_event_message_start() {\n        // WHY: StreamEvent uses tagged enum - verify it deserializes correctly\n        let json = r#\"{\n            \"type\": \"message_start\",\n            \"message\": {\n                \"id\": \"msg_123\",\n                \"type\": \"message\",\n                \"role\": \"assistant\",\n                \"content\": [],\n                \"model\": \"claude-3-5-sonnet\",\n                \"stop_reason\": null,\n                \"usage\": {\"input_tokens\": 10, \"output_tokens\": 0}\n            }\n        }\"#;\n        let event: StreamEvent = serde_json::from_str(json).unwrap();\n        \n        match event {\n            StreamEvent::MessageStart { message } => {\n                assert_eq!(message.id, \"msg_123\");\n                assert_eq!(message.role, \"assistant\");\n            }\n            _ => panic!(\"Expected MessageStart event\"),\n        }\n    }\n\n    #[test]\n    fn test_stream_event_ping() {\n        let json = r#\"{\"type\": \"ping\"}\"#;\n        let event: StreamEvent = serde_json::from_str(json).unwrap();\n        \n        matches!(event, StreamEvent::Ping);\n    }\n\n    #[test]\n    fn test_image_source_serialization() {\n        // WHY: Verify ImageSource serializes correctly for Anthropic API\n        let source = ImageSource {\n            source_type: \"base64\".to_string(),\n            media_type: \"image/png\".to_string(),\n            data: \"aGVsbG8=\".to_string(),\n        };\n        \n        let json = serde_json::to_value(&source).unwrap();\n        assert_eq!(json[\"type\"], \"base64\");\n        assert_eq!(json[\"media_type\"], \"image/png\");\n        assert_eq!(json[\"data\"], \"aGVsbG8=\");\n    }\n\n    #[test]\n    fn test_content_block_tool_use() {\n        let block = ContentBlock {\n            content_type: \"tool_use\".to_string(),\n            text: None,\n            id: Some(\"tool_123\".to_string()),\n            name: Some(\"get_weather\".to_string()),\n            input: Some(serde_json::json!({\"location\": \"NYC\"})),\n            tool_use_id: None,\n            content: None,\n            source: None,\n        };\n        \n        let json = serde_json::to_value(&block).unwrap();\n        assert_eq!(json[\"type\"], \"tool_use\");\n        assert_eq!(json[\"id\"], \"tool_123\");\n        assert_eq!(json[\"name\"], \"get_weather\");\n        assert_eq!(json[\"input\"][\"location\"], \"NYC\");\n    }\n}\n","traces":[{"line":261,"address":[],"length":0,"stats":{"Line":11}},{"line":262,"address":[],"length":0,"stats":{"Line":33}},{"line":264,"address":[],"length":0,"stats":{"Line":22}},{"line":265,"address":[],"length":0,"stats":{"Line":33}},{"line":266,"address":[],"length":0,"stats":{"Line":33}},{"line":268,"address":[],"length":0,"stats":{"Line":33}},{"line":269,"address":[],"length":0,"stats":{"Line":11}},{"line":290,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":2}},{"line":354,"address":[],"length":0,"stats":{"Line":2}},{"line":376,"address":[],"length":0,"stats":{"Line":1}},{"line":377,"address":[],"length":0,"stats":{"Line":1}},{"line":379,"address":[],"length":0,"stats":{"Line":2}},{"line":403,"address":[],"length":0,"stats":{"Line":1}},{"line":404,"address":[],"length":0,"stats":{"Line":1}},{"line":405,"address":[],"length":0,"stats":{"Line":2}},{"line":406,"address":[],"length":0,"stats":{"Line":2}},{"line":410,"address":[],"length":0,"stats":{"Line":6}},{"line":411,"address":[],"length":0,"stats":{"Line":18}},{"line":412,"address":[],"length":0,"stats":{"Line":6}},{"line":413,"address":[],"length":0,"stats":{"Line":12}},{"line":414,"address":[],"length":0,"stats":{"Line":6}},{"line":418,"address":[],"length":0,"stats":{"Line":6}},{"line":419,"address":[],"length":0,"stats":{"Line":18}},{"line":420,"address":[],"length":0,"stats":{"Line":6}},{"line":424,"address":[],"length":0,"stats":{"Line":0}},{"line":425,"address":[],"length":0,"stats":{"Line":0}},{"line":426,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":21}},{"line":431,"address":[],"length":0,"stats":{"Line":21}},{"line":433,"address":[],"length":0,"stats":{"Line":84}},{"line":434,"address":[],"length":0,"stats":{"Line":64}},{"line":437,"address":[],"length":0,"stats":{"Line":20}},{"line":438,"address":[],"length":0,"stats":{"Line":12}},{"line":441,"address":[],"length":0,"stats":{"Line":12}},{"line":442,"address":[],"length":0,"stats":{"Line":12}},{"line":443,"address":[],"length":0,"stats":{"Line":12}},{"line":446,"address":[],"length":0,"stats":{"Line":14}},{"line":447,"address":[],"length":0,"stats":{"Line":10}},{"line":449,"address":[],"length":0,"stats":{"Line":5}},{"line":454,"address":[],"length":0,"stats":{"Line":3}},{"line":455,"address":[],"length":0,"stats":{"Line":6}},{"line":459,"address":[],"length":0,"stats":{"Line":1}},{"line":460,"address":[],"length":0,"stats":{"Line":2}},{"line":461,"address":[],"length":0,"stats":{"Line":2}},{"line":463,"address":[],"length":0,"stats":{"Line":3}},{"line":465,"address":[],"length":0,"stats":{"Line":2}},{"line":467,"address":[],"length":0,"stats":{"Line":3}},{"line":469,"address":[],"length":0,"stats":{"Line":2}},{"line":470,"address":[],"length":0,"stats":{"Line":1}},{"line":471,"address":[],"length":0,"stats":{"Line":3}},{"line":473,"address":[],"length":0,"stats":{"Line":1}},{"line":480,"address":[],"length":0,"stats":{"Line":5}},{"line":481,"address":[],"length":0,"stats":{"Line":10}},{"line":482,"address":[],"length":0,"stats":{"Line":10}},{"line":484,"address":[],"length":0,"stats":{"Line":19}},{"line":485,"address":[],"length":0,"stats":{"Line":7}},{"line":486,"address":[],"length":0,"stats":{"Line":1}},{"line":488,"address":[],"length":0,"stats":{"Line":2}},{"line":492,"address":[],"length":0,"stats":{"Line":10}},{"line":493,"address":[],"length":0,"stats":{"Line":4}},{"line":496,"address":[],"length":0,"stats":{"Line":4}},{"line":497,"address":[],"length":0,"stats":{"Line":6}},{"line":498,"address":[],"length":0,"stats":{"Line":6}},{"line":499,"address":[],"length":0,"stats":{"Line":4}},{"line":500,"address":[],"length":0,"stats":{"Line":4}},{"line":501,"address":[],"length":0,"stats":{"Line":4}},{"line":502,"address":[],"length":0,"stats":{"Line":4}},{"line":503,"address":[],"length":0,"stats":{"Line":4}},{"line":504,"address":[],"length":0,"stats":{"Line":2}},{"line":505,"address":[],"length":0,"stats":{"Line":2}},{"line":510,"address":[],"length":0,"stats":{"Line":4}},{"line":511,"address":[],"length":0,"stats":{"Line":11}},{"line":512,"address":[],"length":0,"stats":{"Line":9}},{"line":513,"address":[],"length":0,"stats":{"Line":9}},{"line":514,"address":[],"length":0,"stats":{"Line":6}},{"line":515,"address":[],"length":0,"stats":{"Line":6}},{"line":516,"address":[],"length":0,"stats":{"Line":6}},{"line":517,"address":[],"length":0,"stats":{"Line":6}},{"line":518,"address":[],"length":0,"stats":{"Line":6}},{"line":519,"address":[],"length":0,"stats":{"Line":6}},{"line":520,"address":[],"length":0,"stats":{"Line":3}},{"line":521,"address":[],"length":0,"stats":{"Line":9}},{"line":522,"address":[],"length":0,"stats":{"Line":9}},{"line":523,"address":[],"length":0,"stats":{"Line":3}},{"line":529,"address":[],"length":0,"stats":{"Line":6}},{"line":530,"address":[],"length":0,"stats":{"Line":4}},{"line":531,"address":[],"length":0,"stats":{"Line":2}},{"line":534,"address":[],"length":0,"stats":{"Line":9}},{"line":535,"address":[],"length":0,"stats":{"Line":9}},{"line":536,"address":[],"length":0,"stats":{"Line":3}},{"line":540,"address":[],"length":0,"stats":{"Line":1}},{"line":541,"address":[],"length":0,"stats":{"Line":3}},{"line":542,"address":[],"length":0,"stats":{"Line":3}},{"line":543,"address":[],"length":0,"stats":{"Line":1}},{"line":548,"address":[],"length":0,"stats":{"Line":0}},{"line":549,"address":[],"length":0,"stats":{"Line":0}},{"line":550,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":0}},{"line":553,"address":[],"length":0,"stats":{"Line":0}},{"line":554,"address":[],"length":0,"stats":{"Line":0}},{"line":555,"address":[],"length":0,"stats":{"Line":0}},{"line":556,"address":[],"length":0,"stats":{"Line":0}},{"line":557,"address":[],"length":0,"stats":{"Line":0}},{"line":558,"address":[],"length":0,"stats":{"Line":0}},{"line":559,"address":[],"length":0,"stats":{"Line":0}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":565,"address":[],"length":0,"stats":{"Line":0}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":570,"address":[],"length":0,"stats":{"Line":0}},{"line":572,"address":[],"length":0,"stats":{"Line":0}},{"line":573,"address":[],"length":0,"stats":{"Line":0}},{"line":574,"address":[],"length":0,"stats":{"Line":0}},{"line":580,"address":[],"length":0,"stats":{"Line":5}},{"line":584,"address":[],"length":0,"stats":{"Line":1}},{"line":585,"address":[],"length":0,"stats":{"Line":1}},{"line":587,"address":[],"length":0,"stats":{"Line":1}},{"line":588,"address":[],"length":0,"stats":{"Line":2}},{"line":589,"address":[],"length":0,"stats":{"Line":2}},{"line":590,"address":[],"length":0,"stats":{"Line":2}},{"line":596,"address":[],"length":0,"stats":{"Line":4}},{"line":597,"address":[],"length":0,"stats":{"Line":2}},{"line":598,"address":[],"length":0,"stats":{"Line":7}},{"line":599,"address":[],"length":0,"stats":{"Line":2}},{"line":600,"address":[],"length":0,"stats":{"Line":2}},{"line":601,"address":[],"length":0,"stats":{"Line":1}},{"line":602,"address":[],"length":0,"stats":{"Line":3}},{"line":608,"address":[],"length":0,"stats":{"Line":2}},{"line":609,"address":[],"length":0,"stats":{"Line":4}},{"line":610,"address":[],"length":0,"stats":{"Line":4}},{"line":611,"address":[],"length":0,"stats":{"Line":4}},{"line":613,"address":[],"length":0,"stats":{"Line":6}},{"line":614,"address":[],"length":0,"stats":{"Line":2}},{"line":615,"address":[],"length":0,"stats":{"Line":2}},{"line":616,"address":[],"length":0,"stats":{"Line":3}},{"line":617,"address":[],"length":0,"stats":{"Line":2}},{"line":620,"address":[],"length":0,"stats":{"Line":1}},{"line":621,"address":[],"length":0,"stats":{"Line":3}},{"line":622,"address":[],"length":0,"stats":{"Line":3}},{"line":624,"address":[],"length":0,"stats":{"Line":3}},{"line":625,"address":[],"length":0,"stats":{"Line":3}},{"line":626,"address":[],"length":0,"stats":{"Line":3}},{"line":627,"address":[],"length":0,"stats":{"Line":1}},{"line":628,"address":[],"length":0,"stats":{"Line":3}},{"line":629,"address":[],"length":0,"stats":{"Line":1}},{"line":635,"address":[],"length":0,"stats":{"Line":0}},{"line":640,"address":[],"length":0,"stats":{"Line":10}},{"line":643,"address":[],"length":0,"stats":{"Line":4}},{"line":644,"address":[],"length":0,"stats":{"Line":2}},{"line":645,"address":[],"length":0,"stats":{"Line":2}},{"line":646,"address":[],"length":0,"stats":{"Line":2}},{"line":650,"address":[],"length":0,"stats":{"Line":4}},{"line":651,"address":[],"length":0,"stats":{"Line":4}},{"line":652,"address":[],"length":0,"stats":{"Line":4}},{"line":653,"address":[],"length":0,"stats":{"Line":4}},{"line":654,"address":[],"length":0,"stats":{"Line":4}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":0}},{"line":705,"address":[],"length":0,"stats":{"Line":0}},{"line":710,"address":[],"length":0,"stats":{"Line":0}},{"line":716,"address":[],"length":0,"stats":{"Line":1}},{"line":717,"address":[],"length":0,"stats":{"Line":1}},{"line":720,"address":[],"length":0,"stats":{"Line":2}},{"line":721,"address":[],"length":0,"stats":{"Line":2}},{"line":724,"address":[],"length":0,"stats":{"Line":1}},{"line":725,"address":[],"length":0,"stats":{"Line":1}},{"line":804,"address":[],"length":0,"stats":{"Line":0}},{"line":808,"address":[],"length":0,"stats":{"Line":0}},{"line":814,"address":[],"length":0,"stats":{"Line":0}},{"line":815,"address":[],"length":0,"stats":{"Line":0}},{"line":816,"address":[],"length":0,"stats":{"Line":0}},{"line":818,"address":[],"length":0,"stats":{"Line":0}},{"line":819,"address":[],"length":0,"stats":{"Line":0}},{"line":820,"address":[],"length":0,"stats":{"Line":0}},{"line":821,"address":[],"length":0,"stats":{"Line":0}},{"line":822,"address":[],"length":0,"stats":{"Line":0}},{"line":824,"address":[],"length":0,"stats":{"Line":0}},{"line":825,"address":[],"length":0,"stats":{"Line":0}},{"line":826,"address":[],"length":0,"stats":{"Line":0}},{"line":827,"address":[],"length":0,"stats":{"Line":0}},{"line":828,"address":[],"length":0,"stats":{"Line":0}},{"line":829,"address":[],"length":0,"stats":{"Line":0}},{"line":833,"address":[],"length":0,"stats":{"Line":0}},{"line":834,"address":[],"length":0,"stats":{"Line":0}},{"line":836,"address":[],"length":0,"stats":{"Line":0}},{"line":841,"address":[],"length":0,"stats":{"Line":0}},{"line":843,"address":[],"length":0,"stats":{"Line":0}},{"line":844,"address":[],"length":0,"stats":{"Line":0}},{"line":845,"address":[],"length":0,"stats":{"Line":0}},{"line":846,"address":[],"length":0,"stats":{"Line":0}},{"line":848,"address":[],"length":0,"stats":{"Line":0}},{"line":854,"address":[],"length":0,"stats":{"Line":1}},{"line":855,"address":[],"length":0,"stats":{"Line":1}},{"line":877,"address":[],"length":0,"stats":{"Line":1}},{"line":878,"address":[],"length":0,"stats":{"Line":1}},{"line":900,"address":[],"length":0,"stats":{"Line":0}},{"line":913,"address":[],"length":0,"stats":{"Line":0}},{"line":917,"address":[],"length":0,"stats":{"Line":0}},{"line":924,"address":[],"length":0,"stats":{"Line":0}},{"line":925,"address":[],"length":0,"stats":{"Line":0}},{"line":926,"address":[],"length":0,"stats":{"Line":0}},{"line":928,"address":[],"length":0,"stats":{"Line":0}},{"line":929,"address":[],"length":0,"stats":{"Line":0}},{"line":930,"address":[],"length":0,"stats":{"Line":0}},{"line":931,"address":[],"length":0,"stats":{"Line":0}},{"line":932,"address":[],"length":0,"stats":{"Line":0}},{"line":934,"address":[],"length":0,"stats":{"Line":0}},{"line":935,"address":[],"length":0,"stats":{"Line":0}},{"line":937,"address":[],"length":0,"stats":{"Line":0}},{"line":938,"address":[],"length":0,"stats":{"Line":0}},{"line":940,"address":[],"length":0,"stats":{"Line":0}},{"line":941,"address":[],"length":0,"stats":{"Line":0}},{"line":942,"address":[],"length":0,"stats":{"Line":0}},{"line":945,"address":[],"length":0,"stats":{"Line":0}},{"line":946,"address":[],"length":0,"stats":{"Line":0}},{"line":947,"address":[],"length":0,"stats":{"Line":0}},{"line":948,"address":[],"length":0,"stats":{"Line":0}},{"line":949,"address":[],"length":0,"stats":{"Line":0}},{"line":954,"address":[],"length":0,"stats":{"Line":0}},{"line":955,"address":[],"length":0,"stats":{"Line":0}},{"line":956,"address":[],"length":0,"stats":{"Line":0}},{"line":957,"address":[],"length":0,"stats":{"Line":0}},{"line":958,"address":[],"length":0,"stats":{"Line":0}},{"line":961,"address":[],"length":0,"stats":{"Line":0}},{"line":962,"address":[],"length":0,"stats":{"Line":0}},{"line":963,"address":[],"length":0,"stats":{"Line":0}},{"line":964,"address":[],"length":0,"stats":{"Line":0}},{"line":965,"address":[],"length":0,"stats":{"Line":0}},{"line":966,"address":[],"length":0,"stats":{"Line":0}},{"line":967,"address":[],"length":0,"stats":{"Line":0}},{"line":972,"address":[],"length":0,"stats":{"Line":0}},{"line":973,"address":[],"length":0,"stats":{"Line":0}},{"line":974,"address":[],"length":0,"stats":{"Line":0}},{"line":975,"address":[],"length":0,"stats":{"Line":0}},{"line":976,"address":[],"length":0,"stats":{"Line":0}},{"line":977,"address":[],"length":0,"stats":{"Line":0}},{"line":981,"address":[],"length":0,"stats":{"Line":0}},{"line":984,"address":[],"length":0,"stats":{"Line":0}},{"line":987,"address":[],"length":0,"stats":{"Line":0}},{"line":988,"address":[],"length":0,"stats":{"Line":0}},{"line":989,"address":[],"length":0,"stats":{"Line":0}},{"line":990,"address":[],"length":0,"stats":{"Line":0}},{"line":993,"address":[],"length":0,"stats":{"Line":0}},{"line":994,"address":[],"length":0,"stats":{"Line":0}},{"line":995,"address":[],"length":0,"stats":{"Line":0}},{"line":998,"address":[],"length":0,"stats":{"Line":0}},{"line":999,"address":[],"length":0,"stats":{"Line":0}},{"line":1001,"address":[],"length":0,"stats":{"Line":0}},{"line":1006,"address":[],"length":0,"stats":{"Line":0}},{"line":1008,"address":[],"length":0,"stats":{"Line":0}},{"line":1009,"address":[],"length":0,"stats":{"Line":0}},{"line":1010,"address":[],"length":0,"stats":{"Line":0}},{"line":1011,"address":[],"length":0,"stats":{"Line":0}}],"covered":143,"coverable":267},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","azure_openai.rs"],"content":"//! Azure OpenAI LLM provider implementation.\n//!\n//! Supports Azure OpenAI Service endpoints with API key or Entra ID authentication.\n//!\n//! # Environment Variables\n//! - `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint (e.g., `https://myresource.openai.azure.com`)\n//! - `AZURE_OPENAI_API_KEY`: API key for authentication\n//! - `AZURE_OPENAI_DEPLOYMENT_NAME`: Deployment name for chat/completion model\n//! - `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Deployment name for embedding model\n//! - `AZURE_OPENAI_API_VERSION`: API version (default: 2024-10-21)\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse futures::StreamExt;\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse tracing::{debug, instrument};\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, FunctionCall, LLMProvider,\n    LLMResponse, StreamChunk, ToolCall, ToolChoice, ToolDefinition,\n};\n\n/// Default Azure OpenAI API version\nconst DEFAULT_API_VERSION: &str = \"2024-10-21\";\n\n/// Azure OpenAI provider configuration\n#[derive(Debug, Clone)]\npub struct AzureOpenAIProvider {\n    client: Client,\n    endpoint: String,\n    api_key: String,\n    deployment_name: String,\n    embedding_deployment_name: String,\n    api_version: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n}\n\n// ============================================================================\n// Azure OpenAI Request/Response Types (same as OpenAI, but different endpoint)\n// ============================================================================\n\n/// Message format for Azure OpenAI\n#[derive(Debug, Clone, Serialize, Deserialize)]\nstruct AzureMessage {\n    role: String,\n    content: String,\n}\n\n/// Chat completion request\n#[derive(Debug, Clone, Serialize)]\nstruct ChatCompletionRequest {\n    messages: Vec<AzureMessage>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    max_tokens: Option<usize>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    max_completion_tokens: Option<usize>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    top_p: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stop: Option<Vec<String>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    frequency_penalty: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    presence_penalty: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stream: Option<bool>,\n    // OODA-11: Tool calling support\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<AzureToolDefinition>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_choice: Option<String>,\n}\n\n/// Chat completion response choice\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\nstruct ChatChoice {\n    index: usize,\n    message: AzureMessageResponse,\n    finish_reason: Option<String>,\n}\n\n/// Response message from Azure OpenAI (includes tool_calls)\n#[derive(Debug, Clone, Deserialize)]\nstruct AzureMessageResponse {\n    #[allow(dead_code)]\n    role: String,\n    #[serde(default)]\n    content: Option<String>,\n    #[serde(default)]\n    tool_calls: Option<Vec<AzureToolCall>>,\n}\n\n/// Token usage information\n#[derive(Debug, Clone, Deserialize, Default)]\nstruct Usage {\n    #[serde(default)]\n    prompt_tokens: usize,\n    #[serde(default)]\n    completion_tokens: usize,\n    #[serde(default)]\n    total_tokens: usize,\n    /// Breakdown of prompt tokens for cache tracking\n    #[serde(default)]\n    prompt_tokens_details: Option<PromptTokensDetails>,\n}\n\n/// Prompt token details including cache hits\n#[derive(Debug, Clone, Deserialize, Default)]\nstruct PromptTokensDetails {\n    /// Number of tokens served from cache\n    #[serde(default)]\n    cached_tokens: Option<usize>,\n}\n\n/// Chat completion response\n#[derive(Debug, Clone, Deserialize)]\nstruct ChatCompletionResponse {\n    id: String,\n    choices: Vec<ChatChoice>,\n    #[serde(default)]\n    usage: Usage,\n    model: String,\n}\n\n/// Embedding request\n#[derive(Debug, Clone, Serialize)]\nstruct EmbeddingRequest {\n    input: Vec<String>,\n}\n\n/// Embedding data\n#[derive(Debug, Clone, Deserialize)]\nstruct EmbeddingData {\n    embedding: Vec<f32>,\n    index: usize,\n}\n\n/// Embedding response\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\nstruct EmbeddingResponse {\n    data: Vec<EmbeddingData>,\n    #[serde(default)]\n    usage: Usage,\n}\n\n/// Error response from Azure OpenAI\n#[derive(Debug, Clone, Deserialize)]\nstruct AzureErrorResponse {\n    error: AzureError,\n}\n\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\nstruct AzureError {\n    code: Option<String>,\n    message: String,\n    #[serde(rename = \"type\")]\n    error_type: Option<String>,\n}\n\n// =========================================================================\n// OODA-11: Tool calling types for Azure OpenAI\n// =========================================================================\n\n/// Tool definition for Azure OpenAI (OpenAI-compatible format)\n#[derive(Debug, Clone, Serialize)]\nstruct AzureToolDefinition {\n    #[serde(rename = \"type\")]\n    type_: String,\n    function: AzureFunctionDefinition,\n}\n\n/// Function definition for Azure OpenAI\n#[derive(Debug, Clone, Serialize)]\nstruct AzureFunctionDefinition {\n    name: String,\n    description: String,\n    parameters: serde_json::Value,\n}\n\n/// Tool call from Azure OpenAI response\n#[derive(Debug, Clone, Deserialize)]\nstruct AzureToolCall {\n    id: String,\n    #[serde(rename = \"type\")]\n    #[allow(dead_code)]\n    type_: String,\n    function: AzureFunctionCall,\n}\n\n/// Function call details\n#[derive(Debug, Clone, Deserialize)]\nstruct AzureFunctionCall {\n    name: String,\n    arguments: String,\n}\n\n/// Streaming chunk for Azure OpenAI\n#[derive(Debug, Clone, Deserialize)]\nstruct StreamingChunk {\n    choices: Vec<StreamingChoice>,\n}\n\n/// Streaming choice\n#[derive(Debug, Clone, Deserialize)]\nstruct StreamingChoice {\n    delta: StreamingDelta,\n    finish_reason: Option<String>,\n}\n\n/// Streaming delta content\n#[derive(Debug, Clone, Deserialize)]\nstruct StreamingDelta {\n    #[serde(default)]\n    content: Option<String>,\n    #[serde(default)]\n    tool_calls: Option<Vec<StreamingToolCall>>,\n}\n\n/// Tool call in streaming format\n#[derive(Debug, Clone, Deserialize)]\nstruct StreamingToolCall {\n    index: usize,\n    #[serde(default)]\n    id: Option<String>,\n    #[serde(default)]\n    function: Option<StreamingFunction>,\n}\n\n/// Function in streaming format\n#[derive(Debug, Clone, Deserialize)]\nstruct StreamingFunction {\n    #[serde(default)]\n    name: Option<String>,\n    #[serde(default)]\n    arguments: Option<String>,\n}\n\n// ============================================================================\n// AzureOpenAIProvider Implementation\n// ============================================================================\n\nimpl AzureOpenAIProvider {\n    /// Create a new Azure OpenAI provider.\n    ///\n    /// # Arguments\n    /// * `endpoint` - Azure OpenAI endpoint (e.g., `https://myresource.openai.azure.com`)\n    /// * `api_key` - API key for authentication\n    /// * `deployment_name` - Deployment name for chat/completion model\n    pub fn new(\n        endpoint: impl Into<String>,\n        api_key: impl Into<String>,\n        deployment_name: impl Into<String>,\n    ) -> Self {\n        let deployment = deployment_name.into();\n        Self {\n            client: Client::new(),\n            endpoint: endpoint.into().trim_end_matches('/').to_string(),\n            api_key: api_key.into(),\n            deployment_name: deployment.clone(),\n            embedding_deployment_name: deployment,\n            api_version: DEFAULT_API_VERSION.to_string(),\n            max_context_length: 128_000, // Default for GPT-4o\n            embedding_dimension: 1536,   // Default for text-embedding-ada-002\n        }\n    }\n\n    /// Create a provider from environment variables.\n    ///\n    /// Reads from:\n    /// - `AZURE_OPENAI_ENDPOINT`\n    /// - `AZURE_OPENAI_API_KEY`\n    /// - `AZURE_OPENAI_DEPLOYMENT_NAME`\n    /// - `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME` (optional)\n    /// - `AZURE_OPENAI_API_VERSION` (optional)\n    pub fn from_env() -> Result<Self> {\n        let endpoint = std::env::var(\"AZURE_OPENAI_ENDPOINT\")\n            .map_err(|_| LlmError::ConfigError(\"AZURE_OPENAI_ENDPOINT not set\".to_string()))?;\n\n        let api_key = std::env::var(\"AZURE_OPENAI_API_KEY\")\n            .map_err(|_| LlmError::ConfigError(\"AZURE_OPENAI_API_KEY not set\".to_string()))?;\n\n        let deployment_name = std::env::var(\"AZURE_OPENAI_DEPLOYMENT_NAME\").map_err(|_| {\n            LlmError::ConfigError(\"AZURE_OPENAI_DEPLOYMENT_NAME not set\".to_string())\n        })?;\n\n        let mut provider = Self::new(endpoint, api_key, deployment_name);\n\n        if let Ok(embedding_deployment) = std::env::var(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\") {\n            provider = provider.with_embedding_deployment(embedding_deployment);\n        }\n\n        if let Ok(api_version) = std::env::var(\"AZURE_OPENAI_API_VERSION\") {\n            provider = provider.with_api_version(api_version);\n        }\n\n        Ok(provider)\n    }\n\n    /// Set the embedding deployment name.\n    pub fn with_embedding_deployment(mut self, deployment_name: impl Into<String>) -> Self {\n        self.embedding_deployment_name = deployment_name.into();\n        self\n    }\n\n    /// Set the API version.\n    pub fn with_api_version(mut self, api_version: impl Into<String>) -> Self {\n        self.api_version = api_version.into();\n        self\n    }\n\n    /// Set the max context length.\n    pub fn with_max_context_length(mut self, max_context_length: usize) -> Self {\n        self.max_context_length = max_context_length;\n        self\n    }\n\n    /// Set the embedding dimension.\n    pub fn with_embedding_dimension(mut self, dimension: usize) -> Self {\n        self.embedding_dimension = dimension;\n        self\n    }\n\n    /// Build URL for a deployment operation.\n    fn build_url(&self, deployment: &str, operation: &str) -> String {\n        format!(\n            \"{}/openai/deployments/{}/{}?api-version={}\",\n            self.endpoint, deployment, operation, self.api_version\n        )\n    }\n\n    /// Convert ChatMessage to Azure format.\n    fn convert_messages(messages: &[ChatMessage]) -> Vec<AzureMessage> {\n        messages\n            .iter()\n            .map(|msg| AzureMessage {\n                role: match msg.role {\n                    ChatRole::System => \"system\".to_string(),\n                    ChatRole::User => \"user\".to_string(),\n                    ChatRole::Assistant => \"assistant\".to_string(),\n                    ChatRole::Tool => \"tool\".to_string(),\n                    ChatRole::Function => \"function\".to_string(),\n                },\n                content: msg.content.clone(),\n            })\n            .collect()\n    }\n\n    /// Send a request and handle errors.\n    async fn send_request<T: for<'de> Deserialize<'de>>(\n        &self,\n        url: &str,\n        body: &impl Serialize,\n    ) -> Result<T> {\n        let response = self\n            .client\n            .post(url)\n            .header(\"api-key\", &self.api_key)\n            .header(\"Content-Type\", \"application/json\")\n            .json(body)\n            .send()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Request failed: {}\", e)))?;\n\n        let status = response.status();\n        let text = response\n            .text()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to read response: {}\", e)))?;\n\n        if !status.is_success() {\n            if let Ok(error_response) = serde_json::from_str::<AzureErrorResponse>(&text) {\n                return Err(LlmError::ApiError(format!(\n                    \"Azure OpenAI error: {}\",\n                    error_response.error.message\n                )));\n            }\n            return Err(LlmError::ApiError(format!(\n                \"Azure OpenAI error ({}): {}\",\n                status, text\n            )));\n        }\n\n        serde_json::from_str(&text).map_err(|e| {\n            LlmError::ApiError(format!(\"Failed to parse response: {}. Body: {}\", e, text))\n        })\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for AzureOpenAIProvider {\n    fn name(&self) -> &str {\n        \"azure-openai\"\n    }\n\n    fn model(&self) -> &str {\n        &self.deployment_name\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    #[instrument(skip(self, prompt), fields(deployment = %self.deployment_name))]\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    #[instrument(skip(self, prompt, options), fields(deployment = %self.deployment_name))]\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    #[instrument(skip(self, messages, options), fields(deployment = %self.deployment_name))]\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let azure_messages = Self::convert_messages(messages);\n        let options = options.cloned().unwrap_or_default();\n\n        let request = ChatCompletionRequest {\n            messages: azure_messages,\n            max_tokens: options.max_tokens,\n            max_completion_tokens: None,\n            temperature: options.temperature,\n            top_p: options.top_p,\n            stop: options.stop,\n            frequency_penalty: options.frequency_penalty,\n            presence_penalty: options.presence_penalty,\n            stream: None,\n            tools: None,\n            tool_choice: None,\n        };\n\n        let url = self.build_url(&self.deployment_name, \"chat/completions\");\n        debug!(\"Sending request to Azure OpenAI: {}\", url);\n\n        let response: ChatCompletionResponse = self.send_request(&url, &request).await?;\n\n        let choice = response\n            .choices\n            .first()\n            .ok_or_else(|| LlmError::ApiError(\"No choices in response\".to_string()))?;\n\n        // Extract cache hit tokens from prompt_tokens_details if available\n        // This enables tracking of KV-cache effectiveness for context engineering\n        let cache_hit_tokens = response\n            .usage\n            .prompt_tokens_details\n            .as_ref()\n            .and_then(|d| d.cached_tokens);\n\n        // Log token usage with cache info\n        if let Some(cached) = cache_hit_tokens {\n            debug!(\n                \"Azure OpenAI token usage - prompt: {}, completion: {}, cached: {} ({:.1}%)\",\n                response.usage.prompt_tokens,\n                response.usage.completion_tokens,\n                cached,\n                (cached as f64 / response.usage.prompt_tokens as f64) * 100.0\n            );\n        } else {\n            debug!(\n                \"Azure OpenAI token usage - prompt: {}, completion: {}, total: {}\",\n                response.usage.prompt_tokens,\n                response.usage.completion_tokens,\n                response.usage.total_tokens\n            );\n        }\n\n        let mut metadata = HashMap::new();\n        metadata.insert(\"response_id\".to_string(), serde_json::json!(response.id));\n\n        Ok(LLMResponse {\n            content: choice.message.content.clone().unwrap_or_default(),\n            prompt_tokens: response.usage.prompt_tokens,\n            completion_tokens: response.usage.completion_tokens,\n            total_tokens: response.usage.total_tokens,\n            model: response.model,\n            finish_reason: choice.finish_reason.clone(),\n            tool_calls: Vec::new(),\n            metadata,\n            cache_hit_tokens,\n            // OODA-15: Reasoning tokens not yet extracted\n            thinking_tokens: None,\n            thinking_content: None,\n        })\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        let messages = vec![ChatMessage::user(prompt)];\n        let azure_messages = Self::convert_messages(&messages);\n\n        let request = ChatCompletionRequest {\n            messages: azure_messages,\n            max_tokens: None,\n            max_completion_tokens: None,\n            temperature: None,\n            top_p: None,\n            stop: None,\n            frequency_penalty: None,\n            presence_penalty: None,\n            stream: Some(true),\n            tools: None,\n            tool_choice: None,\n        };\n\n        let url = self.build_url(&self.deployment_name, \"chat/completions\");\n\n        let response = self\n            .client\n            .post(&url)\n            .header(\"api-key\", &self.api_key)\n            .header(\"Content-Type\", \"application/json\")\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Stream request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\"Stream error: {}\", text)));\n        }\n\n        let stream = response.bytes_stream();\n\n        let mapped_stream = stream.map(|result| {\n            match result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    // Parse SSE format\n                    let mut content = String::new();\n                    for line in text.lines() {\n                        if let Some(data) = line.strip_prefix(\"data: \") {\n                            if data == \"[DONE]\" {\n                                continue;\n                            }\n                            if let Ok(chunk) = serde_json::from_str::<serde_json::Value>(data) {\n                                if let Some(delta_content) = chunk\n                                    .get(\"choices\")\n                                    .and_then(|c| c.get(0))\n                                    .and_then(|c| c.get(\"delta\"))\n                                    .and_then(|d| d.get(\"content\"))\n                                    .and_then(|c| c.as_str())\n                                {\n                                    content.push_str(delta_content);\n                                }\n                            }\n                        }\n                    }\n                    Ok(content)\n                }\n                Err(e) => Err(LlmError::ApiError(format!(\"Stream error: {}\", e))),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_json_mode(&self) -> bool {\n        true\n    }\n    \n    fn supports_function_calling(&self) -> bool {\n        true\n    }\n    \n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n    \n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let azure_messages = Self::convert_messages(messages);\n        let opts = options.cloned().unwrap_or_default();\n        \n        // Convert tools to Azure format\n        let azure_tools: Vec<AzureToolDefinition> = tools\n            .iter()\n            .map(|t| AzureToolDefinition {\n                type_: \"function\".to_string(),\n                function: AzureFunctionDefinition {\n                    name: t.function.name.clone(),\n                    description: t.function.description.clone(),\n                    parameters: t.function.parameters.clone(),\n                },\n            })\n            .collect();\n        \n        // Convert tool_choice\n        let azure_tool_choice = tool_choice.map(|tc| match tc {\n            ToolChoice::Auto(_) => \"auto\".to_string(),\n            ToolChoice::Required(_) => \"required\".to_string(),\n            ToolChoice::Function { function, .. } => {\n                format!(\"{{\\\"type\\\":\\\"function\\\",\\\"function\\\":{{\\\"name\\\":\\\"{}\\\"}}}}\", function.name)\n            }\n        });\n        \n        let request = ChatCompletionRequest {\n            messages: azure_messages,\n            max_tokens: opts.max_tokens,\n            max_completion_tokens: None,\n            temperature: opts.temperature,\n            top_p: opts.top_p,\n            stop: opts.stop,\n            frequency_penalty: opts.frequency_penalty,\n            presence_penalty: opts.presence_penalty,\n            stream: None,\n            tools: Some(azure_tools),\n            tool_choice: azure_tool_choice,\n        };\n        \n        let url = self.build_url(&self.deployment_name, \"chat/completions\");\n        debug!(\"Sending tool request to Azure OpenAI: {}\", url);\n        \n        let response: ChatCompletionResponse = self.send_request(&url, &request).await?;\n        \n        let choice = response\n            .choices\n            .first()\n            .ok_or_else(|| LlmError::ApiError(\"No choices in response\".to_string()))?;\n        \n        // Convert tool calls\n        let tool_calls: Vec<ToolCall> = choice\n            .message\n            .tool_calls\n            .as_ref()\n            .map(|tcs| {\n                tcs.iter()\n                    .map(|tc| ToolCall {\n                        id: tc.id.clone(),\n                        call_type: \"function\".to_string(),\n                        function: FunctionCall {\n                            name: tc.function.name.clone(),\n                            arguments: tc.function.arguments.clone(),\n                        },\n                    })\n                    .collect()\n            })\n            .unwrap_or_default();\n        \n        let cache_hit_tokens = response\n            .usage\n            .prompt_tokens_details\n            .as_ref()\n            .and_then(|d| d.cached_tokens);\n        \n        let mut metadata = HashMap::new();\n        metadata.insert(\"response_id\".to_string(), serde_json::json!(response.id));\n        \n        Ok(LLMResponse {\n            content: choice.message.content.clone().unwrap_or_default(),\n            prompt_tokens: response.usage.prompt_tokens,\n            completion_tokens: response.usage.completion_tokens,\n            total_tokens: response.usage.total_tokens,\n            model: response.model,\n            finish_reason: choice.finish_reason.clone(),\n            tool_calls,\n            metadata,\n            cache_hit_tokens,\n            thinking_tokens: None,\n            thinking_content: None,\n        })\n    }\n    \n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        let azure_messages = Self::convert_messages(messages);\n        let opts = options.cloned().unwrap_or_default();\n        \n        let azure_tools: Vec<AzureToolDefinition> = tools\n            .iter()\n            .map(|t| AzureToolDefinition {\n                type_: \"function\".to_string(),\n                function: AzureFunctionDefinition {\n                    name: t.function.name.clone(),\n                    description: t.function.description.clone(),\n                    parameters: t.function.parameters.clone(),\n                },\n            })\n            .collect();\n        \n        let azure_tool_choice = tool_choice.map(|tc| match tc {\n            ToolChoice::Auto(_) => \"auto\".to_string(),\n            ToolChoice::Required(_) => \"required\".to_string(),\n            ToolChoice::Function { function, .. } => {\n                format!(\"{{\\\"type\\\":\\\"function\\\",\\\"function\\\":{{\\\"name\\\":\\\"{}\\\"}}}}\", function.name)\n            }\n        });\n        \n        let request = ChatCompletionRequest {\n            messages: azure_messages,\n            max_tokens: opts.max_tokens,\n            max_completion_tokens: None,\n            temperature: opts.temperature,\n            top_p: opts.top_p,\n            stop: opts.stop,\n            frequency_penalty: opts.frequency_penalty,\n            presence_penalty: opts.presence_penalty,\n            stream: Some(true),\n            tools: Some(azure_tools),\n            tool_choice: azure_tool_choice,\n        };\n        \n        let url = self.build_url(&self.deployment_name, \"chat/completions\");\n        \n        let response = self\n            .client\n            .post(&url)\n            .header(\"api-key\", &self.api_key)\n            .header(\"Content-Type\", \"application/json\")\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Stream request failed: {}\", e)))?;\n        \n        if !response.status().is_success() {\n            let text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\"Stream error: {}\", text)));\n        }\n        \n        let stream = response.bytes_stream();\n        \n        let mapped_stream = stream.map(|result| {\n            match result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    \n                    for line in text.lines() {\n                        let line = line.trim();\n                        if line.is_empty() {\n                            continue;\n                        }\n                        if line == \"data: [DONE]\" {\n                            return Ok(StreamChunk::Finished {\n                                reason: \"stop\".to_string(),\n                                ttft_ms: None,\n                            });\n                        }\n                        if let Some(json_str) = line.strip_prefix(\"data: \") {\n                            if let Ok(chunk) = serde_json::from_str::<StreamingChunk>(json_str) {\n                                if let Some(choice) = chunk.choices.first() {\n                                    // Check finish reason\n                                    if let Some(reason) = &choice.finish_reason {\n                                        return Ok(StreamChunk::Finished {\n                                            reason: reason.clone(),\n                                            ttft_ms: None,\n                                        });\n                                    }\n                                    \n                                    // Check for tool calls\n                                    if let Some(tool_calls) = &choice.delta.tool_calls {\n                                        if let Some(tc) = tool_calls.first() {\n                                            return Ok(StreamChunk::ToolCallDelta {\n                                                index: tc.index,\n                                                id: tc.id.clone(),\n                                                function_name: tc.function.as_ref().and_then(|f| f.name.clone()),\n                                                function_arguments: tc.function.as_ref().and_then(|f| f.arguments.clone()),\n                                            });\n                                        }\n                                    }\n                                    \n                                    // Check for content\n                                    if let Some(content) = &choice.delta.content {\n                                        if !content.is_empty() {\n                                            return Ok(StreamChunk::Content(content.clone()));\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \n                    Ok(StreamChunk::Content(String::new()))\n                }\n                Err(e) => Err(LlmError::ApiError(format!(\"Stream error: {}\", e))),\n            }\n        });\n        \n        Ok(mapped_stream.boxed())\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for AzureOpenAIProvider {\n    fn name(&self) -> &str {\n        \"azure-openai\"\n    }\n\n    fn model(&self) -> &str {\n        &self.embedding_deployment_name\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        8191 // Azure OpenAI embedding models support 8191 tokens\n    }\n\n    #[instrument(skip(self, texts), fields(deployment = %self.embedding_deployment_name, count = texts.len()))]\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        if texts.is_empty() {\n            return Ok(Vec::new());\n        }\n\n        let request = EmbeddingRequest {\n            input: texts.to_vec(),\n        };\n\n        let url = self.build_url(&self.embedding_deployment_name, \"embeddings\");\n        debug!(\"Sending embedding request to Azure OpenAI: {}\", url);\n\n        let response: EmbeddingResponse = self.send_request(&url, &request).await?;\n\n        // Sort by index to ensure correct ordering\n        let mut embeddings: Vec<_> = response.data.into_iter().collect();\n        embeddings.sort_by_key(|e| e.index);\n\n        Ok(embeddings.into_iter().map(|e| e.embedding).collect())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_provider_creation() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://myresource.openai.azure.com\",\n            \"test-api-key\",\n            \"gpt-4o\",\n        );\n\n        assert_eq!(LLMProvider::name(&provider), \"azure-openai\");\n        assert_eq!(LLMProvider::model(&provider), \"gpt-4o\");\n        assert_eq!(provider.endpoint, \"https://myresource.openai.azure.com\");\n    }\n\n    #[test]\n    fn test_provider_with_options() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://myresource.openai.azure.com/\",\n            \"test-api-key\",\n            \"gpt-4o\",\n        )\n        .with_embedding_deployment(\"text-embedding-ada-002\")\n        .with_api_version(\"2024-06-01\")\n        .with_max_context_length(128_000)\n        .with_embedding_dimension(1536);\n\n        assert_eq!(\n            EmbeddingProvider::model(&provider),\n            \"text-embedding-ada-002\"\n        );\n        assert_eq!(provider.api_version, \"2024-06-01\");\n        assert_eq!(provider.max_context_length(), 128_000);\n        assert_eq!(provider.dimension(), 1536);\n        // Trailing slash should be stripped\n        assert_eq!(provider.endpoint, \"https://myresource.openai.azure.com\");\n    }\n\n    #[test]\n    fn test_build_url() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://myresource.openai.azure.com\",\n            \"test-api-key\",\n            \"gpt-4o\",\n        );\n\n        let url = provider.build_url(\"gpt-4o\", \"chat/completions\");\n        assert_eq!(\n            url,\n            \"https://myresource.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-10-21\"\n        );\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let messages = vec![\n            ChatMessage::system(\"You are helpful\"),\n            ChatMessage::user(\"Hello\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let converted = AzureOpenAIProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 3);\n        assert_eq!(converted[0].role, \"system\");\n        assert_eq!(converted[1].role, \"user\");\n        assert_eq!(converted[2].role, \"assistant\");\n    }\n\n    #[test]\n    fn test_message_conversion_tool_role() {\n        let messages = vec![\n            ChatMessage::tool_result(\"tool-1\", \"Tool result\"),\n        ];\n\n        let converted = AzureOpenAIProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 1);\n        assert_eq!(converted[0].role, \"tool\");\n        assert_eq!(converted[0].content, \"Tool result\");\n    }\n\n    #[test]\n    fn test_provider_defaults() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"test-deployment\",\n        );\n\n        assert_eq!(provider.api_version, \"2024-10-21\");\n        assert_eq!(provider.max_context_length, 128_000);\n        assert_eq!(provider.embedding_dimension, 1536);\n        // Deployment name should be used for both chat and embedding by default\n        assert_eq!(provider.deployment_name, \"test-deployment\");\n        assert_eq!(provider.embedding_deployment_name, \"test-deployment\");\n    }\n\n    #[test]\n    fn test_supports_streaming() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        );\n        assert!(provider.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_json_mode() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        );\n        assert!(provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_embedding_provider_name() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        )\n        .with_embedding_deployment(\"text-embedding-ada-002\");\n\n        assert_eq!(EmbeddingProvider::name(&provider), \"azure-openai\");\n    }\n\n    #[test]\n    fn test_embedding_provider_dimension() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        )\n        .with_embedding_dimension(3072);\n\n        assert_eq!(provider.dimension(), 3072);\n    }\n\n    #[test]\n    fn test_endpoint_trailing_slash_handling() {\n        let provider1 = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com/\",\n            \"key\",\n            \"deployment\",\n        );\n        let _provider2 = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com///\",\n            \"key\",\n            \"deployment\",\n        );\n        let provider3 = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"deployment\",\n        );\n\n        // All should have the same endpoint without trailing slashes\n        assert_eq!(provider1.endpoint, \"https://test.openai.azure.com\");\n        // Note: current impl only strips one trailing slash at a time\n        // This is acceptable behavior - URL should be normalized by caller\n        assert_eq!(provider3.endpoint, \"https://test.openai.azure.com\");\n    }\n\n    #[test]\n    fn test_build_url_embeddings() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://myresource.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        )\n        .with_embedding_deployment(\"text-embedding-ada-002\");\n\n        let url = provider.build_url(\"text-embedding-ada-002\", \"embeddings\");\n        assert!(url.contains(\"/openai/deployments/text-embedding-ada-002/embeddings\"));\n        assert!(url.contains(\"api-version=2024-10-21\"));\n    }\n\n    #[test]\n    fn test_build_url_custom_api_version() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://myresource.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        )\n        .with_api_version(\"2024-06-01\");\n\n        let url = provider.build_url(\"gpt-4o\", \"chat/completions\");\n        assert!(url.contains(\"api-version=2024-06-01\"));\n    }\n\n    #[test]\n    fn test_from_env_missing_endpoint() {\n        // Clear env vars to ensure clean test\n        std::env::remove_var(\"AZURE_OPENAI_ENDPOINT\");\n        std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n        std::env::remove_var(\"AZURE_OPENAI_DEPLOYMENT_NAME\");\n\n        let result = AzureOpenAIProvider::from_env();\n        assert!(result.is_err());\n        let err = result.unwrap_err();\n        assert!(err.to_string().contains(\"AZURE_OPENAI_ENDPOINT\"));\n    }\n\n    #[test]\n    fn test_max_context_length() {\n        let provider = AzureOpenAIProvider::new(\n            \"https://test.openai.azure.com\",\n            \"key\",\n            \"gpt-4o\",\n        )\n        .with_max_context_length(200_000);\n\n        assert_eq!(provider.max_context_length(), 200_000);\n    }\n\n    #[test]\n    fn test_azure_message_serialization() {\n        let msg = AzureMessage {\n            role: \"user\".to_string(),\n            content: \"Hello world\".to_string(),\n        };\n\n        let json = serde_json::to_string(&msg).unwrap();\n        assert!(json.contains(\"\\\"role\\\":\\\"user\\\"\"));\n        assert!(json.contains(\"\\\"content\\\":\\\"Hello world\\\"\"));\n    }\n}\n","traces":[{"line":258,"address":[],"length":0,"stats":{"Line":14}},{"line":263,"address":[],"length":0,"stats":{"Line":42}},{"line":265,"address":[],"length":0,"stats":{"Line":28}},{"line":266,"address":[],"length":0,"stats":{"Line":42}},{"line":267,"address":[],"length":0,"stats":{"Line":42}},{"line":268,"address":[],"length":0,"stats":{"Line":42}},{"line":270,"address":[],"length":0,"stats":{"Line":14}},{"line":284,"address":[],"length":0,"stats":{"Line":1}},{"line":285,"address":[],"length":0,"stats":{"Line":1}},{"line":286,"address":[],"length":0,"stats":{"Line":4}},{"line":288,"address":[],"length":0,"stats":{"Line":0}},{"line":289,"address":[],"length":0,"stats":{"Line":0}},{"line":291,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":302,"address":[],"length":0,"stats":{"Line":0}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":3}},{"line":310,"address":[],"length":0,"stats":{"Line":9}},{"line":311,"address":[],"length":0,"stats":{"Line":3}},{"line":315,"address":[],"length":0,"stats":{"Line":2}},{"line":316,"address":[],"length":0,"stats":{"Line":6}},{"line":317,"address":[],"length":0,"stats":{"Line":2}},{"line":321,"address":[],"length":0,"stats":{"Line":2}},{"line":322,"address":[],"length":0,"stats":{"Line":2}},{"line":323,"address":[],"length":0,"stats":{"Line":2}},{"line":327,"address":[],"length":0,"stats":{"Line":2}},{"line":328,"address":[],"length":0,"stats":{"Line":2}},{"line":329,"address":[],"length":0,"stats":{"Line":2}},{"line":333,"address":[],"length":0,"stats":{"Line":3}},{"line":334,"address":[],"length":0,"stats":{"Line":3}},{"line":341,"address":[],"length":0,"stats":{"Line":2}},{"line":342,"address":[],"length":0,"stats":{"Line":2}},{"line":344,"address":[],"length":0,"stats":{"Line":2}},{"line":345,"address":[],"length":0,"stats":{"Line":4}},{"line":346,"address":[],"length":0,"stats":{"Line":2}},{"line":347,"address":[],"length":0,"stats":{"Line":2}},{"line":348,"address":[],"length":0,"stats":{"Line":2}},{"line":349,"address":[],"length":0,"stats":{"Line":2}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":352,"address":[],"length":0,"stats":{"Line":8}},{"line":358,"address":[],"length":0,"stats":{"Line":0}},{"line":363,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[],"length":0,"stats":{"Line":0}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":373,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":377,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[],"length":0,"stats":{"Line":0}},{"line":380,"address":[],"length":0,"stats":{"Line":0}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":0}},{"line":383,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[],"length":0,"stats":{"Line":0}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":388,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":400,"address":[],"length":0,"stats":{"Line":1}},{"line":401,"address":[],"length":0,"stats":{"Line":1}},{"line":404,"address":[],"length":0,"stats":{"Line":1}},{"line":405,"address":[],"length":0,"stats":{"Line":1}},{"line":408,"address":[],"length":0,"stats":{"Line":2}},{"line":409,"address":[],"length":0,"stats":{"Line":2}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":540,"address":[],"length":0,"stats":{"Line":0}},{"line":549,"address":[],"length":0,"stats":{"Line":0}},{"line":550,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":0}},{"line":554,"address":[],"length":0,"stats":{"Line":0}},{"line":555,"address":[],"length":0,"stats":{"Line":0}},{"line":556,"address":[],"length":0,"stats":{"Line":0}},{"line":557,"address":[],"length":0,"stats":{"Line":0}},{"line":558,"address":[],"length":0,"stats":{"Line":0}},{"line":560,"address":[],"length":0,"stats":{"Line":0}},{"line":561,"address":[],"length":0,"stats":{"Line":0}},{"line":562,"address":[],"length":0,"stats":{"Line":0}},{"line":563,"address":[],"length":0,"stats":{"Line":0}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":565,"address":[],"length":0,"stats":{"Line":0}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":568,"address":[],"length":0,"stats":{"Line":0}},{"line":573,"address":[],"length":0,"stats":{"Line":0}},{"line":575,"address":[],"length":0,"stats":{"Line":0}},{"line":582,"address":[],"length":0,"stats":{"Line":1}},{"line":583,"address":[],"length":0,"stats":{"Line":1}},{"line":586,"address":[],"length":0,"stats":{"Line":1}},{"line":587,"address":[],"length":0,"stats":{"Line":1}},{"line":590,"address":[],"length":0,"stats":{"Line":0}},{"line":591,"address":[],"length":0,"stats":{"Line":0}},{"line":594,"address":[],"length":0,"stats":{"Line":0}},{"line":595,"address":[],"length":0,"stats":{"Line":0}},{"line":612,"address":[],"length":0,"stats":{"Line":0}},{"line":613,"address":[],"length":0,"stats":{"Line":0}},{"line":614,"address":[],"length":0,"stats":{"Line":0}},{"line":615,"address":[],"length":0,"stats":{"Line":0}},{"line":616,"address":[],"length":0,"stats":{"Line":0}},{"line":622,"address":[],"length":0,"stats":{"Line":0}},{"line":623,"address":[],"length":0,"stats":{"Line":0}},{"line":624,"address":[],"length":0,"stats":{"Line":0}},{"line":625,"address":[],"length":0,"stats":{"Line":0}},{"line":626,"address":[],"length":0,"stats":{"Line":0}},{"line":652,"address":[],"length":0,"stats":{"Line":0}},{"line":659,"address":[],"length":0,"stats":{"Line":0}},{"line":660,"address":[],"length":0,"stats":{"Line":0}},{"line":661,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[],"length":0,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":664,"address":[],"length":0,"stats":{"Line":0}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":666,"address":[],"length":0,"stats":{"Line":0}},{"line":669,"address":[],"length":0,"stats":{"Line":0}},{"line":710,"address":[],"length":0,"stats":{"Line":0}},{"line":711,"address":[],"length":0,"stats":{"Line":0}},{"line":712,"address":[],"length":0,"stats":{"Line":0}},{"line":713,"address":[],"length":0,"stats":{"Line":0}},{"line":714,"address":[],"length":0,"stats":{"Line":0}},{"line":719,"address":[],"length":0,"stats":{"Line":0}},{"line":720,"address":[],"length":0,"stats":{"Line":0}},{"line":721,"address":[],"length":0,"stats":{"Line":0}},{"line":722,"address":[],"length":0,"stats":{"Line":0}},{"line":723,"address":[],"length":0,"stats":{"Line":0}},{"line":751,"address":[],"length":0,"stats":{"Line":0}},{"line":760,"address":[],"length":0,"stats":{"Line":0}},{"line":761,"address":[],"length":0,"stats":{"Line":0}},{"line":762,"address":[],"length":0,"stats":{"Line":0}},{"line":763,"address":[],"length":0,"stats":{"Line":0}},{"line":765,"address":[],"length":0,"stats":{"Line":0}},{"line":766,"address":[],"length":0,"stats":{"Line":0}},{"line":767,"address":[],"length":0,"stats":{"Line":0}},{"line":768,"address":[],"length":0,"stats":{"Line":0}},{"line":770,"address":[],"length":0,"stats":{"Line":0}},{"line":771,"address":[],"length":0,"stats":{"Line":0}},{"line":772,"address":[],"length":0,"stats":{"Line":0}},{"line":773,"address":[],"length":0,"stats":{"Line":0}},{"line":776,"address":[],"length":0,"stats":{"Line":0}},{"line":777,"address":[],"length":0,"stats":{"Line":0}},{"line":778,"address":[],"length":0,"stats":{"Line":0}},{"line":780,"address":[],"length":0,"stats":{"Line":0}},{"line":781,"address":[],"length":0,"stats":{"Line":0}},{"line":782,"address":[],"length":0,"stats":{"Line":0}},{"line":783,"address":[],"length":0,"stats":{"Line":0}},{"line":788,"address":[],"length":0,"stats":{"Line":0}},{"line":789,"address":[],"length":0,"stats":{"Line":0}},{"line":791,"address":[],"length":0,"stats":{"Line":0}},{"line":792,"address":[],"length":0,"stats":{"Line":0}},{"line":793,"address":[],"length":0,"stats":{"Line":0}},{"line":794,"address":[],"length":0,"stats":{"Line":0}},{"line":800,"address":[],"length":0,"stats":{"Line":0}},{"line":801,"address":[],"length":0,"stats":{"Line":0}},{"line":802,"address":[],"length":0,"stats":{"Line":0}},{"line":810,"address":[],"length":0,"stats":{"Line":0}},{"line":812,"address":[],"length":0,"stats":{"Line":0}},{"line":822,"address":[],"length":0,"stats":{"Line":1}},{"line":823,"address":[],"length":0,"stats":{"Line":1}},{"line":826,"address":[],"length":0,"stats":{"Line":1}},{"line":827,"address":[],"length":0,"stats":{"Line":1}},{"line":830,"address":[],"length":0,"stats":{"Line":2}},{"line":831,"address":[],"length":0,"stats":{"Line":2}},{"line":834,"address":[],"length":0,"stats":{"Line":0}},{"line":835,"address":[],"length":0,"stats":{"Line":0}}],"covered":49,"coverable":171},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","gemini.rs"],"content":"//! Gemini LLM provider implementation.\n//!\n//! Supports both Google AI Gemini API and VertexAI endpoints.\n//!\n//! # Environment Variables\n//! - `GEMINI_API_KEY`: API key for Google AI Gemini API\n//! - `GOOGLE_APPLICATION_CREDENTIALS`: Path to service account JSON for VertexAI\n//! - `GOOGLE_CLOUD_PROJECT`: GCP project ID for VertexAI\n//! - `GOOGLE_CLOUD_REGION`: GCP region for VertexAI (default: us-central1)\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse tracing::{debug, instrument};\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse,\n    StreamChunk, ToolCall, ToolChoice, ToolDefinition, // OODA-06/07/08: Tool + streaming support\n};\n\n/// Gemini API endpoints\nconst GEMINI_API_BASE: &str = \"https://generativelanguage.googleapis.com/v1beta\";\n\n/// Default models\n// WHY: gemini-2.5-flash is the stable production model as of Jan 2026\n// gemini-3-flash exists in docs but may not be available for all API keys\n// See: https://ai.google.dev/gemini-api/docs/models\nconst DEFAULT_GEMINI_MODEL: &str = \"gemini-2.5-flash\";\nconst DEFAULT_EMBEDDING_MODEL: &str = \"text-embedding-004\";\n\n/// Gemini provider configuration\n#[derive(Debug, Clone)]\npub enum GeminiEndpoint {\n    /// Google AI Gemini API (uses API key)\n    GoogleAI { api_key: String },\n    /// VertexAI endpoint (uses OAuth2/service account)\n    VertexAI {\n        project_id: String,\n        region: String,\n        access_token: String,\n    },\n}\n\n/// Cache state for interior mutability.\n#[derive(Debug, Default)]\nstruct CacheState {\n    content_id: Option<String>,\n    system_hash: Option<u64>,\n}\n\n/// Gemini LLM provider.\n#[derive(Debug)]\npub struct GeminiProvider {\n    client: Client,\n    endpoint: GeminiEndpoint,\n    model: String,\n    embedding_model: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n    /// Cache TTL (time-to-live)\n    cache_ttl: String,\n    /// Cache state with interior mutability for Arc compatibility\n    cache_state: tokio::sync::RwLock<CacheState>,\n}\n\n// ============================================================================\n// Gemini API Request/Response Types\n// ============================================================================\n\n// ============================================================================\n// Image Support Types (OODA-54)\n// ============================================================================\n//\n// Gemini uses inline_data for images:\n//\n// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n// â”‚ parts: [                â”‚\n// â”‚   { text: \"...\" },      â”‚\n// â”‚   { inlineData: {       â”‚\n// â”‚       mimeType: \"...\",  â”‚\n// â”‚       data: \"base64...\" â”‚\n// â”‚     }                   â”‚\n// â”‚   }                     â”‚\n// â”‚ ]                       â”‚\n// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//\n// WHY: Blob struct matches Gemini API Blob type exactly\n// ============================================================================\n\n/// Blob for inline media data (images, etc.)\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct Blob {\n    pub mime_type: String,  // MIME type, e.g., \"image/png\"\n    pub data: String,       // Base64-encoded data\n}\n\n/// Content part for Gemini API (text, inline data, or function call/response)\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n#[serde(rename_all = \"camelCase\")]\npub struct Part {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub text: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub inline_data: Option<Blob>,\n    // OODA-06: Function calling support\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub function_call: Option<FunctionCall>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub function_response: Option<FunctionResponse>,\n    // OODA-25: Thinking support - indicates if this part is a thought summary\n    // When true, this part contains thinking/reasoning content, not final response\n    #[serde(default)]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub thought: Option<bool>,\n}\n\n// ============================================================================\n// Function Calling Types (OODA-06)\n// ============================================================================\n//\n// Gemini function calling API structure:\n//\n// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n// â”‚ Request:                                                      â”‚\n// â”‚   tools: [{ functionDeclarations: [{ name, description }]}]   â”‚\n// â”‚   toolConfig: { functionCallingConfig: { mode: \"AUTO\" }}      â”‚\n// â”‚                                                               â”‚\n// â”‚ Response:                                                     â”‚\n// â”‚   parts: [{ functionCall: { name, args }}]                    â”‚\n// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n/// Function declaration for Gemini tool calling\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct FunctionDeclaration {\n    pub name: String,\n    pub description: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub parameters: Option<serde_json::Value>,\n}\n\n/// Tool wrapper containing function declarations\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct GeminiTool {\n    pub function_declarations: Vec<FunctionDeclaration>,\n}\n\n/// Function calling configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct FunctionCallingConfig {\n    pub mode: String, // AUTO, ANY, NONE\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub allowed_function_names: Option<Vec<String>>,\n}\n\n/// Tool configuration\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct ToolConfig {\n    pub function_calling_config: FunctionCallingConfig,\n}\n\n/// Function call in response\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct FunctionCall {\n    pub name: String,\n    pub args: serde_json::Value,\n}\n\n/// Function response for tool results\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct FunctionResponse {\n    pub name: String,\n    pub response: serde_json::Value,\n}\n\n/// Content for Gemini API\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct Content {\n    pub parts: Vec<Part>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub role: Option<String>,\n}\n\n/// Generation config for Gemini API\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n#[serde(rename_all = \"camelCase\")]\npub struct GenerationConfig {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub max_output_tokens: Option<usize>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub top_p: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub top_k: Option<i32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub stop_sequences: Option<Vec<String>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub response_mime_type: Option<String>,\n}\n\n// ============================================================================\n// OODA-25: Thinking Configuration Types\n// ============================================================================\n//\n// Gemini 2.5 and 3.x models support \"thinking\" - internal reasoning before\n// responding. This config controls thinking behavior:\n//\n// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n// â”‚ include_thoughts: true  â†’ Response includes thought summaries            â”‚\n// â”‚ thinking_level: \"high\"  â†’ Maximum reasoning depth (Gemini 3)             â”‚\n// â”‚ thinking_budget: 1024   â†’ Token budget for thinking (Gemini 2.5)         â”‚\n// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//\n// Reference: https://ai.google.dev/gemini-api/docs/thinking\n// ============================================================================\n\n/// Thinking configuration for Gemini 2.5+/3.x models\n#[derive(Debug, Clone, Serialize, Deserialize, Default)]\n#[serde(rename_all = \"camelCase\")]\npub struct ThinkingConfig {\n    /// Include thought summaries in response (shows model's reasoning)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub include_thoughts: Option<bool>,\n    /// Thinking level for Gemini 3 models: \"minimal\", \"low\", \"medium\", \"high\"\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub thinking_level: Option<String>,\n    /// Token budget for thinking (Gemini 2.5): 0 to 24576, -1 for dynamic\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub thinking_budget: Option<i32>,\n}\n\n/// Safety setting for Gemini API\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct SafetySetting {\n    pub category: String,\n    pub threshold: String,\n}\n\n/// Request to create cached content\n#[derive(Debug, Clone, Serialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct CreateCachedContentRequest {\n    model: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    contents: Option<Vec<Content>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    system_instruction: Option<Content>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<serde_json::Value>>,\n    ttl: String,\n}\n\n/// Response from cachedContents.create\n#[derive(Debug, Clone, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct CachedContentResponse {\n    name: String,\n    /// Usage metadata for the cached content.\n    /// Currently unused but preserved for future cache token tracking.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[allow(dead_code)]\n    usage_metadata: Option<UsageMetadata>,\n}\n\n/// Request body for generateContent\n#[derive(Debug, Clone, Serialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct GenerateContentRequest {\n    contents: Vec<Content>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    generation_config: Option<GenerationConfig>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    system_instruction: Option<Content>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    safety_settings: Option<Vec<SafetySetting>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    cached_content: Option<String>,\n    // OODA-06: Tool support\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<GeminiTool>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_config: Option<ToolConfig>,\n    // OODA-25: Thinking config for Gemini 2.5+/3.x models\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    thinking_config: Option<ThinkingConfig>,\n}\n\n/// Candidate from Gemini response\n#[derive(Debug, Clone, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct Candidate {\n    content: Content,\n    finish_reason: Option<String>,\n    #[serde(default)]\n    safety_ratings: Vec<serde_json::Value>,\n}\n\n/// Usage metadata from Gemini response\n#[derive(Debug, Clone, Deserialize, Default)]\n#[serde(rename_all = \"camelCase\")]\nstruct UsageMetadata {\n    #[serde(default)]\n    prompt_token_count: usize,\n    #[serde(default)]\n    candidates_token_count: usize,\n    #[serde(default)]\n    total_token_count: usize,\n    #[serde(default)]\n    cached_content_token_count: usize,\n    // OODA-25: Track thinking tokens used by Gemini 2.5+/3.x\n    #[serde(default)]\n    thoughts_token_count: usize,\n}\n\n/// Response from generateContent\n#[derive(Debug, Clone, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct GenerateContentResponse {\n    candidates: Option<Vec<Candidate>>,\n    usage_metadata: Option<UsageMetadata>,\n}\n\n/// Request body for embedContent\n#[derive(Debug, Clone, Serialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct EmbedContentRequest {\n    content: Content,\n    /// WHY: Required for batchEmbedContents - each request must specify the model\n    /// Format: \"models/{model}\" e.g. \"models/text-embedding-004\"\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    model: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    task_type: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    title: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    output_dimensionality: Option<usize>,\n}\n\n/// Request body for batchEmbedContents\n#[derive(Debug, Clone, Serialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct BatchEmbedContentsRequest {\n    requests: Vec<EmbedContentRequest>,\n}\n\n/// Embedding values from Gemini response\n#[derive(Debug, Clone, Deserialize)]\nstruct EmbeddingValues {\n    values: Vec<f32>,\n}\n\n/// Response from embedContent\n#[derive(Debug, Clone, Deserialize)]\nstruct EmbedContentResponse {\n    embedding: EmbeddingValues,\n}\n\n/// Response from batchEmbedContents\n#[derive(Debug, Clone, Deserialize)]\nstruct BatchEmbedContentsResponse {\n    embeddings: Vec<EmbeddingValues>,\n}\n\n/// Error response from Gemini API\n#[derive(Debug, Clone, Deserialize)]\nstruct GeminiErrorResponse {\n    error: GeminiError,\n}\n\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\nstruct GeminiError {\n    code: i32,\n    message: String,\n    status: String,\n}\n\n/// OODA-40: Response structure for Gemini /models endpoint\n#[derive(Debug, Clone, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct GeminiModelsResponse {\n    #[serde(default)]\n    pub models: Vec<GeminiModelInfo>,\n}\n\n/// OODA-40: Individual model info from Gemini API\n#[derive(Debug, Clone, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct GeminiModelInfo {\n    /// Full model name (e.g., \"models/gemini-2.5-flash\")\n    pub name: String,\n    /// Display name (e.g., \"Gemini 2.5 Flash\")\n    #[serde(default)]\n    pub display_name: String,\n    /// Description of the model\n    #[serde(default)]\n    pub description: String,\n    /// Max input tokens\n    #[serde(default)]\n    pub input_token_limit: Option<u32>,\n    /// Max output tokens\n    #[serde(default)]\n    pub output_token_limit: Option<u32>,\n    /// Supported generation methods\n    #[serde(default)]\n    pub supported_generation_methods: Vec<String>,\n}\n\n// ============================================================================\n// GeminiProvider Implementation\n// ============================================================================\n\nimpl GeminiProvider {\n    /// Create a new Gemini provider using Google AI API key.\n    ///\n    /// # Arguments\n    /// * `api_key` - Google AI API key (from <https://aistudio.google.com/app/apikey>)\n    pub fn new(api_key: impl Into<String>) -> Self {\n        Self {\n            client: Client::new(),\n            endpoint: GeminiEndpoint::GoogleAI {\n                api_key: api_key.into(),\n            },\n            model: DEFAULT_GEMINI_MODEL.to_string(),\n            embedding_model: DEFAULT_EMBEDDING_MODEL.to_string(),\n            max_context_length: 2_000_000, // Gemini 3 supports up to 2M tokens\n            embedding_dimension: 768,      // text-embedding-004 default\n            cache_ttl: \"3600s\".to_string(),\n            cache_state: tokio::sync::RwLock::new(CacheState::default()),\n        }\n    }\n\n    /// Create a provider from environment variables.\n    ///\n    /// Checks for `GEMINI_API_KEY` first, then falls back to VertexAI credentials.\n    pub fn from_env() -> Result<Self> {\n        // Try Google AI API key first\n        if let Ok(api_key) = std::env::var(\"GEMINI_API_KEY\") {\n            return Ok(Self::new(api_key));\n        }\n\n        // Try VertexAI credentials\n        Self::from_env_vertex_ai()\n    }\n\n    /// Create a VertexAI provider from environment variables.\n    ///\n    /// OODA-95: This method is specifically for VertexAI endpoint usage.\n    /// It will auto-obtain the access token via `gcloud auth print-access-token`\n    /// if `GOOGLE_ACCESS_TOKEN` is not set.\n    ///\n    /// # Environment Variables\n    /// - `GOOGLE_CLOUD_PROJECT`: Required. GCP project ID.\n    /// - `GOOGLE_CLOUD_REGION`: Optional. GCP region (default: us-central1).\n    /// - `GOOGLE_ACCESS_TOKEN`: Optional. If not set, obtained via gcloud CLI.\n    pub fn from_env_vertex_ai() -> Result<Self> {\n        let project_id = std::env::var(\"GOOGLE_CLOUD_PROJECT\").map_err(|_| {\n            LlmError::ConfigError(\n                \"VertexAI requires GOOGLE_CLOUD_PROJECT environment variable. \\\n                 Run: export GOOGLE_CLOUD_PROJECT=your-project-id\".to_string()\n            )\n        })?;\n\n        let region = std::env::var(\"GOOGLE_CLOUD_REGION\")\n            .unwrap_or_else(|_| \"us-central1\".to_string());\n\n        // Try to get access token from env, or obtain via gcloud CLI\n        let access_token = match std::env::var(\"GOOGLE_ACCESS_TOKEN\") {\n            Ok(token) if !token.is_empty() => token,\n            _ => Self::get_access_token_from_gcloud()?,\n        };\n\n        Ok(Self::vertex_ai(project_id, region, access_token))\n    }\n\n    /// Get access token from gcloud CLI.\n    ///\n    /// OODA-95: Runs `gcloud auth print-access-token` to obtain OAuth2 token.\n    fn get_access_token_from_gcloud() -> Result<String> {\n        use std::process::Command;\n\n        debug!(\"Obtaining access token via gcloud auth print-access-token\");\n\n        let output = Command::new(\"gcloud\")\n            .args([\"auth\", \"print-access-token\"])\n            .output()\n            .map_err(|e| {\n                LlmError::ConfigError(format!(\n                    \"Failed to run 'gcloud auth print-access-token': {}. \\\n                     Make sure gcloud CLI is installed and you're authenticated. \\\n                     Run: gcloud auth login\",\n                    e\n                ))\n            })?;\n\n        if !output.status.success() {\n            let stderr = String::from_utf8_lossy(&output.stderr);\n            return Err(LlmError::ConfigError(format!(\n                \"gcloud auth print-access-token failed: {}. \\\n                 Run: gcloud auth login\",\n                stderr.trim()\n            )));\n        }\n\n        let token = String::from_utf8_lossy(&output.stdout).trim().to_string();\n        if token.is_empty() {\n            return Err(LlmError::ConfigError(\n                \"gcloud auth print-access-token returned empty token. \\\n                 Run: gcloud auth login\".to_string()\n            ));\n        }\n\n        Ok(token)\n    }\n\n    /// Create a VertexAI provider.\n    ///\n    /// # Arguments\n    /// * `project_id` - GCP project ID\n    /// * `region` - GCP region (e.g., \"us-central1\")\n    /// * `access_token` - OAuth2 access token\n    pub fn vertex_ai(\n        project_id: impl Into<String>,\n        region: impl Into<String>,\n        access_token: impl Into<String>,\n    ) -> Self {\n        Self {\n            client: Client::new(),\n            endpoint: GeminiEndpoint::VertexAI {\n                project_id: project_id.into(),\n                region: region.into(),\n                access_token: access_token.into(),\n            },\n            model: DEFAULT_GEMINI_MODEL.to_string(),\n            embedding_model: DEFAULT_EMBEDDING_MODEL.to_string(),\n            max_context_length: 1_000_000,\n            embedding_dimension: 768,\n            cache_ttl: \"3600s\".to_string(),\n            cache_state: tokio::sync::RwLock::new(CacheState::default()),\n        }\n    }\n\n    /// Configure cache TTL (time-to-live).\n    ///\n    /// Default: \"3600s\" (1 hour). Format: duration with 's' suffix (e.g., \"7200s\" for 2 hours).\n    pub fn with_cache_ttl(mut self, ttl: impl Into<String>) -> Self {\n        self.cache_ttl = ttl.into();\n        self\n    }\n\n    /// Set the model to use.\n    pub fn with_model(mut self, model: impl Into<String>) -> Self {\n        let model_name = model.into();\n        self.max_context_length = Self::context_length_for_model(&model_name);\n        self.model = model_name;\n        self\n    }\n\n    /// Set the embedding model to use.\n    pub fn with_embedding_model(mut self, model: impl Into<String>) -> Self {\n        let model_name = model.into();\n        self.embedding_dimension = Self::dimension_for_model(&model_name);\n        self.embedding_model = model_name;\n        self\n    }\n\n    /// Get context length for a given model.\n    pub fn context_length_for_model(model: &str) -> usize {\n        match model {\n            // Gemini 3 series (2026 models)\n            m if m.contains(\"gemini-3-pro\") => 2_000_000,\n            m if m.contains(\"gemini-3-flash\") => 2_000_000,\n\n            // Gemini 2.5 series\n            m if m.contains(\"gemini-2.5-pro\") => 1_000_000,\n            m if m.contains(\"gemini-2.5-flash\") => 1_000_000,\n\n            // Gemini 2.0 series\n            m if m.contains(\"gemini-2.0\") => 1_000_000,\n\n            // Gemini 1.5 series\n            m if m.contains(\"gemini-1.5-pro\") => 2_000_000,\n            m if m.contains(\"gemini-1.5-flash\") => 1_000_000,\n\n            // Gemini 1.0 series\n            m if m.contains(\"gemini-1.0\") => 32_000,\n\n            _ => 1_000_000, // Updated default\n        }\n    }\n\n    /// Get embedding dimension for a given model.\n    pub fn dimension_for_model(model: &str) -> usize {\n        match model {\n            m if m.contains(\"text-embedding-004\") => 768,\n            m if m.contains(\"text-embedding-005\") => 768,\n            m if m.contains(\"embedding-001\") => 768,\n            m if m.contains(\"text-multilingual-embedding-002\") => 768,\n            _ => 768, // Default dimension\n        }\n    }\n\n    /// List available Gemini models via the API.\n    ///\n    /// # OODA-40: Dynamic Model Discovery\n    ///\n    /// Fetches the list of available models from the Gemini API.\n    /// This enables dynamic model selection instead of relying on a static registry.\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let provider = GeminiProvider::from_env()?;\n    /// let models = provider.list_models().await?;\n    /// for model in models.models {\n    ///     println!(\"Available: {} ({})\", model.name, model.display_name);\n    /// }\n    /// ```\n    pub async fn list_models(&self) -> Result<GeminiModelsResponse> {\n        let url = match &self.endpoint {\n            GeminiEndpoint::GoogleAI { api_key } => {\n                format!(\"{}/models?key={}\", GEMINI_API_BASE, api_key)\n            }\n            GeminiEndpoint::VertexAI {\n                project_id,\n                region,\n                access_token: _,\n            } => {\n                format!(\n                    \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/publishers/google/models\",\n                    region, project_id, region\n                )\n            }\n        };\n\n        debug!(url = %url, \"Fetching Gemini models list\");\n\n        let mut req = self.client.get(&url);\n\n        // Add auth header for VertexAI\n        if let GeminiEndpoint::VertexAI { access_token, .. } = &self.endpoint {\n            req = req.bearer_auth(access_token);\n        }\n\n        let response = req.send().await.map_err(|e| {\n            LlmError::NetworkError(format!(\"Failed to fetch Gemini models: {}\", e))\n        })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Gemini /models returned {}: {}\",\n                status, body\n            )));\n        }\n\n        response.json::<GeminiModelsResponse>().await.map_err(|e| {\n            LlmError::ProviderError(format!(\"Failed to parse models response: {}\", e))\n        })\n    }\n\n    /// Create or reuse cached content for system instruction.\n    ///\n    /// Returns cached_content ID (e.g., \"cachedContents/abc123\").\n    /// Creates new cache if:\n    /// - No cache exists (cached_content_id is None)\n    /// - System instruction changed (cached_system_hash mismatch)\n    ///\n    /// Reuses existing cache otherwise.\n    #[instrument(skip(self, system_instruction))]\n    async fn ensure_cache(&self, system_instruction: &Content) -> Result<String> {\n        use std::collections::hash_map::DefaultHasher;\n        use std::hash::{Hash, Hasher};\n\n        // Hash system instruction to detect changes\n        let mut hasher = DefaultHasher::new();\n        serde_json::to_string(system_instruction)\n            .map_err(LlmError::SerializationError)?\n            .hash(&mut hasher);\n        let current_hash = hasher.finish();\n\n        // Check if we can reuse existing cache (read lock)\n        {\n            let cache = self.cache_state.read().await;\n            if let (Some(cache_id), Some(cached_hash)) = (&cache.content_id, cache.system_hash) {\n                if cached_hash == current_hash {\n                    debug!(\"Reusing cached content: {}\", cache_id);\n                    return Ok(cache_id.clone());\n                } else {\n                    debug!(\"System instruction changed, creating new cache\");\n                }\n            }\n        }\n\n        // Create new cache\n        debug!(\"Creating cached content (ttl: {})\", self.cache_ttl);\n\n        let request = CreateCachedContentRequest {\n            model: format!(\"models/{}\", self.model),\n            contents: None,\n            system_instruction: Some(system_instruction.clone()),\n            tools: None,\n            ttl: self.cache_ttl.clone(),\n        };\n\n        let url = match &self.endpoint {\n            GeminiEndpoint::GoogleAI { api_key } => {\n                format!(\"{}/cachedContents?key={}\", GEMINI_API_BASE, api_key)\n            }\n            GeminiEndpoint::VertexAI {\n                project_id, region, ..\n            } => {\n                format!(\n                    \"https://{}-aiplatform.googleapis.com/v1beta/projects/{}/locations/{}/cachedContents\",\n                    region, project_id, region\n                )\n            }\n        };\n\n        let mut req = self.client.post(&url).json(&request);\n\n        // Add auth header for VertexAI\n        if let GeminiEndpoint::VertexAI { access_token, .. } = &self.endpoint {\n            req = req.bearer_auth(access_token);\n        }\n\n        let response = req\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n            return Err(LlmError::ApiError(format!(\n                \"Failed to create cached content (status {}): {}\",\n                status, error_text\n            )));\n        }\n\n        let cache_response: CachedContentResponse = response.json().await.map_err(|e| {\n            LlmError::NetworkError(format!(\"Failed to parse cache response: {}\", e))\n        })?;\n\n        // Store cache ID and hash (write lock)\n        {\n            let mut cache = self.cache_state.write().await;\n            cache.content_id = Some(cache_response.name.clone());\n            cache.system_hash = Some(current_hash);\n        }\n\n        debug!(\"Created cached content: {}\", cache_response.name);\n        Ok(cache_response.name)\n    }\n\n    /// Build the URL for a Gemini API endpoint.\n    fn build_url(&self, model: &str, action: &str) -> String {\n        // Strip provider prefix from model name (e.g., \"vertexai:gemini-2.5-flash\" -> \"gemini-2.5-flash\")\n        // This allows the same model ID to work for both GoogleAI and VertexAI endpoints\n        let model_without_prefix = model\n            .strip_prefix(\"vertexai:\")\n            .or_else(|| model.strip_prefix(\"gemini:\"))\n            .or_else(|| model.strip_prefix(\"google:\"))\n            .unwrap_or(model);\n        \n        // Gemini 3 models require the \"-preview\" suffix on VertexAI\n        // Auto-add it for user convenience if they forget\n        let model_name = if model_without_prefix.starts_with(\"gemini-3-\") \n            && !model_without_prefix.ends_with(\"-preview\") \n        {\n            format!(\"{}-preview\", model_without_prefix)\n        } else {\n            model_without_prefix.to_string()\n        };\n        \n        match &self.endpoint {\n            GeminiEndpoint::GoogleAI { api_key } => {\n                format!(\n                    \"{}/models/{}:{}?key={}\",\n                    GEMINI_API_BASE, model_name, action, api_key\n                )\n            }\n            GeminiEndpoint::VertexAI {\n                project_id, region, ..\n            } => {\n                format!(\n                    \"https://{}-aiplatform.googleapis.com/v1/projects/{}/locations/{}/publishers/google/models/{}:{}\",\n                    region, project_id, region, model_name, action\n                )\n            }\n        }\n    }\n\n    /// Get authorization headers for the request.\n    fn auth_headers(&self) -> Vec<(&'static str, String)> {\n        match &self.endpoint {\n            GeminiEndpoint::GoogleAI { .. } => {\n                // API key is in URL for Google AI\n                vec![]\n            }\n            GeminiEndpoint::VertexAI { access_token, .. } => {\n                vec![(\"Authorization\", format!(\"Bearer {}\", access_token))]\n            }\n        }\n    }\n\n    /// Convert ChatMessage to Gemini Content format.\n    fn convert_messages(messages: &[ChatMessage]) -> (Option<Content>, Vec<Content>) {\n        let mut system_instruction = None;\n        let mut contents = Vec::new();\n\n        for msg in messages {\n            match msg.role {\n                ChatRole::System => {\n                    // Gemini uses system_instruction field for system prompts\n                    system_instruction = Some(Content {\n                        parts: vec![Part {\n                            text: Some(msg.content.clone()),\n                            ..Default::default()\n                        }],\n                        role: None,\n                    });\n                }\n                ChatRole::User => {\n                    // OODA-54: Check if message has images for multipart content\n                    if msg.has_images() {\n                        let mut parts = Vec::new();\n                        \n                        // Add text part first (if non-empty)\n                        if !msg.content.is_empty() {\n                            parts.push(Part {\n                                text: Some(msg.content.clone()),\n                                ..Default::default()\n                            });\n                        }\n                        \n                        // Add image parts\n                        if let Some(ref images) = msg.images {\n                            for img in images {\n                                parts.push(Part {\n                                    inline_data: Some(Blob {\n                                        mime_type: img.mime_type.clone(),\n                                        data: img.data.clone(),\n                                    }),\n                                    ..Default::default()\n                                });\n                            }\n                        }\n                        \n                        contents.push(Content {\n                            parts,\n                            role: Some(\"user\".to_string()),\n                        });\n                    } else {\n                        contents.push(Content {\n                            parts: vec![Part {\n                                text: Some(msg.content.clone()),\n                                ..Default::default()\n                            }],\n                            role: Some(\"user\".to_string()),\n                        });\n                    }\n                }\n                ChatRole::Assistant => {\n                    contents.push(Content {\n                        parts: vec![Part {\n                            text: Some(msg.content.clone()),\n                            ..Default::default()\n                        }],\n                        role: Some(\"model\".to_string()),\n                    });\n                }\n                ChatRole::Tool | ChatRole::Function => {\n                    // Handle tool/function messages as user messages with context\n                    contents.push(Content {\n                        parts: vec![Part {\n                            text: Some(msg.content.clone()),\n                            ..Default::default()\n                        }],\n                        role: Some(\"user\".to_string()),\n                    });\n                }\n            }\n        }\n\n        (system_instruction, contents)\n    }\n\n    /// Send a request and handle errors.\n    async fn send_request<T: for<'de> Deserialize<'de>>(\n        &self,\n        url: &str,\n        body: &impl Serialize,\n    ) -> Result<T> {\n        let mut request = self.client.post(url).json(body);\n\n        for (key, value) in self.auth_headers() {\n            request = request.header(key, value);\n        }\n\n        let response = request\n            .send()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Request failed: {}\", e)))?;\n\n        let status = response.status();\n        let text = response\n            .text()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to read response: {}\", e)))?;\n\n        if !status.is_success() {\n            // Try to parse error response\n            if let Ok(error_response) = serde_json::from_str::<GeminiErrorResponse>(&text) {\n                return Err(LlmError::ApiError(format!(\n                    \"Gemini API error ({}): {}\",\n                    error_response.error.code, error_response.error.message\n                )));\n            }\n            return Err(LlmError::ApiError(format!(\n                \"Gemini API error ({}): {}\",\n                status, text\n            )));\n        }\n\n        serde_json::from_str(&text).map_err(|e| {\n            LlmError::ApiError(format!(\"Failed to parse response: {}. Body: {}\", e, text))\n        })\n    }\n    \n    // =========================================================================\n    // OODA-06: Tool Conversion Methods\n    // =========================================================================\n    \n    /// Remove `$schema` field from JSON object (Gemini doesn't accept it)\n    fn sanitize_parameters(mut params: serde_json::Value) -> serde_json::Value {\n        if let Some(obj) = params.as_object_mut() {\n            obj.remove(\"$schema\");\n            \n            // Also sanitize nested objects in properties, items, etc.\n            for (_key, value) in obj.iter_mut() {\n                if value.is_object() || value.is_array() {\n                    *value = Self::sanitize_parameters(value.clone());\n                }\n            }\n        } else if let Some(arr) = params.as_array_mut() {\n            for item in arr.iter_mut() {\n                if item.is_object() || item.is_array() {\n                    *item = Self::sanitize_parameters(item.clone());\n                }\n            }\n        }\n        params\n    }\n    \n    /// Convert EdgeCode ToolDefinition to Gemini FunctionDeclaration format\n    fn convert_tools(tools: &[ToolDefinition]) -> Vec<GeminiTool> {\n        let declarations: Vec<FunctionDeclaration> = tools\n            .iter()\n            .map(|tool| {\n                // GEMINI-SCHEMA-FIX: Remove $schema field that Gemini doesn't accept\n                let sanitized_params = Self::sanitize_parameters(tool.function.parameters.clone());\n                \n                FunctionDeclaration {\n                    name: tool.function.name.clone(),\n                    description: tool.function.description.clone(),\n                    parameters: Some(sanitized_params),\n                }\n            })\n            .collect();\n        \n        vec![GeminiTool { function_declarations: declarations }]\n    }\n    \n    /// Convert ToolChoice to Gemini ToolConfig\n    fn convert_tool_choice(tool_choice: Option<ToolChoice>) -> Option<ToolConfig> {\n        let mode = match &tool_choice {\n            None => \"AUTO\",\n            Some(ToolChoice::Auto(s)) if s == \"auto\" => \"AUTO\",\n            Some(ToolChoice::Auto(s)) if s == \"none\" => \"NONE\",\n            Some(ToolChoice::Auto(_)) => \"AUTO\",\n            Some(ToolChoice::Required(_)) => \"ANY\",\n            Some(ToolChoice::Function { function, .. }) => {\n                return Some(ToolConfig {\n                    function_calling_config: FunctionCallingConfig {\n                        mode: \"ANY\".to_string(),\n                        allowed_function_names: Some(vec![function.name.clone()]),\n                    },\n                });\n            }\n        };\n        \n        Some(ToolConfig {\n            function_calling_config: FunctionCallingConfig {\n                mode: mode.to_string(),\n                allowed_function_names: None,\n            },\n        })\n    }\n    \n    // =========================================================================\n    // OODA-25: Thinking Support Detection\n    // =========================================================================\n    \n    /// Check if the current model supports thinking\n    /// \n    /// CRITICAL: As of January 2026, NO Gemini models support thinkingConfig via\n    /// the Google AI API (generativelanguage.googleapis.com). All models return\n    /// \"Unknown name 'thinkingConfig'\" errors (400 Bad Request).\n    /// \n    /// This includes:\n    /// - âŒ Gemini 3 Flash (gemini-3-flash)\n    /// - âŒ Gemini 3 Pro (gemini-3-pro)  \n    /// - âŒ Gemini 2.5 Flash (gemini-2.5-flash)\n    /// - âŒ Gemini 2.5 Pro (gemini-2.5-pro)\n    /// \n    /// The thinkingConfig feature appears to be:\n    /// 1. Documentation-only (not yet in production API)\n    /// 2. Preview SDK-only (official Python/Node SDKs only)\n    /// 3. Or requiring different API endpoint\n    /// \n    /// DISABLE thinking for ALL models until Google enables it in the REST API.\n    /// \n    /// See: <https://ai.google.dev/gemini-api/docs/thinking>\n    /// API Ref: <https://ai.google.dev/api/generate-content>\n    pub fn supports_thinking(&self) -> bool {\n        // DISABLED: API doesn't support thinkingConfig as of Jan 2026\n        false\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for GeminiProvider {\n    fn name(&self) -> &str {\n        match &self.endpoint {\n            GeminiEndpoint::GoogleAI { .. } => \"gemini\",\n            GeminiEndpoint::VertexAI { .. } => \"vertex-ai\",\n        }\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    #[instrument(skip(self, prompt), fields(model = %self.model))]\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    #[instrument(skip(self, prompt, options), fields(model = %self.model))]\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    #[instrument(skip(self, messages, options), fields(model = %self.model))]\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let (system_instruction, contents) = Self::convert_messages(messages);\n\n        if contents.is_empty() {\n            return Err(LlmError::InvalidRequest(\n                \"No user messages provided\".to_string(),\n            ));\n        }\n\n        let options = options.cloned().unwrap_or_default();\n\n        // Build generation config\n        let mut generation_config = GenerationConfig::default();\n\n        if let Some(max_tokens) = options.max_tokens {\n            generation_config.max_output_tokens = Some(max_tokens);\n        }\n        if let Some(temp) = options.temperature {\n            generation_config.temperature = Some(temp);\n        }\n        if let Some(top_p) = options.top_p {\n            generation_config.top_p = Some(top_p);\n        }\n        if let Some(stop) = options.stop {\n            generation_config.stop_sequences = Some(stop);\n        }\n        if options.response_format.as_deref() == Some(\"json_object\") {\n            generation_config.response_mime_type = Some(\"application/json\".to_string());\n        }\n\n        // Create or reuse cache if system instruction exists\n        let cached_content = if let Some(system_inst) = system_instruction.as_ref() {\n            match self.ensure_cache(system_inst).await {\n                Ok(cache_id) => Some(cache_id),\n                Err(e) => {\n                    // Log error but continue without cache\n                    debug!(\n                        \"Failed to create/reuse cache: {}, continuing without cache\",\n                        e\n                    );\n                    None\n                }\n            }\n        } else {\n            None\n        };\n\n        let request = GenerateContentRequest {\n            contents,\n            generation_config: Some(generation_config),\n            system_instruction: if cached_content.is_none() {\n                system_instruction\n            } else {\n                None\n            },\n            safety_settings: None,\n            cached_content,\n            // OODA-06: No tools for regular chat\n            tools: None,\n            tool_config: None,\n            // OODA-25: Enable thinking for Gemini 2.5+/3.x models\n            thinking_config: if self.supports_thinking() {\n                Some(ThinkingConfig {\n                    include_thoughts: Some(true),\n                    thinking_level: None,\n                    thinking_budget: None,\n                })\n            } else {\n                None\n            },\n        };\n\n        let url = self.build_url(&self.model, \"generateContent\");\n        debug!(\"Sending request to Gemini: {}\", url);\n\n        let response: GenerateContentResponse = self.send_request(&url, &request).await?;\n\n        // Extract content from response\n        let candidates = response\n            .candidates\n            .ok_or_else(|| LlmError::ApiError(\"No candidates in response\".to_string()))?;\n\n        let candidate = candidates\n            .first()\n            .ok_or_else(|| LlmError::ApiError(\"Empty candidates array\".to_string()))?;\n\n        // OODA-25: Separate thinking content from regular content\n        let mut content = String::new();\n        let mut thinking_content_parts: Vec<String> = Vec::new();\n        \n        for part in &candidate.content.parts {\n            if let Some(text) = &part.text {\n                if part.thought == Some(true) {\n                    thinking_content_parts.push(text.clone());\n                } else {\n                    content.push_str(text);\n                }\n            }\n        }\n        \n        let thinking_content = if thinking_content_parts.is_empty() {\n            None\n        } else {\n            Some(thinking_content_parts.join(\"\"))\n        };\n\n        let usage = response.usage_metadata.unwrap_or_default();\n\n        let mut metadata = HashMap::new();\n        if !candidate.safety_ratings.is_empty() {\n            metadata.insert(\n                \"safety_ratings\".to_string(),\n                serde_json::json!(candidate.safety_ratings),\n            );\n        }\n\n        Ok(LLMResponse {\n            content,\n            prompt_tokens: usage.prompt_token_count,\n            completion_tokens: usage.candidates_token_count,\n            total_tokens: usage.total_token_count,\n            model: self.model.clone(),\n            finish_reason: candidate.finish_reason.clone(),\n            tool_calls: Vec::new(),\n            metadata,\n            cache_hit_tokens: if usage.cached_content_token_count > 0 {\n                Some(usage.cached_content_token_count)\n            } else {\n                None\n            },\n            // OODA-25: Track thinking tokens and content from Gemini 2.5+/3.x\n            thinking_tokens: if usage.thoughts_token_count > 0 {\n                Some(usage.thoughts_token_count)\n            } else {\n                None\n            },\n            thinking_content,\n        })\n    }\n\n    // =========================================================================\n    // OODA-07: Tool Calling Implementation\n    // =========================================================================\n    //\n    // Gemini function calling enables the model to call tools/functions.\n    // The response may contain functionCall parts that need to be converted\n    // to EdgeCode's ToolCall format.\n    //\n    // Request: { tools: [{ functionDeclarations: [...] }], toolConfig: {...} }\n    // Response: { parts: [{ functionCall: { name, args } }] }\n    // =========================================================================\n    \n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let (system_instruction, contents) = Self::convert_messages(messages);\n        \n        if contents.is_empty() {\n            return Err(LlmError::InvalidRequest(\n                \"No user messages provided\".to_string(),\n            ));\n        }\n        \n        let options = options.cloned().unwrap_or_default();\n        \n        // Build generation config\n        let mut generation_config = GenerationConfig::default();\n        \n        if let Some(max_tokens) = options.max_tokens {\n            generation_config.max_output_tokens = Some(max_tokens);\n        }\n        if let Some(temp) = options.temperature {\n            generation_config.temperature = Some(temp);\n        }\n        if let Some(top_p) = options.top_p {\n            generation_config.top_p = Some(top_p);\n        }\n        if let Some(stop) = options.stop {\n            generation_config.stop_sequences = Some(stop);\n        }\n        \n        // Convert tools to Gemini format\n        let gemini_tools = if tools.is_empty() {\n            None\n        } else {\n            Some(Self::convert_tools(tools))\n        };\n        \n        let gemini_tool_config = Self::convert_tool_choice(tool_choice);\n        \n        // OODA-25: Enable thinking for Gemini 2.5+/3.x models\n        let thinking_config = if self.supports_thinking() {\n            Some(ThinkingConfig {\n                include_thoughts: Some(true),\n                thinking_level: None,\n                thinking_budget: None,\n            })\n        } else {\n            None\n        };\n        \n        let request = GenerateContentRequest {\n            contents,\n            generation_config: Some(generation_config),\n            system_instruction,\n            safety_settings: None,\n            cached_content: None,\n            tools: gemini_tools,\n            tool_config: gemini_tool_config,\n            thinking_config,\n        };\n        \n        let url = self.build_url(&self.model, \"generateContent\");\n        debug!(\"Sending chat_with_tools request to Gemini: {}\", url);\n        \n        let response: GenerateContentResponse = self.send_request(&url, &request).await?;\n        \n        // Parse response\n        let candidates = response\n            .candidates\n            .ok_or_else(|| LlmError::ApiError(\"No candidates in response\".to_string()))?;\n        \n        let candidate = candidates\n            .first()\n            .ok_or_else(|| LlmError::ApiError(\"Empty candidates array\".to_string()))?;\n        \n        let mut content = String::new();\n        let mut tool_calls = Vec::new();\n        let mut thinking_content_parts: Vec<String> = Vec::new();\n        \n        // OODA-25: Parse all parts - separate thinking content from regular content\n        for part in &candidate.content.parts {\n            // Text content - check if thinking or regular\n            if let Some(text) = &part.text {\n                if part.thought == Some(true) {\n                    thinking_content_parts.push(text.clone());\n                } else {\n                    content.push_str(text);\n                }\n            }\n            // Function call - convert to ToolCall\n            if let Some(fc) = &part.function_call {\n                tool_calls.push(ToolCall {\n                    id: format!(\"call_{}\", uuid::Uuid::new_v4().to_string().replace('-', \"\")),\n                    call_type: \"function\".to_string(),\n                    function: crate::traits::FunctionCall {\n                        name: fc.name.clone(),\n                        arguments: fc.args.to_string(),\n                    },\n                });\n            }\n        }\n        \n        let thinking_content = if thinking_content_parts.is_empty() {\n            None\n        } else {\n            Some(thinking_content_parts.join(\"\"))\n        };\n        \n        let usage = response.usage_metadata.unwrap_or_default();\n        \n        let mut metadata = HashMap::new();\n        if !candidate.safety_ratings.is_empty() {\n            metadata.insert(\n                \"safety_ratings\".to_string(),\n                serde_json::json!(candidate.safety_ratings),\n            );\n        }\n        \n        Ok(LLMResponse {\n            content,\n            prompt_tokens: usage.prompt_token_count,\n            completion_tokens: usage.candidates_token_count,\n            total_tokens: usage.total_token_count,\n            model: self.model.clone(),\n            finish_reason: candidate.finish_reason.clone(),\n            tool_calls,\n            metadata,\n            cache_hit_tokens: if usage.cached_content_token_count > 0 {\n                Some(usage.cached_content_token_count)\n            } else {\n                None\n            },\n            // OODA-25: Track thinking tokens and content from Gemini 2.5+/3.x\n            thinking_tokens: if usage.thoughts_token_count > 0 {\n                Some(usage.thoughts_token_count)\n            } else {\n                None\n            },\n            thinking_content,\n        })\n    }\n\n    // ============================================================================\n    // Streaming Implementation\n    // ============================================================================\n    //\n    // WHY: Gemini streaming requires `alt=sse` parameter for proper SSE format.\n    // Without `alt=sse`, Gemini returns a JSON array which is harder to parse\n    // incrementally. With `alt=sse`, we get standard SSE format:\n    //\n    //   data: {\"candidates\":[...],\"usageMetadata\":{...}}\n    //   data: {\"candidates\":[...],\"finishReason\":\"STOP\"}\n    //\n    // Each `data:` line contains a complete GenerateContentResponse JSON object.\n    // ============================================================================\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        use futures::StreamExt;\n\n        let messages = vec![ChatMessage::user(prompt)];\n        let (system_instruction, contents) = Self::convert_messages(&messages);\n\n        let request = GenerateContentRequest {\n            contents,\n            generation_config: None,\n            system_instruction,\n            safety_settings: None,\n            cached_content: None, // TODO: Add caching support for streaming\n            // OODA-06: No tools for basic streaming\n            tools: None,\n            tool_config: None,\n            // OODA-25: Enable thinking for streaming\n            thinking_config: if self.supports_thinking() {\n                Some(ThinkingConfig {\n                    include_thoughts: Some(true),\n                    thinking_level: None,\n                    thinking_budget: None,\n                })\n            } else {\n                None\n            },\n        };\n\n        // WHY: Add `alt=sse` parameter for proper Server-Sent Events format\n        // This makes parsing simpler and more reliable\n        let base_url = self.build_url(&self.model, \"streamGenerateContent\");\n        let url = if base_url.contains('?') {\n            format!(\"{}&alt=sse\", base_url)\n        } else {\n            format!(\"{}?alt=sse\", base_url)\n        };\n\n        let mut req = self.client.post(&url).json(&request);\n        for (key, value) in self.auth_headers() {\n            req = req.header(key, value);\n        }\n\n        let response = req\n            .send()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Stream request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\"Stream error: {}\", text)));\n        }\n\n        let stream = response.bytes_stream();\n\n        // WHY: Parse SSE format - each chunk may contain multiple `data:` lines\n        // We need to handle partial chunks and accumulate data across boundaries\n        let mapped_stream = stream.map(|result| {\n            match result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    let mut content_parts = Vec::new();\n                    \n                    // Parse each `data:` line in the SSE response\n                    for line in text.lines() {\n                        if let Some(json_str) = line.strip_prefix(\"data: \") {\n                            if let Ok(chunk) = serde_json::from_str::<GenerateContentResponse>(json_str) {\n                                if let Some(candidates) = chunk.candidates {\n                                    if let Some(candidate) = candidates.first() {\n                                        let content: String = candidate\n                                            .content\n                                            .parts\n                                            .iter()\n                                            .filter_map(|p| p.text.clone())\n                                            .collect();\n                                        if !content.is_empty() {\n                                            content_parts.push(content);\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \n                    Ok(content_parts.join(\"\"))\n                }\n                Err(e) => Err(LlmError::ApiError(format!(\"Stream error: {}\", e))),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_json_mode(&self) -> bool {\n        // Gemini 1.5+ supports JSON mode\n        self.model.contains(\"gemini-1.5\") || self.model.contains(\"gemini-2\")\n    }\n    \n    // OODA-07: Enable function calling for Gemini\n    fn supports_function_calling(&self) -> bool {\n        // Gemini 1.5+ and 2.x support function calling\n        self.model.contains(\"gemini-1.5\") || self.model.contains(\"gemini-2\") || self.model.contains(\"gemini-3\")\n    }\n\n    // =========================================================================\n    // OODA-08: Streaming with Tool Calling\n    // =========================================================================\n    //\n    // Combines streaming (SSE) with tool calling support.\n    // Emits StreamChunk events for content, tool calls, and finish reasons.\n    //\n    // Response format:\n    //   data: {\"candidates\":[{\"content\":{\"parts\":[{\"text\":\"...\"}]}}]}\n    //   data: {\"candidates\":[{\"content\":{\"parts\":[{\"functionCall\":{\"name\":\"...\",\"args\":{...}}}]}}]}\n    //\n    // =========================================================================\n    \n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        use futures::StreamExt;\n        \n        // Convert messages to Gemini format\n        let (system_instruction, contents) = Self::convert_messages(messages);\n        \n        if contents.is_empty() {\n            return Err(LlmError::InvalidRequest(\n                \"No user messages provided\".to_string(),\n            ));\n        }\n        \n        // Convert tools using OODA-06 helpers\n        let gemini_tools = if !tools.is_empty() {\n            Some(Self::convert_tools(tools))\n        } else {\n            None\n        };\n        \n        let tool_config = Self::convert_tool_choice(tool_choice);\n        \n        // Build generation config\n        let options = options.cloned().unwrap_or_default();\n        let mut generation_config = GenerationConfig::default();\n        \n        if let Some(max_tokens) = options.max_tokens {\n            generation_config.max_output_tokens = Some(max_tokens);\n        }\n        if let Some(temp) = options.temperature {\n            generation_config.temperature = Some(temp);\n        }\n        if let Some(top_p) = options.top_p {\n            generation_config.top_p = Some(top_p);\n        }\n        if let Some(stop) = options.stop {\n            generation_config.stop_sequences = Some(stop);\n        }\n        \n        // OODA-25: Enable thinking for Gemini 2.5+/3.x models\n        let thinking_config = if self.supports_thinking() {\n            Some(ThinkingConfig {\n                include_thoughts: Some(true),\n                thinking_level: None, // Use model default\n                thinking_budget: None, // Use model default (-1 dynamic)\n            })\n        } else {\n            None\n        };\n        \n        // Build request with tools\n        let request = GenerateContentRequest {\n            contents,\n            generation_config: Some(generation_config),\n            system_instruction,\n            safety_settings: None,\n            cached_content: None,\n            tools: gemini_tools,\n            tool_config,\n            thinking_config,\n        };\n        \n        // Build streaming URL with alt=sse\n        let base_url = self.build_url(&self.model, \"streamGenerateContent\");\n        let url = if base_url.contains('?') {\n            format!(\"{}&alt=sse\", base_url)\n        } else {\n            format!(\"{}?alt=sse\", base_url)\n        };\n        \n        // Send streaming request\n        let mut req = self.client.post(&url).json(&request);\n        for (key, value) in self.auth_headers() {\n            req = req.header(key, value);\n        }\n        \n        let response = req\n            .send()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Stream request failed: {}\", e)))?;\n        \n        if !response.status().is_success() {\n            let text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\"Stream error: {}\", text)));\n        }\n        \n        let stream = response.bytes_stream();\n        \n        // Map SSE stream to StreamChunk events\n        let mapped_stream = stream.map(|result| {\n            match result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    let mut chunks = Vec::new();\n                    \n                    // Parse each `data:` line in the SSE response\n                    for line in text.lines() {\n                        if let Some(json_str) = line.strip_prefix(\"data: \") {\n                            if let Ok(response) = serde_json::from_str::<GenerateContentResponse>(json_str) {\n                                if let Some(candidates) = response.candidates {\n                                    if let Some(candidate) = candidates.first() {\n                                        // Process each part in the response\n                                        for part in &candidate.content.parts {\n                                            // Handle text content\n                                            // OODA-25: Check if this is thinking content\n                                            if let Some(text_content) = &part.text {\n                                                if !text_content.is_empty() {\n                                                    if part.thought == Some(true) {\n                                                        // This is thinking/reasoning content\n                                                        chunks.push(StreamChunk::ThinkingContent {\n                                                            text: text_content.clone(),\n                                                            tokens_used: None, // Gemini doesn't report per-chunk tokens\n                                                            budget_total: None, // Could be enhanced with thinking_budget\n                                                        });\n                                                    } else {\n                                                        // This is regular content\n                                                        chunks.push(StreamChunk::Content(text_content.clone()));\n                                                    }\n                                                }\n                                            }\n                                            \n                                            // Handle function calls\n                                            if let Some(func_call) = &part.function_call {\n                                                // Serialize args to JSON string\n                                                let args_json = serde_json::to_string(&func_call.args).ok();\n                                                \n                                                chunks.push(StreamChunk::ToolCallDelta {\n                                                    index: 0, // Gemini doesn't use indexing like OpenAI\n                                                    id: Some(uuid::Uuid::new_v4().to_string()),\n                                                    function_name: Some(func_call.name.clone()),\n                                                    function_arguments: args_json,\n                                                });\n                                            }\n                                        }\n                                        \n                                        // Check for finish reason\n                                        if let Some(ref reason) = candidate.finish_reason {\n                                            let mapped_reason = match reason.as_str() {\n                                                \"STOP\" => \"stop\",\n                                                \"MAX_TOKENS\" => \"length\",\n                                                \"SAFETY\" => \"content_filter\",\n                                                _ => reason.as_str(),\n                                            };\n                                            chunks.push(StreamChunk::Finished {\n                                                reason: mapped_reason.to_string(),\n                                                ttft_ms: None,\n                                            });\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \n                    // Return first chunk or empty content\n                    // Note: In real usage, we might want to flatten this better\n                    if let Some(chunk) = chunks.into_iter().next() {\n                        Ok(chunk)\n                    } else {\n                        Ok(StreamChunk::Content(String::new()))\n                    }\n                }\n                Err(e) => Err(LlmError::ApiError(format!(\"Stream error: {}\", e))),\n            }\n        });\n        \n        Ok(mapped_stream.boxed())\n    }\n    \n    // OODA-08: Enable streaming with tools for Gemini\n    fn supports_tool_streaming(&self) -> bool {\n        // Same models that support function calling also support streaming with tools\n        self.model.contains(\"gemini-1.5\") || self.model.contains(\"gemini-2\") || self.model.contains(\"gemini-3\")\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for GeminiProvider {\n    fn name(&self) -> &str {\n        \"gemini\"\n    }\n\n    /// Returns the embedding model name (not completion model).\n    #[allow(clippy::misnamed_getters)]\n    fn model(&self) -> &str {\n        &self.embedding_model\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        2048 // Gemini embedding models max tokens\n    }\n\n    #[instrument(skip(self, texts), fields(model = %self.embedding_model, count = texts.len()))]\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        if texts.is_empty() {\n            return Ok(Vec::new());\n        }\n\n        // Use batch endpoint for multiple texts\n        if texts.len() > 1 {\n            return self.embed_batch(texts).await;\n        }\n\n        // Single text - use embedContent\n        let request = EmbedContentRequest {\n            content: Content {\n                parts: vec![Part {\n                    text: Some(texts[0].clone()),\n                    ..Default::default()\n                }],\n                role: None,\n            },\n            model: None, // Not needed for single embedContent\n            task_type: Some(\"RETRIEVAL_DOCUMENT\".to_string()),\n            title: None,\n            output_dimensionality: None,\n        };\n\n        let url = self.build_url(&self.embedding_model, \"embedContent\");\n        debug!(\"Sending embedding request to Gemini: {}\", url);\n\n        let response: EmbedContentResponse = self.send_request(&url, &request).await?;\n\n        Ok(vec![response.embedding.values])\n    }\n\n    async fn embed_one(&self, text: &str) -> Result<Vec<f32>> {\n        let results = self.embed(&[text.to_string()]).await?;\n        results\n            .into_iter()\n            .next()\n            .ok_or_else(|| LlmError::Unknown(\"Empty embedding result\".to_string()))\n    }\n}\n\nimpl GeminiProvider {\n    /// Embed multiple texts using batch endpoint.\n    /// Embed multiple texts using batch endpoint.\n    ///\n    /// WHY: Batch endpoint is more efficient for multiple texts (single API call).\n    /// Each request in batch MUST include the model field per Gemini API spec.\n    async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // WHY: batchEmbedContents requires model field in each request\n        // Format: \"models/{model_name}\"\n        let model_path = format!(\"models/{}\", self.embedding_model);\n        \n        let requests: Vec<EmbedContentRequest> = texts\n            .iter()\n            .map(|text| EmbedContentRequest {\n                content: Content {\n                    parts: vec![Part { text: Some(text.clone()), ..Default::default() }],\n                    role: None,\n                },\n                model: Some(model_path.clone()),\n                task_type: Some(\"RETRIEVAL_DOCUMENT\".to_string()),\n                title: None,\n                output_dimensionality: None,\n            })\n            .collect();\n\n        let batch_request = BatchEmbedContentsRequest { requests };\n\n        let url = self.build_url(&self.embedding_model, \"batchEmbedContents\");\n        debug!(\"Sending batch embedding request to Gemini: {}\", url);\n\n        let response: BatchEmbedContentsResponse = self.send_request(&url, &batch_request).await?;\n\n        Ok(response.embeddings.into_iter().map(|e| e.values).collect())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_context_length_detection() {\n        assert_eq!(\n            GeminiProvider::context_length_for_model(\"gemini-2.0-flash\"),\n            1_000_000\n        );\n        assert_eq!(\n            GeminiProvider::context_length_for_model(\"gemini-1.5-pro\"),\n            2_000_000\n        );\n        assert_eq!(\n            GeminiProvider::context_length_for_model(\"gemini-1.0-pro\"),\n            32_000\n        );\n    }\n\n    #[test]\n    fn test_embedding_dimension_detection() {\n        assert_eq!(\n            GeminiProvider::dimension_for_model(\"text-embedding-004\"),\n            768\n        );\n        assert_eq!(\n            GeminiProvider::dimension_for_model(\"text-embedding-005\"),\n            768\n        );\n    }\n\n    #[test]\n    fn test_provider_builder() {\n        let provider = GeminiProvider::new(\"test-key\")\n            .with_model(\"gemini-1.5-pro\")\n            .with_embedding_model(\"text-embedding-004\");\n\n        assert_eq!(LLMProvider::model(&provider), \"gemini-1.5-pro\");\n        assert_eq!(provider.dimension(), 768);\n        assert_eq!(provider.max_context_length(), 2_000_000);\n    }\n\n    #[test]\n    fn test_vertex_ai_provider() {\n        let provider = GeminiProvider::vertex_ai(\"my-project\", \"us-central1\", \"test-token\");\n        assert_eq!(LLMProvider::name(&provider), \"vertex-ai\");\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let messages = vec![\n            ChatMessage::system(\"You are helpful\"),\n            ChatMessage::user(\"Hello\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let (system, contents) = GeminiProvider::convert_messages(&messages);\n\n        assert!(system.is_some());\n        assert_eq!(system.unwrap().parts[0].text, Some(\"You are helpful\".to_string()));\n        assert_eq!(contents.len(), 2);\n        assert_eq!(contents[0].role.as_deref(), Some(\"user\"));\n        assert_eq!(contents[1].role.as_deref(), Some(\"model\"));\n    }\n\n    // =========================================================================\n    // Image Support Tests (OODA-54)\n    // =========================================================================\n\n    #[test]\n    fn test_convert_messages_text_only() {\n        // WHY: Verify text-only messages serialize correctly with new Part structure\n        let messages = vec![ChatMessage::user(\"Hello, world!\")];\n        let (_, contents) = GeminiProvider::convert_messages(&messages);\n\n        assert_eq!(contents.len(), 1);\n        \n        // Text-only should serialize with text field, no inline_data\n        let json = serde_json::to_value(&contents[0]).unwrap();\n        let parts = &json[\"parts\"];\n        \n        assert!(parts.is_array());\n        assert_eq!(parts.as_array().unwrap().len(), 1);\n        assert_eq!(parts[0][\"text\"], \"Hello, world!\");\n        assert!(parts[0].get(\"inlineData\").is_none());\n    }\n\n    #[test]\n    fn test_convert_messages_with_images() {\n        use crate::traits::ImageData;\n        \n        // WHY: Verify images use Gemini's inlineData format\n        let images = vec![ImageData::new(\"base64data\", \"image/png\")];\n        let messages = vec![ChatMessage::user_with_images(\"What's this?\", images)];\n        let (_, contents) = GeminiProvider::convert_messages(&messages);\n\n        assert_eq!(contents.len(), 1);\n        \n        // With images should have multiple parts\n        let json = serde_json::to_value(&contents[0]).unwrap();\n        let parts = &json[\"parts\"];\n        \n        assert!(parts.is_array());\n        assert_eq!(parts.as_array().unwrap().len(), 2);\n        \n        // First part: text\n        assert_eq!(parts[0][\"text\"], \"What's this?\");\n        \n        // Second part: inlineData (Gemini format)\n        assert!(parts[1].get(\"inlineData\").is_some(), \"Should have inlineData for image\");\n        assert_eq!(parts[1][\"inlineData\"][\"mimeType\"], \"image/png\");\n        assert_eq!(parts[1][\"inlineData\"][\"data\"], \"base64data\");\n    }\n\n    #[test]\n    fn test_convert_messages_multiple_images() {\n        use crate::traits::ImageData;\n        \n        // WHY: Verify multiple images are handled correctly\n        let images = vec![\n            ImageData::new(\"img1data\", \"image/png\"),\n            ImageData::new(\"img2data\", \"image/jpeg\"),\n        ];\n        let messages = vec![ChatMessage::user_with_images(\"Compare these\", images)];\n        let (_, contents) = GeminiProvider::convert_messages(&messages);\n\n        let json = serde_json::to_value(&contents[0]).unwrap();\n        let parts = &json[\"parts\"];\n        \n        assert_eq!(parts.as_array().unwrap().len(), 3); // 1 text + 2 images\n        \n        // Verify both images\n        assert_eq!(parts[1][\"inlineData\"][\"mimeType\"], \"image/png\");\n        assert_eq!(parts[2][\"inlineData\"][\"mimeType\"], \"image/jpeg\");\n    }\n\n    #[test]\n    fn test_build_url_google_ai() {\n        let provider = GeminiProvider::new(\"test-api-key\");\n        let url = provider.build_url(\"gemini-2.0-flash\", \"generateContent\");\n\n        assert!(url.contains(\"generativelanguage.googleapis.com\"));\n        assert!(url.contains(\"gemini-2.0-flash\"));\n        assert!(url.contains(\"key=test-api-key\"));\n    }\n\n    #[test]\n    fn test_build_url_vertex_ai() {\n        let provider = GeminiProvider::vertex_ai(\"my-project\", \"us-central1\", \"token\");\n        let url = provider.build_url(\"gemini-2.0-flash\", \"generateContent\");\n\n        assert!(url.contains(\"aiplatform.googleapis.com\"));\n        assert!(url.contains(\"my-project\"));\n        assert!(url.contains(\"us-central1\"));\n    }\n\n    // =========================================================================\n    // OODA-25: Thinking Support Tests\n    // =========================================================================\n\n    #[test]\n    fn test_supports_thinking_gemini_25() {\n        // Gemini 2.5 models do NOT support thinking (API rejects thinkingConfig)\n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-2.5-flash\");\n        assert!(!provider.supports_thinking());\n        \n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-2.5-pro\");\n        assert!(!provider.supports_thinking());\n    }\n\n    #[test]\n    fn test_supports_thinking_gemini_3() {\n        // Gemini 3 models also do NOT support thinking via REST API (as of Jan 2026)\n        // Documentation shows it, but API rejects with 400 error\n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-3-flash\");\n        assert!(!provider.supports_thinking());\n        \n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-3-pro\");\n        assert!(!provider.supports_thinking());\n    }\n\n    #[test]\n    fn test_supports_thinking_gemini_1x() {\n        // Gemini 1.x models do NOT support thinking\n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-1.5-flash\");\n        assert!(!provider.supports_thinking());\n        \n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-1.0-pro\");\n        assert!(!provider.supports_thinking());\n    }\n\n    #[test]\n    fn test_thinking_config_serialization() {\n        // Verify ThinkingConfig serializes correctly to camelCase\n        let config = ThinkingConfig {\n            include_thoughts: Some(true),\n            thinking_level: Some(\"high\".to_string()),\n            thinking_budget: Some(1024),\n        };\n        \n        let json = serde_json::to_value(&config).unwrap();\n        assert_eq!(json[\"includeThoughts\"], true);\n        assert_eq!(json[\"thinkingLevel\"], \"high\");\n        assert_eq!(json[\"thinkingBudget\"], 1024);\n    }\n\n    #[test]\n    fn test_part_thought_deserialization() {\n        // Verify Part deserializes thought field correctly\n        let json = r#\"{\"text\": \"thinking...\", \"thought\": true}\"#;\n        let part: Part = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(part.text, Some(\"thinking...\".to_string()));\n        assert_eq!(part.thought, Some(true));\n    }\n\n    #[test]\n    fn test_part_thought_defaults_to_none() {\n        // Verify Part without thought field defaults to None\n        let json = r#\"{\"text\": \"response\"}\"#;\n        let part: Part = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(part.text, Some(\"response\".to_string()));\n        assert_eq!(part.thought, None);\n    }\n\n    #[test]\n    fn test_usage_metadata_thoughts_token_count() {\n        // Verify UsageMetadata deserializes thoughtsTokenCount\n        let json = r#\"{\"promptTokenCount\": 100, \"candidatesTokenCount\": 50, \"totalTokenCount\": 150, \"thoughtsTokenCount\": 25}\"#;\n        let usage: UsageMetadata = serde_json::from_str(json).unwrap();\n        \n        assert_eq!(usage.prompt_token_count, 100);\n        assert_eq!(usage.candidates_token_count, 50);\n        assert_eq!(usage.thoughts_token_count, 25);\n    }\n\n    // =========================================================================\n    // OODA-34: Additional Unit Tests\n    // =========================================================================\n\n    #[test]\n    fn test_constants() {\n        // WHY: Verify constants are as expected for API compatibility\n        assert_eq!(GEMINI_API_BASE, \"https://generativelanguage.googleapis.com/v1beta\");\n        assert_eq!(DEFAULT_GEMINI_MODEL, \"gemini-2.5-flash\");\n        assert_eq!(DEFAULT_EMBEDDING_MODEL, \"text-embedding-004\");\n    }\n\n    #[test]\n    fn test_google_ai_provider_name() {\n        // WHY: Google AI endpoint should return \"gemini\" as name\n        let provider = GeminiProvider::new(\"test-key\");\n        assert_eq!(LLMProvider::name(&provider), \"gemini\");\n    }\n\n    #[test]\n    fn test_supports_streaming() {\n        let provider = GeminiProvider::new(\"test-key\");\n        assert!(provider.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_json_mode_gemini_25() {\n        // WHY: Gemini 2.x models support JSON mode\n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-2.5-flash\");\n        assert!(provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_supports_json_mode_gemini_15() {\n        // WHY: Gemini 1.5 models support JSON mode\n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-1.5-pro\");\n        assert!(provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_supports_json_mode_gemini_10() {\n        // WHY: Gemini 1.0 does NOT support JSON mode\n        let provider = GeminiProvider::new(\"key\").with_model(\"gemini-1.0-pro\");\n        assert!(!provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_with_cache_ttl() {\n        let provider = GeminiProvider::new(\"key\").with_cache_ttl(\"7200s\");\n        assert_eq!(provider.cache_ttl, \"7200s\");\n    }\n\n    #[test]\n    fn test_embedding_provider_name() {\n        let provider = GeminiProvider::new(\"key\");\n        assert_eq!(EmbeddingProvider::name(&provider), \"gemini\");\n    }\n\n    #[test]\n    fn test_embedding_provider_model() {\n        let provider = GeminiProvider::new(\"key\").with_embedding_model(\"text-embedding-005\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"text-embedding-005\");\n    }\n\n    #[test]\n    fn test_embedding_provider_max_tokens() {\n        let provider = GeminiProvider::new(\"key\");\n        // Gemini embedding models support large input\n        assert!(EmbeddingProvider::max_tokens(&provider) > 0);\n    }\n\n    #[tokio::test]\n    async fn test_embed_empty_input() {\n        let provider = GeminiProvider::new(\"key\");\n        let texts: Vec<String> = vec![];\n        let result = provider.embed_batch(&texts).await;\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_generation_config_serialization() {\n        let config = GenerationConfig {\n            max_output_tokens: Some(1000),\n            temperature: Some(0.7),\n            top_p: Some(0.9),\n            top_k: Some(40),\n            stop_sequences: Some(vec![\"END\".to_string()]),\n            response_mime_type: Some(\"application/json\".to_string()),\n        };\n        let json = serde_json::to_value(&config).unwrap();\n        assert_eq!(json[\"maxOutputTokens\"], 1000);\n        // WHY: f32 serialization may have precision differences, check approximate\n        let temp = json[\"temperature\"].as_f64().unwrap();\n        assert!((temp - 0.7).abs() < 0.001);\n        let top_p = json[\"topP\"].as_f64().unwrap();\n        assert!((top_p - 0.9).abs() < 0.001);\n        assert_eq!(json[\"topK\"], 40);\n        assert_eq!(json[\"stopSequences\"], serde_json::json!([\"END\"]));\n        assert_eq!(json[\"responseMimeType\"], \"application/json\");\n    }\n\n    #[test]\n    fn test_gemini_models_response_deserialization() {\n        let json = r#\"{\n            \"models\": [\n                {\n                    \"name\": \"models/gemini-2.5-flash\",\n                    \"displayName\": \"Gemini 2.5 Flash\",\n                    \"description\": \"Fast model\",\n                    \"inputTokenLimit\": 1000000,\n                    \"outputTokenLimit\": 8192,\n                    \"supportedGenerationMethods\": [\"generateContent\"]\n                }\n            ]\n        }\"#;\n        let response: GeminiModelsResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.models.len(), 1);\n        assert_eq!(response.models[0].name, \"models/gemini-2.5-flash\");\n        assert_eq!(response.models[0].display_name, \"Gemini 2.5 Flash\");\n        assert_eq!(response.models[0].input_token_limit, Some(1000000));\n    }\n\n    #[test]\n    fn test_function_call_deserialization() {\n        let json = r#\"{\"name\": \"get_weather\", \"args\": {\"location\": \"London\"}}\"#;\n        let fc: FunctionCall = serde_json::from_str(json).unwrap();\n        assert_eq!(fc.name, \"get_weather\");\n        assert_eq!(fc.args[\"location\"], \"London\");\n    }\n}\n","traces":[{"line":431,"address":[],"length":0,"stats":{"Line":19}},{"line":433,"address":[],"length":0,"stats":{"Line":38}},{"line":434,"address":[],"length":0,"stats":{"Line":38}},{"line":437,"address":[],"length":0,"stats":{"Line":57}},{"line":438,"address":[],"length":0,"stats":{"Line":57}},{"line":441,"address":[],"length":0,"stats":{"Line":57}},{"line":442,"address":[],"length":0,"stats":{"Line":19}},{"line":449,"address":[],"length":0,"stats":{"Line":9}},{"line":451,"address":[],"length":0,"stats":{"Line":9}},{"line":452,"address":[],"length":0,"stats":{"Line":0}},{"line":456,"address":[],"length":0,"stats":{"Line":9}},{"line":469,"address":[],"length":0,"stats":{"Line":9}},{"line":470,"address":[],"length":0,"stats":{"Line":27}},{"line":471,"address":[],"length":0,"stats":{"Line":0}},{"line":472,"address":[],"length":0,"stats":{"Line":0}},{"line":473,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":18}},{"line":478,"address":[],"length":0,"stats":{"Line":27}},{"line":481,"address":[],"length":0,"stats":{"Line":9}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":483,"address":[],"length":0,"stats":{"Line":18}},{"line":486,"address":[],"length":0,"stats":{"Line":0}},{"line":492,"address":[],"length":0,"stats":{"Line":9}},{"line":495,"address":[],"length":0,"stats":{"Line":9}},{"line":497,"address":[],"length":0,"stats":{"Line":27}},{"line":498,"address":[],"length":0,"stats":{"Line":9}},{"line":500,"address":[],"length":0,"stats":{"Line":9}},{"line":501,"address":[],"length":0,"stats":{"Line":0}},{"line":502,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[],"length":0,"stats":{"Line":0}},{"line":505,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":9}},{"line":510,"address":[],"length":0,"stats":{"Line":27}},{"line":511,"address":[],"length":0,"stats":{"Line":18}},{"line":512,"address":[],"length":0,"stats":{"Line":18}},{"line":513,"address":[],"length":0,"stats":{"Line":18}},{"line":514,"address":[],"length":0,"stats":{"Line":9}},{"line":518,"address":[],"length":0,"stats":{"Line":0}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":520,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":522,"address":[],"length":0,"stats":{"Line":0}},{"line":526,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":2}},{"line":541,"address":[],"length":0,"stats":{"Line":4}},{"line":542,"address":[],"length":0,"stats":{"Line":4}},{"line":547,"address":[],"length":0,"stats":{"Line":6}},{"line":548,"address":[],"length":0,"stats":{"Line":6}},{"line":551,"address":[],"length":0,"stats":{"Line":6}},{"line":552,"address":[],"length":0,"stats":{"Line":2}},{"line":559,"address":[],"length":0,"stats":{"Line":1}},{"line":560,"address":[],"length":0,"stats":{"Line":3}},{"line":561,"address":[],"length":0,"stats":{"Line":1}},{"line":565,"address":[],"length":0,"stats":{"Line":10}},{"line":566,"address":[],"length":0,"stats":{"Line":30}},{"line":567,"address":[],"length":0,"stats":{"Line":10}},{"line":568,"address":[],"length":0,"stats":{"Line":20}},{"line":569,"address":[],"length":0,"stats":{"Line":10}},{"line":573,"address":[],"length":0,"stats":{"Line":2}},{"line":574,"address":[],"length":0,"stats":{"Line":6}},{"line":575,"address":[],"length":0,"stats":{"Line":2}},{"line":576,"address":[],"length":0,"stats":{"Line":4}},{"line":577,"address":[],"length":0,"stats":{"Line":2}},{"line":581,"address":[],"length":0,"stats":{"Line":13}},{"line":582,"address":[],"length":0,"stats":{"Line":13}},{"line":584,"address":[],"length":0,"stats":{"Line":28}},{"line":585,"address":[],"length":0,"stats":{"Line":26}},{"line":588,"address":[],"length":0,"stats":{"Line":24}},{"line":589,"address":[],"length":0,"stats":{"Line":24}},{"line":592,"address":[],"length":0,"stats":{"Line":18}},{"line":595,"address":[],"length":0,"stats":{"Line":20}},{"line":596,"address":[],"length":0,"stats":{"Line":10}},{"line":599,"address":[],"length":0,"stats":{"Line":12}},{"line":601,"address":[],"length":0,"stats":{"Line":0}},{"line":606,"address":[],"length":0,"stats":{"Line":4}},{"line":607,"address":[],"length":0,"stats":{"Line":4}},{"line":608,"address":[],"length":0,"stats":{"Line":12}},{"line":609,"address":[],"length":0,"stats":{"Line":8}},{"line":610,"address":[],"length":0,"stats":{"Line":0}},{"line":611,"address":[],"length":0,"stats":{"Line":0}},{"line":612,"address":[],"length":0,"stats":{"Line":0}},{"line":632,"address":[],"length":0,"stats":{"Line":0}},{"line":633,"address":[],"length":0,"stats":{"Line":0}},{"line":634,"address":[],"length":0,"stats":{"Line":0}},{"line":635,"address":[],"length":0,"stats":{"Line":0}},{"line":638,"address":[],"length":0,"stats":{"Line":0}},{"line":639,"address":[],"length":0,"stats":{"Line":0}},{"line":642,"address":[],"length":0,"stats":{"Line":0}},{"line":649,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":0}},{"line":654,"address":[],"length":0,"stats":{"Line":0}},{"line":655,"address":[],"length":0,"stats":{"Line":0}},{"line":658,"address":[],"length":0,"stats":{"Line":0}},{"line":659,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[],"length":0,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":664,"address":[],"length":0,"stats":{"Line":0}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":666,"address":[],"length":0,"stats":{"Line":0}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":671,"address":[],"length":0,"stats":{"Line":0}},{"line":672,"address":[],"length":0,"stats":{"Line":0}},{"line":685,"address":[],"length":0,"stats":{"Line":0}},{"line":744,"address":[],"length":0,"stats":{"Line":0}},{"line":751,"address":[],"length":0,"stats":{"Line":0}},{"line":758,"address":[],"length":0,"stats":{"Line":0}},{"line":759,"address":[],"length":0,"stats":{"Line":0}},{"line":774,"address":[],"length":0,"stats":{"Line":4}},{"line":777,"address":[],"length":0,"stats":{"Line":8}},{"line":779,"address":[],"length":0,"stats":{"Line":12}},{"line":780,"address":[],"length":0,"stats":{"Line":12}},{"line":781,"address":[],"length":0,"stats":{"Line":8}},{"line":785,"address":[],"length":0,"stats":{"Line":12}},{"line":786,"address":[],"length":0,"stats":{"Line":0}},{"line":788,"address":[],"length":0,"stats":{"Line":0}},{"line":790,"address":[],"length":0,"stats":{"Line":8}},{"line":793,"address":[],"length":0,"stats":{"Line":4}},{"line":794,"address":[],"length":0,"stats":{"Line":3}},{"line":795,"address":[],"length":0,"stats":{"Line":3}},{"line":801,"address":[],"length":0,"stats":{"Line":2}},{"line":803,"address":[],"length":0,"stats":{"Line":1}},{"line":812,"address":[],"length":0,"stats":{"Line":2}},{"line":813,"address":[],"length":0,"stats":{"Line":2}},{"line":816,"address":[],"length":0,"stats":{"Line":2}},{"line":818,"address":[],"length":0,"stats":{"Line":0}},{"line":819,"address":[],"length":0,"stats":{"Line":0}},{"line":825,"address":[],"length":0,"stats":{"Line":5}},{"line":826,"address":[],"length":0,"stats":{"Line":10}},{"line":827,"address":[],"length":0,"stats":{"Line":10}},{"line":829,"address":[],"length":0,"stats":{"Line":19}},{"line":830,"address":[],"length":0,"stats":{"Line":7}},{"line":831,"address":[],"length":0,"stats":{"Line":1}},{"line":833,"address":[],"length":0,"stats":{"Line":2}},{"line":834,"address":[],"length":0,"stats":{"Line":2}},{"line":835,"address":[],"length":0,"stats":{"Line":1}},{"line":836,"address":[],"length":0,"stats":{"Line":1}},{"line":838,"address":[],"length":0,"stats":{"Line":1}},{"line":843,"address":[],"length":0,"stats":{"Line":10}},{"line":844,"address":[],"length":0,"stats":{"Line":4}},{"line":847,"address":[],"length":0,"stats":{"Line":4}},{"line":848,"address":[],"length":0,"stats":{"Line":6}},{"line":849,"address":[],"length":0,"stats":{"Line":2}},{"line":850,"address":[],"length":0,"stats":{"Line":2}},{"line":855,"address":[],"length":0,"stats":{"Line":4}},{"line":856,"address":[],"length":0,"stats":{"Line":11}},{"line":857,"address":[],"length":0,"stats":{"Line":9}},{"line":858,"address":[],"length":0,"stats":{"Line":3}},{"line":859,"address":[],"length":0,"stats":{"Line":9}},{"line":860,"address":[],"length":0,"stats":{"Line":3}},{"line":862,"address":[],"length":0,"stats":{"Line":3}},{"line":867,"address":[],"length":0,"stats":{"Line":6}},{"line":868,"address":[],"length":0,"stats":{"Line":4}},{"line":869,"address":[],"length":0,"stats":{"Line":2}},{"line":872,"address":[],"length":0,"stats":{"Line":9}},{"line":873,"address":[],"length":0,"stats":{"Line":9}},{"line":874,"address":[],"length":0,"stats":{"Line":6}},{"line":875,"address":[],"length":0,"stats":{"Line":6}},{"line":877,"address":[],"length":0,"stats":{"Line":3}},{"line":881,"address":[],"length":0,"stats":{"Line":1}},{"line":882,"address":[],"length":0,"stats":{"Line":3}},{"line":883,"address":[],"length":0,"stats":{"Line":3}},{"line":884,"address":[],"length":0,"stats":{"Line":2}},{"line":885,"address":[],"length":0,"stats":{"Line":2}},{"line":887,"address":[],"length":0,"stats":{"Line":1}},{"line":890,"address":[],"length":0,"stats":{"Line":0}},{"line":892,"address":[],"length":0,"stats":{"Line":0}},{"line":893,"address":[],"length":0,"stats":{"Line":0}},{"line":894,"address":[],"length":0,"stats":{"Line":0}},{"line":895,"address":[],"length":0,"stats":{"Line":0}},{"line":897,"address":[],"length":0,"stats":{"Line":0}},{"line":903,"address":[],"length":0,"stats":{"Line":5}},{"line":907,"address":[],"length":0,"stats":{"Line":2}},{"line":912,"address":[],"length":0,"stats":{"Line":12}},{"line":914,"address":[],"length":0,"stats":{"Line":4}},{"line":915,"address":[],"length":0,"stats":{"Line":0}},{"line":918,"address":[],"length":0,"stats":{"Line":6}},{"line":920,"address":[],"length":0,"stats":{"Line":2}},{"line":921,"address":[],"length":0,"stats":{"Line":2}},{"line":923,"address":[],"length":0,"stats":{"Line":6}},{"line":924,"address":[],"length":0,"stats":{"Line":6}},{"line":926,"address":[],"length":0,"stats":{"Line":2}},{"line":927,"address":[],"length":0,"stats":{"Line":2}},{"line":929,"address":[],"length":0,"stats":{"Line":2}},{"line":931,"address":[],"length":0,"stats":{"Line":4}},{"line":932,"address":[],"length":0,"stats":{"Line":2}},{"line":933,"address":[],"length":0,"stats":{"Line":2}},{"line":934,"address":[],"length":0,"stats":{"Line":2}},{"line":937,"address":[],"length":0,"stats":{"Line":0}},{"line":938,"address":[],"length":0,"stats":{"Line":0}},{"line":939,"address":[],"length":0,"stats":{"Line":0}},{"line":943,"address":[],"length":0,"stats":{"Line":0}},{"line":944,"address":[],"length":0,"stats":{"Line":0}},{"line":953,"address":[],"length":0,"stats":{"Line":0}},{"line":954,"address":[],"length":0,"stats":{"Line":0}},{"line":955,"address":[],"length":0,"stats":{"Line":0}},{"line":958,"address":[],"length":0,"stats":{"Line":0}},{"line":959,"address":[],"length":0,"stats":{"Line":0}},{"line":960,"address":[],"length":0,"stats":{"Line":0}},{"line":963,"address":[],"length":0,"stats":{"Line":0}},{"line":964,"address":[],"length":0,"stats":{"Line":0}},{"line":965,"address":[],"length":0,"stats":{"Line":0}},{"line":966,"address":[],"length":0,"stats":{"Line":0}},{"line":970,"address":[],"length":0,"stats":{"Line":0}},{"line":974,"address":[],"length":0,"stats":{"Line":0}},{"line":975,"address":[],"length":0,"stats":{"Line":0}},{"line":977,"address":[],"length":0,"stats":{"Line":0}},{"line":979,"address":[],"length":0,"stats":{"Line":0}},{"line":981,"address":[],"length":0,"stats":{"Line":0}},{"line":982,"address":[],"length":0,"stats":{"Line":0}},{"line":983,"address":[],"length":0,"stats":{"Line":0}},{"line":984,"address":[],"length":0,"stats":{"Line":0}},{"line":989,"address":[],"length":0,"stats":{"Line":0}},{"line":993,"address":[],"length":0,"stats":{"Line":0}},{"line":994,"address":[],"length":0,"stats":{"Line":0}},{"line":995,"address":[],"length":0,"stats":{"Line":0}},{"line":996,"address":[],"length":0,"stats":{"Line":0}},{"line":997,"address":[],"length":0,"stats":{"Line":0}},{"line":998,"address":[],"length":0,"stats":{"Line":0}},{"line":999,"address":[],"length":0,"stats":{"Line":0}},{"line":1000,"address":[],"length":0,"stats":{"Line":0}},{"line":1001,"address":[],"length":0,"stats":{"Line":0}},{"line":1002,"address":[],"length":0,"stats":{"Line":0}},{"line":1003,"address":[],"length":0,"stats":{"Line":0}},{"line":1004,"address":[],"length":0,"stats":{"Line":0}},{"line":1010,"address":[],"length":0,"stats":{"Line":0}},{"line":1011,"address":[],"length":0,"stats":{"Line":0}},{"line":1012,"address":[],"length":0,"stats":{"Line":0}},{"line":1013,"address":[],"length":0,"stats":{"Line":0}},{"line":1043,"address":[],"length":0,"stats":{"Line":7}},{"line":1045,"address":[],"length":0,"stats":{"Line":7}},{"line":1051,"address":[],"length":0,"stats":{"Line":2}},{"line":1052,"address":[],"length":0,"stats":{"Line":2}},{"line":1053,"address":[],"length":0,"stats":{"Line":1}},{"line":1054,"address":[],"length":0,"stats":{"Line":1}},{"line":1058,"address":[],"length":0,"stats":{"Line":1}},{"line":1059,"address":[],"length":0,"stats":{"Line":1}},{"line":1062,"address":[],"length":0,"stats":{"Line":1}},{"line":1063,"address":[],"length":0,"stats":{"Line":1}},{"line":1173,"address":[],"length":0,"stats":{"Line":0}},{"line":1177,"address":[],"length":0,"stats":{"Line":0}},{"line":1317,"address":[],"length":0,"stats":{"Line":0}},{"line":1321,"address":[],"length":0,"stats":{"Line":0}},{"line":1403,"address":[],"length":0,"stats":{"Line":0}},{"line":1447,"address":[],"length":0,"stats":{"Line":0}},{"line":1458,"address":[],"length":0,"stats":{"Line":0}},{"line":1459,"address":[],"length":0,"stats":{"Line":0}},{"line":1460,"address":[],"length":0,"stats":{"Line":0}},{"line":1461,"address":[],"length":0,"stats":{"Line":0}},{"line":1462,"address":[],"length":0,"stats":{"Line":0}},{"line":1465,"address":[],"length":0,"stats":{"Line":0}},{"line":1466,"address":[],"length":0,"stats":{"Line":0}},{"line":1467,"address":[],"length":0,"stats":{"Line":0}},{"line":1468,"address":[],"length":0,"stats":{"Line":0}},{"line":1469,"address":[],"length":0,"stats":{"Line":0}},{"line":1470,"address":[],"length":0,"stats":{"Line":0}},{"line":1471,"address":[],"length":0,"stats":{"Line":0}},{"line":1472,"address":[],"length":0,"stats":{"Line":0}},{"line":1473,"address":[],"length":0,"stats":{"Line":0}},{"line":1474,"address":[],"length":0,"stats":{"Line":0}},{"line":1475,"address":[],"length":0,"stats":{"Line":0}},{"line":1476,"address":[],"length":0,"stats":{"Line":0}},{"line":1477,"address":[],"length":0,"stats":{"Line":0}},{"line":1485,"address":[],"length":0,"stats":{"Line":0}},{"line":1487,"address":[],"length":0,"stats":{"Line":0}},{"line":1494,"address":[],"length":0,"stats":{"Line":1}},{"line":1495,"address":[],"length":0,"stats":{"Line":1}},{"line":1498,"address":[],"length":0,"stats":{"Line":3}},{"line":1500,"address":[],"length":0,"stats":{"Line":5}},{"line":1504,"address":[],"length":0,"stats":{"Line":0}},{"line":1506,"address":[],"length":0,"stats":{"Line":0}},{"line":1606,"address":[],"length":0,"stats":{"Line":0}},{"line":1616,"address":[],"length":0,"stats":{"Line":0}},{"line":1617,"address":[],"length":0,"stats":{"Line":0}},{"line":1618,"address":[],"length":0,"stats":{"Line":0}},{"line":1619,"address":[],"length":0,"stats":{"Line":0}},{"line":1620,"address":[],"length":0,"stats":{"Line":0}},{"line":1623,"address":[],"length":0,"stats":{"Line":0}},{"line":1624,"address":[],"length":0,"stats":{"Line":0}},{"line":1625,"address":[],"length":0,"stats":{"Line":0}},{"line":1626,"address":[],"length":0,"stats":{"Line":0}},{"line":1627,"address":[],"length":0,"stats":{"Line":0}},{"line":1629,"address":[],"length":0,"stats":{"Line":0}},{"line":1632,"address":[],"length":0,"stats":{"Line":0}},{"line":1633,"address":[],"length":0,"stats":{"Line":0}},{"line":1634,"address":[],"length":0,"stats":{"Line":0}},{"line":1636,"address":[],"length":0,"stats":{"Line":0}},{"line":1637,"address":[],"length":0,"stats":{"Line":0}},{"line":1638,"address":[],"length":0,"stats":{"Line":0}},{"line":1639,"address":[],"length":0,"stats":{"Line":0}},{"line":1643,"address":[],"length":0,"stats":{"Line":0}},{"line":1649,"address":[],"length":0,"stats":{"Line":0}},{"line":1651,"address":[],"length":0,"stats":{"Line":0}},{"line":1653,"address":[],"length":0,"stats":{"Line":0}},{"line":1654,"address":[],"length":0,"stats":{"Line":0}},{"line":1655,"address":[],"length":0,"stats":{"Line":0}},{"line":1656,"address":[],"length":0,"stats":{"Line":0}},{"line":1657,"address":[],"length":0,"stats":{"Line":0}},{"line":1663,"address":[],"length":0,"stats":{"Line":0}},{"line":1664,"address":[],"length":0,"stats":{"Line":0}},{"line":1665,"address":[],"length":0,"stats":{"Line":0}},{"line":1666,"address":[],"length":0,"stats":{"Line":0}},{"line":1667,"address":[],"length":0,"stats":{"Line":0}},{"line":1668,"address":[],"length":0,"stats":{"Line":0}},{"line":1670,"address":[],"length":0,"stats":{"Line":0}},{"line":1671,"address":[],"length":0,"stats":{"Line":0}},{"line":1672,"address":[],"length":0,"stats":{"Line":0}},{"line":1683,"address":[],"length":0,"stats":{"Line":0}},{"line":1684,"address":[],"length":0,"stats":{"Line":0}},{"line":1686,"address":[],"length":0,"stats":{"Line":0}},{"line":1689,"address":[],"length":0,"stats":{"Line":0}},{"line":1697,"address":[],"length":0,"stats":{"Line":0}},{"line":1699,"address":[],"length":0,"stats":{"Line":0}},{"line":1705,"address":[],"length":0,"stats":{"Line":1}},{"line":1706,"address":[],"length":0,"stats":{"Line":1}},{"line":1711,"address":[],"length":0,"stats":{"Line":1}},{"line":1712,"address":[],"length":0,"stats":{"Line":1}},{"line":1715,"address":[],"length":0,"stats":{"Line":1}},{"line":1716,"address":[],"length":0,"stats":{"Line":1}},{"line":1719,"address":[],"length":0,"stats":{"Line":1}},{"line":1720,"address":[],"length":0,"stats":{"Line":1}},{"line":1757,"address":[],"length":0,"stats":{"Line":0}},{"line":1762,"address":[],"length":0,"stats":{"Line":0}},{"line":1772,"address":[],"length":0,"stats":{"Line":2}},{"line":1775,"address":[],"length":0,"stats":{"Line":3}},{"line":1777,"address":[],"length":0,"stats":{"Line":3}},{"line":1779,"address":[],"length":0,"stats":{"Line":1}},{"line":1780,"address":[],"length":0,"stats":{"Line":0}},{"line":1781,"address":[],"length":0,"stats":{"Line":0}},{"line":1782,"address":[],"length":0,"stats":{"Line":0}},{"line":1784,"address":[],"length":0,"stats":{"Line":0}},{"line":1785,"address":[],"length":0,"stats":{"Line":0}},{"line":1786,"address":[],"length":0,"stats":{"Line":0}},{"line":1787,"address":[],"length":0,"stats":{"Line":0}},{"line":1791,"address":[],"length":0,"stats":{"Line":2}},{"line":1793,"address":[],"length":0,"stats":{"Line":5}},{"line":1794,"address":[],"length":0,"stats":{"Line":1}},{"line":1796,"address":[],"length":0,"stats":{"Line":5}},{"line":1798,"address":[],"length":0,"stats":{"Line":0}}],"covered":160,"coverable":339},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","genai_events.rs"],"content":"//! GenAI Event Emission following OpenTelemetry Semantic Conventions\n//!\n//! This module implements event emission according to:\n//! <https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-events/>\n//!\n//! Events are recorded as OpenTelemetry span events with the event name:\n//! \"gen_ai.client.inference.operation.details\"\n//!\n//! The events contain structured JSON data with the following attributes:\n//! - gen_ai.input.messages: Array of input messages\n//! - gen_ai.output.messages: Array of output messages\n\nuse crate::traits::{ChatMessage, ChatRole};\nuse serde::{Deserialize, Serialize};\nuse std::env;\n\n/// Check if content capture is enabled via environment variable\npub fn should_capture_content() -> bool {\n    env::var(\"EDGECODE_CAPTURE_CONTENT\")\n        .map(|v| v.to_lowercase() == \"true\" || v == \"1\")\n        .unwrap_or(false)\n}\n\n// No need for a separate logger getter function\n\n/// GenAI message part (text or tool_call or tool_result)\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"snake_case\")]\npub enum GenAIMessagePart {\n    Text { text: String },\n    ToolCall { tool_call: GenAIToolCall },\n    ToolResult { tool_result: GenAIToolResult },\n}\n\n/// GenAI tool call structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GenAIToolCall {\n    pub id: String,\n    pub name: String,\n    pub arguments: String,\n}\n\n/// GenAI tool result structure\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GenAIToolResult {\n    pub tool_call_id: String,\n    pub content: String,\n}\n\n/// GenAI message following OpenTelemetry schema\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GenAIMessage {\n    pub role: String,\n    pub content: Vec<GenAIMessagePart>,\n}\n\n/// Convert ChatMessage to GenAI message format\npub fn convert_to_genai_messages(messages: &[ChatMessage]) -> Vec<GenAIMessage> {\n    messages\n        .iter()\n        .map(|msg| {\n            let role = match msg.role {\n                ChatRole::System => \"system\",\n                ChatRole::User => \"user\",\n                ChatRole::Assistant => \"assistant\",\n                ChatRole::Tool => \"tool\",\n                ChatRole::Function => \"function\",\n            };\n\n            let mut all_parts = vec![GenAIMessagePart::Text {\n                text: msg.content.clone(),\n            }];\n\n            // Add tool calls if present\n            if let Some(tool_calls) = &msg.tool_calls {\n                for tc in tool_calls {\n                    all_parts.push(GenAIMessagePart::ToolCall {\n                        tool_call: GenAIToolCall {\n                            id: tc.id.clone(),\n                            name: tc.function.name.clone(),\n                            arguments: tc.function.arguments.clone(),\n                        },\n                    });\n                }\n            }\n\n            GenAIMessage {\n                role: role.to_string(),\n                content: all_parts,\n            }\n        })\n        .collect()\n}\n\n/// Emit a gen_ai.client.inference.operation.details event as a span event\n///\n/// # Arguments\n/// * `input_messages` - Input messages sent to the LLM\n/// * `output_messages` - Output messages received from the LLM\n/// * `response` - The LLM response containing metadata (response_id, finish_reason, etc.)\n/// * `options` - Optional request options (temperature, max_tokens, etc.)\n///\n/// This function adds an event to the current active span using tracing macros.\n/// The event will be exported as part of the span to Jaeger.\n///\n/// # OODA-13: Extended Metadata Capture\n/// This function now captures comprehensive metadata following OpenTelemetry GenAI semantic conventions:\n/// - Response ID (gen_ai.response.id) - Unique identifier from LLM provider\n/// - Finish reason (gen_ai.response.finish_reasons) - Why generation stopped\n/// - Cache hits (gen_ai.usage.cache_hit_tokens) - Tokens served from cache\n/// - Request options (temperature, max_tokens, top_p, penalties)\npub fn emit_inference_event(\n    input_messages: &[ChatMessage],\n    output_messages: &[ChatMessage],\n    response: &crate::traits::LLMResponse,\n    options: Option<&crate::traits::CompletionOptions>,\n) {\n    // Check if content capture is enabled\n    if !should_capture_content() {\n        tracing::debug!(\"Content capture disabled (EDGECODE_CAPTURE_CONTENT not set to true)\");\n        return;\n    }\n\n    // Convert messages to GenAI format\n    let input = convert_to_genai_messages(input_messages);\n    let output = convert_to_genai_messages(output_messages);\n\n    // Serialize to JSON\n    let input_json = match serde_json::to_string(&input) {\n        Ok(json) => json,\n        Err(e) => {\n            tracing::warn!(\"Failed to serialize input messages: {}\", e);\n            return;\n        }\n    };\n\n    let output_json = match serde_json::to_string(&output) {\n        Ok(json) => json,\n        Err(e) => {\n            tracing::warn!(\"Failed to serialize output messages: {}\", e);\n            return;\n        }\n    };\n\n    // OODA-13: Extract metadata for event emission\n    // Extract response_id from metadata HashMap\n    let response_id = response\n        .metadata\n        .get(\"id\")\n        .and_then(|v| v.as_str())\n        .unwrap_or(\"\");\n\n    let finish_reason = response.finish_reason.as_deref().unwrap_or(\"\");\n\n    // OODA-13: Extract optional parameters with defaults for Jaeger compatibility\n    // tracing-opentelemetry may not properly export Option<T> fields, so we use f64 with sentinel\n    let temperature_val = options.and_then(|o| o.temperature).unwrap_or(-1.0) as f64;\n    let max_tokens_val = options.and_then(|o| o.max_tokens).unwrap_or(0) as i64;\n    let top_p_val = options.and_then(|o| o.top_p).unwrap_or(-1.0) as f64;\n    let frequency_penalty_val = options.and_then(|o| o.frequency_penalty).unwrap_or(-999.0) as f64;\n    let presence_penalty_val = options.and_then(|o| o.presence_penalty).unwrap_or(-999.0) as f64;\n    let cache_hit_tokens_val = response.cache_hit_tokens.unwrap_or(0) as i64;\n\n    // Emit the event using tracing::event! macro which adds it to the current span\n    // The event will appear in Jaeger as a span event (log entry within the span timeline)\n    // OODA-13: Now includes comprehensive metadata per OpenTelemetry GenAI conventions\n    tracing::event!(\n        target: \"gen_ai.events\",\n        tracing::Level::INFO,\n        event.name = \"gen_ai.client.inference.operation.details\",\n        gen_ai.input.messages = %input_json,\n        gen_ai.output.messages = %output_json,\n        gen_ai.response.id = %response_id,\n        gen_ai.response.finish_reasons = %finish_reason,\n        gen_ai.usage.input_tokens = response.prompt_tokens as i64,\n        gen_ai.usage.output_tokens = response.completion_tokens as i64,\n        gen_ai.usage.cache_hit_tokens = cache_hit_tokens_val,\n        gen_ai.request.temperature = temperature_val,\n        gen_ai.request.max_tokens = max_tokens_val,\n        gen_ai.request.top_p = top_p_val,\n        gen_ai.request.frequency_penalty = frequency_penalty_val,\n        gen_ai.request.presence_penalty = presence_penalty_val,\n        \"GenAI inference completed\"\n    );\n\n    tracing::debug!(\n        \"Emitted gen_ai.client.inference.operation.details event with response_id={} finish_reason={}\",\n        response_id,\n        finish_reason\n    );\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::ToolCall;\n\n    #[test]\n    fn test_convert_simple_text_message() {\n        let messages = vec![ChatMessage {\n            role: ChatRole::User,\n            content: \"Hello, world!\".to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai.len(), 1);\n        assert_eq!(genai[0].role, \"user\");\n        assert_eq!(genai[0].content.len(), 1);\n\n        match &genai[0].content[0] {\n            GenAIMessagePart::Text { text } => {\n                assert_eq!(text, \"Hello, world!\");\n            }\n            _ => panic!(\"Expected text part\"),\n        }\n    }\n\n    #[test]\n    fn test_convert_with_tool_calls() {\n        let messages = vec![ChatMessage {\n            role: ChatRole::Assistant,\n            content: \"Let me search for that.\".to_string(),\n            name: None,\n            tool_calls: Some(vec![ToolCall {\n                id: \"call_123\".to_string(),\n                call_type: \"function\".to_string(),\n                function: crate::traits::FunctionCall {\n                    name: \"web_search\".to_string(),\n                    arguments: r#\"{\"query\":\"test\"}\"#.to_string(),\n                },\n            }]),\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai.len(), 1);\n        assert_eq!(genai[0].role, \"assistant\");\n        assert_eq!(genai[0].content.len(), 2); // text + tool_call\n    }\n\n    #[test]\n    fn test_should_capture_content_enabled() {\n        env::set_var(\"EDGECODE_CAPTURE_CONTENT\", \"true\");\n        assert!(should_capture_content());\n        env::remove_var(\"EDGECODE_CAPTURE_CONTENT\");\n    }\n\n    #[test]\n    fn test_should_capture_content_disabled() {\n        env::remove_var(\"EDGECODE_CAPTURE_CONTENT\");\n        assert!(!should_capture_content());\n    }\n\n    #[test]\n    fn test_json_serialization() {\n        let genai = GenAIMessage {\n            role: \"user\".to_string(),\n            content: vec![GenAIMessagePart::Text {\n                text: \"Test message\".to_string(),\n            }],\n        };\n\n        let json = serde_json::to_string(&genai).unwrap();\n        assert!(json.contains(\"user\"));\n        assert!(json.contains(\"Test message\"));\n        assert!(json.contains(\"\\\"type\\\":\\\"text\\\"\"));\n    }\n\n    #[test]\n    fn test_convert_system_role() {\n        let messages = vec![ChatMessage {\n            role: ChatRole::System,\n            content: \"You are a helper.\".to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai[0].role, \"system\");\n    }\n\n    #[test]\n    fn test_convert_tool_role() {\n        let messages = vec![ChatMessage {\n            role: ChatRole::Tool,\n            content: \"result data\".to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: Some(\"call_123\".to_string()),\n            cache_control: None,\n            images: None,\n        }];\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai[0].role, \"tool\");\n    }\n\n    #[test]\n    fn test_convert_function_role() {\n        let messages = vec![ChatMessage {\n            role: ChatRole::Function,\n            content: \"function output\".to_string(),\n            name: Some(\"my_function\".to_string()),\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai[0].role, \"function\");\n    }\n\n    #[test]\n    fn test_convert_assistant_role() {\n        let messages = vec![ChatMessage {\n            role: ChatRole::Assistant,\n            content: \"I can help.\".to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai[0].role, \"assistant\");\n    }\n\n    #[test]\n    fn test_should_capture_content_with_1() {\n        env::set_var(\"EDGECODE_CAPTURE_CONTENT\", \"1\");\n        assert!(should_capture_content());\n        env::remove_var(\"EDGECODE_CAPTURE_CONTENT\");\n    }\n\n    #[test]\n    fn test_should_capture_content_false_string() {\n        env::set_var(\"EDGECODE_CAPTURE_CONTENT\", \"false\");\n        assert!(!should_capture_content());\n        env::remove_var(\"EDGECODE_CAPTURE_CONTENT\");\n    }\n\n    #[test]\n    fn test_tool_call_serialization() {\n        let part = GenAIMessagePart::ToolCall {\n            tool_call: GenAIToolCall {\n                id: \"call_1\".to_string(),\n                name: \"search\".to_string(),\n                arguments: r#\"{\"q\":\"test\"}\"#.to_string(),\n            },\n        };\n        let json = serde_json::to_string(&part).unwrap();\n        assert!(json.contains(\"tool_call\"));\n        assert!(json.contains(\"search\"));\n    }\n\n    #[test]\n    fn test_tool_result_serialization() {\n        let part = GenAIMessagePart::ToolResult {\n            tool_result: GenAIToolResult {\n                tool_call_id: \"call_1\".to_string(),\n                content: \"search results\".to_string(),\n            },\n        };\n        let json = serde_json::to_string(&part).unwrap();\n        assert!(json.contains(\"tool_result\"));\n        assert!(json.contains(\"search results\"));\n    }\n\n    #[test]\n    fn test_genai_message_deserialization() {\n        let json = r#\"{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"hello\"}]}\"#;\n        let msg: GenAIMessage = serde_json::from_str(json).unwrap();\n        assert_eq!(msg.role, \"user\");\n        assert_eq!(msg.content.len(), 1);\n    }\n\n    #[test]\n    fn test_convert_multiple_messages() {\n        let messages = vec![\n            ChatMessage {\n                role: ChatRole::System,\n                content: \"System prompt\".to_string(),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n                images: None,\n            },\n            ChatMessage {\n                role: ChatRole::User,\n                content: \"User message\".to_string(),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n                images: None,\n            },\n        ];\n        let genai = convert_to_genai_messages(&messages);\n        assert_eq!(genai.len(), 2);\n        assert_eq!(genai[0].role, \"system\");\n        assert_eq!(genai[1].role, \"user\");\n    }\n\n    #[test]\n    fn test_emit_inference_event_disabled() {\n        // Ensure capture is disabled\n        env::remove_var(\"EDGECODE_CAPTURE_CONTENT\");\n\n        let input = vec![ChatMessage {\n            role: ChatRole::User,\n            content: \"Hello\".to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n        let output = vec![ChatMessage {\n            role: ChatRole::Assistant,\n            content: \"Hi there\".to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n        let response = crate::traits::LLMResponse {\n            content: \"Hi there\".to_string(),\n            prompt_tokens: 10,\n            completion_tokens: 5,\n            total_tokens: 15,\n            model: \"gpt-4\".to_string(),\n            finish_reason: Some(\"stop\".to_string()),\n            metadata: Default::default(),\n            cache_hit_tokens: None,\n            tool_calls: vec![],\n            thinking_tokens: None,\n            thinking_content: None,\n        };\n\n        // Should not panic even when disabled\n        emit_inference_event(&input, &output, &response, None);\n    }\n}\n","traces":[{"line":18,"address":[],"length":0,"stats":{"Line":8}},{"line":19,"address":[],"length":0,"stats":{"Line":8}},{"line":20,"address":[],"length":0,"stats":{"Line":13}},{"line":58,"address":[],"length":0,"stats":{"Line":7}},{"line":59,"address":[],"length":0,"stats":{"Line":7}},{"line":61,"address":[],"length":0,"stats":{"Line":15}},{"line":62,"address":[],"length":0,"stats":{"Line":16}},{"line":63,"address":[],"length":0,"stats":{"Line":2}},{"line":64,"address":[],"length":0,"stats":{"Line":2}},{"line":65,"address":[],"length":0,"stats":{"Line":2}},{"line":66,"address":[],"length":0,"stats":{"Line":1}},{"line":67,"address":[],"length":0,"stats":{"Line":1}},{"line":70,"address":[],"length":0,"stats":{"Line":24}},{"line":71,"address":[],"length":0,"stats":{"Line":8}},{"line":75,"address":[],"length":0,"stats":{"Line":9}},{"line":76,"address":[],"length":0,"stats":{"Line":4}},{"line":77,"address":[],"length":0,"stats":{"Line":3}},{"line":78,"address":[],"length":0,"stats":{"Line":1}},{"line":79,"address":[],"length":0,"stats":{"Line":3}},{"line":80,"address":[],"length":0,"stats":{"Line":3}},{"line":81,"address":[],"length":0,"stats":{"Line":1}},{"line":87,"address":[],"length":0,"stats":{"Line":8}},{"line":88,"address":[],"length":0,"stats":{"Line":16}},{"line":89,"address":[],"length":0,"stats":{"Line":8}},{"line":112,"address":[],"length":0,"stats":{"Line":4}},{"line":119,"address":[],"length":0,"stats":{"Line":4}},{"line":120,"address":[],"length":0,"stats":{"Line":4}},{"line":121,"address":[],"length":0,"stats":{"Line":4}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":126,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":130,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":0}}],"covered":28,"coverable":57},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","huggingface.rs"],"content":"//! HuggingFace Hub Provider - Access to open-source models via HuggingFace.\n//!\n//! @implements OODA-80: HuggingFace Hub Integration\n//!\n//! # Overview\n//!\n//! This provider connects to HuggingFace's Inference API for access to\n//! open-source models like Llama, Mistral, Qwen, and Gemma. HuggingFace's API\n//! is OpenAI-compatible, so we leverage `OpenAICompatibleProvider` internally.\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                  HuggingFace Provider Architecture                       â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                          â”‚\n//! â”‚   User Request                                                           â”‚\n//! â”‚        â”‚                                                                 â”‚\n//! â”‚        â–¼                                                                 â”‚\n//! â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n//! â”‚   â”‚ HuggingFaceProviderâ”‚â”€â–ºâ”‚ OpenAICompatibleProviderâ”‚â”€â–ºâ”‚ api-inference  â”‚ â”‚\n//! â”‚   â”‚ (wrapper)          â”‚  â”‚ (implementation)        â”‚  â”‚ .huggingface   â”‚ â”‚\n//! â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ .co/v1          â”‚ â”‚\n//! â”‚                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n//! â”‚                                                                          â”‚\n//! â”‚   HuggingFaceProvider provides:                                          â”‚\n//! â”‚   - HF_TOKEN environment detection                                       â”‚\n//! â”‚   - Dynamic base URL per model                                           â”‚\n//! â”‚   - Default model: meta-llama/Meta-Llama-3.1-70B-Instruct               â”‚\n//! â”‚   - Model catalog with context sizes                                    â”‚\n//! â”‚                                                                          â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Environment Variables\n//!\n//! | Variable | Required | Default | Description |\n//! |----------|----------|---------|-------------|\n//! | `HF_TOKEN` | âœ… Yes | - | HuggingFace access token |\n//! | `HUGGINGFACE_TOKEN` | âŒ Alt | - | Alternative token variable |\n//! | `HF_MODEL` | âŒ No | `meta-llama/Meta-Llama-3.1-70B-Instruct` | Default model |\n//! | `HF_BASE_URL` | âŒ No | Auto | Custom API endpoint |\n//!\n//! # Available Models\n//!\n//! | Model | Context | Description |\n//! |-------|---------|-------------|\n//! | `meta-llama/Meta-Llama-3.1-70B-Instruct` | 128K | Llama 3.1 70B |\n//! | `meta-llama/Meta-Llama-3.1-8B-Instruct` | 128K | Llama 3.1 8B |\n//! | `mistralai/Mistral-7B-Instruct-v0.3` | 32K | Mistral 7B |\n//! | `Qwen/Qwen2.5-72B-Instruct` | 128K | Qwen 2.5 72B |\n//! | `microsoft/Phi-3-medium-4k-instruct` | 4K | Phi-3 Medium |\n//! | `google/gemma-7b-it` | 8K | Gemma 7B IT |\n//!\n//! # Example\n//!\n//! ```bash\n//! # Set HuggingFace token (from https://huggingface.co/settings/tokens)\n//! export HF_TOKEN=hf_xxxxxxxxxx\n//!\n//! # Use with EdgeCode (auto-detected)\n//! edgecode react \"Write hello world in Rust\"\n//!\n//! # Explicit provider selection\n//! edgecode react --provider huggingface \"Write hello world in Rust\"\n//!\n//! # Use specific model\n//! export HF_MODEL=mistralai/Mistral-7B-Instruct-v0.3\n//! edgecode react \"Explain quantum computing\"\n//! ```\n//!\n//! # Free Tier\n//!\n//! HuggingFace offers a free tier with limited requests. For higher throughput,\n//! consider a Pro subscription or Inference Endpoints for dedicated infrastructure.\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse tracing::debug;\n\nuse crate::error::{LlmError, Result};\nuse crate::model_config::{\n    ModelCapabilities, ModelCard, ModelType, ProviderConfig, ProviderType as ConfigProviderType,\n};\nuse crate::providers::openai_compatible::OpenAICompatibleProvider;\nuse crate::traits::{ChatMessage, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse, StreamChunk};\n\n// ============================================================================\n// Constants\n// ============================================================================\n\n/// HuggingFace Inference API base URL template\n///\n/// WHY: HuggingFace uses per-model URLs for the serverless inference API\n/// Format: https://api-inference.huggingface.co/models/{model_id}/v1\n#[allow(dead_code)]\nconst HF_BASE_URL_TEMPLATE: &str = \"https://api-inference.huggingface.co/models\";\n\n/// Alternative: Router-based URL for Inference Providers\n/// This provides load balancing and provider selection\nconst HF_ROUTER_URL: &str = \"https://router.huggingface.co/hf-inference/v1\";\n\n/// Default model - Llama 3.1 70B is a strong general-purpose model\nconst HF_DEFAULT_MODEL: &str = \"meta-llama/Meta-Llama-3.1-70B-Instruct\";\n\n/// Provider display name\nconst HF_PROVIDER_NAME: &str = \"huggingface\";\n\n/// HuggingFace model catalog with context lengths.\n///\n/// WHY: Pre-defined models ensure users get correct context limits without\n/// having to check documentation. Updated from HuggingFace Hub.\nconst HF_MODELS: &[(&str, &str, usize)] = &[\n    // Meta Llama models\n    (\n        \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n        \"Llama 3.1 70B Instruct\",\n        128000,\n    ),\n    (\n        \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n        \"Llama 3.1 8B Instruct\",\n        128000,\n    ),\n    (\n        \"meta-llama/Meta-Llama-3-8B-Instruct\",\n        \"Llama 3 8B Instruct\",\n        8192,\n    ),\n    (\n        \"meta-llama/Meta-Llama-3-70B-Instruct\",\n        \"Llama 3 70B Instruct\",\n        8192,\n    ),\n    // Mistral models\n    (\n        \"mistralai/Mistral-7B-Instruct-v0.3\",\n        \"Mistral 7B Instruct v0.3\",\n        32000,\n    ),\n    (\n        \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n        \"Mixtral 8x7B Instruct\",\n        32000,\n    ),\n    // Qwen models\n    (\"Qwen/Qwen2.5-72B-Instruct\", \"Qwen 2.5 72B Instruct\", 128000),\n    (\"Qwen/Qwen2.5-7B-Instruct\", \"Qwen 2.5 7B Instruct\", 128000),\n    (\"Qwen/Qwen2.5-Coder-32B-Instruct\", \"Qwen 2.5 Coder 32B\", 128000),\n    // Microsoft Phi models\n    (\n        \"microsoft/Phi-3-medium-4k-instruct\",\n        \"Phi-3 Medium 4K\",\n        4096,\n    ),\n    (\n        \"microsoft/Phi-3-mini-4k-instruct\",\n        \"Phi-3 Mini 4K\",\n        4096,\n    ),\n    // Google Gemma models\n    (\"google/gemma-7b-it\", \"Gemma 7B IT\", 8192),\n    (\"google/gemma-2b-it\", \"Gemma 2B IT\", 8192),\n    // DeepSeek models\n    (\n        \"deepseek-ai/DeepSeek-Coder-V2-Instruct\",\n        \"DeepSeek Coder V2\",\n        128000,\n    ),\n];\n\n// ============================================================================\n// HuggingFaceProvider\n// ============================================================================\n\n/// HuggingFace Hub provider for open-source model access.\n///\n/// This is a thin wrapper around `OpenAICompatibleProvider` that provides:\n/// - Automatic `HF_TOKEN` detection\n/// - Default configuration for HuggingFace's API\n/// - Model catalog with correct context sizes\n///\n/// # Why Wrap OpenAICompatibleProvider?\n///\n/// HuggingFace's Inference API is OpenAI-compatible, so we get:\n/// - Battle-tested HTTP client\n/// - Streaming support\n/// - Tool/function calling\n/// - JSON mode\n/// - Error handling\n/// - Retry logic\n///\n/// Without code duplication!\n#[derive(Debug)]\npub struct HuggingFaceProvider {\n    /// Inner OpenAI-compatible provider\n    inner: OpenAICompatibleProvider,\n    /// Current model name\n    model: String,\n}\n\nimpl HuggingFaceProvider {\n    /// Create provider from environment variables.\n    ///\n    /// # Environment Variables\n    ///\n    /// - `HF_TOKEN` or `HUGGINGFACE_TOKEN`: Required API token\n    /// - `HF_MODEL`: Model name (default: meta-llama/Meta-Llama-3.1-70B-Instruct)\n    /// - `HF_BASE_URL`: Custom base URL (default: auto-generated per model)\n    ///\n    /// # Errors\n    ///\n    /// Returns error if neither `HF_TOKEN` nor `HUGGINGFACE_TOKEN` is set.\n    pub fn from_env() -> Result<Self> {\n        // Check both common token variable names\n        let api_key = std::env::var(\"HF_TOKEN\")\n            .or_else(|_| std::env::var(\"HUGGINGFACE_TOKEN\"))\n            .map_err(|_| {\n                LlmError::ConfigError(\n                    \"HF_TOKEN or HUGGINGFACE_TOKEN environment variable not set. \\\n                     Get your token from https://huggingface.co/settings/tokens\"\n                        .to_string(),\n                )\n            })?;\n\n        if api_key.is_empty() {\n            return Err(LlmError::ConfigError(\n                \"HF_TOKEN is empty. Please set a valid token.\".to_string(),\n            ));\n        }\n\n        let model =\n            std::env::var(\"HF_MODEL\").unwrap_or_else(|_| HF_DEFAULT_MODEL.to_string());\n        let base_url = std::env::var(\"HF_BASE_URL\").ok();\n\n        Self::new(api_key, model, base_url)\n    }\n\n    /// Create provider with explicit configuration.\n    ///\n    /// # Arguments\n    ///\n    /// * `api_key` - HuggingFace access token\n    /// * `model` - Model name (e.g., \"meta-llama/Meta-Llama-3.1-70B-Instruct\")\n    /// * `base_url` - Optional custom base URL\n    pub fn new(api_key: String, model: String, base_url: Option<String>) -> Result<Self> {\n        // Set HF_TOKEN env var for OpenAICompatibleProvider to read\n        std::env::set_var(\"HF_TOKEN\", &api_key);\n\n        // Build ProviderConfig\n        let config = Self::build_config(&model, base_url.as_deref());\n\n        // Create inner provider\n        let inner = OpenAICompatibleProvider::from_config(config)?;\n\n        debug!(\n            provider = HF_PROVIDER_NAME,\n            model = %model,\n            \"Created HuggingFace provider\"\n        );\n\n        Ok(Self { inner, model })\n    }\n\n    /// Create with a different model.\n    ///\n    /// Returns a new provider instance configured for the specified model.\n    pub fn with_model(mut self, model: &str) -> Self {\n        self.model = model.to_string();\n        // Update inner provider with new model\n        // Note: Base URL stays the same (router URL handles routing)\n        self.inner = self.inner.with_model(model);\n        self\n    }\n\n    /// Generate the API URL for a specific model.\n    ///\n    /// WHY: HuggingFace uses per-model URLs for serverless inference.\n    /// We use the router URL for simpler implementation and load balancing.\n    #[allow(dead_code)]\n    fn model_url(_model: &str) -> String {\n        // Use router URL which handles model routing automatically\n        // This is simpler than constructing per-model URLs\n        HF_ROUTER_URL.to_string()\n    }\n\n    /// Build ProviderConfig for OpenAICompatibleProvider.\n    fn build_config(model: &str, base_url: Option<&str>) -> ProviderConfig {\n        // Build model cards from HF_MODELS with proper capabilities\n        let models: Vec<ModelCard> = HF_MODELS\n            .iter()\n            .map(|(name, display, context)| ModelCard {\n                name: name.to_string(),\n                display_name: display.to_string(),\n                model_type: ModelType::Llm,\n                capabilities: ModelCapabilities {\n                    context_length: *context,\n                    supports_function_calling: true, // Most HF models support this\n                    supports_json_mode: true,\n                    supports_streaming: true,\n                    supports_system_message: true,\n                    supports_vision: false, // Set true for vision models if needed\n                    ..Default::default()\n                },\n                ..Default::default()\n            })\n            .collect();\n\n        // Use provided base_url or default to router URL\n        let effective_base_url = base_url\n            .map(|s| s.to_string())\n            .unwrap_or_else(|| HF_ROUTER_URL.to_string());\n\n        ProviderConfig {\n            name: HF_PROVIDER_NAME.to_string(),\n            display_name: \"HuggingFace Hub\".to_string(),\n            provider_type: ConfigProviderType::OpenAICompatible,\n            api_key_env: Some(\"HF_TOKEN\".to_string()),\n            base_url: Some(effective_base_url),\n            base_url_env: Some(\"HF_BASE_URL\".to_string()),\n            default_llm_model: Some(model.to_string()),\n            default_embedding_model: None,\n            models,\n            headers: std::collections::HashMap::new(),\n            enabled: true,\n            ..Default::default()\n        }\n    }\n\n    /// Get context length for a model.\n    pub fn context_length(model: &str) -> usize {\n        HF_MODELS\n            .iter()\n            .find(|(name, _, _)| *name == model)\n            .map(|(_, _, ctx)| *ctx)\n            .unwrap_or(8192) // Conservative default\n    }\n\n    /// List available models.\n    pub fn available_models() -> Vec<(&'static str, &'static str, usize)> {\n        HF_MODELS.to_vec()\n    }\n\n    /// Check if a token looks like a HuggingFace token.\n    ///\n    /// HuggingFace tokens start with \"hf_\" prefix.\n    pub fn is_hf_token(token: &str) -> bool {\n        token.starts_with(\"hf_\")\n    }\n}\n\n// ============================================================================\n// LLMProvider Implementation (delegates to inner OpenAICompatibleProvider)\n// ============================================================================\n\n#[async_trait]\nimpl LLMProvider for HuggingFaceProvider {\n    fn name(&self) -> &str {\n        HF_PROVIDER_NAME\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        Self::context_length(&self.model)\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.inner.complete(prompt).await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        self.inner.complete_with_options(prompt, options).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.inner.chat(messages, options).await\n    }\n\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.inner\n            .chat_with_tools(messages, tools, tool_choice, options)\n            .await\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        self.inner.stream(prompt).await\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        self.inner.supports_function_calling()\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        self.inner.supports_tool_streaming()\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        self.inner.chat_with_tools_stream(messages, tools, tool_choice, options).await\n    }\n}\n\n// ============================================================================\n// EmbeddingProvider Implementation\n// ============================================================================\n\n#[async_trait]\nimpl EmbeddingProvider for HuggingFaceProvider {\n    fn name(&self) -> &str {\n        HF_PROVIDER_NAME\n    }\n\n    fn model(&self) -> &str {\n        \"none\"\n    }\n\n    fn dimension(&self) -> usize {\n        0 // Embedding support would require a separate implementation\n    }\n\n    fn max_tokens(&self) -> usize {\n        0\n    }\n\n    async fn embed(&self, _texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // Embeddings require a different API endpoint\n        // Could be added in future with a dedicated embedding model\n        Err(LlmError::ConfigError(\n            \"HuggingFace embeddings require a separate provider configuration. \\\n             Use the HuggingFace Inference API directly for embeddings.\"\n                .to_string(),\n        ))\n    }\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_context_length_known_model() {\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n            128000\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"mistralai/Mistral-7B-Instruct-v0.3\"),\n            32000\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"microsoft/Phi-3-medium-4k-instruct\"),\n            4096\n        );\n    }\n\n    #[test]\n    fn test_context_length_unknown_model() {\n        // Unknown models default to 8K\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"unknown/model\"),\n            8192\n        );\n    }\n\n    #[test]\n    fn test_available_models() {\n        let models = HuggingFaceProvider::available_models();\n        assert!(!models.is_empty());\n        assert!(models\n            .iter()\n            .any(|(name, _, _)| *name == \"meta-llama/Meta-Llama-3.1-70B-Instruct\"));\n        assert!(models\n            .iter()\n            .any(|(name, _, _)| *name == \"Qwen/Qwen2.5-72B-Instruct\"));\n    }\n\n    #[test]\n    fn test_build_config() {\n        let config = HuggingFaceProvider::build_config(\n            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            None,\n        );\n        assert_eq!(config.name, \"huggingface\");\n        assert_eq!(\n            config.base_url,\n            Some(\"https://router.huggingface.co/hf-inference/v1\".to_string())\n        );\n        assert_eq!(\n            config.default_llm_model,\n            Some(\"meta-llama/Meta-Llama-3.1-70B-Instruct\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_build_config_custom_url() {\n        let config = HuggingFaceProvider::build_config(\n            \"mistralai/Mistral-7B-Instruct-v0.3\",\n            Some(\"https://custom.api\"),\n        );\n        assert_eq!(config.base_url, Some(\"https://custom.api\".to_string()));\n    }\n\n    #[test]\n    fn test_is_hf_token() {\n        assert!(HuggingFaceProvider::is_hf_token(\"hf_xxxxx\"));\n        assert!(HuggingFaceProvider::is_hf_token(\"hf_abc123\"));\n        assert!(!HuggingFaceProvider::is_hf_token(\"sk-xxxxx\"));\n        assert!(!HuggingFaceProvider::is_hf_token(\"xxxxx\"));\n    }\n\n    #[test]\n    fn test_model_url() {\n        let url = HuggingFaceProvider::model_url(\"meta-llama/Meta-Llama-3.1-70B-Instruct\");\n        assert_eq!(url, \"https://router.huggingface.co/hf-inference/v1\");\n    }\n\n    #[test]\n    fn test_context_length_llama_models() {\n        // Llama 3.1 series - 128K\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"meta-llama/Meta-Llama-3.1-70B-Instruct\"),\n            128000\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n            128000\n        );\n        // Llama 3 series - 8K\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"meta-llama/Meta-Llama-3-8B-Instruct\"),\n            8192\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"meta-llama/Meta-Llama-3-70B-Instruct\"),\n            8192\n        );\n    }\n\n    #[test]\n    fn test_context_length_mistral_models() {\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"mistralai/Mistral-7B-Instruct-v0.3\"),\n            32000\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"mistralai/Mixtral-8x7B-Instruct-v0.1\"),\n            32000\n        );\n    }\n\n    #[test]\n    fn test_context_length_qwen_models() {\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"Qwen/Qwen2.5-72B-Instruct\"),\n            128000\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"Qwen/Qwen2.5-7B-Instruct\"),\n            128000\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n            128000\n        );\n    }\n\n    #[test]\n    fn test_context_length_phi_models() {\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"microsoft/Phi-3-medium-4k-instruct\"),\n            4096\n        );\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"microsoft/Phi-3-mini-4k-instruct\"),\n            4096\n        );\n    }\n\n    #[test]\n    fn test_context_length_gemma_and_deepseek() {\n        // Gemma models - 8K\n        assert_eq!(HuggingFaceProvider::context_length(\"google/gemma-7b-it\"), 8192);\n        assert_eq!(HuggingFaceProvider::context_length(\"google/gemma-2b-it\"), 8192);\n        // DeepSeek - 128K\n        assert_eq!(\n            HuggingFaceProvider::context_length(\"deepseek-ai/DeepSeek-Coder-V2-Instruct\"),\n            128000\n        );\n    }\n\n    #[test]\n    fn test_available_models_contains_all_families() {\n        let models = HuggingFaceProvider::available_models();\n        \n        // Meta Llama\n        assert!(models.iter().any(|(name, _, _)| name.contains(\"meta-llama\")));\n        // Mistral\n        assert!(models.iter().any(|(name, _, _)| name.contains(\"mistralai\")));\n        // Qwen\n        assert!(models.iter().any(|(name, _, _)| name.contains(\"Qwen\")));\n        // Microsoft Phi\n        assert!(models.iter().any(|(name, _, _)| name.contains(\"microsoft\")));\n        // Google Gemma\n        assert!(models.iter().any(|(name, _, _)| name.contains(\"google\")));\n        // DeepSeek\n        assert!(models.iter().any(|(name, _, _)| name.contains(\"deepseek\")));\n    }\n\n    #[test]\n    fn test_available_models_has_positive_context() {\n        let models = HuggingFaceProvider::available_models();\n        for (name, _desc, context_len) in models {\n            assert!(context_len > 0, \"Model {} should have positive context length\", name);\n        }\n    }\n\n    #[test]\n    fn test_constants() {\n        assert_eq!(HF_DEFAULT_MODEL, \"meta-llama/Meta-Llama-3.1-70B-Instruct\");\n        assert_eq!(HF_PROVIDER_NAME, \"huggingface\");\n        assert_eq!(HF_ROUTER_URL, \"https://router.huggingface.co/hf-inference/v1\");\n    }\n\n    #[test]\n    fn test_from_env_missing_token() {\n        // Clear env vars\n        std::env::remove_var(\"HF_TOKEN\");\n        std::env::remove_var(\"HUGGINGFACE_TOKEN\");\n        std::env::remove_var(\"HF_MODEL\");\n        std::env::remove_var(\"HF_BASE_URL\");\n\n        let result = HuggingFaceProvider::from_env();\n        assert!(result.is_err());\n        let err = result.unwrap_err();\n        assert!(err.to_string().contains(\"HF_TOKEN\") || err.to_string().contains(\"HUGGINGFACE_TOKEN\"));\n    }\n\n    #[test]\n    fn test_build_config_has_models() {\n        let config = HuggingFaceProvider::build_config(\n            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            None,\n        );\n        assert!(!config.models.is_empty());\n        // Should contain the requested model\n        assert!(config.models.iter().any(|m| m.name == \"meta-llama/Meta-Llama-3.1-70B-Instruct\"));\n    }\n\n    #[test]\n    fn test_build_config_api_key_env() {\n        let config = HuggingFaceProvider::build_config(\n            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            None,\n        );\n        assert_eq!(config.api_key_env, Some(\"HF_TOKEN\".to_string()));\n    }\n\n    #[test]\n    fn test_is_hf_token_edge_cases() {\n        // Valid prefixes - function just checks starts_with(\"hf_\")\n        assert!(HuggingFaceProvider::is_hf_token(\"hf_a\"));\n        assert!(HuggingFaceProvider::is_hf_token(\"hf_verylongtokenstring123\"));\n        assert!(HuggingFaceProvider::is_hf_token(\"hf_\")); // Technically valid prefix match\n        \n        // Invalid - wrong prefix\n        assert!(!HuggingFaceProvider::is_hf_token(\"sk_xxxxx\"));\n        assert!(!HuggingFaceProvider::is_hf_token(\"api_key\"));\n        assert!(!HuggingFaceProvider::is_hf_token(\"HF_token\")); // Case sensitive\n        \n        // Invalid - too short or empty\n        assert!(!HuggingFaceProvider::is_hf_token(\"\"));\n        assert!(!HuggingFaceProvider::is_hf_token(\"h\"));\n        assert!(!HuggingFaceProvider::is_hf_token(\"hf\"));\n    }\n\n    #[test]\n    fn test_model_url_always_returns_router() {\n        // All models should use the router URL\n        let models = vec![\n            \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n            \"mistralai/Mistral-7B-Instruct-v0.3\",\n            \"Qwen/Qwen2.5-72B-Instruct\",\n            \"unknown/model\",\n        ];\n        \n        for model in models {\n            assert_eq!(\n                HuggingFaceProvider::model_url(model),\n                \"https://router.huggingface.co/hf-inference/v1\"\n            );\n        }\n    }\n}\n","traces":[{"line":213,"address":[],"length":0,"stats":{"Line":2}},{"line":215,"address":[],"length":0,"stats":{"Line":3}},{"line":216,"address":[],"length":0,"stats":{"Line":3}},{"line":217,"address":[],"length":0,"stats":{"Line":3}},{"line":218,"address":[],"length":0,"stats":{"Line":1}},{"line":219,"address":[],"length":0,"stats":{"Line":1}},{"line":220,"address":[],"length":0,"stats":{"Line":1}},{"line":221,"address":[],"length":0,"stats":{"Line":1}},{"line":225,"address":[],"length":0,"stats":{"Line":2}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":1}},{"line":232,"address":[],"length":0,"stats":{"Line":4}},{"line":233,"address":[],"length":0,"stats":{"Line":3}},{"line":235,"address":[],"length":0,"stats":{"Line":4}},{"line":245,"address":[],"length":0,"stats":{"Line":1}},{"line":247,"address":[],"length":0,"stats":{"Line":2}},{"line":250,"address":[],"length":0,"stats":{"Line":5}},{"line":253,"address":[],"length":0,"stats":{"Line":3}},{"line":255,"address":[],"length":0,"stats":{"Line":1}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":1}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":5}},{"line":283,"address":[],"length":0,"stats":{"Line":10}},{"line":287,"address":[],"length":0,"stats":{"Line":5}},{"line":289,"address":[],"length":0,"stats":{"Line":15}},{"line":291,"address":[],"length":0,"stats":{"Line":5}},{"line":292,"address":[],"length":0,"stats":{"Line":140}},{"line":293,"address":[],"length":0,"stats":{"Line":140}},{"line":294,"address":[],"length":0,"stats":{"Line":70}},{"line":295,"address":[],"length":0,"stats":{"Line":70}},{"line":296,"address":[],"length":0,"stats":{"Line":70}},{"line":297,"address":[],"length":0,"stats":{"Line":70}},{"line":298,"address":[],"length":0,"stats":{"Line":70}},{"line":299,"address":[],"length":0,"stats":{"Line":70}},{"line":300,"address":[],"length":0,"stats":{"Line":70}},{"line":301,"address":[],"length":0,"stats":{"Line":70}},{"line":302,"address":[],"length":0,"stats":{"Line":70}},{"line":304,"address":[],"length":0,"stats":{"Line":70}},{"line":309,"address":[],"length":0,"stats":{"Line":10}},{"line":310,"address":[],"length":0,"stats":{"Line":7}},{"line":311,"address":[],"length":0,"stats":{"Line":13}},{"line":314,"address":[],"length":0,"stats":{"Line":15}},{"line":315,"address":[],"length":0,"stats":{"Line":15}},{"line":317,"address":[],"length":0,"stats":{"Line":10}},{"line":318,"address":[],"length":0,"stats":{"Line":10}},{"line":319,"address":[],"length":0,"stats":{"Line":10}},{"line":320,"address":[],"length":0,"stats":{"Line":10}},{"line":323,"address":[],"length":0,"stats":{"Line":5}},{"line":330,"address":[],"length":0,"stats":{"Line":18}},{"line":331,"address":[],"length":0,"stats":{"Line":18}},{"line":333,"address":[],"length":0,"stats":{"Line":288}},{"line":334,"address":[],"length":0,"stats":{"Line":18}},{"line":339,"address":[],"length":0,"stats":{"Line":3}},{"line":340,"address":[],"length":0,"stats":{"Line":6}},{"line":346,"address":[],"length":0,"stats":{"Line":13}},{"line":347,"address":[],"length":0,"stats":{"Line":26}},{"line":357,"address":[],"length":0,"stats":{"Line":0}},{"line":358,"address":[],"length":0,"stats":{"Line":0}},{"line":361,"address":[],"length":0,"stats":{"Line":0}},{"line":362,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[],"length":0,"stats":{"Line":0}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":406,"address":[],"length":0,"stats":{"Line":0}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":410,"address":[],"length":0,"stats":{"Line":0}},{"line":413,"address":[],"length":0,"stats":{"Line":0}},{"line":414,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":438,"address":[],"length":0,"stats":{"Line":0}},{"line":439,"address":[],"length":0,"stats":{"Line":0}},{"line":442,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":0}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":447,"address":[],"length":0,"stats":{"Line":0}},{"line":450,"address":[],"length":0,"stats":{"Line":0}}],"covered":54,"coverable":84},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","jina.rs"],"content":"//! Jina AI Embeddings provider implementation.\n//!\n//! This module provides integration with Jina AI's embedding API.\n//!\n//! # Environment Variables\n//!\n//! - `JINA_API_KEY`: Your Jina AI API key (required)\n//!\n//! # Example\n//!\n//! ```rust,ignore\n//! use edgequake_llm::JinaProvider;\n//!\n//! let provider = JinaProvider::from_env()?;\n//! let embeddings = provider.embed(vec![\"Hello, world!\".to_string()]).await?;\n//! ```\n\nuse async_trait::async_trait;\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse tracing::debug;\n\nuse crate::error::LlmError;\nuse crate::traits::EmbeddingProvider;\n\n/// Default Jina embedding model\nconst DEFAULT_JINA_EMBEDDING_MODEL: &str = \"jina-embeddings-v3\";\n\n/// Default Jina API base URL\nconst DEFAULT_JINA_BASE_URL: &str = \"https://api.jina.ai\";\n\n/// Jina AI Embeddings provider.\n///\n/// Supports Jina's embedding models including:\n/// - `jina-embeddings-v4`: Multimodal multilingual (3.8B parameters)\n/// - `jina-embeddings-v3`: Multilingual with task LoRA\n/// - `jina-embeddings-v2-*`: Various specialized models\n/// - `jina-clip-v2`: CLIP-style multimodal embeddings\n#[derive(Debug, Clone)]\npub struct JinaProvider {\n    client: Client,\n    api_key: String,\n    base_url: String,\n    embedding_model: String,\n    embedding_dimension: usize,\n    /// Task type for embeddings (retrieval.query, retrieval.passage, etc.)\n    task: Option<String>,\n    /// Whether to normalize embeddings\n    normalized: bool,\n}\n\n/// Builder for JinaProvider\n#[derive(Debug, Clone)]\npub struct JinaProviderBuilder {\n    api_key: Option<String>,\n    base_url: String,\n    embedding_model: String,\n    embedding_dimension: usize,\n    task: Option<String>,\n    normalized: bool,\n}\n\nimpl Default for JinaProviderBuilder {\n    fn default() -> Self {\n        Self {\n            api_key: None,\n            base_url: DEFAULT_JINA_BASE_URL.to_string(),\n            embedding_model: DEFAULT_JINA_EMBEDDING_MODEL.to_string(),\n            embedding_dimension: 1024, // jina-embeddings-v3 default\n            task: None,\n            normalized: true,\n        }\n    }\n}\n\nimpl JinaProviderBuilder {\n    /// Create a new builder with default settings\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Set the API key\n    pub fn api_key(mut self, api_key: impl Into<String>) -> Self {\n        self.api_key = Some(api_key.into());\n        self\n    }\n\n    /// Set the base URL\n    pub fn base_url(mut self, base_url: impl Into<String>) -> Self {\n        self.base_url = base_url.into();\n        self\n    }\n\n    /// Set the embedding model\n    pub fn embedding_model(mut self, model: impl Into<String>) -> Self {\n        self.embedding_model = model.into();\n        self\n    }\n\n    /// Set the embedding dimension\n    pub fn embedding_dimension(mut self, dimension: usize) -> Self {\n        self.embedding_dimension = dimension;\n        self\n    }\n\n    /// Set the task type for embeddings\n    ///\n    /// Supported tasks:\n    /// - `retrieval.query`: For query embeddings in search\n    /// - `retrieval.passage`: For document/passage embeddings\n    /// - `separation`: For clustering/separation tasks\n    /// - `classification`: For classification tasks\n    /// - `text-matching`: For semantic similarity\n    pub fn task(mut self, task: impl Into<String>) -> Self {\n        self.task = Some(task.into());\n        self\n    }\n\n    /// Set whether to normalize embeddings (L2 normalization)\n    pub fn normalized(mut self, normalized: bool) -> Self {\n        self.normalized = normalized;\n        self\n    }\n\n    /// Build the JinaProvider\n    pub fn build(self) -> Result<JinaProvider, LlmError> {\n        let api_key = self\n            .api_key\n            .ok_or_else(|| LlmError::ConfigError(\"JINA_API_KEY is required\".to_string()))?;\n\n        let client = Client::builder()\n            .timeout(std::time::Duration::from_secs(120))\n            .build()\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        Ok(JinaProvider {\n            client,\n            api_key,\n            base_url: self.base_url,\n            embedding_model: self.embedding_model,\n            embedding_dimension: self.embedding_dimension,\n            task: self.task,\n            normalized: self.normalized,\n        })\n    }\n}\n\nimpl JinaProvider {\n    /// Create a new JinaProvider from environment variables.\n    ///\n    /// Required environment variables:\n    /// - `JINA_API_KEY`: Your Jina AI API key\n    ///\n    /// Optional environment variables:\n    /// - `JINA_BASE_URL`: Custom API base URL\n    /// - `JINA_EMBEDDING_MODEL`: Model to use (default: jina-embeddings-v3)\n    pub fn from_env() -> Result<Self, LlmError> {\n        let api_key = std::env::var(\"JINA_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\"JINA_API_KEY environment variable is required\".to_string())\n        })?;\n\n        let base_url =\n            std::env::var(\"JINA_BASE_URL\").unwrap_or_else(|_| DEFAULT_JINA_BASE_URL.to_string());\n\n        let embedding_model = std::env::var(\"JINA_EMBEDDING_MODEL\")\n            .unwrap_or_else(|_| DEFAULT_JINA_EMBEDDING_MODEL.to_string());\n\n        // Get dimension based on model\n        let embedding_dimension = get_model_dimension(&embedding_model);\n\n        JinaProviderBuilder::new()\n            .api_key(api_key)\n            .base_url(base_url)\n            .embedding_model(embedding_model)\n            .embedding_dimension(embedding_dimension)\n            .build()\n    }\n\n    /// Create a new builder for JinaProvider\n    pub fn builder() -> JinaProviderBuilder {\n        JinaProviderBuilder::new()\n    }\n}\n\n/// Get the default dimension for a Jina model\nfn get_model_dimension(model: &str) -> usize {\n    match model {\n        \"jina-embeddings-v4\" => 1024,\n        \"jina-embeddings-v3\" => 1024,\n        \"jina-embeddings-v2-base-en\" => 768,\n        \"jina-embeddings-v2-small-en\" => 512,\n        \"jina-embeddings-v2-base-de\" => 768,\n        \"jina-embeddings-v2-base-zh\" => 768,\n        \"jina-embeddings-v2-base-code\" => 768,\n        \"jina-clip-v2\" => 1024,\n        \"jina-clip-v1\" => 768,\n        _ => 1024, // Default\n    }\n}\n\n// Request/Response structures for Jina API\n\n#[derive(Debug, Serialize)]\nstruct EmbeddingRequest {\n    model: String,\n    input: Vec<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    task: Option<String>,\n    #[serde(skip_serializing_if = \"std::ops::Not::not\")]\n    normalized: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    dimensions: Option<usize>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    embedding_type: Option<String>,\n}\n\n#[allow(dead_code)]\n#[derive(Debug, Deserialize)]\nstruct EmbeddingResponse {\n    model: String,\n    data: Vec<EmbeddingData>,\n    usage: EmbeddingUsage,\n}\n\n#[derive(Debug, Deserialize)]\nstruct EmbeddingData {\n    index: usize,\n    embedding: Vec<f32>,\n}\n\n#[allow(dead_code)]\n#[derive(Debug, Deserialize)]\nstruct EmbeddingUsage {\n    total_tokens: u32,\n    #[serde(default)]\n    prompt_tokens: u32,\n}\n\n#[derive(Debug, Deserialize)]\nstruct JinaError {\n    detail: String,\n}\n\n#[async_trait]\nimpl EmbeddingProvider for JinaProvider {\n    fn name(&self) -> &str {\n        \"jina\"\n    }\n\n    fn model(&self) -> &str {\n        &self.embedding_model\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        // jina-embeddings-v3 supports 8192 tokens\n        8192\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>, LlmError> {\n        if texts.is_empty() {\n            return Ok(vec![]);\n        }\n\n        debug!(\n            \"Jina embedding request: {} texts with model {}\",\n            texts.len(),\n            self.embedding_model\n        );\n\n        let url = format!(\"{}/v1/embeddings\", self.base_url);\n\n        let request = EmbeddingRequest {\n            model: self.embedding_model.clone(),\n            input: texts.to_vec(),\n            task: self.task.clone(),\n            normalized: self.normalized,\n            dimensions: None,\n            embedding_type: Some(\"float\".to_string()),\n        };\n\n        let response = self\n            .client\n            .post(&url)\n            .header(\"Authorization\", format!(\"Bearer {}\", self.api_key))\n            .header(\"Content-Type\", \"application/json\")\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            if let Ok(error) = serde_json::from_str::<JinaError>(&error_text) {\n                return Err(LlmError::ApiError(format!(\n                    \"Jina API error ({}): {}\",\n                    status, error.detail\n                )));\n            }\n            return Err(LlmError::ApiError(format!(\n                \"Jina API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let response: EmbeddingResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to parse response: {}\", e)))?;\n\n        // Sort by index and extract embeddings\n        let mut embeddings: Vec<_> = response\n            .data\n            .into_iter()\n            .map(|d| (d.index, d.embedding))\n            .collect();\n        embeddings.sort_by_key(|(i, _)| *i);\n\n        let embeddings: Vec<Vec<f32>> = embeddings.into_iter().map(|(_, e)| e).collect();\n\n        debug!(\n            \"Jina embedding response: {} embeddings of dimension {}\",\n            embeddings.len(),\n            embeddings.first().map(|e| e.len()).unwrap_or(0)\n        );\n\n        Ok(embeddings)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_builder_creation() {\n        let result = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .embedding_model(\"jina-embeddings-v3\")\n            .embedding_dimension(1024)\n            .normalized(true)\n            .build();\n\n        assert!(result.is_ok());\n        let provider = result.unwrap();\n        assert_eq!(EmbeddingProvider::name(&provider), \"jina\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"jina-embeddings-v3\");\n        assert_eq!(EmbeddingProvider::dimension(&provider), 1024);\n    }\n\n    #[test]\n    fn test_builder_with_task() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .task(\"retrieval.query\")\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.task, Some(\"retrieval.query\".to_string()));\n    }\n\n    #[test]\n    fn test_builder_missing_api_key() {\n        let result = JinaProviderBuilder::new().build();\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_model_dimensions() {\n        assert_eq!(get_model_dimension(\"jina-embeddings-v4\"), 1024);\n        assert_eq!(get_model_dimension(\"jina-embeddings-v3\"), 1024);\n        assert_eq!(get_model_dimension(\"jina-embeddings-v2-base-en\"), 768);\n        assert_eq!(get_model_dimension(\"jina-embeddings-v2-small-en\"), 512);\n        assert_eq!(get_model_dimension(\"jina-clip-v2\"), 1024);\n        assert_eq!(get_model_dimension(\"unknown-model\"), 1024);\n    }\n\n    #[tokio::test]\n    async fn test_embed_empty_input() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .build()\n            .unwrap();\n\n        let result = provider.embed(&[]).await;\n        assert!(result.is_ok());\n        assert!(result.unwrap().is_empty());\n    }\n\n    #[test]\n    fn test_builder_default_values() {\n        let builder = JinaProviderBuilder::default();\n        \n        assert!(builder.api_key.is_none());\n        assert_eq!(builder.base_url, \"https://api.jina.ai\");\n        assert_eq!(builder.embedding_model, \"jina-embeddings-v3\");\n        assert_eq!(builder.embedding_dimension, 1024);\n        assert!(builder.task.is_none());\n        assert!(builder.normalized);\n    }\n\n    #[test]\n    fn test_builder_custom_base_url() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .base_url(\"https://custom.jina.ai\")\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.base_url, \"https://custom.jina.ai\");\n    }\n\n    #[test]\n    fn test_builder_normalized_false() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .normalized(false)\n            .build()\n            .unwrap();\n\n        assert!(!provider.normalized);\n    }\n\n    #[test]\n    fn test_model_dimension_v2_variants() {\n        assert_eq!(get_model_dimension(\"jina-embeddings-v2-base-de\"), 768);\n        assert_eq!(get_model_dimension(\"jina-embeddings-v2-base-zh\"), 768);\n        assert_eq!(get_model_dimension(\"jina-embeddings-v2-base-code\"), 768);\n    }\n\n    #[test]\n    fn test_model_dimension_clip() {\n        assert_eq!(get_model_dimension(\"jina-clip-v1\"), 768);\n        assert_eq!(get_model_dimension(\"jina-clip-v2\"), 1024);\n    }\n\n    #[test]\n    fn test_constants() {\n        assert_eq!(DEFAULT_JINA_EMBEDDING_MODEL, \"jina-embeddings-v3\");\n        assert_eq!(DEFAULT_JINA_BASE_URL, \"https://api.jina.ai\");\n    }\n\n    #[test]\n    fn test_from_env_missing_api_key() {\n        // Clear env vars to ensure clean test\n        std::env::remove_var(\"JINA_API_KEY\");\n        std::env::remove_var(\"JINA_BASE_URL\");\n        std::env::remove_var(\"JINA_EMBEDDING_MODEL\");\n\n        let result = JinaProvider::from_env();\n        assert!(result.is_err());\n        let err = result.unwrap_err();\n        assert!(err.to_string().contains(\"JINA_API_KEY\"));\n    }\n\n    #[test]\n    fn test_embedding_provider_max_tokens() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.max_tokens(), 8192);\n    }\n\n    #[test]\n    fn test_embedding_provider_name_is_jina() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .build()\n            .unwrap();\n\n        assert_eq!(EmbeddingProvider::name(&provider), \"jina\");\n    }\n\n    #[test]\n    fn test_builder_all_tasks() {\n        let tasks = vec![\n            \"retrieval.query\",\n            \"retrieval.passage\",\n            \"separation\",\n            \"classification\",\n            \"text-matching\",\n        ];\n\n        for task in tasks {\n            let provider = JinaProviderBuilder::new()\n                .api_key(\"test-key\")\n                .task(task)\n                .build()\n                .unwrap();\n            assert_eq!(provider.task, Some(task.to_string()));\n        }\n    }\n\n    #[test]\n    fn test_builder_chaining() {\n        let provider = JinaProviderBuilder::new()\n            .api_key(\"test-key\")\n            .base_url(\"https://custom.api\")\n            .embedding_model(\"jina-clip-v2\")\n            .embedding_dimension(1024)\n            .task(\"retrieval.query\")\n            .normalized(false)\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.base_url, \"https://custom.api\");\n        assert_eq!(provider.embedding_model, \"jina-clip-v2\");\n        assert_eq!(provider.embedding_dimension, 1024);\n        assert_eq!(provider.task, Some(\"retrieval.query\".to_string()));\n        assert!(!provider.normalized);\n    }\n}\n","traces":[{"line":64,"address":[],"length":0,"stats":{"Line":15}},{"line":67,"address":[],"length":0,"stats":{"Line":45}},{"line":68,"address":[],"length":0,"stats":{"Line":30}},{"line":78,"address":[],"length":0,"stats":{"Line":14}},{"line":79,"address":[],"length":0,"stats":{"Line":14}},{"line":83,"address":[],"length":0,"stats":{"Line":13}},{"line":84,"address":[],"length":0,"stats":{"Line":26}},{"line":85,"address":[],"length":0,"stats":{"Line":13}},{"line":89,"address":[],"length":0,"stats":{"Line":2}},{"line":90,"address":[],"length":0,"stats":{"Line":6}},{"line":91,"address":[],"length":0,"stats":{"Line":2}},{"line":95,"address":[],"length":0,"stats":{"Line":2}},{"line":96,"address":[],"length":0,"stats":{"Line":6}},{"line":97,"address":[],"length":0,"stats":{"Line":2}},{"line":101,"address":[],"length":0,"stats":{"Line":2}},{"line":102,"address":[],"length":0,"stats":{"Line":2}},{"line":103,"address":[],"length":0,"stats":{"Line":2}},{"line":114,"address":[],"length":0,"stats":{"Line":7}},{"line":115,"address":[],"length":0,"stats":{"Line":14}},{"line":116,"address":[],"length":0,"stats":{"Line":7}},{"line":120,"address":[],"length":0,"stats":{"Line":3}},{"line":121,"address":[],"length":0,"stats":{"Line":3}},{"line":122,"address":[],"length":0,"stats":{"Line":3}},{"line":126,"address":[],"length":0,"stats":{"Line":14}},{"line":127,"address":[],"length":0,"stats":{"Line":27}},{"line":128,"address":[],"length":0,"stats":{"Line":14}},{"line":129,"address":[],"length":0,"stats":{"Line":17}},{"line":131,"address":[],"length":0,"stats":{"Line":26}},{"line":132,"address":[],"length":0,"stats":{"Line":26}},{"line":134,"address":[],"length":0,"stats":{"Line":13}},{"line":136,"address":[],"length":0,"stats":{"Line":13}},{"line":137,"address":[],"length":0,"stats":{"Line":26}},{"line":138,"address":[],"length":0,"stats":{"Line":26}},{"line":139,"address":[],"length":0,"stats":{"Line":26}},{"line":140,"address":[],"length":0,"stats":{"Line":26}},{"line":141,"address":[],"length":0,"stats":{"Line":26}},{"line":142,"address":[],"length":0,"stats":{"Line":13}},{"line":143,"address":[],"length":0,"stats":{"Line":13}},{"line":157,"address":[],"length":0,"stats":{"Line":10}},{"line":158,"address":[],"length":0,"stats":{"Line":30}},{"line":159,"address":[],"length":0,"stats":{"Line":10}},{"line":162,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":11}},{"line":187,"address":[],"length":0,"stats":{"Line":11}},{"line":188,"address":[],"length":0,"stats":{"Line":12}},{"line":189,"address":[],"length":0,"stats":{"Line":11}},{"line":190,"address":[],"length":0,"stats":{"Line":10}},{"line":191,"address":[],"length":0,"stats":{"Line":9}},{"line":192,"address":[],"length":0,"stats":{"Line":8}},{"line":193,"address":[],"length":0,"stats":{"Line":7}},{"line":194,"address":[],"length":0,"stats":{"Line":6}},{"line":195,"address":[],"length":0,"stats":{"Line":6}},{"line":196,"address":[],"length":0,"stats":{"Line":3}},{"line":197,"address":[],"length":0,"stats":{"Line":1}},{"line":246,"address":[],"length":0,"stats":{"Line":2}},{"line":247,"address":[],"length":0,"stats":{"Line":2}},{"line":250,"address":[],"length":0,"stats":{"Line":1}},{"line":251,"address":[],"length":0,"stats":{"Line":1}},{"line":254,"address":[],"length":0,"stats":{"Line":1}},{"line":255,"address":[],"length":0,"stats":{"Line":1}},{"line":258,"address":[],"length":0,"stats":{"Line":1}},{"line":260,"address":[],"length":0,"stats":{"Line":1}},{"line":263,"address":[],"length":0,"stats":{"Line":1}},{"line":293,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":319,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}}],"covered":62,"coverable":78},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","lmstudio.rs"],"content":"//! LM Studio provider implementation.\n//!\n//! This module provides integration with LM Studio's local OpenAI-compatible API.\n//! LM Studio runs local models and exposes them via an OpenAI-compatible HTTP API.\n//!\n//! # Default Configuration\n//!\n//! - Base URL: `http://localhost:1234`\n//! - Default model: `gemma2-9b-it` (chat), `nomic-embed-text-v1.5` (embeddings, 768 dimensions)\n//!\n//! # Environment Variables\n//!\n//! - `LMSTUDIO_HOST`: LM Studio server URL (default: http://localhost:1234)\n//! - `LMSTUDIO_MODEL`: Default chat model\n//! - `LMSTUDIO_EMBEDDING_MODEL`: Default embedding model\n//! - `LMSTUDIO_EMBEDDING_DIM`: Embedding dimension (default: 768)\n//!\n//! # Example\n//!\n//! ```rust,ignore\n//! use edgequake_llm::LMStudioProvider;\n//!\n//! // Connect to local LM Studio with defaults\n//! let provider = LMStudioProvider::from_env()?;\n//!\n//! // Or specify custom settings\n//! let provider = LMStudioProvider::builder()\n//!     .host(\"http://localhost:1234\")\n//!     .model(\"mistral-7b-instruct\")\n//!     .embedding_model(\"nomic-embed-text-v1.5\")\n//!     .build()?;\n//! ```\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse futures::StreamExt;\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse std::collections::HashMap;\nuse tracing::debug;\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, LLMProvider,\n    LLMResponse, StreamChunk, ToolChoice, ToolDefinition,\n};\n\n/// Default LM Studio host URL\nconst DEFAULT_LMSTUDIO_HOST: &str = \"http://localhost:1234\";\n\n/// Default LM Studio chat model\nconst DEFAULT_LMSTUDIO_MODEL: &str = \"gemma2-9b-it\";\n\n/// Default LM Studio embedding model\nconst DEFAULT_LMSTUDIO_EMBEDDING_MODEL: &str = \"nomic-embed-text-v1.5\";\n\n/// Default embedding dimension for nomic-embed-text-v1.5\nconst DEFAULT_LMSTUDIO_EMBEDDING_DIM: usize = 768;\n\n/// LM Studio LLM and embedding provider.\n///\n/// Provides integration with locally running LM Studio instance.\n/// Uses OpenAI-compatible API format.\n#[derive(Debug, Clone)]\npub struct LMStudioProvider {\n    client: Client,\n    host: String,\n    model: String,\n    embedding_model: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n    auto_load_models: bool,\n}\n\n/// Builder for LMStudioProvider\n#[derive(Debug, Clone)]\npub struct LMStudioProviderBuilder {\n    host: String,\n    model: String,\n    embedding_model: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n    auto_load_models: bool,\n}\n\nimpl Default for LMStudioProviderBuilder {\n    fn default() -> Self {\n        Self {\n            host: DEFAULT_LMSTUDIO_HOST.to_string(),\n            model: DEFAULT_LMSTUDIO_MODEL.to_string(),\n            embedding_model: DEFAULT_LMSTUDIO_EMBEDDING_MODEL.to_string(),\n            max_context_length: 131072, // OODA-99: Increased to 128K (131072)\n            embedding_dimension: DEFAULT_LMSTUDIO_EMBEDDING_DIM,\n            auto_load_models: true,\n        }\n    }\n}\n\nimpl LMStudioProviderBuilder {\n    /// Create a new builder with default settings\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Set the LM Studio host URL\n    pub fn host(mut self, host: impl Into<String>) -> Self {\n        self.host = host.into();\n        self\n    }\n\n    /// Set the chat model\n    pub fn model(mut self, model: impl Into<String>) -> Self {\n        self.model = model.into();\n        self\n    }\n\n    /// Set the embedding model\n    pub fn embedding_model(mut self, model: impl Into<String>) -> Self {\n        self.embedding_model = model.into();\n        self\n    }\n\n    /// Set the maximum context length\n    pub fn max_context_length(mut self, length: usize) -> Self {\n        self.max_context_length = length;\n        self\n    }\n\n    /// Set the embedding dimension\n    pub fn embedding_dimension(mut self, dimension: usize) -> Self {\n        self.embedding_dimension = dimension;\n        self\n    }\n\n    /// Enable or disable automatic model loading (default: true)\n    pub fn auto_load_models(mut self, enabled: bool) -> Self {\n        self.auto_load_models = enabled;\n        self\n    }\n\n    /// Build the LMStudioProvider\n    pub fn build(self) -> Result<LMStudioProvider> {\n        let client = Client::builder()\n            .timeout(std::time::Duration::from_secs(300)) // Longer timeout for local models\n            .no_proxy() // CRITICAL: Disable all proxies for localhost connections\n            .build()\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        Ok(LMStudioProvider {\n            client,\n            host: self.host,\n            model: self.model,\n            embedding_model: self.embedding_model,\n            max_context_length: self.max_context_length,\n            embedding_dimension: self.embedding_dimension,\n            auto_load_models: self.auto_load_models,\n        })\n    }\n}\n\nimpl LMStudioProvider {\n    /// Create a new LMStudioProvider from environment variables.\n    ///\n    /// Environment variables:\n    /// - `LMSTUDIO_HOST`: Server URL (default: http://localhost:1234)\n    /// - `LMSTUDIO_MODEL`: Chat model (default: gemma2-9b-it)\n    /// - `LMSTUDIO_EMBEDDING_MODEL`: Embedding model (default: nomic-embed-text-v1.5)\n    /// - `LMSTUDIO_EMBEDDING_DIM`: Embedding dimension (default: 768)\n    /// - `LMSTUDIO_CONTEXT_LENGTH`: Max context length (default: 32768)\n    ///\n    /// # OODA-99: Context Length Configuration\n    ///\n    /// The context length determines how much text can be sent to LMStudio.\n    /// If you get \"400 Bad Request\" errors, increase this value to match\n    /// your LMStudio model's configured context window.\n    ///\n    /// Example: `export LMSTUDIO_CONTEXT_LENGTH=65536`\n    pub fn from_env() -> Result<Self> {\n        let host =\n            std::env::var(\"LMSTUDIO_HOST\").unwrap_or_else(|_| DEFAULT_LMSTUDIO_HOST.to_string());\n\n        let model =\n            std::env::var(\"LMSTUDIO_MODEL\").unwrap_or_else(|_| DEFAULT_LMSTUDIO_MODEL.to_string());\n\n        let embedding_model = std::env::var(\"LMSTUDIO_EMBEDDING_MODEL\")\n            .unwrap_or_else(|_| DEFAULT_LMSTUDIO_EMBEDDING_MODEL.to_string());\n\n        let embedding_dimension = std::env::var(\"LMSTUDIO_EMBEDDING_DIM\")\n            .ok()\n            .and_then(|s| s.parse().ok())\n            .unwrap_or(DEFAULT_LMSTUDIO_EMBEDDING_DIM);\n\n        // OODA-99: Allow context length override\n        // Default to 128K (131072) for modern models with large contexts\n        // Falls back to 8192 only if parsing fails\n        let max_context_length = std::env::var(\"LMSTUDIO_CONTEXT_LENGTH\")\n            .ok()\n            .and_then(|s| s.parse().ok())\n            .unwrap_or(131072);\n\n        LMStudioProviderBuilder::new()\n            .host(host)\n            .model(model)\n            .embedding_model(embedding_model)\n            .embedding_dimension(embedding_dimension)\n            .max_context_length(max_context_length)\n            .build()\n    }\n\n    /// Create a new builder for LMStudioProvider\n    pub fn builder() -> LMStudioProviderBuilder {\n        LMStudioProviderBuilder::new()\n    }\n\n    /// Create with default settings (localhost:1234)\n    pub fn default_local() -> Result<Self> {\n        LMStudioProviderBuilder::new().build()\n    }\n\n    /// Get the API base URL with /v1 suffix\n    fn api_base(&self) -> String {\n        if self.host.ends_with(\"/v1\") {\n            self.host.clone()\n        } else {\n            format!(\"{}/v1\", self.host)\n        }\n    }\n\n    /// Get the REST API base URL for native LMStudio features\n    /// OODA-30: Used for reasoning support via /api/v1/chat\n    fn rest_api_base(&self) -> String {\n        let base = self.host.trim_end_matches(\"/v1\");\n        format!(\"{}/api/v1\", base)\n    }\n\n    /// Check if LM Studio is running and accessible\n    pub async fn health_check(&self) -> Result<()> {\n        let url = format!(\"{}/models\", self.api_base());\n        \n        let response = self\n            .client\n            .get(&url)\n            .timeout(std::time::Duration::from_secs(5)) // Quick check\n            .send()\n            .await\n            .map_err(|e| {\n                if e.is_timeout() {\n                    LlmError::NetworkError(format!(\n                        \"LM Studio not responding at {}. Is LM Studio running?\",\n                        self.host\n                    ))\n                } else if e.is_connect() {\n                    LlmError::NetworkError(format!(\n                        \"Cannot connect to LM Studio at {}. Please start LM Studio and load a model.\",\n                        self.host\n                    ))\n                } else {\n                    LlmError::NetworkError(format!(\"LM Studio health check failed: {}\", e))\n                }\n            })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio returned error status {}: {}. Please check that a model is loaded.\",\n                status, error_text\n            )));\n        }\n\n        // Verify models are available\n        let models_text = response.text().await\n            .map_err(|e| LlmError::NetworkError(format!(\"Failed to read models response: {}\", e)))?;\n        \n        debug!(\"LM Studio models response: {}\", models_text);\n        \n        // Basic check that response contains \"data\" field\n        if !models_text.contains(\"\\\"data\\\"\") && !models_text.contains(\"\\\"object\\\":\") {\n            return Err(LlmError::ApiError(\n                \"LM Studio /v1/models returned unexpected format. Please ensure LM Studio is properly initialized.\".to_string()\n            ));\n        }\n\n        Ok(())\n    }\n\n    /// Check if an error indicates the model is not loaded\n    fn is_model_not_loaded_error(error_text: &str) -> bool {\n        error_text.contains(\"not a valid model ID\")\n            || error_text.contains(\"model not found\")\n            || error_text.contains(\"model not loaded\")\n            || error_text.contains(\"No model loaded\")\n    }\n\n    /// Attempt to load a model using the lms CLI\n    async fn load_model_via_cli(&self, model_id: &str) -> Result<()> {\n        eprintln!(\"â³ Model '{}' not loaded. Loading automatically via lms CLI...\", model_id);\n        eprintln!(\"   This may take 15-60 seconds depending on model size.\");\n        \n        let start = std::time::Instant::now();\n        \n        // Try to load the model using lms CLI\n        let output = tokio::process::Command::new(\"lms\")\n            .args([\"load\", model_id, \"--gpu\", \"max\", \"-y\"])\n            .output()\n            .await\n            .map_err(|e| {\n                LlmError::ApiError(format!(\n                    \"Failed to run 'lms load' command: {}\\n\\n\\\n                    Troubleshooting:\\n\\\n                    1. Ensure LM Studio is installed\\n\\\n                    2. Make sure 'lms' CLI is in your PATH\\n\\\n                    3. Run 'lms --help' to verify installation\\n\\\n                    4. Alternatively, manually load the model in LM Studio GUI\",\n                    e\n                ))\n            })?;\n        \n        if !output.status.success() {\n            let stderr = String::from_utf8_lossy(&output.stderr);\n            let stdout = String::from_utf8_lossy(&output.stdout);\n            \n            return Err(LlmError::ApiError(format!(\n                \"Failed to load model '{}' via lms CLI:\\n{}\\n{}\\n\\n\\\n                Please manually load the model in LM Studio GUI or check:\\n\\\n                1. Model is downloaded locally (run 'lms ls' to check)\\n\\\n                2. Enough RAM/VRAM available\\n\\\n                3. LM Studio is running\",\n                model_id, stdout, stderr\n            )));\n        }\n        \n        let elapsed = start.elapsed();\n        eprintln!(\"âœ… Model '{}' loaded successfully in {:.1}s\", model_id, elapsed.as_secs_f64());\n        \n        Ok(())\n    }\n\n    /// Execute a chat request with automatic model loading on failure\n    async fn chat_with_auto_load(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        // Try the request first\n        match self.chat_internal(messages, options).await {\n            Ok(response) => Ok(response),\n            Err(e) => {\n                // Check if error is due to model not loaded and auto-load is enabled\n                if self.auto_load_models && Self::is_model_not_loaded_error(&e.to_string()) {\n                    debug!(\n                        provider = \"lmstudio\",\n                        model = %self.model,\n                        \"Model not loaded, attempting automatic load\"\n                    );\n                    \n                    // Try to load the model\n                    self.load_model_via_cli(&self.model).await?;\n                    \n                    // Retry the request\n                    debug!(\n                        provider = \"lmstudio\",\n                        model = %self.model,\n                        \"Retrying request after model load\"\n                    );\n                    self.chat_internal(messages, options).await\n                } else {\n                    // Not a model-not-loaded error, or auto-load disabled\n                    Err(e)\n                }\n            }\n        }\n    }\n\n    /// Execute a chat with tools request with automatic model loading on failure\n    async fn chat_with_tools_auto_load(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        // Try the request first\n        match self.chat_with_tools_internal(messages, tools, tool_choice.clone(), options).await {\n            Ok(response) => Ok(response),\n            Err(e) => {\n                // Check if error is due to model not loaded and auto-load is enabled\n                if self.auto_load_models && Self::is_model_not_loaded_error(&e.to_string()) {\n                    debug!(\n                        provider = \"lmstudio\",\n                        model = %self.model,\n                        \"Model not loaded for tools request, attempting automatic load\"\n                    );\n                    \n                    // Try to load the model\n                    self.load_model_via_cli(&self.model).await?;\n                    \n                    // Retry the request\n                    debug!(\n                        provider = \"lmstudio\",\n                        model = %self.model,\n                        \"Retrying tools request after model load\"\n                    );\n                    self.chat_with_tools_internal(messages, tools, tool_choice, options).await\n                } else {\n                    // Not a model-not-loaded error, or auto-load disabled\n                    Err(e)\n                }\n            }\n        }\n    }\n\n    /// Internal chat implementation (without auto-load retry logic)\n    async fn chat_internal(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        // OODA-30: Route reasoning models to REST API for thinking support\n        if is_reasoning_model(&self.model) {\n            debug!(\n                provider = \"lmstudio\",\n                model = %self.model,\n                \"Using REST API for reasoning model\"\n            );\n            return self.chat_with_reasoning(messages, options).await;\n        }\n\n        let api_messages: Vec<ChatMessageRequest> = messages\n            .iter()\n            .map(|m| ChatMessageRequest {\n                role: match m.role {\n                    ChatRole::System => \"system\".to_string(),\n                    ChatRole::User => \"user\".to_string(),\n                    ChatRole::Assistant => \"assistant\".to_string(),\n                    ChatRole::Tool => \"tool\".to_string(),\n                    ChatRole::Function => \"function\".to_string(),\n                },\n                content: m.content.clone(),\n            })\n            .collect();\n\n        let opts = options.cloned().unwrap_or_default();\n        let request = ChatCompletionRequest {\n            model: self.model.clone(),\n            messages: api_messages,\n            temperature: opts.temperature,\n            max_tokens: opts.max_tokens.map(|t| t as i32),\n            stream: false,\n            tools: None,\n            tool_choice: None,\n        };\n\n        let url = format!(\"{}/chat/completions\", self.api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.model,\n            url = %url,\n            message_count = messages.len(),\n            \"Sending chat completion request\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"LM Studio request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            // Try to parse as API error\n            if let Ok(api_error) = serde_json::from_str::<ApiError>(&error_text) {\n                return Err(LlmError::ApiError(format!(\n                    \"LM Studio API error ({}): {}\",\n                    status, api_error.error.message\n                )));\n            }\n\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let completion: ChatCompletionResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"Failed to parse response: {}\", e)))?;\n\n        let content = completion\n            .choices\n            .first()\n            .map(|c| c.message.content.clone())\n            .unwrap_or_default();\n\n        let (prompt_tokens, completion_tokens) = completion\n            .usage\n            .map(|u| (u.prompt_tokens, u.completion_tokens))\n            .unwrap_or((0, 0));\n\n        debug!(\n            provider = \"lmstudio\",\n            prompt_tokens = prompt_tokens,\n            completion_tokens = completion_tokens,\n            content_length = content.len(),\n            \"Received chat completion response\"\n        );\n\n        Ok(LLMResponse {\n            content,\n            prompt_tokens,\n            completion_tokens,\n            model: self.model.clone(),\n            total_tokens: prompt_tokens + completion_tokens,\n            finish_reason: completion\n                .choices\n                .first()\n                .and_then(|c| c.finish_reason.clone()),\n            tool_calls: Vec::new(),\n            metadata: HashMap::new(),\n            cache_hit_tokens: None,\n            // OODA-15: LMStudio doesn't have thinking API\n            thinking_tokens: None,\n            thinking_content: None,\n        })\n    }\n\n    /// Internal chat with tools implementation (without auto-load retry logic)\n    async fn chat_with_tools_internal(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let api_messages: Vec<ChatMessageRequest> = messages\n            .iter()\n            .map(|m| ChatMessageRequest {\n                role: match m.role {\n                    ChatRole::System => \"system\".to_string(),\n                    ChatRole::User => \"user\".to_string(),\n                    ChatRole::Assistant => \"assistant\".to_string(),\n                    ChatRole::Tool => \"tool\".to_string(),\n                    ChatRole::Function => \"function\".to_string(),\n                },\n                content: m.content.clone(),\n            })\n            .collect();\n\n        // Convert tools to OpenAI-compatible format\n        let api_tools: Vec<ToolDefinitionRequest> = tools\n            .iter()\n            .map(|tool| ToolDefinitionRequest {\n                type_: \"function\".to_string(),\n                function: FunctionDefinitionRequest {\n                    name: tool.function.name.clone(),\n                    description: tool.function.description.clone(),\n                    parameters: tool.function.parameters.clone(),\n                },\n            })\n            .collect();\n\n        // Convert tool_choice to API format\n        // LMStudio only supports: none, auto, required (not specific functions)\n        let api_tool_choice = tool_choice.map(|tc| match tc {\n            crate::traits::ToolChoice::Auto(_) => \"auto\".to_string(),\n            crate::traits::ToolChoice::Required(_) => \"required\".to_string(),\n            crate::traits::ToolChoice::Function { .. } => {\n                // LMStudio doesn't support specific function selection\n                // Fall back to required mode to ensure a tool is called\n                \"required\".to_string()\n            }\n        });\n\n        let opts = options.cloned().unwrap_or_default();\n        let request = ChatCompletionRequest {\n            model: self.model.clone(),\n            messages: api_messages,\n            temperature: opts.temperature,\n            max_tokens: opts.max_tokens.map(|t| t as i32),\n            stream: false,\n            tools: Some(api_tools),\n            tool_choice: api_tool_choice,\n        };\n\n        let url = format!(\"{}/chat/completions\", self.api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.model,\n            url = %url,\n            message_count = messages.len(),\n            tool_count = tools.len(),\n            \"Sending chat completion request with tools\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| {\n                if e.is_timeout() {\n                    LlmError::NetworkError(format!(\n                        \"LM Studio request timed out at {}. The model may be taking too long to respond.\",\n                        self.host\n                    ))\n                } else if e.is_connect() {\n                    LlmError::NetworkError(format!(\n                        \"Lost connection to LM Studio at {}. Was LM Studio closed?\",\n                        self.host\n                    ))\n                } else {\n                    LlmError::NetworkError(format!(\n                        \"LM Studio request failed: {}\\n\\nTroubleshooting:\\n\\\n                         1. Check LM Studio is still running\\n\\\n                         2. Verify model '{}' is loaded\\n\\\n                         3. Check LM Studio console for errors\",\n                        e, self.model\n                    ))\n                }\n            })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            // Try to parse as API error\n            if let Ok(api_error) = serde_json::from_str::<ApiError>(&error_text) {\n                return Err(LlmError::ApiError(format!(\n                    \"LM Studio API error ({}): {}\",\n                    status, api_error.error.message\n                )));\n            }\n\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let completion: ChatCompletionResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"Failed to parse response: {}\", e)))?;\n\n        let choice = completion\n            .choices\n            .first()\n            .ok_or_else(|| LlmError::ApiError(\"No choices in response\".to_string()))?;\n\n        let content = choice.message.content.clone();\n        let api_tool_calls = &choice.message.tool_calls;\n\n        // Convert tool calls to our format\n        let tool_calls: Vec<crate::traits::ToolCall> = api_tool_calls\n            .iter()\n            .map(|tc| crate::traits::ToolCall {\n                id: tc.id.clone(),\n                call_type: \"function\".to_string(),\n                function: crate::traits::FunctionCall {\n                    name: tc.function.name.clone(),\n                    arguments: tc.function.arguments.clone(),\n                },\n            })\n            .collect();\n\n        let (prompt_tokens, completion_tokens) = completion\n            .usage\n            .map(|u| (u.prompt_tokens, u.completion_tokens))\n            .unwrap_or((0, 0));\n\n        debug!(\n            provider = \"lmstudio\",\n            prompt_tokens = prompt_tokens,\n            completion_tokens = completion_tokens,\n            content_length = content.len(),\n            tool_call_count = tool_calls.len(),\n            \"Received chat completion response with tools\"\n        );\n\n        Ok(LLMResponse {\n            content,\n            prompt_tokens,\n            completion_tokens,\n            model: self.model.clone(),\n            total_tokens: prompt_tokens + completion_tokens,\n            finish_reason: choice.finish_reason.clone(),\n            tool_calls,\n            metadata: HashMap::new(),\n            cache_hit_tokens: None,\n            thinking_tokens: None,\n            thinking_content: None,\n        })\n    }\n}\n\n// =========================================================================\n// OODA-30: Helper function to detect reasoning models\n// =========================================================================\n\n/// Check if a model supports reasoning (thinking) capabilities\nfn is_reasoning_model(model: &str) -> bool {\n    let model_lower = model.to_lowercase();\n    model_lower.contains(\"deepseek-r1\")\n        || model_lower.contains(\"qwen3\")\n        || model_lower.contains(\"qwq\")\n        || model_lower.contains(\"phi4-reasoning\")\n        || model_lower.contains(\"granite-4\")\n        || model_lower.contains(\"reasoning\")\n        || model_lower.contains(\"thinking\")\n}\n\n// =========================================================================\n// OODA-30: Native REST API structures for reasoning support\n// =========================================================================\n\n/// REST API request for /api/v1/chat\n#[derive(Debug, Serialize)]\nstruct RestChatRequest {\n    model: String,\n    input: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    reasoning: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stream: Option<bool>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    max_output_tokens: Option<i32>,\n}\n\n/// REST API response from /api/v1/chat\n#[derive(Debug, Deserialize)]\nstruct RestChatResponse {\n    #[allow(dead_code)]\n    model_instance_id: String,\n    output: Vec<RestOutputItem>,\n    stats: RestStats,\n}\n\n/// Output item in REST API response\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\")]\nenum RestOutputItem {\n    #[serde(rename = \"reasoning\")]\n    Reasoning { content: String },\n    #[serde(rename = \"message\")]\n    Message { content: String },\n}\n\n/// Stats from REST API response\n#[derive(Debug, Deserialize)]\nstruct RestStats {\n    input_tokens: usize,\n    total_output_tokens: usize,\n    #[serde(default)]\n    reasoning_output_tokens: usize,\n    #[allow(dead_code)]\n    tokens_per_second: f64,\n    #[allow(dead_code)]\n    time_to_first_token_seconds: f64,\n}\n\n/// Streaming event from REST API\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\")]\nenum RestStreamEvent {\n    #[serde(rename = \"chat.start\")]\n    ChatStart {\n        #[allow(dead_code)]\n        model_instance_id: Option<String>,\n    },\n    #[serde(rename = \"reasoning.start\")]\n    ReasoningStart,\n    #[serde(rename = \"reasoning.delta\")]\n    ReasoningDelta { content: String },\n    #[serde(rename = \"reasoning.end\")]\n    ReasoningEnd,\n    #[serde(rename = \"message.start\")]\n    MessageStart,\n    #[serde(rename = \"message.delta\")]\n    MessageDelta { content: String },\n    #[serde(rename = \"message.end\")]\n    MessageEnd,\n    #[serde(rename = \"chat.end\")]\n    ChatEnd { result: RestChatResponse },\n    #[serde(rename = \"prompt_processing.start\")]\n    PromptProcessingStart,\n    #[serde(rename = \"prompt_processing.progress\")]\n    PromptProcessingProgress {\n        #[allow(dead_code)]\n        progress: f64,\n    },\n    #[serde(rename = \"prompt_processing.end\")]\n    PromptProcessingEnd,\n    #[serde(rename = \"model_load.start\")]\n    ModelLoadStart {\n        #[allow(dead_code)]\n        model_instance_id: Option<String>,\n    },\n    #[serde(rename = \"model_load.progress\")]\n    ModelLoadProgress {\n        #[allow(dead_code)]\n        progress: f64,\n    },\n    #[serde(rename = \"model_load.end\")]\n    ModelLoadEnd {\n        #[allow(dead_code)]\n        load_time_seconds: Option<f64>,\n    },\n}\n\n// =========================================================================\n// OpenAI-compatible API request/response structures\n// =========================================================================\n\n#[derive(Debug, Serialize)]\nstruct ChatCompletionRequest {\n    model: String,\n    messages: Vec<ChatMessageRequest>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    max_tokens: Option<i32>,\n    stream: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<ToolDefinitionRequest>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_choice: Option<String>,\n}\n\n#[derive(Debug, Serialize)]\nstruct ToolDefinitionRequest {\n    #[serde(rename = \"type\")]\n    type_: String,\n    function: FunctionDefinitionRequest,\n}\n\n#[derive(Debug, Serialize)]\nstruct FunctionDefinitionRequest {\n    name: String,\n    description: String,\n    parameters: serde_json::Value,\n}\n\n#[derive(Debug, Serialize)]\nstruct ChatMessageRequest {\n    role: String,\n    content: String,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ChatCompletionResponse {\n    choices: Vec<ChatChoice>,\n    usage: Option<UsageInfo>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ChatChoice {\n    message: ChatMessageResponse,\n    finish_reason: Option<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ChatMessageResponse {\n    role: String,\n    content: String,\n    #[serde(default)]\n    tool_calls: Vec<ToolCallResponse>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ToolCallResponse {\n    id: String,\n    #[serde(rename = \"type\")]\n    #[allow(dead_code)]\n    type_: String,\n    function: FunctionCallResponse,\n}\n\n#[derive(Debug, Deserialize)]\nstruct FunctionCallResponse {\n    name: String,\n    arguments: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct UsageInfo {\n    prompt_tokens: usize,\n    completion_tokens: usize,\n    total_tokens: usize,\n}\n\n#[derive(Debug, Serialize)]\nstruct EmbeddingRequest {\n    model: String,\n    input: Vec<String>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct EmbeddingResponse {\n    data: Vec<EmbeddingData>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct EmbeddingData {\n    embedding: Vec<f32>,\n}\n\n// =========================================================================\n// OODA-10: Streaming response types for LM Studio\n// =========================================================================\n//\n// LM Studio uses SSE (Server-Sent Events) with OpenAI-compatible format.\n// Each chunk starts with \"data: \" prefix and contains delta content.\n// =========================================================================\n\n/// Streaming response chunk from LM Studio\n#[derive(Debug, Deserialize)]\nstruct StreamingChunk {\n    #[allow(dead_code)]\n    id: Option<String>,\n    choices: Vec<StreamingChoice>,\n}\n\n/// Individual choice in a streaming chunk\n#[derive(Debug, Deserialize)]\nstruct StreamingChoice {\n    delta: StreamingDelta,\n    #[allow(dead_code)]\n    index: usize,\n    finish_reason: Option<String>,\n}\n\n/// Delta content in a streaming chunk\n#[derive(Debug, Deserialize)]\nstruct StreamingDelta {\n    #[serde(default)]\n    content: Option<String>,\n    #[serde(default)]\n    tool_calls: Option<Vec<StreamingToolCall>>,\n}\n\n/// Tool call in streaming format\n#[derive(Debug, Deserialize)]\nstruct StreamingToolCall {\n    index: usize,\n    #[serde(default)]\n    id: Option<String>,\n    #[serde(default)]\n    function: Option<StreamingFunction>,\n}\n\n/// Function details in streaming format\n#[derive(Debug, Deserialize)]\nstruct StreamingFunction {\n    #[serde(default)]\n    name: Option<String>,\n    #[serde(default)]\n    arguments: Option<String>,\n}\n\n// API error handling\n\n#[derive(Debug, Deserialize)]\nstruct ApiError {\n    error: ApiErrorDetail,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ApiErrorDetail {\n    message: String,\n    #[serde(rename = \"type\")]\n    error_type: Option<String>,\n}\n\n/// OODA-39: Response structure for LM Studio /v1/models endpoint\n#[derive(Debug, Deserialize)]\npub struct LMStudioModelsResponse {\n    pub data: Vec<LMStudioModel>,\n}\n\n/// OODA-39: Individual model info from LM Studio\n#[derive(Debug, Deserialize)]\npub struct LMStudioModel {\n    pub id: String,\n    #[serde(default)]\n    pub object: String,\n    #[serde(default)]\n    pub created: u64,\n    #[serde(default)]\n    pub owned_by: String,\n}\n\nimpl LMStudioProvider {\n    /// List locally available LM Studio models.\n    ///\n    /// # OODA-39: Dynamic Model Discovery\n    ///\n    /// Fetches the list of models currently loaded in LM Studio\n    /// via the OpenAI-compatible GET /v1/models endpoint. This enables\n    /// dynamic model selection instead of relying on a static registry.\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let provider = LMStudioProvider::default_local()?;\n    /// let models = provider.list_models().await?;\n    /// for model in models.data {\n    ///     println!(\"Available: {}\", model.id);\n    /// }\n    /// ```\n    pub async fn list_models(&self) -> Result<LMStudioModelsResponse> {\n        let url = format!(\"{}/models\", self.api_base());\n\n        debug!(url = %url, \"Fetching LM Studio models list\");\n\n        let response = self\n            .client\n            .get(&url)\n            .send()\n            .await\n            .map_err(|e| {\n                LlmError::NetworkError(format!(\n                    \"Failed to connect to LM Studio at {}: {}. Is LM Studio running?\",\n                    self.host, e\n                ))\n            })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio /v1/models returned {}: {}\",\n                status, body\n            )));\n        }\n\n        response\n            .json::<LMStudioModelsResponse>()\n            .await\n            .map_err(|e| LlmError::ProviderError(format!(\"Failed to parse models response: {}\", e)))\n    }\n\n    /// OODA-30: Chat using native REST API with reasoning support\n    ///\n    /// Uses the LMStudio REST API endpoint /api/v1/chat which provides\n    /// reasoning output for thinking models like DeepSeek-R1, Qwen3, etc.\n    async fn chat_with_reasoning(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        // Build input from messages (REST API uses a simpler input format)\n        let input = messages\n            .iter()\n            .map(|m| {\n                let role = match m.role {\n                    ChatRole::System => \"system\",\n                    ChatRole::User => \"user\",\n                    ChatRole::Assistant => \"assistant\",\n                    ChatRole::Tool => \"tool\",\n                    ChatRole::Function => \"function\",\n                };\n                format!(\"[{}]: {}\", role, m.content)\n            })\n            .collect::<Vec<_>>()\n            .join(\"\\n\\n\");\n\n        let opts = options.cloned().unwrap_or_default();\n        let request = RestChatRequest {\n            model: self.model.clone(),\n            input,\n            reasoning: Some(\"on\".to_string()),\n            stream: Some(false),\n            temperature: opts.temperature,\n            max_output_tokens: opts.max_tokens.map(|t| t as i32),\n        };\n\n        let url = format!(\"{}/chat\", self.rest_api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.model,\n            url = %url,\n            message_count = messages.len(),\n            \"Sending REST API chat request with reasoning\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"LM Studio REST API request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            // Try to parse as API error\n            if let Ok(api_error) = serde_json::from_str::<ApiError>(&error_text) {\n                return Err(LlmError::ApiError(format!(\n                    \"LM Studio REST API error ({}): {}\",\n                    status, api_error.error.message\n                )));\n            }\n\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio REST API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let rest_response: RestChatResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"Failed to parse REST API response: {}\", e)))?;\n\n        // Extract content and reasoning from output\n        let mut content = String::new();\n        let mut reasoning_content = String::new();\n\n        for item in &rest_response.output {\n            match item {\n                RestOutputItem::Reasoning { content: text } => {\n                    reasoning_content.push_str(text);\n                }\n                RestOutputItem::Message { content: text } => {\n                    content.push_str(text);\n                }\n            }\n        }\n\n        debug!(\n            provider = \"lmstudio\",\n            prompt_tokens = rest_response.stats.input_tokens,\n            completion_tokens = rest_response.stats.total_output_tokens,\n            reasoning_tokens = rest_response.stats.reasoning_output_tokens,\n            content_length = content.len(),\n            reasoning_length = reasoning_content.len(),\n            \"Received REST API response with reasoning\"\n        );\n\n        let mut response = LLMResponse {\n            content,\n            prompt_tokens: rest_response.stats.input_tokens,\n            completion_tokens: rest_response.stats.total_output_tokens,\n            model: self.model.clone(),\n            total_tokens: rest_response.stats.input_tokens + rest_response.stats.total_output_tokens,\n            finish_reason: Some(\"stop\".to_string()),\n            tool_calls: Vec::new(),\n            metadata: HashMap::new(),\n            cache_hit_tokens: None,\n            thinking_tokens: None,\n            thinking_content: None,\n        };\n\n        // Set thinking/reasoning content if present\n        if !reasoning_content.is_empty() {\n            response = response\n                .with_thinking_content(reasoning_content)\n                .with_thinking_tokens(rest_response.stats.reasoning_output_tokens);\n        }\n\n        Ok(response)\n    }\n\n    /// OODA-32: Stream chat using native REST API with reasoning support\n    ///\n    /// Uses LMStudio's SSE streaming with reasoning events:\n    /// - `reasoning.delta` â†’ `StreamChunk::ThinkingContent`\n    /// - `message.delta` â†’ `StreamChunk::Content`\n    /// - `chat.end` â†’ `StreamChunk::Finished`\n    async fn chat_with_reasoning_stream(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        // Build input from messages\n        let input = messages\n            .iter()\n            .map(|m| {\n                let role = match m.role {\n                    ChatRole::System => \"system\",\n                    ChatRole::User => \"user\",\n                    ChatRole::Assistant => \"assistant\",\n                    ChatRole::Tool => \"tool\",\n                    ChatRole::Function => \"function\",\n                };\n                format!(\"[{}]: {}\", role, m.content)\n            })\n            .collect::<Vec<_>>()\n            .join(\"\\n\\n\");\n\n        let opts = options.cloned().unwrap_or_default();\n        let request = RestChatRequest {\n            model: self.model.clone(),\n            input,\n            reasoning: Some(\"on\".to_string()),\n            stream: Some(true),\n            temperature: opts.temperature,\n            max_output_tokens: opts.max_tokens.map(|t| t as i32),\n        };\n\n        let url = format!(\"{}/chat\", self.rest_api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.model,\n            url = %url,\n            message_count = messages.len(),\n            \"Starting REST API streaming with reasoning\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"LM Studio REST API request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio REST API streaming error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let stream = response.bytes_stream();\n\n        let mapped_stream = stream.map(|chunk_result| {\n            match chunk_result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    \n                    // Parse SSE lines (event: X\\ndata: {JSON})\n                    for line in text.lines() {\n                        let line = line.trim();\n                        if line.is_empty() || line.starts_with(\"event:\") {\n                            continue;\n                        }\n                        \n                        if let Some(json_str) = line.strip_prefix(\"data: \") {\n                            // Try to parse as RestStreamEvent\n                            if let Ok(event) = serde_json::from_str::<RestStreamEvent>(json_str) {\n                                match event {\n                                    RestStreamEvent::ReasoningDelta { content } => {\n                                        // Estimate tokens (~4 chars per token)\n                                        let tokens_used = content.len() / 4;\n                                        return Ok(StreamChunk::ThinkingContent {\n                                            text: content,\n                                            tokens_used: Some(tokens_used),\n                                            budget_total: None,\n                                        });\n                                    }\n                                    RestStreamEvent::MessageDelta { content } => {\n                                        if !content.is_empty() {\n                                            return Ok(StreamChunk::Content(content));\n                                        }\n                                    }\n                                    RestStreamEvent::ChatEnd { result } => {\n                                        // OODA-39: Extract native TTFT from REST API stats\n                                        let ttft_ms = Some(result.stats.time_to_first_token_seconds * 1000.0);\n                                        return Ok(StreamChunk::Finished {\n                                            reason: \"stop\".to_string(),\n                                            ttft_ms,\n                                        });\n                                    }\n                                    // Ignore other events (start, end, progress)\n                                    _ => {}\n                                }\n                            }\n                        }\n                    }\n                    \n                    // Default empty content\n                    Ok(StreamChunk::Content(String::new()))\n                }\n                Err(e) => Err(LlmError::NetworkError(e.to_string())),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for LMStudioProvider {\n    fn name(&self) -> &str {\n        \"lmstudio\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        let messages = vec![ChatMessage::user(prompt)];\n        self.chat(&messages, None).await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let messages = vec![ChatMessage::user(prompt)];\n        self.chat(&messages, Some(options)).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.chat_with_auto_load(messages, options).await\n    }\n\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.chat_with_tools_auto_load(messages, tools, tool_choice, options).await\n    }\n    \n    // =========================================================================\n    // OODA-10: Streaming methods for LM Studio\n    // =========================================================================\n    \n    fn supports_streaming(&self) -> bool {\n        true\n    }\n    \n    fn supports_function_calling(&self) -> bool {\n        true\n    }\n    \n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n    \n    async fn stream(\n        &self,\n        prompt: &str,\n    ) -> Result<BoxStream<'static, Result<String>>> {\n        let api_messages = vec![ChatMessageRequest {\n            role: \"user\".to_string(),\n            content: prompt.to_string(),\n        }];\n\n        let request = ChatCompletionRequest {\n            model: self.model.clone(),\n            messages: api_messages,\n            temperature: None,\n            max_tokens: None,\n            stream: true,\n            tools: None,\n            tool_choice: None,\n        };\n\n        let url = format!(\"{}/chat/completions\", self.api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.model,\n            url = %url,\n            \"Starting streaming request\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"LM Studio request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio streaming error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let stream = response.bytes_stream();\n\n        let mapped_stream = stream.map(|chunk_result| {\n            match chunk_result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    let mut content = String::new();\n                    \n                    // Parse SSE lines (data: prefix)\n                    for line in text.lines() {\n                        let line = line.trim();\n                        if line.is_empty() || line == \"data: [DONE]\" {\n                            continue;\n                        }\n                        if let Some(json_str) = line.strip_prefix(\"data: \") {\n                            if let Ok(chunk) = serde_json::from_str::<StreamingChunk>(json_str) {\n                                if let Some(choice) = chunk.choices.first() {\n                                    if let Some(c) = &choice.delta.content {\n                                        content.push_str(c);\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    Ok(content)\n                }\n                Err(e) => Err(LlmError::NetworkError(e.to_string())),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n    \n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        // OODA-32: Route reasoning models to REST API streaming\n        // Note: REST API doesn't support tool calling during streaming,\n        // so we only use it for reasoning without tools\n        if is_reasoning_model(&self.model) && tools.is_empty() {\n            debug!(\n                provider = \"lmstudio\",\n                model = %self.model,\n                \"Using REST API streaming for reasoning model\"\n            );\n            return self.chat_with_reasoning_stream(messages, options).await;\n        }\n\n        let api_messages: Vec<ChatMessageRequest> = messages\n            .iter()\n            .map(|m| ChatMessageRequest {\n                role: match m.role {\n                    ChatRole::System => \"system\".to_string(),\n                    ChatRole::User => \"user\".to_string(),\n                    ChatRole::Assistant => \"assistant\".to_string(),\n                    ChatRole::Tool => \"tool\".to_string(),\n                    ChatRole::Function => \"function\".to_string(),\n                },\n                content: m.content.clone(),\n            })\n            .collect();\n\n        // Convert tools to OpenAI-compatible format\n        let api_tools: Vec<ToolDefinitionRequest> = tools\n            .iter()\n            .map(|tool| ToolDefinitionRequest {\n                type_: \"function\".to_string(),\n                function: FunctionDefinitionRequest {\n                    name: tool.function.name.clone(),\n                    description: tool.function.description.clone(),\n                    parameters: tool.function.parameters.clone(),\n                },\n            })\n            .collect();\n\n        // Convert tool_choice to API format\n        // LMStudio only supports: none, auto, required (not specific functions)\n        let api_tool_choice = tool_choice.map(|tc| match tc {\n            ToolChoice::Auto(_) => \"auto\".to_string(),\n            ToolChoice::Required(_) => \"required\".to_string(),\n            ToolChoice::Function { .. } => {\n                // LMStudio doesn't support specific function selection\n                // Fall back to required mode to ensure a tool is called\n                \"required\".to_string()\n            }\n        });\n\n        let opts = options.cloned().unwrap_or_default();\n        let request = ChatCompletionRequest {\n            model: self.model.clone(),\n            messages: api_messages,\n            temperature: opts.temperature,\n            max_tokens: opts.max_tokens.map(|t| t as i32),\n            stream: true,\n            tools: Some(api_tools),\n            tool_choice: api_tool_choice,\n        };\n\n        let url = format!(\"{}/chat/completions\", self.api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.model,\n            url = %url,\n            tool_count = tools.len(),\n            \"Starting streaming request with tools\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(format!(\"LM Studio request failed: {}\", e)))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio streaming error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let stream = response.bytes_stream();\n\n        let mapped_stream = stream.map(|chunk_result| {\n            match chunk_result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    \n                    // Parse SSE lines\n                    for line in text.lines() {\n                        let line = line.trim();\n                        if line.is_empty() {\n                            continue;\n                        }\n                        if line == \"data: [DONE]\" {\n                            return Ok(StreamChunk::Finished {\n                                reason: \"stop\".to_string(),\n                                ttft_ms: None,\n                            });\n                        }\n                        if let Some(json_str) = line.strip_prefix(\"data: \") {\n                            if let Ok(chunk) = serde_json::from_str::<StreamingChunk>(json_str) {\n                                if let Some(choice) = chunk.choices.first() {\n                                    // Check for finish reason\n                                    if let Some(reason) = &choice.finish_reason {\n                                        return Ok(StreamChunk::Finished {\n                                            reason: reason.clone(),\n                                            ttft_ms: None,\n                                        });\n                                    }\n                                    \n                                    // Check for tool calls\n                                    if let Some(tool_calls) = &choice.delta.tool_calls {\n                                        if let Some(tc) = tool_calls.first() {\n                                            return Ok(StreamChunk::ToolCallDelta {\n                                                index: tc.index,\n                                                id: tc.id.clone(),\n                                                function_name: tc.function.as_ref().and_then(|f| f.name.clone()),\n                                                function_arguments: tc.function.as_ref().and_then(|f| f.arguments.clone()),\n                                            });\n                                        }\n                                    }\n                                    \n                                    // Check for content\n                                    if let Some(content) = &choice.delta.content {\n                                        if !content.is_empty() {\n                                            return Ok(StreamChunk::Content(content.clone()));\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \n                    // Default empty content\n                    Ok(StreamChunk::Content(String::new()))\n                }\n                Err(e) => Err(LlmError::NetworkError(e.to_string())),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for LMStudioProvider {\n    fn name(&self) -> &str {\n        \"lmstudio\"\n    }\n\n    #[allow(clippy::misnamed_getters)]\n    fn model(&self) -> &str {\n        // Note: Returns embedding_model, not the chat model - this is intentional\n        // as per EmbeddingProvider trait contract\n        &self.embedding_model\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        8192 // Default max tokens for embedding models\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        if texts.is_empty() {\n            return Ok(vec![]);\n        }\n\n        let request = EmbeddingRequest {\n            model: self.embedding_model.clone(),\n            input: texts.to_vec(),\n        };\n\n        let url = format!(\"{}/embeddings\", self.api_base());\n\n        debug!(\n            provider = \"lmstudio\",\n            model = %self.embedding_model,\n            url = %url,\n            text_count = texts.len(),\n            \"Sending embedding request\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| {\n                LlmError::NetworkError(format!(\"LM Studio embedding request failed: {}\", e))\n            })?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            // Try to parse as API error\n            if let Ok(api_error) = serde_json::from_str::<ApiError>(&error_text) {\n                return Err(LlmError::ApiError(format!(\n                    \"LM Studio embedding API error ({}): {}\",\n                    status, api_error.error.message\n                )));\n            }\n\n            return Err(LlmError::ApiError(format!(\n                \"LM Studio embedding API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let embedding_response: EmbeddingResponse = response.json().await.map_err(|e| {\n            LlmError::NetworkError(format!(\"Failed to parse embedding response: {}\", e))\n        })?;\n\n        let embeddings: Vec<Vec<f32>> = embedding_response\n            .data\n            .into_iter()\n            .map(|d| d.embedding)\n            .collect();\n\n        debug!(\n            provider = \"lmstudio\",\n            embedding_count = embeddings.len(),\n            dimension = embeddings.first().map(|e: &Vec<f32>| e.len()).unwrap_or(0),\n            \"Received embeddings\"\n        );\n\n        Ok(embeddings)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_builder_defaults() {\n        let builder = LMStudioProviderBuilder::new();\n        assert_eq!(builder.host, DEFAULT_LMSTUDIO_HOST);\n        assert_eq!(builder.model, DEFAULT_LMSTUDIO_MODEL);\n        assert_eq!(builder.embedding_model, DEFAULT_LMSTUDIO_EMBEDDING_MODEL);\n        assert_eq!(builder.embedding_dimension, DEFAULT_LMSTUDIO_EMBEDDING_DIM);\n    }\n\n    #[test]\n    fn test_builder_custom() {\n        let builder = LMStudioProviderBuilder::new()\n            .host(\"http://custom:8080\")\n            .model(\"custom-model\")\n            .embedding_model(\"custom-embed\")\n            .embedding_dimension(1024);\n\n        assert_eq!(builder.host, \"http://custom:8080\");\n        assert_eq!(builder.model, \"custom-model\");\n        assert_eq!(builder.embedding_model, \"custom-embed\");\n        assert_eq!(builder.embedding_dimension, 1024);\n    }\n\n    #[test]\n    fn test_provider_build() {\n        use crate::traits::{EmbeddingProvider, LLMProvider};\n\n        let provider = LMStudioProviderBuilder::new().build().unwrap();\n        assert_eq!(LLMProvider::name(&provider), \"lmstudio\");\n        assert_eq!(LLMProvider::model(&provider), DEFAULT_LMSTUDIO_MODEL);\n        assert_eq!(\n            EmbeddingProvider::dimension(&provider),\n            DEFAULT_LMSTUDIO_EMBEDDING_DIM\n        );\n    }\n\n    #[test]\n    fn test_api_base_with_v1() {\n        let provider = LMStudioProviderBuilder::new()\n            .host(\"http://localhost:1234/v1\")\n            .build()\n            .unwrap();\n        assert_eq!(provider.api_base(), \"http://localhost:1234/v1\");\n    }\n\n    #[test]\n    fn test_api_base_without_v1() {\n        let provider = LMStudioProviderBuilder::new()\n            .host(\"http://localhost:1234\")\n            .build()\n            .unwrap();\n        assert_eq!(provider.api_base(), \"http://localhost:1234/v1\");\n    }\n\n    #[test]\n    fn test_from_env_defaults() {\n        // Clean environment\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n        std::env::remove_var(\"LMSTUDIO_MODEL\");\n        std::env::remove_var(\"LMSTUDIO_EMBEDDING_MODEL\");\n        std::env::remove_var(\"LMSTUDIO_EMBEDDING_DIM\");\n\n        let provider = LMStudioProvider::from_env().unwrap();\n        assert_eq!(provider.host, DEFAULT_LMSTUDIO_HOST);\n        assert_eq!(provider.model, DEFAULT_LMSTUDIO_MODEL);\n    }\n\n    #[test]\n    fn test_from_env_custom() {\n        std::env::set_var(\"LMSTUDIO_HOST\", \"http://custom:9999\");\n        std::env::set_var(\"LMSTUDIO_MODEL\", \"test-model\");\n        std::env::set_var(\"LMSTUDIO_EMBEDDING_MODEL\", \"test-embed\");\n        std::env::set_var(\"LMSTUDIO_EMBEDDING_DIM\", \"512\");\n\n        let provider = LMStudioProvider::from_env().unwrap();\n        assert_eq!(provider.host, \"http://custom:9999\");\n        assert_eq!(provider.model, \"test-model\");\n        assert_eq!(provider.embedding_model, \"test-embed\");\n        assert_eq!(provider.embedding_dimension, 512);\n\n        // Clean up\n        std::env::remove_var(\"LMSTUDIO_HOST\");\n        std::env::remove_var(\"LMSTUDIO_MODEL\");\n        std::env::remove_var(\"LMSTUDIO_EMBEDDING_MODEL\");\n        std::env::remove_var(\"LMSTUDIO_EMBEDDING_DIM\");\n    }\n\n    // =========================================================================\n    // OODA-30: Tests for reasoning model detection\n    // =========================================================================\n\n    #[test]\n    fn test_is_reasoning_model_deepseek_r1() {\n        assert!(is_reasoning_model(\"deepseek-r1\"));\n        assert!(is_reasoning_model(\"deepseek-r1-7b\"));\n        assert!(is_reasoning_model(\"DEEPSEEK-R1-distill\"));\n        assert!(is_reasoning_model(\"bartowski/DeepSeek-R1-Distill-Qwen-7B-GGUF\"));\n    }\n\n    #[test]\n    fn test_is_reasoning_model_qwen3() {\n        assert!(is_reasoning_model(\"qwen3\"));\n        assert!(is_reasoning_model(\"qwen3-14b\"));\n        assert!(is_reasoning_model(\"Qwen3-Thinking\"));\n    }\n\n    #[test]\n    fn test_is_reasoning_model_others() {\n        assert!(is_reasoning_model(\"qwq\"));\n        assert!(is_reasoning_model(\"phi4-reasoning\"));\n        assert!(is_reasoning_model(\"granite-4\"));\n        assert!(is_reasoning_model(\"model-with-reasoning\"));\n        assert!(is_reasoning_model(\"model-thinking\"));\n    }\n\n    #[test]\n    fn test_is_reasoning_model_non_reasoning() {\n        assert!(!is_reasoning_model(\"llama-3\"));\n        assert!(!is_reasoning_model(\"gpt-oss-20b\"));\n        assert!(!is_reasoning_model(\"mistral-7b\"));\n        assert!(!is_reasoning_model(\"gemma2-9b\"));\n    }\n\n    // =========================================================================\n    // OODA-30: Tests for REST API base URL\n    // =========================================================================\n\n    #[test]\n    fn test_rest_api_base() {\n        let provider = LMStudioProviderBuilder::new()\n            .host(\"http://localhost:1234\")\n            .build()\n            .unwrap();\n        assert_eq!(provider.rest_api_base(), \"http://localhost:1234/api/v1\");\n    }\n\n    #[test]\n    fn test_rest_api_base_with_v1() {\n        let provider = LMStudioProviderBuilder::new()\n            .host(\"http://localhost:1234/v1\")\n            .build()\n            .unwrap();\n        assert_eq!(provider.rest_api_base(), \"http://localhost:1234/api/v1\");\n    }\n\n    // =========================================================================\n    // OODA-30: Tests for REST API response parsing\n    // =========================================================================\n\n    #[test]\n    fn test_rest_output_item_parsing_reasoning() {\n        let json = r#\"{\"type\": \"reasoning\", \"content\": \"Let me think...\"}\"#;\n        let item: RestOutputItem = serde_json::from_str(json).unwrap();\n        match item {\n            RestOutputItem::Reasoning { content } => {\n                assert_eq!(content, \"Let me think...\");\n            }\n            _ => panic!(\"Expected Reasoning variant\"),\n        }\n    }\n\n    #[test]\n    fn test_rest_output_item_parsing_message() {\n        let json = r#\"{\"type\": \"message\", \"content\": \"The answer is 42.\"}\"#;\n        let item: RestOutputItem = serde_json::from_str(json).unwrap();\n        match item {\n            RestOutputItem::Message { content } => {\n                assert_eq!(content, \"The answer is 42.\");\n            }\n            _ => panic!(\"Expected Message variant\"),\n        }\n    }\n\n    #[test]\n    fn test_rest_stats_parsing() {\n        let json = r#\"{\n            \"input_tokens\": 100,\n            \"total_output_tokens\": 150,\n            \"reasoning_output_tokens\": 50,\n            \"tokens_per_second\": 43.73,\n            \"time_to_first_token_seconds\": 0.781\n        }\"#;\n        let stats: RestStats = serde_json::from_str(json).unwrap();\n        assert_eq!(stats.input_tokens, 100);\n        assert_eq!(stats.total_output_tokens, 150);\n        assert_eq!(stats.reasoning_output_tokens, 50);\n    }\n\n    #[test]\n    fn test_rest_chat_request_serialization() {\n        let request = RestChatRequest {\n            model: \"deepseek-r1\".to_string(),\n            input: \"What is 2+2?\".to_string(),\n            reasoning: Some(\"on\".to_string()),\n            stream: Some(false),\n            temperature: Some(0.7),\n            max_output_tokens: Some(1000),\n        };\n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"\\\"model\\\":\\\"deepseek-r1\\\"\"));\n        assert!(json.contains(\"\\\"reasoning\\\":\\\"on\\\"\"));\n        assert!(json.contains(\"\\\"stream\\\":false\"));\n    }\n\n    #[test]\n    fn test_rest_stream_event_parsing_reasoning_delta() {\n        let json = r#\"{\"type\": \"reasoning.delta\", \"content\": \"Step 1...\"}\"#;\n        let event: RestStreamEvent = serde_json::from_str(json).unwrap();\n        match event {\n            RestStreamEvent::ReasoningDelta { content } => {\n                assert_eq!(content, \"Step 1...\");\n            }\n            _ => panic!(\"Expected ReasoningDelta variant\"),\n        }\n    }\n\n    #[test]\n    fn test_rest_stream_event_parsing_message_delta() {\n        let json = r#\"{\"type\": \"message.delta\", \"content\": \"Hello\"}\"#;\n        let event: RestStreamEvent = serde_json::from_str(json).unwrap();\n        match event {\n            RestStreamEvent::MessageDelta { content } => {\n                assert_eq!(content, \"Hello\");\n            }\n            _ => panic!(\"Expected MessageDelta variant\"),\n        }\n    }\n\n    #[test]\n    fn test_constants() {\n        assert_eq!(DEFAULT_LMSTUDIO_HOST, \"http://localhost:1234\");\n        assert_eq!(DEFAULT_LMSTUDIO_MODEL, \"gemma2-9b-it\");\n        assert_eq!(DEFAULT_LMSTUDIO_EMBEDDING_MODEL, \"nomic-embed-text-v1.5\");\n        assert_eq!(DEFAULT_LMSTUDIO_EMBEDDING_DIM, 768);\n    }\n\n    #[test]\n    fn test_builder_auto_load_models_default() {\n        let builder = LMStudioProviderBuilder::default();\n        assert!(builder.auto_load_models);\n    }\n\n    #[test]\n    fn test_builder_auto_load_models_disabled() {\n        let provider = LMStudioProviderBuilder::new()\n            .auto_load_models(false)\n            .build()\n            .unwrap();\n        assert!(!provider.auto_load_models);\n    }\n\n    #[test]\n    fn test_builder_max_context_length() {\n        let provider = LMStudioProviderBuilder::new()\n            .max_context_length(65536)\n            .build()\n            .unwrap();\n        assert_eq!(provider.max_context_length(), 65536);\n    }\n\n    #[test]\n    fn test_supports_streaming() {\n        let provider = LMStudioProviderBuilder::new().build().unwrap();\n        assert!(provider.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_json_mode() {\n        let provider = LMStudioProviderBuilder::new().build().unwrap();\n        // LM Studio doesn't override supports_json_mode, so default is false\n        assert!(!provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_embedding_provider_name() {\n        let provider = LMStudioProviderBuilder::new().build().unwrap();\n        assert_eq!(EmbeddingProvider::name(&provider), \"lmstudio\");\n    }\n\n    #[test]\n    fn test_embedding_provider_model() {\n        let provider = LMStudioProviderBuilder::new()\n            .embedding_model(\"custom-embed\")\n            .build()\n            .unwrap();\n        assert_eq!(EmbeddingProvider::model(&provider), \"custom-embed\");\n    }\n\n    #[test]\n    fn test_embedding_provider_max_tokens() {\n        let provider = LMStudioProviderBuilder::new().build().unwrap();\n        assert_eq!(provider.max_tokens(), 8192);\n    }\n\n    #[tokio::test]\n    async fn test_embed_empty_input() {\n        let provider = LMStudioProviderBuilder::new().build().unwrap();\n        let result = provider.embed(&[]).await;\n        assert!(result.is_ok());\n        assert!(result.unwrap().is_empty());\n    }\n\n    #[test]\n    fn test_rest_chat_response_parsing() {\n        let json = r#\"{\n            \"model_instance_id\": \"test-instance\",\n            \"output\": [\n                {\"type\": \"reasoning\", \"content\": \"Thinking...\"},\n                {\"type\": \"message\", \"content\": \"Answer\"}\n            ],\n            \"stats\": {\n                \"input_tokens\": 10,\n                \"total_output_tokens\": 20,\n                \"reasoning_output_tokens\": 5,\n                \"tokens_per_second\": 30.0,\n                \"time_to_first_token_seconds\": 0.2\n            }\n        }\"#;\n        let response: RestChatResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.output.len(), 2);\n    }\n}\n","traces":[{"line":87,"address":[],"length":0,"stats":{"Line":34}},{"line":89,"address":[],"length":0,"stats":{"Line":102}},{"line":90,"address":[],"length":0,"stats":{"Line":102}},{"line":91,"address":[],"length":0,"stats":{"Line":34}},{"line":101,"address":[],"length":0,"stats":{"Line":33}},{"line":102,"address":[],"length":0,"stats":{"Line":33}},{"line":106,"address":[],"length":0,"stats":{"Line":23}},{"line":107,"address":[],"length":0,"stats":{"Line":69}},{"line":108,"address":[],"length":0,"stats":{"Line":23}},{"line":112,"address":[],"length":0,"stats":{"Line":19}},{"line":113,"address":[],"length":0,"stats":{"Line":57}},{"line":114,"address":[],"length":0,"stats":{"Line":19}},{"line":118,"address":[],"length":0,"stats":{"Line":20}},{"line":119,"address":[],"length":0,"stats":{"Line":60}},{"line":120,"address":[],"length":0,"stats":{"Line":20}},{"line":124,"address":[],"length":0,"stats":{"Line":19}},{"line":125,"address":[],"length":0,"stats":{"Line":19}},{"line":126,"address":[],"length":0,"stats":{"Line":19}},{"line":130,"address":[],"length":0,"stats":{"Line":19}},{"line":131,"address":[],"length":0,"stats":{"Line":19}},{"line":132,"address":[],"length":0,"stats":{"Line":19}},{"line":136,"address":[],"length":0,"stats":{"Line":1}},{"line":137,"address":[],"length":0,"stats":{"Line":1}},{"line":138,"address":[],"length":0,"stats":{"Line":1}},{"line":142,"address":[],"length":0,"stats":{"Line":31}},{"line":143,"address":[],"length":0,"stats":{"Line":62}},{"line":144,"address":[],"length":0,"stats":{"Line":62}},{"line":147,"address":[],"length":0,"stats":{"Line":31}},{"line":149,"address":[],"length":0,"stats":{"Line":31}},{"line":150,"address":[],"length":0,"stats":{"Line":62}},{"line":151,"address":[],"length":0,"stats":{"Line":62}},{"line":152,"address":[],"length":0,"stats":{"Line":62}},{"line":153,"address":[],"length":0,"stats":{"Line":62}},{"line":154,"address":[],"length":0,"stats":{"Line":62}},{"line":155,"address":[],"length":0,"stats":{"Line":31}},{"line":156,"address":[],"length":0,"stats":{"Line":31}},{"line":178,"address":[],"length":0,"stats":{"Line":18}},{"line":179,"address":[],"length":0,"stats":{"Line":18}},{"line":180,"address":[],"length":0,"stats":{"Line":62}},{"line":182,"address":[],"length":0,"stats":{"Line":18}},{"line":183,"address":[],"length":0,"stats":{"Line":68}},{"line":185,"address":[],"length":0,"stats":{"Line":36}},{"line":186,"address":[],"length":0,"stats":{"Line":52}},{"line":188,"address":[],"length":0,"stats":{"Line":36}},{"line":190,"address":[],"length":0,"stats":{"Line":20}},{"line":191,"address":[],"length":0,"stats":{"Line":18}},{"line":196,"address":[],"length":0,"stats":{"Line":36}},{"line":198,"address":[],"length":0,"stats":{"Line":18}},{"line":201,"address":[],"length":0,"stats":{"Line":18}},{"line":202,"address":[],"length":0,"stats":{"Line":36}},{"line":203,"address":[],"length":0,"stats":{"Line":36}},{"line":204,"address":[],"length":0,"stats":{"Line":36}},{"line":205,"address":[],"length":0,"stats":{"Line":36}},{"line":206,"address":[],"length":0,"stats":{"Line":36}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":2}},{"line":222,"address":[],"length":0,"stats":{"Line":2}},{"line":223,"address":[],"length":0,"stats":{"Line":2}},{"line":225,"address":[],"length":0,"stats":{"Line":2}},{"line":231,"address":[],"length":0,"stats":{"Line":2}},{"line":232,"address":[],"length":0,"stats":{"Line":4}},{"line":233,"address":[],"length":0,"stats":{"Line":4}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":249,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":291,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":306,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":0}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":312,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":317,"address":[],"length":0,"stats":{"Line":0}},{"line":318,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":324,"address":[],"length":0,"stats":{"Line":0}},{"line":326,"address":[],"length":0,"stats":{"Line":0}},{"line":327,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":0}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":339,"address":[],"length":0,"stats":{"Line":0}},{"line":343,"address":[],"length":0,"stats":{"Line":0}},{"line":349,"address":[],"length":0,"stats":{"Line":0}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":351,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":0}},{"line":354,"address":[],"length":0,"stats":{"Line":0}},{"line":357,"address":[],"length":0,"stats":{"Line":0}},{"line":361,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[],"length":0,"stats":{"Line":0}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":388,"address":[],"length":0,"stats":{"Line":0}},{"line":389,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":399,"address":[],"length":0,"stats":{"Line":0}},{"line":402,"address":[],"length":0,"stats":{"Line":0}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":407,"address":[],"length":0,"stats":{"Line":0}},{"line":410,"address":[],"length":0,"stats":{"Line":0}},{"line":417,"address":[],"length":0,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":436,"address":[],"length":0,"stats":{"Line":0}},{"line":437,"address":[],"length":0,"stats":{"Line":0}},{"line":438,"address":[],"length":0,"stats":{"Line":0}},{"line":439,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[],"length":0,"stats":{"Line":0}},{"line":442,"address":[],"length":0,"stats":{"Line":0}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":450,"address":[],"length":0,"stats":{"Line":0}},{"line":451,"address":[],"length":0,"stats":{"Line":0}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":459,"address":[],"length":0,"stats":{"Line":0}},{"line":463,"address":[],"length":0,"stats":{"Line":0}},{"line":464,"address":[],"length":0,"stats":{"Line":0}},{"line":467,"address":[],"length":0,"stats":{"Line":0}},{"line":468,"address":[],"length":0,"stats":{"Line":0}},{"line":469,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":472,"address":[],"length":0,"stats":{"Line":0}},{"line":473,"address":[],"length":0,"stats":{"Line":0}},{"line":475,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":477,"address":[],"length":0,"stats":{"Line":0}},{"line":479,"address":[],"length":0,"stats":{"Line":0}},{"line":480,"address":[],"length":0,"stats":{"Line":0}},{"line":483,"address":[],"length":0,"stats":{"Line":0}},{"line":484,"address":[],"length":0,"stats":{"Line":0}},{"line":485,"address":[],"length":0,"stats":{"Line":0}},{"line":486,"address":[],"length":0,"stats":{"Line":0}},{"line":490,"address":[],"length":0,"stats":{"Line":0}},{"line":491,"address":[],"length":0,"stats":{"Line":0}},{"line":492,"address":[],"length":0,"stats":{"Line":0}},{"line":496,"address":[],"length":0,"stats":{"Line":0}},{"line":498,"address":[],"length":0,"stats":{"Line":0}},{"line":499,"address":[],"length":0,"stats":{"Line":0}},{"line":501,"address":[],"length":0,"stats":{"Line":0}},{"line":502,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":508,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":0}},{"line":517,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":522,"address":[],"length":0,"stats":{"Line":0}},{"line":523,"address":[],"length":0,"stats":{"Line":0}},{"line":524,"address":[],"length":0,"stats":{"Line":0}},{"line":525,"address":[],"length":0,"stats":{"Line":0}},{"line":526,"address":[],"length":0,"stats":{"Line":0}},{"line":527,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":0}},{"line":529,"address":[],"length":0,"stats":{"Line":0}},{"line":530,"address":[],"length":0,"stats":{"Line":0}},{"line":531,"address":[],"length":0,"stats":{"Line":0}},{"line":532,"address":[],"length":0,"stats":{"Line":0}},{"line":534,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":540,"address":[],"length":0,"stats":{"Line":0}},{"line":547,"address":[],"length":0,"stats":{"Line":0}},{"line":549,"address":[],"length":0,"stats":{"Line":0}},{"line":550,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":0}},{"line":553,"address":[],"length":0,"stats":{"Line":0}},{"line":554,"address":[],"length":0,"stats":{"Line":0}},{"line":555,"address":[],"length":0,"stats":{"Line":0}},{"line":557,"address":[],"length":0,"stats":{"Line":0}},{"line":562,"address":[],"length":0,"stats":{"Line":0}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":565,"address":[],"length":0,"stats":{"Line":0}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":567,"address":[],"length":0,"stats":{"Line":0}},{"line":568,"address":[],"length":0,"stats":{"Line":0}},{"line":569,"address":[],"length":0,"stats":{"Line":0}},{"line":576,"address":[],"length":0,"stats":{"Line":0}},{"line":577,"address":[],"length":0,"stats":{"Line":0}},{"line":578,"address":[],"length":0,"stats":{"Line":0}},{"line":582,"address":[],"length":0,"stats":{"Line":0}},{"line":586,"address":[],"length":0,"stats":{"Line":0}},{"line":588,"address":[],"length":0,"stats":{"Line":0}},{"line":590,"address":[],"length":0,"stats":{"Line":0}},{"line":591,"address":[],"length":0,"stats":{"Line":0}},{"line":593,"address":[],"length":0,"stats":{"Line":0}},{"line":597,"address":[],"length":0,"stats":{"Line":0}},{"line":599,"address":[],"length":0,"stats":{"Line":0}},{"line":603,"address":[],"length":0,"stats":{"Line":0}},{"line":604,"address":[],"length":0,"stats":{"Line":0}},{"line":605,"address":[],"length":0,"stats":{"Line":0}},{"line":608,"address":[],"length":0,"stats":{"Line":0}},{"line":609,"address":[],"length":0,"stats":{"Line":0}},{"line":610,"address":[],"length":0,"stats":{"Line":0}},{"line":611,"address":[],"length":0,"stats":{"Line":0}},{"line":613,"address":[],"length":0,"stats":{"Line":0}},{"line":614,"address":[],"length":0,"stats":{"Line":0}},{"line":615,"address":[],"length":0,"stats":{"Line":0}},{"line":616,"address":[],"length":0,"stats":{"Line":0}},{"line":617,"address":[],"length":0,"stats":{"Line":0}},{"line":618,"address":[],"length":0,"stats":{"Line":0}},{"line":620,"address":[],"length":0,"stats":{"Line":0}},{"line":621,"address":[],"length":0,"stats":{"Line":0}},{"line":622,"address":[],"length":0,"stats":{"Line":0}},{"line":623,"address":[],"length":0,"stats":{"Line":0}},{"line":626,"address":[],"length":0,"stats":{"Line":0}},{"line":627,"address":[],"length":0,"stats":{"Line":0}},{"line":628,"address":[],"length":0,"stats":{"Line":0}},{"line":629,"address":[],"length":0,"stats":{"Line":0}},{"line":630,"address":[],"length":0,"stats":{"Line":0}},{"line":631,"address":[],"length":0,"stats":{"Line":0}},{"line":636,"address":[],"length":0,"stats":{"Line":0}},{"line":637,"address":[],"length":0,"stats":{"Line":0}},{"line":638,"address":[],"length":0,"stats":{"Line":0}},{"line":640,"address":[],"length":0,"stats":{"Line":0}},{"line":641,"address":[],"length":0,"stats":{"Line":0}},{"line":644,"address":[],"length":0,"stats":{"Line":0}},{"line":645,"address":[],"length":0,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":647,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":0}},{"line":652,"address":[],"length":0,"stats":{"Line":0}},{"line":653,"address":[],"length":0,"stats":{"Line":0}},{"line":657,"address":[],"length":0,"stats":{"Line":0}},{"line":659,"address":[],"length":0,"stats":{"Line":0}},{"line":660,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[],"length":0,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":665,"address":[],"length":0,"stats":{"Line":0}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[],"length":0,"stats":{"Line":0}},{"line":671,"address":[],"length":0,"stats":{"Line":0}},{"line":673,"address":[],"length":0,"stats":{"Line":0}},{"line":674,"address":[],"length":0,"stats":{"Line":0}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":676,"address":[],"length":0,"stats":{"Line":0}},{"line":677,"address":[],"length":0,"stats":{"Line":0}},{"line":678,"address":[],"length":0,"stats":{"Line":0}},{"line":683,"address":[],"length":0,"stats":{"Line":0}},{"line":684,"address":[],"length":0,"stats":{"Line":0}},{"line":685,"address":[],"length":0,"stats":{"Line":0}},{"line":686,"address":[],"length":0,"stats":{"Line":0}},{"line":688,"address":[],"length":0,"stats":{"Line":0}},{"line":692,"address":[],"length":0,"stats":{"Line":0}},{"line":693,"address":[],"length":0,"stats":{"Line":0}},{"line":694,"address":[],"length":0,"stats":{"Line":0}},{"line":697,"address":[],"length":0,"stats":{"Line":0}},{"line":698,"address":[],"length":0,"stats":{"Line":0}},{"line":699,"address":[],"length":0,"stats":{"Line":0}},{"line":700,"address":[],"length":0,"stats":{"Line":0}},{"line":701,"address":[],"length":0,"stats":{"Line":0}},{"line":702,"address":[],"length":0,"stats":{"Line":0}},{"line":703,"address":[],"length":0,"stats":{"Line":0}},{"line":704,"address":[],"length":0,"stats":{"Line":0}},{"line":705,"address":[],"length":0,"stats":{"Line":0}},{"line":706,"address":[],"length":0,"stats":{"Line":0}},{"line":707,"address":[],"length":0,"stats":{"Line":0}},{"line":708,"address":[],"length":0,"stats":{"Line":0}},{"line":718,"address":[],"length":0,"stats":{"Line":16}},{"line":719,"address":[],"length":0,"stats":{"Line":48}},{"line":720,"address":[],"length":0,"stats":{"Line":16}},{"line":721,"address":[],"length":0,"stats":{"Line":12}},{"line":722,"address":[],"length":0,"stats":{"Line":9}},{"line":723,"address":[],"length":0,"stats":{"Line":8}},{"line":724,"address":[],"length":0,"stats":{"Line":7}},{"line":725,"address":[],"length":0,"stats":{"Line":6}},{"line":726,"address":[],"length":0,"stats":{"Line":5}},{"line":1032,"address":[],"length":0,"stats":{"Line":0}},{"line":1033,"address":[],"length":0,"stats":{"Line":0}},{"line":1035,"address":[],"length":0,"stats":{"Line":0}},{"line":1037,"address":[],"length":0,"stats":{"Line":0}},{"line":1038,"address":[],"length":0,"stats":{"Line":0}},{"line":1039,"address":[],"length":0,"stats":{"Line":0}},{"line":1041,"address":[],"length":0,"stats":{"Line":0}},{"line":1042,"address":[],"length":0,"stats":{"Line":0}},{"line":1043,"address":[],"length":0,"stats":{"Line":0}},{"line":1044,"address":[],"length":0,"stats":{"Line":0}},{"line":1045,"address":[],"length":0,"stats":{"Line":0}},{"line":1049,"address":[],"length":0,"stats":{"Line":0}},{"line":1050,"address":[],"length":0,"stats":{"Line":0}},{"line":1051,"address":[],"length":0,"stats":{"Line":0}},{"line":1052,"address":[],"length":0,"stats":{"Line":0}},{"line":1053,"address":[],"length":0,"stats":{"Line":0}},{"line":1054,"address":[],"length":0,"stats":{"Line":0}},{"line":1058,"address":[],"length":0,"stats":{"Line":0}},{"line":1060,"address":[],"length":0,"stats":{"Line":0}},{"line":1061,"address":[],"length":0,"stats":{"Line":0}},{"line":1068,"address":[],"length":0,"stats":{"Line":0}},{"line":1074,"address":[],"length":0,"stats":{"Line":0}},{"line":1076,"address":[],"length":0,"stats":{"Line":0}},{"line":1077,"address":[],"length":0,"stats":{"Line":0}},{"line":1078,"address":[],"length":0,"stats":{"Line":0}},{"line":1079,"address":[],"length":0,"stats":{"Line":0}},{"line":1080,"address":[],"length":0,"stats":{"Line":0}},{"line":1081,"address":[],"length":0,"stats":{"Line":0}},{"line":1082,"address":[],"length":0,"stats":{"Line":0}},{"line":1084,"address":[],"length":0,"stats":{"Line":0}},{"line":1089,"address":[],"length":0,"stats":{"Line":0}},{"line":1091,"address":[],"length":0,"stats":{"Line":0}},{"line":1093,"address":[],"length":0,"stats":{"Line":0}},{"line":1094,"address":[],"length":0,"stats":{"Line":0}},{"line":1095,"address":[],"length":0,"stats":{"Line":0}},{"line":1096,"address":[],"length":0,"stats":{"Line":0}},{"line":1099,"address":[],"length":0,"stats":{"Line":0}},{"line":1101,"address":[],"length":0,"stats":{"Line":0}},{"line":1105,"address":[],"length":0,"stats":{"Line":0}},{"line":1106,"address":[],"length":0,"stats":{"Line":0}},{"line":1109,"address":[],"length":0,"stats":{"Line":0}},{"line":1110,"address":[],"length":0,"stats":{"Line":0}},{"line":1111,"address":[],"length":0,"stats":{"Line":0}},{"line":1112,"address":[],"length":0,"stats":{"Line":0}},{"line":1114,"address":[],"length":0,"stats":{"Line":0}},{"line":1115,"address":[],"length":0,"stats":{"Line":0}},{"line":1117,"address":[],"length":0,"stats":{"Line":0}},{"line":1118,"address":[],"length":0,"stats":{"Line":0}},{"line":1119,"address":[],"length":0,"stats":{"Line":0}},{"line":1121,"address":[],"length":0,"stats":{"Line":0}},{"line":1122,"address":[],"length":0,"stats":{"Line":0}},{"line":1125,"address":[],"length":0,"stats":{"Line":0}},{"line":1126,"address":[],"length":0,"stats":{"Line":0}},{"line":1127,"address":[],"length":0,"stats":{"Line":0}},{"line":1128,"address":[],"length":0,"stats":{"Line":0}},{"line":1132,"address":[],"length":0,"stats":{"Line":0}},{"line":1133,"address":[],"length":0,"stats":{"Line":0}},{"line":1134,"address":[],"length":0,"stats":{"Line":0}},{"line":1138,"address":[],"length":0,"stats":{"Line":0}},{"line":1140,"address":[],"length":0,"stats":{"Line":0}},{"line":1141,"address":[],"length":0,"stats":{"Line":0}},{"line":1144,"address":[],"length":0,"stats":{"Line":0}},{"line":1145,"address":[],"length":0,"stats":{"Line":0}},{"line":1147,"address":[],"length":0,"stats":{"Line":0}},{"line":1148,"address":[],"length":0,"stats":{"Line":0}},{"line":1149,"address":[],"length":0,"stats":{"Line":0}},{"line":1150,"address":[],"length":0,"stats":{"Line":0}},{"line":1152,"address":[],"length":0,"stats":{"Line":0}},{"line":1153,"address":[],"length":0,"stats":{"Line":0}},{"line":1158,"address":[],"length":0,"stats":{"Line":0}},{"line":1163,"address":[],"length":0,"stats":{"Line":0}},{"line":1164,"address":[],"length":0,"stats":{"Line":0}},{"line":1165,"address":[],"length":0,"stats":{"Line":0}},{"line":1170,"address":[],"length":0,"stats":{"Line":0}},{"line":1171,"address":[],"length":0,"stats":{"Line":0}},{"line":1172,"address":[],"length":0,"stats":{"Line":0}},{"line":1173,"address":[],"length":0,"stats":{"Line":0}},{"line":1174,"address":[],"length":0,"stats":{"Line":0}},{"line":1175,"address":[],"length":0,"stats":{"Line":0}},{"line":1176,"address":[],"length":0,"stats":{"Line":0}},{"line":1183,"address":[],"length":0,"stats":{"Line":0}},{"line":1184,"address":[],"length":0,"stats":{"Line":0}},{"line":1185,"address":[],"length":0,"stats":{"Line":0}},{"line":1186,"address":[],"length":0,"stats":{"Line":0}},{"line":1189,"address":[],"length":0,"stats":{"Line":0}},{"line":1198,"address":[],"length":0,"stats":{"Line":0}},{"line":1204,"address":[],"length":0,"stats":{"Line":0}},{"line":1206,"address":[],"length":0,"stats":{"Line":0}},{"line":1207,"address":[],"length":0,"stats":{"Line":0}},{"line":1208,"address":[],"length":0,"stats":{"Line":0}},{"line":1209,"address":[],"length":0,"stats":{"Line":0}},{"line":1210,"address":[],"length":0,"stats":{"Line":0}},{"line":1211,"address":[],"length":0,"stats":{"Line":0}},{"line":1212,"address":[],"length":0,"stats":{"Line":0}},{"line":1214,"address":[],"length":0,"stats":{"Line":0}},{"line":1219,"address":[],"length":0,"stats":{"Line":0}},{"line":1221,"address":[],"length":0,"stats":{"Line":0}},{"line":1223,"address":[],"length":0,"stats":{"Line":0}},{"line":1224,"address":[],"length":0,"stats":{"Line":0}},{"line":1225,"address":[],"length":0,"stats":{"Line":0}},{"line":1226,"address":[],"length":0,"stats":{"Line":0}},{"line":1229,"address":[],"length":0,"stats":{"Line":0}},{"line":1231,"address":[],"length":0,"stats":{"Line":0}},{"line":1235,"address":[],"length":0,"stats":{"Line":0}},{"line":1236,"address":[],"length":0,"stats":{"Line":0}},{"line":1239,"address":[],"length":0,"stats":{"Line":0}},{"line":1240,"address":[],"length":0,"stats":{"Line":0}},{"line":1241,"address":[],"length":0,"stats":{"Line":0}},{"line":1242,"address":[],"length":0,"stats":{"Line":0}},{"line":1244,"address":[],"length":0,"stats":{"Line":0}},{"line":1245,"address":[],"length":0,"stats":{"Line":0}},{"line":1247,"address":[],"length":0,"stats":{"Line":0}},{"line":1248,"address":[],"length":0,"stats":{"Line":0}},{"line":1249,"address":[],"length":0,"stats":{"Line":0}},{"line":1250,"address":[],"length":0,"stats":{"Line":0}},{"line":1251,"address":[],"length":0,"stats":{"Line":0}},{"line":1252,"address":[],"length":0,"stats":{"Line":0}},{"line":1256,"address":[],"length":0,"stats":{"Line":0}},{"line":1258,"address":[],"length":0,"stats":{"Line":0}},{"line":1259,"address":[],"length":0,"stats":{"Line":0}},{"line":1260,"address":[],"length":0,"stats":{"Line":0}},{"line":1261,"address":[],"length":0,"stats":{"Line":0}},{"line":1264,"address":[],"length":0,"stats":{"Line":0}},{"line":1265,"address":[],"length":0,"stats":{"Line":0}},{"line":1266,"address":[],"length":0,"stats":{"Line":0}},{"line":1267,"address":[],"length":0,"stats":{"Line":0}},{"line":1270,"address":[],"length":0,"stats":{"Line":0}},{"line":1272,"address":[],"length":0,"stats":{"Line":0}},{"line":1273,"address":[],"length":0,"stats":{"Line":0}},{"line":1274,"address":[],"length":0,"stats":{"Line":0}},{"line":1276,"address":[],"length":0,"stats":{"Line":0}},{"line":1277,"address":[],"length":0,"stats":{"Line":0}},{"line":1278,"address":[],"length":0,"stats":{"Line":0}},{"line":1279,"address":[],"length":0,"stats":{"Line":0}},{"line":1280,"address":[],"length":0,"stats":{"Line":0}},{"line":1283,"address":[],"length":0,"stats":{"Line":0}},{"line":1284,"address":[],"length":0,"stats":{"Line":0}},{"line":1285,"address":[],"length":0,"stats":{"Line":0}},{"line":1288,"address":[],"length":0,"stats":{"Line":0}},{"line":1290,"address":[],"length":0,"stats":{"Line":0}},{"line":1291,"address":[],"length":0,"stats":{"Line":0}},{"line":1292,"address":[],"length":0,"stats":{"Line":0}},{"line":1293,"address":[],"length":0,"stats":{"Line":0}},{"line":1297,"address":[],"length":0,"stats":{"Line":0}},{"line":1304,"address":[],"length":0,"stats":{"Line":0}},{"line":1306,"address":[],"length":0,"stats":{"Line":0}},{"line":1310,"address":[],"length":0,"stats":{"Line":0}},{"line":1316,"address":[],"length":0,"stats":{"Line":7}},{"line":1317,"address":[],"length":0,"stats":{"Line":7}},{"line":1320,"address":[],"length":0,"stats":{"Line":1}},{"line":1321,"address":[],"length":0,"stats":{"Line":1}},{"line":1324,"address":[],"length":0,"stats":{"Line":1}},{"line":1325,"address":[],"length":0,"stats":{"Line":1}},{"line":1328,"address":[],"length":0,"stats":{"Line":0}},{"line":1364,"address":[],"length":0,"stats":{"Line":1}},{"line":1365,"address":[],"length":0,"stats":{"Line":1}},{"line":1368,"address":[],"length":0,"stats":{"Line":0}},{"line":1369,"address":[],"length":0,"stats":{"Line":0}},{"line":1372,"address":[],"length":0,"stats":{"Line":0}},{"line":1373,"address":[],"length":0,"stats":{"Line":0}},{"line":1410,"address":[],"length":0,"stats":{"Line":0}},{"line":1423,"address":[],"length":0,"stats":{"Line":0}},{"line":1424,"address":[],"length":0,"stats":{"Line":0}},{"line":1425,"address":[],"length":0,"stats":{"Line":0}},{"line":1426,"address":[],"length":0,"stats":{"Line":0}},{"line":1427,"address":[],"length":0,"stats":{"Line":0}},{"line":1430,"address":[],"length":0,"stats":{"Line":0}},{"line":1431,"address":[],"length":0,"stats":{"Line":0}},{"line":1432,"address":[],"length":0,"stats":{"Line":0}},{"line":1433,"address":[],"length":0,"stats":{"Line":0}},{"line":1435,"address":[],"length":0,"stats":{"Line":0}},{"line":1436,"address":[],"length":0,"stats":{"Line":0}},{"line":1437,"address":[],"length":0,"stats":{"Line":0}},{"line":1438,"address":[],"length":0,"stats":{"Line":0}},{"line":1439,"address":[],"length":0,"stats":{"Line":0}},{"line":1445,"address":[],"length":0,"stats":{"Line":0}},{"line":1447,"address":[],"length":0,"stats":{"Line":0}},{"line":1476,"address":[],"length":0,"stats":{"Line":0}},{"line":1477,"address":[],"length":0,"stats":{"Line":0}},{"line":1478,"address":[],"length":0,"stats":{"Line":0}},{"line":1479,"address":[],"length":0,"stats":{"Line":0}},{"line":1480,"address":[],"length":0,"stats":{"Line":0}},{"line":1481,"address":[],"length":0,"stats":{"Line":0}},{"line":1483,"address":[],"length":0,"stats":{"Line":0}},{"line":1491,"address":[],"length":0,"stats":{"Line":0}},{"line":1492,"address":[],"length":0,"stats":{"Line":0}},{"line":1493,"address":[],"length":0,"stats":{"Line":0}},{"line":1494,"address":[],"length":0,"stats":{"Line":0}},{"line":1495,"address":[],"length":0,"stats":{"Line":0}},{"line":1502,"address":[],"length":0,"stats":{"Line":0}},{"line":1503,"address":[],"length":0,"stats":{"Line":0}},{"line":1504,"address":[],"length":0,"stats":{"Line":0}},{"line":1508,"address":[],"length":0,"stats":{"Line":0}},{"line":1517,"address":[],"length":0,"stats":{"Line":0}},{"line":1539,"address":[],"length":0,"stats":{"Line":0}},{"line":1552,"address":[],"length":0,"stats":{"Line":0}},{"line":1553,"address":[],"length":0,"stats":{"Line":0}},{"line":1554,"address":[],"length":0,"stats":{"Line":0}},{"line":1555,"address":[],"length":0,"stats":{"Line":0}},{"line":1558,"address":[],"length":0,"stats":{"Line":0}},{"line":1559,"address":[],"length":0,"stats":{"Line":0}},{"line":1560,"address":[],"length":0,"stats":{"Line":0}},{"line":1561,"address":[],"length":0,"stats":{"Line":0}},{"line":1563,"address":[],"length":0,"stats":{"Line":0}},{"line":1564,"address":[],"length":0,"stats":{"Line":0}},{"line":1565,"address":[],"length":0,"stats":{"Line":0}},{"line":1566,"address":[],"length":0,"stats":{"Line":0}},{"line":1569,"address":[],"length":0,"stats":{"Line":0}},{"line":1570,"address":[],"length":0,"stats":{"Line":0}},{"line":1571,"address":[],"length":0,"stats":{"Line":0}},{"line":1573,"address":[],"length":0,"stats":{"Line":0}},{"line":1574,"address":[],"length":0,"stats":{"Line":0}},{"line":1575,"address":[],"length":0,"stats":{"Line":0}},{"line":1576,"address":[],"length":0,"stats":{"Line":0}},{"line":1581,"address":[],"length":0,"stats":{"Line":0}},{"line":1582,"address":[],"length":0,"stats":{"Line":0}},{"line":1584,"address":[],"length":0,"stats":{"Line":0}},{"line":1585,"address":[],"length":0,"stats":{"Line":0}},{"line":1586,"address":[],"length":0,"stats":{"Line":0}},{"line":1587,"address":[],"length":0,"stats":{"Line":0}},{"line":1593,"address":[],"length":0,"stats":{"Line":0}},{"line":1594,"address":[],"length":0,"stats":{"Line":0}},{"line":1595,"address":[],"length":0,"stats":{"Line":0}},{"line":1604,"address":[],"length":0,"stats":{"Line":0}},{"line":1606,"address":[],"length":0,"stats":{"Line":0}},{"line":1616,"address":[],"length":0,"stats":{"Line":4}},{"line":1617,"address":[],"length":0,"stats":{"Line":4}},{"line":1621,"address":[],"length":0,"stats":{"Line":1}},{"line":1624,"address":[],"length":0,"stats":{"Line":1}},{"line":1627,"address":[],"length":0,"stats":{"Line":5}},{"line":1628,"address":[],"length":0,"stats":{"Line":5}},{"line":1631,"address":[],"length":0,"stats":{"Line":1}},{"line":1632,"address":[],"length":0,"stats":{"Line":1}},{"line":1635,"address":[],"length":0,"stats":{"Line":1}},{"line":1661,"address":[],"length":0,"stats":{"Line":0}},{"line":1662,"address":[],"length":0,"stats":{"Line":0}},{"line":1670,"address":[],"length":0,"stats":{"Line":0}},{"line":1686,"address":[],"length":0,"stats":{"Line":0}},{"line":1687,"address":[],"length":0,"stats":{"Line":0}},{"line":1699,"address":[],"length":0,"stats":{"Line":0}}],"covered":87,"coverable":569},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","mock.rs"],"content":"//! Mock LLM and Embedding provider for testing.\n//!\n//! # OODA-45: E2E Testing Infrastructure\n//! \n//! This module provides deterministic mock providers for E2E testing:\n//! - MockProvider: Basic LLM mock with queue-based responses\n//! - MockAgentProvider: Advanced mock with tool call support for React agent testing\n//!\n//! ## Architecture\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    Mock Provider System                         â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚  MockProvider (basic)     MockAgentProvider (advanced)          â”‚\n//! â”‚  â”œâ”€â”€ add_response()       â”œâ”€â”€ add_response()                    â”‚\n//! â”‚  â””â”€â”€ complete()           â”œâ”€â”€ add_tool_response()               â”‚\n//! â”‚                           â”œâ”€â”€ chat_with_tools()                 â”‚\n//! â”‚                           â””â”€â”€ chat_with_tools_stream()          â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n\nuse async_trait::async_trait;\nuse std::sync::atomic::{AtomicUsize, Ordering};\nuse std::sync::Arc;\nuse tokio::sync::Mutex;\n\nuse crate::error::Result;\nuse crate::traits::{\n    ChatMessage, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse,\n    StreamChunk, ToolCall, ToolChoice, ToolDefinition,\n};\n\n/// Mock LLM provider for testing.\n#[derive(Debug, Clone)]\npub struct MockProvider {\n    responses: Arc<Mutex<Vec<String>>>,\n    embeddings: Arc<Mutex<Vec<Vec<f32>>>>,\n}\n\n/// Advanced mock provider for React agent E2E testing.\n/// \n/// Supports deterministic tool calling responses for testing agent workflows.\n/// \n/// # Example\n/// ```\n/// use edgequake_llm::providers::MockAgentProvider;\n/// use edgequake_llm::traits::ToolCall;\n/// \n/// let provider = MockAgentProvider::new();\n/// // Add a response with tool calls\n/// provider.add_tool_response_sync(\"I'll create that file\", vec![\n///     ToolCall {\n///         id: \"call_1\".to_string(),\n///         call_type: \"function\".to_string(),\n///         function: edgequake_llm::traits::FunctionCall {\n///             name: \"write_file\".to_string(),\n///             arguments: r#\"{\"path\": \"test.txt\", \"content\": \"hello\"}\"#.to_string(),\n///         },\n///     }\n/// ]);\n/// ```\n#[derive(Debug, Clone)]\npub struct MockAgentProvider {\n    /// Queue of responses (content + tool calls)\n    responses: Arc<Mutex<Vec<MockResponse>>>,\n    /// Number of responses consumed\n    call_count: Arc<AtomicUsize>,\n    /// Model name for testing model-specific behavior\n    model_name: String,\n}\n\n/// A mock response containing content and tool calls.\n#[derive(Debug, Clone)]\npub struct MockResponse {\n    pub content: String,\n    pub tool_calls: Vec<ToolCall>,\n}\n\nimpl MockProvider {\n    /// Create a new mock provider with default responses.\n    pub fn new() -> Self {\n        Self {\n            responses: Arc::new(Mutex::new(Vec::new())),\n            embeddings: Arc::new(Mutex::new(vec![\n                vec![0.1; 1536], // Default 1536-dim embedding\n            ])),\n        }\n    }\n\n    /// Add a response to the queue.\n    pub async fn add_response(&self, response: impl Into<String>) {\n        self.responses.lock().await.push(response.into());\n    }\n\n    /// Add an embedding to the queue.\n    pub async fn add_embedding(&self, embedding: Vec<f32>) {\n        self.embeddings.lock().await.push(embedding);\n    }\n}\n\nimpl Default for MockProvider {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for MockProvider {\n    fn name(&self) -> &str {\n        \"mock\"\n    }\n\n    fn model(&self) -> &str {\n        \"mock-model\"\n    }\n\n    fn max_context_length(&self) -> usize {\n        4096\n    }\n\n    async fn complete(&self, _prompt: &str) -> Result<LLMResponse> {\n        let mut responses = self.responses.lock().await;\n        let content = if responses.is_empty() {\n            \"Mock response\".to_string()\n        } else {\n            responses.remove(0)\n        };\n\n        Ok(LLMResponse::new(content, \"mock-model\"))\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        _options: &crate::traits::CompletionOptions,\n    ) -> Result<LLMResponse> {\n        self.complete(prompt).await\n    }\n\n    async fn chat(\n        &self,\n        _messages: &[crate::traits::ChatMessage],\n        _options: Option<&crate::traits::CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.complete(\"\").await\n    }\n\n    async fn stream(\n        &self,\n        prompt: &str,\n    ) -> Result<futures::stream::BoxStream<'static, Result<String>>> {\n        use futures::StreamExt;\n        let response = self.complete(prompt).await?;\n        let stream = futures::stream::iter(vec![Ok(response.content)]);\n        Ok(stream.boxed())\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for MockProvider {\n    fn name(&self) -> &str {\n        \"mock\"\n    }\n\n    fn model(&self) -> &str {\n        \"mock-embedding\"\n    }\n\n    fn dimension(&self) -> usize {\n        1536\n    }\n\n    fn max_tokens(&self) -> usize {\n        512\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        let mut results = Vec::with_capacity(texts.len());\n        for _ in texts {\n            let mut embeddings = self.embeddings.lock().await;\n            let emb = if embeddings.is_empty() {\n                vec![0.1; 1536]\n            } else {\n                embeddings.remove(0)\n            };\n            results.push(emb);\n        }\n        Ok(results)\n    }\n}\n\n// ============================================================================\n// MockAgentProvider - Advanced E2E Testing Provider\n// ============================================================================\n\nimpl MockAgentProvider {\n    /// Create a new mock agent provider.\n    pub fn new() -> Self {\n        Self {\n            responses: Arc::new(Mutex::new(Vec::new())),\n            call_count: Arc::new(AtomicUsize::new(0)),\n            model_name: \"mock-agent\".to_string(),\n        }\n    }\n\n    /// Create with a specific model name for testing model-specific behavior.\n    pub fn with_model(model_name: impl Into<String>) -> Self {\n        Self {\n            responses: Arc::new(Mutex::new(Vec::new())),\n            call_count: Arc::new(AtomicUsize::new(0)),\n            model_name: model_name.into(),\n        }\n    }\n\n    /// Add a text-only response (no tool calls).\n    pub async fn add_response(&self, content: impl Into<String>) {\n        self.responses.lock().await.push(MockResponse {\n            content: content.into(),\n            tool_calls: vec![],\n        });\n    }\n\n    /// Add a response with tool calls.\n    pub async fn add_tool_response(&self, content: impl Into<String>, tool_calls: Vec<ToolCall>) {\n        self.responses.lock().await.push(MockResponse {\n            content: content.into(),\n            tool_calls,\n        });\n    }\n\n    /// Synchronous version of add_response for test setup.\n    pub fn add_response_sync(&self, content: impl Into<String>) {\n        // Use try_lock for synchronous contexts - should always succeed in test setup\n        if let Ok(mut responses) = self.responses.try_lock() {\n            responses.push(MockResponse {\n                content: content.into(),\n                tool_calls: vec![],\n            });\n        }\n    }\n\n    /// Synchronous version of add_tool_response for test setup.\n    pub fn add_tool_response_sync(&self, content: impl Into<String>, tool_calls: Vec<ToolCall>) {\n        if let Ok(mut responses) = self.responses.try_lock() {\n            responses.push(MockResponse {\n                content: content.into(),\n                tool_calls,\n            });\n        }\n    }\n\n    /// Get the number of responses consumed.\n    pub fn call_count(&self) -> usize {\n        self.call_count.load(Ordering::SeqCst)\n    }\n\n    /// Check if all queued responses have been consumed.\n    pub async fn is_exhausted(&self) -> bool {\n        self.responses.lock().await.is_empty()\n    }\n\n    /// Get next response from queue.\n    async fn next_response(&self) -> MockResponse {\n        self.call_count.fetch_add(1, Ordering::SeqCst);\n        let mut responses = self.responses.lock().await;\n        if responses.is_empty() {\n            // Default: task_complete with success message\n            MockResponse {\n                content: \"Task completed successfully.\".to_string(),\n                tool_calls: vec![ToolCall {\n                    id: format!(\"call_{}\", self.call_count.load(Ordering::SeqCst)),\n                    call_type: \"function\".to_string(),\n                    function: crate::traits::FunctionCall {\n                        name: \"task_complete\".to_string(),\n                        arguments: r#\"{\"result\": \"success\"}\"#.to_string(),\n                    },\n                }],\n            }\n        } else {\n            responses.remove(0)\n        }\n    }\n}\n\nimpl Default for MockAgentProvider {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for MockAgentProvider {\n    fn name(&self) -> &str {\n        \"mock-agent\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model_name\n    }\n\n    fn max_context_length(&self) -> usize {\n        128_000 // Simulate large context window\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        true\n    }\n\n    async fn complete(&self, _prompt: &str) -> Result<LLMResponse> {\n        let response = self.next_response().await;\n        Ok(LLMResponse::new(response.content, &self.model_name))\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        _options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        self.complete(prompt).await\n    }\n\n    async fn chat(\n        &self,\n        _messages: &[ChatMessage],\n        _options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.complete(\"\").await\n    }\n\n    async fn chat_with_tools(\n        &self,\n        _messages: &[ChatMessage],\n        _tools: &[ToolDefinition],\n        _tool_choice: Option<ToolChoice>,\n        _options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let mock_response = self.next_response().await;\n        let mut response = LLMResponse::new(mock_response.content, &self.model_name);\n        response.tool_calls = mock_response.tool_calls;\n        Ok(response)\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        _messages: &[ChatMessage],\n        _tools: &[ToolDefinition],\n        _tool_choice: Option<ToolChoice>,\n        _options: Option<&CompletionOptions>,\n    ) -> Result<futures::stream::BoxStream<'static, Result<StreamChunk>>> {\n        use futures::StreamExt;\n\n        let mock_response = self.next_response().await;\n        \n        // Build stream chunks that simulate real streaming behavior\n        // StreamChunk is an enum with Content, ToolCallDelta, and Finished variants\n        let mut chunks = Vec::new();\n        \n        // Content chunk (if any)\n        if !mock_response.content.is_empty() {\n            chunks.push(Ok(StreamChunk::Content(mock_response.content.clone())));\n        }\n        \n        // Tool call chunks (if any) - emit one delta per tool call\n        for (index, tool_call) in mock_response.tool_calls.iter().enumerate() {\n            chunks.push(Ok(StreamChunk::ToolCallDelta {\n                index,\n                id: Some(tool_call.id.clone()),\n                function_name: Some(tool_call.function.name.clone()),\n                function_arguments: Some(tool_call.function.arguments.clone()),\n            }));\n        }\n        \n        // Final chunk with finish reason\n        chunks.push(Ok(StreamChunk::Finished {\n            reason: \"stop\".to_string(),\n            ttft_ms: None,\n        }));\n        \n        let stream = futures::stream::iter(chunks);\n        Ok(stream.boxed())\n    }\n\n    async fn stream(\n        &self,\n        prompt: &str,\n    ) -> Result<futures::stream::BoxStream<'static, Result<String>>> {\n        use futures::StreamExt;\n        let response = self.complete(prompt).await?;\n        let stream = futures::stream::iter(vec![Ok(response.content)]);\n        Ok(stream.boxed())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::traits::FunctionCall;\n\n    #[tokio::test]\n    async fn test_mock_provider() {\n        let provider = MockProvider::new();\n        provider.add_response(\"This is a mock response.\").await;\n\n        // Test LLM\n        let response = provider.complete(\"test\").await.unwrap();\n        assert_eq!(response.content, \"This is a mock response.\");\n\n        // Test embedding\n        let embedding = provider.embed_one(\"test\").await.unwrap();\n        assert_eq!(embedding.len(), 1536);\n    }\n\n    #[tokio::test]\n    async fn test_custom_responses() {\n        let provider = MockProvider::new();\n        provider.add_response(\"Custom response\").await;\n\n        let response = provider.complete(\"test\").await.unwrap();\n        assert_eq!(response.content, \"Custom response\");\n    }\n\n    // ========================================================================\n    // MockAgentProvider Tests\n    // ========================================================================\n\n    #[tokio::test]\n    async fn test_mock_agent_provider_basic() {\n        let provider = MockAgentProvider::new();\n        provider.add_response(\"Hello from mock agent\").await;\n\n        let response = provider.complete(\"test\").await.unwrap();\n        assert_eq!(response.content, \"Hello from mock agent\");\n        assert_eq!(provider.call_count(), 1);\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_provider_with_tools() {\n        let provider = MockAgentProvider::new();\n        \n        // Add response with tool call\n        provider.add_tool_response(\n            \"I'll create that file for you.\",\n            vec![ToolCall {\n                id: \"call_1\".to_string(),\n                call_type: \"function\".to_string(),\n                function: FunctionCall {\n                    name: \"write_file\".to_string(),\n                    arguments: r#\"{\"path\": \"test.txt\", \"content\": \"hello world\"}\"#.to_string(),\n                },\n            }],\n        ).await;\n\n        let response = provider.chat_with_tools(\n            &[ChatMessage::user(\"Create test.txt\")],\n            &[],\n            None,\n            None,\n        ).await.unwrap();\n\n        assert!(response.content.contains(\"create that file\"));\n        assert!(!response.tool_calls.is_empty());\n        assert_eq!(response.tool_calls.len(), 1);\n        assert_eq!(response.tool_calls[0].function.name, \"write_file\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_provider_stream() {\n        use futures::StreamExt;\n\n        let provider = MockAgentProvider::new();\n        provider.add_tool_response(\n            \"Creating file...\",\n            vec![ToolCall {\n                id: \"call_1\".to_string(),\n                call_type: \"function\".to_string(),\n                function: FunctionCall {\n                    name: \"write_file\".to_string(),\n                    arguments: r#\"{\"path\": \"test.txt\", \"content\": \"hello\"}\"#.to_string(),\n                },\n            }],\n        ).await;\n\n        let mut stream = provider.chat_with_tools_stream(\n            &[ChatMessage::user(\"Create file\")],\n            &[],\n            None,\n            None,\n        ).await.unwrap();\n\n        let mut content = String::new();\n        let mut tool_call_count = 0;\n        let mut finish_reason = None;\n\n        while let Some(chunk_result) = stream.next().await {\n            let chunk = chunk_result.unwrap();\n            match chunk {\n                StreamChunk::Content(delta) => content.push_str(&delta),\n                StreamChunk::ThinkingContent { text, .. } => content.push_str(&text),\n                StreamChunk::ToolCallDelta { .. } => tool_call_count += 1,\n                StreamChunk::Finished { reason, .. } => finish_reason = Some(reason),\n            }\n        }\n\n        assert_eq!(content, \"Creating file...\");\n        assert_eq!(tool_call_count, 1);\n        assert_eq!(finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_default_task_complete() {\n        // When queue is empty, should return task_complete\n        let provider = MockAgentProvider::new();\n        \n        let response = provider.chat_with_tools(\n            &[ChatMessage::user(\"Do something\")],\n            &[],\n            None,\n            None,\n        ).await.unwrap();\n\n        assert!(!response.tool_calls.is_empty());\n        assert_eq!(response.tool_calls[0].function.name, \"task_complete\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_sync_setup() {\n        let provider = MockAgentProvider::new();\n        \n        // Use sync methods for test setup\n        provider.add_response_sync(\"Sync response 1\");\n        provider.add_tool_response_sync(\"With tools\", vec![ToolCall {\n            id: \"call_1\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"read_file\".to_string(),\n                arguments: r#\"{\"path\": \"test.txt\"}\"#.to_string(),\n            },\n        }]);\n\n        let r1 = provider.complete(\"test\").await.unwrap();\n        assert_eq!(r1.content, \"Sync response 1\");\n\n        let r2 = provider.chat_with_tools(&[], &[], None, None).await.unwrap();\n        assert_eq!(r2.content, \"With tools\");\n        assert!(!r2.tool_calls.is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_model_specific() {\n        let provider = MockAgentProvider::with_model(\"claude-sonnet-4-20250514\");\n        assert_eq!(provider.model(), \"claude-sonnet-4-20250514\");\n        \n        let provider2 = MockAgentProvider::with_model(\"gpt-4-turbo\");\n        assert_eq!(provider2.model(), \"gpt-4-turbo\");\n    }\n\n    // ---- Iteration 25: Additional mock provider tests ----\n\n    #[test]\n    fn test_mock_provider_default() {\n        let p = MockProvider::default();\n        assert_eq!(LLMProvider::name(&p), \"mock\");\n        assert_eq!(LLMProvider::model(&p), \"mock-model\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_default_response_when_empty() {\n        let p = MockProvider::new();\n        // No responses queued â†’ returns \"Mock response\"\n        let resp = p.complete(\"anything\").await.unwrap();\n        assert_eq!(resp.content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_multiple_responses_fifo() {\n        let p = MockProvider::new();\n        p.add_response(\"first\").await;\n        p.add_response(\"second\").await;\n        p.add_response(\"third\").await;\n\n        assert_eq!(p.complete(\"\").await.unwrap().content, \"first\");\n        assert_eq!(p.complete(\"\").await.unwrap().content, \"second\");\n        assert_eq!(p.complete(\"\").await.unwrap().content, \"third\");\n        // Queue empty â†’ default\n        assert_eq!(p.complete(\"\").await.unwrap().content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_custom_embedding() {\n        let p = MockProvider::new();\n        // MockProvider::new() starts with one default [0.1; 1536] embedding.\n        // Consume the default first.\n        let _ = p.embed(&[\"consume_default\".to_string()]).await.unwrap();\n        // Now add custom and it will be next\n        p.add_embedding(vec![1.0, 2.0, 3.0]).await;\n\n        let embs = p.embed(&[\"hello\".to_string()]).await.unwrap();\n        assert_eq!(embs[0], vec![1.0, 2.0, 3.0]);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_embed_multiple_texts() {\n        let p = MockProvider::new();\n        // Default embedding is used when queue is empty\n        let embs = p\n            .embed(&[\"a\".to_string(), \"b\".to_string()])\n            .await\n            .unwrap();\n        assert_eq!(embs.len(), 2);\n        assert_eq!(embs[0].len(), 1536);\n        assert_eq!(embs[1].len(), 1536);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_embedding_provider_trait() {\n        let p = MockProvider::new();\n        assert_eq!(EmbeddingProvider::name(&p), \"mock\");\n        assert_eq!(EmbeddingProvider::model(&p), \"mock-embedding\");\n        assert_eq!(p.dimension(), 1536);\n        assert_eq!(EmbeddingProvider::max_tokens(&p), 512);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_max_context_length() {\n        let p = MockProvider::new();\n        assert_eq!(p.max_context_length(), 4096);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_chat_delegation() {\n        let p = MockProvider::new();\n        p.add_response(\"chat response\").await;\n        let resp = p\n            .chat(&[ChatMessage::user(\"hi\")], None)\n            .await\n            .unwrap();\n        assert_eq!(resp.content, \"chat response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_complete_with_options() {\n        let p = MockProvider::new();\n        p.add_response(\"opts response\").await;\n        let opts = CompletionOptions::with_temperature(0.5);\n        let resp = p.complete_with_options(\"prompt\", &opts).await.unwrap();\n        assert_eq!(resp.content, \"opts response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_stream() {\n        use futures::StreamExt;\n        let p = MockProvider::new();\n        p.add_response(\"streamed\").await;\n        let mut stream = p.stream(\"test\").await.unwrap();\n        let chunk = stream.next().await.unwrap().unwrap();\n        assert_eq!(chunk, \"streamed\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_default_impl() {\n        let p = MockAgentProvider::default();\n        assert_eq!(LLMProvider::name(&p), \"mock-agent\");\n        assert_eq!(p.model(), \"mock-agent\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_supports_traits() {\n        let p = MockAgentProvider::new();\n        assert!(p.supports_streaming());\n        assert!(p.supports_tool_streaming());\n        assert!(p.supports_function_calling());\n        assert_eq!(p.max_context_length(), 128_000);\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_call_count_tracking() {\n        let p = MockAgentProvider::new();\n        assert_eq!(p.call_count(), 0);\n\n        p.add_response(\"a\").await;\n        p.add_response(\"b\").await;\n        p.complete(\"\").await.unwrap();\n        assert_eq!(p.call_count(), 1);\n        p.complete(\"\").await.unwrap();\n        assert_eq!(p.call_count(), 2);\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_is_exhausted() {\n        let p = MockAgentProvider::new();\n        assert!(p.is_exhausted().await);\n\n        p.add_response(\"one\").await;\n        assert!(!p.is_exhausted().await);\n\n        p.complete(\"\").await.unwrap();\n        assert!(p.is_exhausted().await);\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_chat_delegation() {\n        let p = MockAgentProvider::new();\n        p.add_response(\"agent chat\").await;\n        let resp = p\n            .chat(&[ChatMessage::user(\"hi\")], None)\n            .await\n            .unwrap();\n        assert_eq!(resp.content, \"agent chat\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_agent_complete_with_options() {\n        let p = MockAgentProvider::new();\n        p.add_response(\"agent opts\").await;\n        let opts = CompletionOptions::default();\n        let resp = p.complete_with_options(\"prompt\", &opts).await.unwrap();\n        assert_eq!(resp.content, \"agent opts\");\n    }\n}\n","traces":[{"line":81,"address":[],"length":0,"stats":{"Line":100}},{"line":83,"address":[],"length":0,"stats":{"Line":400}},{"line":84,"address":[],"length":0,"stats":{"Line":300}},{"line":91,"address":[],"length":0,"stats":{"Line":66}},{"line":92,"address":[],"length":0,"stats":{"Line":165}},{"line":96,"address":[],"length":0,"stats":{"Line":2}},{"line":97,"address":[],"length":0,"stats":{"Line":4}},{"line":102,"address":[],"length":0,"stats":{"Line":2}},{"line":103,"address":[],"length":0,"stats":{"Line":2}},{"line":109,"address":[],"length":0,"stats":{"Line":21}},{"line":110,"address":[],"length":0,"stats":{"Line":21}},{"line":113,"address":[],"length":0,"stats":{"Line":6}},{"line":114,"address":[],"length":0,"stats":{"Line":6}},{"line":117,"address":[],"length":0,"stats":{"Line":5}},{"line":118,"address":[],"length":0,"stats":{"Line":5}},{"line":121,"address":[],"length":0,"stats":{"Line":43}},{"line":161,"address":[],"length":0,"stats":{"Line":8}},{"line":162,"address":[],"length":0,"stats":{"Line":8}},{"line":165,"address":[],"length":0,"stats":{"Line":3}},{"line":166,"address":[],"length":0,"stats":{"Line":3}},{"line":169,"address":[],"length":0,"stats":{"Line":11}},{"line":170,"address":[],"length":0,"stats":{"Line":11}},{"line":173,"address":[],"length":0,"stats":{"Line":3}},{"line":174,"address":[],"length":0,"stats":{"Line":3}},{"line":177,"address":[],"length":0,"stats":{"Line":17}},{"line":198,"address":[],"length":0,"stats":{"Line":12}},{"line":200,"address":[],"length":0,"stats":{"Line":48}},{"line":201,"address":[],"length":0,"stats":{"Line":36}},{"line":202,"address":[],"length":0,"stats":{"Line":12}},{"line":207,"address":[],"length":0,"stats":{"Line":2}},{"line":209,"address":[],"length":0,"stats":{"Line":8}},{"line":210,"address":[],"length":0,"stats":{"Line":6}},{"line":211,"address":[],"length":0,"stats":{"Line":2}},{"line":216,"address":[],"length":0,"stats":{"Line":14}},{"line":217,"address":[],"length":0,"stats":{"Line":28}},{"line":218,"address":[],"length":0,"stats":{"Line":14}},{"line":219,"address":[],"length":0,"stats":{"Line":7}},{"line":224,"address":[],"length":0,"stats":{"Line":4}},{"line":225,"address":[],"length":0,"stats":{"Line":8}},{"line":226,"address":[],"length":0,"stats":{"Line":4}},{"line":227,"address":[],"length":0,"stats":{"Line":2}},{"line":232,"address":[],"length":0,"stats":{"Line":1}},{"line":234,"address":[],"length":0,"stats":{"Line":3}},{"line":235,"address":[],"length":0,"stats":{"Line":3}},{"line":236,"address":[],"length":0,"stats":{"Line":2}},{"line":237,"address":[],"length":0,"stats":{"Line":1}},{"line":243,"address":[],"length":0,"stats":{"Line":1}},{"line":244,"address":[],"length":0,"stats":{"Line":3}},{"line":245,"address":[],"length":0,"stats":{"Line":3}},{"line":246,"address":[],"length":0,"stats":{"Line":2}},{"line":247,"address":[],"length":0,"stats":{"Line":1}},{"line":253,"address":[],"length":0,"stats":{"Line":4}},{"line":254,"address":[],"length":0,"stats":{"Line":8}},{"line":258,"address":[],"length":0,"stats":{"Line":6}},{"line":259,"address":[],"length":0,"stats":{"Line":9}},{"line":263,"address":[],"length":0,"stats":{"Line":24}},{"line":264,"address":[],"length":0,"stats":{"Line":24}},{"line":265,"address":[],"length":0,"stats":{"Line":24}},{"line":266,"address":[],"length":0,"stats":{"Line":12}},{"line":269,"address":[],"length":0,"stats":{"Line":3}},{"line":270,"address":[],"length":0,"stats":{"Line":2}},{"line":280,"address":[],"length":0,"stats":{"Line":11}},{"line":286,"address":[],"length":0,"stats":{"Line":1}},{"line":287,"address":[],"length":0,"stats":{"Line":1}},{"line":293,"address":[],"length":0,"stats":{"Line":1}},{"line":294,"address":[],"length":0,"stats":{"Line":1}},{"line":297,"address":[],"length":0,"stats":{"Line":3}},{"line":298,"address":[],"length":0,"stats":{"Line":3}},{"line":301,"address":[],"length":0,"stats":{"Line":1}},{"line":302,"address":[],"length":0,"stats":{"Line":1}},{"line":305,"address":[],"length":0,"stats":{"Line":1}},{"line":306,"address":[],"length":0,"stats":{"Line":1}},{"line":309,"address":[],"length":0,"stats":{"Line":1}},{"line":310,"address":[],"length":0,"stats":{"Line":1}},{"line":313,"address":[],"length":0,"stats":{"Line":1}},{"line":314,"address":[],"length":0,"stats":{"Line":1}},{"line":317,"address":[],"length":0,"stats":{"Line":7}}],"covered":77,"coverable":77},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","mod.rs"],"content":"//! LLM provider implementations.\n\npub mod openai;\n\npub mod mock;\n// OODA-45: Export MockAgentProvider for E2E testing\npub use mock::{MockAgentProvider, MockProvider, MockResponse};\n\npub mod gemini;\n\npub mod azure_openai;\n\npub mod jina;\n\npub mod ollama;\n\npub mod lmstudio;\n\npub mod vscode;\n\n// OODA-01: Anthropic (Claude) provider\npub mod anthropic;\npub use anthropic::AnthropicProvider;\n\n// OODA-02: OpenRouter provider (200+ models)\npub mod openrouter;\npub use openrouter::OpenRouterProvider;\n\n// OODA-200: Configurable OpenAI-compatible provider\npub mod openai_compatible;\npub use openai_compatible::OpenAICompatibleProvider;\n\n// OODA-71: xAI Grok provider (api.x.ai)\npub mod xai;\npub use xai::XAIProvider;\n\n// OODA-80: HuggingFace Hub provider (open-source models)\npub mod huggingface;\npub use huggingface::HuggingFaceProvider;\n\n// OODA-LOG-03: Tracing wrapper for GenAI observability\npub mod tracing;\npub use self::tracing::TracingProvider;\n\n// OODA-LOG-11: GenAI event emission for OpenTelemetry\npub mod genai_events;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","ollama.rs"],"content":"//! Ollama provider implementation.\n//!\n//! This module provides integration with Ollama's local LLM API.\n//! Ollama provides an OpenAI-compatible API, so this provider wraps\n//! the functionality with Ollama-specific defaults.\n//!\n//! # Default Configuration\n//!\n//! - Base URL: `http://localhost:11434`\n//! - Default model: `gemma3:12b` (chat), `embeddinggemma:latest` (embeddings, 768 dimensions)\n//!\n//! # Environment Variables\n//!\n//! - `OLLAMA_HOST`: Ollama server URL (default: http://localhost:11434)\n//! - `OLLAMA_MODEL`: Default chat model\n//! - `OLLAMA_EMBEDDING_MODEL`: Default embedding model\n//!\n//! # Example\n//!\n//! ```rust,ignore\n//! use edgequake_llm::OllamaProvider;\n//!\n//! // Connect to local Ollama with defaults\n//! let provider = OllamaProvider::from_env()?;\n//!\n//! // Or specify custom settings\n//! let provider = OllamaProvider::builder()\n//!     .host(\"http://localhost:11434\")\n//!     .model(\"mistral\")\n//!     .embedding_model(\"nomic-embed-text\")\n//!     .build()?;\n//! ```\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse tracing::debug;\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, FunctionCall, LLMProvider,\n    LLMResponse, StreamChunk as TraitStreamChunk, ToolCall, ToolChoice, ToolDefinition,\n};\n\n/// Default Ollama host URL\nconst DEFAULT_OLLAMA_HOST: &str = \"http://localhost:11434\";\n\n/// Default Ollama chat model\nconst DEFAULT_OLLAMA_MODEL: &str = \"gemma3:12b\";\n\n/// Default Ollama embedding model\nconst DEFAULT_OLLAMA_EMBEDDING_MODEL: &str = \"embeddinggemma:latest\";\n\n/// Ollama LLM and embedding provider.\n///\n/// Provides integration with locally running Ollama instance.\n/// Supports both chat completion and text embeddings.\n#[derive(Debug, Clone)]\npub struct OllamaProvider {\n    client: Client,\n    host: String,\n    model: String,\n    embedding_model: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n}\n\n/// Builder for OllamaProvider\n#[derive(Debug, Clone)]\npub struct OllamaProviderBuilder {\n    host: String,\n    model: String,\n    embedding_model: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n}\n\nimpl Default for OllamaProviderBuilder {\n    fn default() -> Self {\n        Self {\n            host: DEFAULT_OLLAMA_HOST.to_string(),\n            model: DEFAULT_OLLAMA_MODEL.to_string(),\n            embedding_model: DEFAULT_OLLAMA_EMBEDDING_MODEL.to_string(),\n            max_context_length: 131072, // OODA-99: Increased to 128K (131072)\n            embedding_dimension: 768, // embeddinggemma:latest default (VERIFIED via Ollama API)\n        }\n    }\n}\n\nimpl OllamaProviderBuilder {\n    /// Create a new builder with default settings\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Set the Ollama host URL\n    pub fn host(mut self, host: impl Into<String>) -> Self {\n        self.host = host.into();\n        self\n    }\n\n    /// Set the chat model\n    pub fn model(mut self, model: impl Into<String>) -> Self {\n        self.model = model.into();\n        self\n    }\n\n    /// Set the embedding model\n    pub fn embedding_model(mut self, model: impl Into<String>) -> Self {\n        self.embedding_model = model.into();\n        self\n    }\n\n    /// Set the maximum context length\n    pub fn max_context_length(mut self, length: usize) -> Self {\n        self.max_context_length = length;\n        self\n    }\n\n    /// Set the embedding dimension\n    pub fn embedding_dimension(mut self, dimension: usize) -> Self {\n        self.embedding_dimension = dimension;\n        self\n    }\n\n    /// Build the OllamaProvider\n    pub fn build(self) -> Result<OllamaProvider> {\n        let client = Client::builder()\n            .timeout(std::time::Duration::from_secs(300)) // Longer timeout for local models\n            .no_proxy() // CRITICAL: Disable all proxies for localhost connections\n            .build()\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        Ok(OllamaProvider {\n            client,\n            host: self.host,\n            model: self.model,\n            embedding_model: self.embedding_model,\n            max_context_length: self.max_context_length,\n            embedding_dimension: self.embedding_dimension,\n        })\n    }\n}\n\nimpl OllamaProvider {\n    /// Create a new OllamaProvider from environment variables.\n    ///\n    /// Environment variables:\n    /// - `OLLAMA_HOST`: Server URL (default: http://localhost:11434)\n    /// - `OLLAMA_MODEL`: Chat model (default: llama3)\n    /// - `OLLAMA_EMBEDDING_MODEL`: Embedding model (default: nomic-embed-text)\n    /// - `OLLAMA_CONTEXT_LENGTH`: Max context length (default: 131072 = 128K)\n    ///\n    /// # OODA-99: Context Length Configuration\n    ///\n    /// Default is 128K which works for most modern models. Override if needed:\n    /// Example: `export OLLAMA_CONTEXT_LENGTH=65536` for 64K\n    pub fn from_env() -> Result<Self> {\n        let host = std::env::var(\"OLLAMA_HOST\").unwrap_or_else(|_| DEFAULT_OLLAMA_HOST.to_string());\n\n        let model =\n            std::env::var(\"OLLAMA_MODEL\").unwrap_or_else(|_| DEFAULT_OLLAMA_MODEL.to_string());\n\n        let embedding_model = std::env::var(\"OLLAMA_EMBEDDING_MODEL\")\n            .unwrap_or_else(|_| DEFAULT_OLLAMA_EMBEDDING_MODEL.to_string());\n\n        // OODA-99: Allow context length override, default 128K\n        let max_context_length = std::env::var(\"OLLAMA_CONTEXT_LENGTH\")\n            .ok()\n            .and_then(|s| s.parse().ok())\n            .unwrap_or(131072);\n\n        OllamaProviderBuilder::new()\n            .host(host)\n            .model(model)\n            .embedding_model(embedding_model)\n            .max_context_length(max_context_length)\n            .build()\n    }\n\n    /// Create a new builder for OllamaProvider\n    pub fn builder() -> OllamaProviderBuilder {\n        OllamaProviderBuilder::new()\n    }\n\n    /// Create with default settings (localhost:11434)\n    pub fn default_local() -> Result<Self> {\n        OllamaProviderBuilder::new().build()\n    }\n}\n\n// Ollama API request/response structures\n\n#[derive(Debug, Serialize)]\nstruct ChatRequest {\n    model: String,\n    messages: Vec<OllamaMessage>,\n    stream: bool,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    options: Option<ChatOptions>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<OllamaTool>>,  // OODA-09: Tool support\n    /// OODA-29: Enable thinking for reasoning models (deepseek-r1, qwen3)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    think: Option<bool>,\n}\n\n#[derive(Debug, Serialize)]\nstruct OllamaMessage {\n    role: String,\n    content: String,\n}\n\n#[derive(Debug, Serialize)]\nstruct ChatOptions {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    num_predict: Option<i32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stop: Option<Vec<String>>,\n    /// Context window size in tokens sent to Ollama (default: provider's max_context_length)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    num_ctx: Option<usize>,\n}\n\n#[allow(dead_code)]\n#[derive(Debug, Deserialize)]\nstruct ChatResponse {\n    model: String,\n    message: ResponseMessage,\n    done: bool,\n    #[serde(default)]\n    total_duration: u64,\n    #[serde(default)]\n    prompt_eval_count: u32,\n    #[serde(default)]\n    eval_count: u32,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ResponseMessage {\n    #[allow(dead_code)]\n    role: String,\n    content: String,\n    /// OODA-29: Thinking content for reasoning models (deepseek-r1, qwen3)\n    #[serde(default)]\n    thinking: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_calls: Option<Vec<OllamaToolCall>>,  // OODA-09: Tool calls in response\n}\n\n// OODA-09: Renamed to avoid conflict with traits::StreamChunk\n#[derive(Debug, Deserialize)]\nstruct OllamaStreamChunk {\n    #[serde(default)]\n    message: Option<ResponseMessage>,\n    #[allow(dead_code)]\n    done: bool,\n}\n\n// ============================================================================\n// OODA-09: Tool Calling Types\n// ============================================================================\n\n/// Tool definition for Ollama (OpenAI-compatible format)\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OllamaTool {\n    pub r#type: String,\n    pub function: OllamaFunction,\n}\n\n/// Function definition within a tool\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OllamaFunction {\n    pub name: String,\n    pub description: String,\n    pub parameters: serde_json::Value,\n}\n\n/// Tool call in response\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OllamaToolCall {\n    pub function: OllamaFunctionCall,\n}\n\n/// Function call details\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct OllamaFunctionCall {\n    pub name: String,\n    pub arguments: serde_json::Value,\n}\n\n#[derive(Debug, Serialize)]\nstruct EmbeddingRequest {\n    model: String,\n    input: Vec<String>,\n}\n\n#[allow(dead_code)]\n#[derive(Debug, Deserialize)]\nstruct EmbeddingResponse {\n    embeddings: Vec<Vec<f32>>,\n}\n\n// ============================================================================\n// OODA-78: List Models API Types\n// ============================================================================\n\n/// Response from GET /api/tags endpoint.\n///\n/// Lists all locally available Ollama models.\n#[derive(Debug, Clone, Deserialize)]\npub struct OllamaModelsResponse {\n    /// Array of available models.\n    pub models: Vec<OllamaModelInfo>,\n}\n\n/// Model details from Ollama API.\n#[derive(Debug, Clone, Deserialize)]\npub struct OllamaModelDetails {\n    /// Parent model (if any).\n    #[serde(default)]\n    pub parent_model: String,\n    /// Model format (usually \"gguf\").\n    #[serde(default)]\n    pub format: String,\n    /// Model family (e.g., \"llama\", \"qwen2\").\n    #[serde(default)]\n    pub family: String,\n    /// List of model families.\n    #[serde(default)]\n    pub families: Vec<String>,\n    /// Parameter size (e.g., \"7.6B\").\n    #[serde(default)]\n    pub parameter_size: String,\n    /// Quantization level (e.g., \"Q4_K_M\").\n    #[serde(default)]\n    pub quantization_level: String,\n}\n\n/// Individual model info from Ollama API.\n#[derive(Debug, Clone, Deserialize)]\npub struct OllamaModelInfo {\n    /// Model name (e.g., \"llama3.2:latest\").\n    pub name: String,\n    /// Model identifier.\n    #[serde(default)]\n    pub model: String,\n    /// Last modified timestamp.\n    #[serde(default)]\n    pub modified_at: String,\n    /// Model size in bytes.\n    #[serde(default)]\n    pub size: u64,\n    /// Model digest.\n    #[serde(default)]\n    pub digest: String,\n    /// Model details.\n    #[serde(default)]\n    pub details: Option<OllamaModelDetails>,\n}\n\nimpl OllamaProvider {\n    fn convert_role(role: &ChatRole) -> &'static str {\n        match role {\n            ChatRole::System => \"system\",\n            ChatRole::User => \"user\",\n            ChatRole::Assistant => \"assistant\",\n            ChatRole::Tool | ChatRole::Function => \"user\", // Ollama doesn't have tool/function role\n        }\n    }\n\n    fn convert_messages(messages: &[ChatMessage]) -> Vec<OllamaMessage> {\n        messages\n            .iter()\n            .map(|msg| OllamaMessage {\n                role: Self::convert_role(&msg.role).to_string(),\n                content: msg.content.clone(),\n            })\n            .collect()\n    }\n    \n    /// Convert tool definitions to Ollama's OpenAI-compatible format.\n    ///\n    /// # OODA-09: Tool Calling Support\n    ///\n    /// Ollama uses the OpenAI-compatible tools format:\n    /// ```json\n    /// {\n    ///   \"type\": \"function\",\n    ///   \"function\": {\n    ///     \"name\": \"tool_name\",\n    ///     \"description\": \"what it does\",\n    ///     \"parameters\": { ... JSON Schema ... }\n    ///   }\n    /// }\n    /// ```\n    fn convert_tools(tools: &[ToolDefinition]) -> Vec<OllamaTool> {\n        tools\n            .iter()\n            .map(|tool| OllamaTool {\n                r#type: \"function\".to_string(),\n                function: OllamaFunction {\n                    name: tool.function.name.clone(),\n                    description: tool.function.description.clone(),\n                    parameters: tool.function.parameters.clone(),\n                },\n            })\n            .collect()\n    }\n\n    /// List locally available Ollama models.\n    ///\n    /// # OODA-78: Dynamic Model Discovery\n    ///\n    /// Fetches the list of models currently installed on the Ollama server\n    /// via the GET /api/tags endpoint. This enables dynamic model selection\n    /// instead of relying on a static registry.\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let provider = OllamaProvider::default_local()?;\n    /// let models = provider.list_models().await?;\n    /// for model in models.models {\n    ///     println!(\"Available: {} ({})\", model.name, model.details.unwrap().parameter_size);\n    /// }\n    /// ```\n    pub async fn list_models(&self) -> Result<OllamaModelsResponse> {\n        let url = format!(\"{}/api/tags\", self.host);\n\n        debug!(url = %url, \"Fetching Ollama models list\");\n\n        let response = self\n            .client\n            .get(&url)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Ollama API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let models: OllamaModelsResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to parse models response: {}\", e)))?;\n\n        debug!(\"Ollama returned {} models\", models.models.len());\n\n        Ok(models)\n    }\n\n    /// Get host URL for this provider.\n    pub fn host(&self) -> &str {\n        &self.host\n    }\n\n    /// OODA-29: Check if model supports thinking/reasoning.\n    ///\n    /// Returns true for models known to support the `think` parameter:\n    /// - DeepSeek R1 models (deepseek-r1)\n    /// - Qwen 3 models (qwen3)\n    /// - Other thinking-tagged models\n    fn is_thinking_model(model: &str) -> bool {\n        let model_lower = model.to_lowercase();\n        model_lower.contains(\"deepseek-r1\")\n            || model_lower.contains(\"qwen3\")\n            || model_lower.contains(\"qwq\")\n            || model_lower.contains(\"openthinker\")\n            || model_lower.contains(\"phi4-reasoning\")\n            || model_lower.contains(\"magistral\")\n            || model_lower.contains(\"cogito\")\n            || model_lower.contains(\"gpt-oss\")  // OpenAI open-weight reasoning\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OllamaProvider {\n    fn name(&self) -> &str {\n        \"ollama\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        debug!(\n            \"Ollama chat request: {} messages to model {}\",\n            messages.len(),\n            self.model\n        );\n\n        let url = format!(\"{}/api/chat\", self.host);\n        let opts = options.cloned().unwrap_or_default();\n\n        let chat_options = ChatOptions {\n            temperature: opts.temperature,\n            num_predict: opts.max_tokens.map(|t| t as i32),\n            stop: opts.stop.clone(),\n            num_ctx: Some(self.max_context_length),\n        };\n\n        // OODA-29: Enable thinking for reasoning models\n        let think = Self::is_thinking_model(&self.model);\n\n        let request = ChatRequest {\n            model: self.model.clone(),\n            messages: Self::convert_messages(messages),\n            stream: false,\n            options: Some(chat_options),\n            tools: None,  // OODA-09: No tools for basic chat\n            think: if think { Some(true) } else { None },\n        };\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Ollama API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let response: ChatResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to parse response: {}\", e)))?;\n\n        // OODA-29: Build response with thinking content if available\n        let mut llm_response = LLMResponse::new(response.message.content, response.model)\n            .with_usage(\n                response.prompt_eval_count as usize,\n                response.eval_count as usize,\n            );\n\n        // Add thinking content if present\n        if let Some(thinking) = &response.message.thinking {\n            if !thinking.is_empty() {\n                llm_response = llm_response.with_thinking_content(thinking.clone());\n                // Estimate thinking tokens (rough: ~4 chars per token)\n                let thinking_tokens = thinking.len() / 4;\n                llm_response = llm_response.with_thinking_tokens(thinking_tokens);\n            }\n        }\n\n        Ok(llm_response)\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        use futures::StreamExt;\n\n        debug!(\"Ollama stream request: prompt to model {}\", self.model);\n\n        let url = format!(\"{}/api/chat\", self.host);\n\n        let chat_options = ChatOptions {\n            temperature: None,\n            num_predict: None,\n            stop: None,\n            num_ctx: Some(self.max_context_length),\n        };\n\n        // OODA-29: Enable thinking for reasoning models\n        let think = Self::is_thinking_model(&self.model);\n\n        let request = ChatRequest {\n            model: self.model.clone(),\n            messages: vec![OllamaMessage {\n                role: \"user\".to_string(),\n                content: prompt.to_string(),\n            }],\n            stream: true,\n            options: Some(chat_options),\n            tools: None,  // OODA-09: No tools for basic stream\n            think: if think { Some(true) } else { None },\n        };\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Ollama API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let stream = response.bytes_stream();\n\n        let mapped_stream = stream.map(|chunk_result| {\n            match chunk_result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    // Parse NDJSON - each line is a separate JSON object\n                    let mut content = String::new();\n                    for line in text.lines() {\n                        if line.is_empty() {\n                            continue;\n                        }\n                        // OODA-09: Use renamed OllamaStreamChunk\n                        if let Ok(chunk) = serde_json::from_str::<OllamaStreamChunk>(line) {\n                            if let Some(msg) = chunk.message {\n                                content.push_str(&msg.content);\n                            }\n                        }\n                    }\n                    Ok(content)\n                }\n                Err(e) => Err(LlmError::NetworkError(e.to_string())),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n    \n    // =========================================================================\n    // OODA-09: Tool Calling Support for Ollama\n    // =========================================================================\n    //\n    // Ollama supports tool calling via the OpenAI-compatible tools parameter.\n    // See: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion\n    //\n    // Models with tool support: llama3.2, mistral, qwen2, etc.\n    // =========================================================================\n    \n    fn supports_function_calling(&self) -> bool {\n        true // Ollama supports tools for compatible models\n    }\n    \n    fn supports_tool_streaming(&self) -> bool {\n        true // Ollama supports streaming with tools\n    }\n    \n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        _tool_choice: Option<ToolChoice>,  // Ollama doesn't support tool_choice\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let url = format!(\"{}/api/chat\", self.host);\n        let opts = options.cloned().unwrap_or_default();\n        \n        let chat_options = ChatOptions {\n            temperature: opts.temperature,\n            num_predict: opts.max_tokens.map(|t| t as i32),\n            stop: opts.stop.clone(),\n            num_ctx: Some(self.max_context_length),\n        };\n\n        // Convert tools to Ollama format\n        let ollama_tools = if !tools.is_empty() {\n            Some(Self::convert_tools(tools))\n        } else {\n            None\n        };\n\n        // OODA-29: Enable thinking for reasoning models\n        let think = Self::is_thinking_model(&self.model);\n        \n        let request = ChatRequest {\n            model: self.model.clone(),\n            messages: Self::convert_messages(messages),\n            stream: false,\n            options: Some(chat_options),\n            tools: ollama_tools,\n            think: if think { Some(true) } else { None },\n        };\n        \n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n        \n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Ollama API error ({}): {}\",\n                status, error_text\n            )));\n        }\n        \n        let response: ChatResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to parse response: {}\", e)))?;\n        \n        // Convert tool calls if present\n        let tool_calls: Vec<ToolCall> = response.message.tool_calls\n            .unwrap_or_default()\n            .into_iter()\n            .map(|tc| ToolCall {\n                id: uuid::Uuid::new_v4().to_string(),\n                call_type: \"function\".to_string(),\n                function: FunctionCall {\n                    name: tc.function.name,\n                    arguments: serde_json::to_string(&tc.function.arguments).unwrap_or_default(),\n                },\n            })\n            .collect();\n\n        // OODA-29: Build response with thinking content if available\n        let mut llm_response = LLMResponse::new(response.message.content, response.model)\n            .with_usage(response.prompt_eval_count as usize, response.eval_count as usize)\n            .with_tool_calls(tool_calls);\n\n        // Add thinking content if present\n        if let Some(thinking) = &response.message.thinking {\n            if !thinking.is_empty() {\n                llm_response = llm_response.with_thinking_content(thinking.clone());\n                // Estimate thinking tokens (rough: ~4 chars per token)\n                let thinking_tokens = thinking.len() / 4;\n                llm_response = llm_response.with_thinking_tokens(thinking_tokens);\n            }\n        }\n\n        Ok(llm_response)\n    }\n    \n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        _tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<TraitStreamChunk>>> {\n        use futures::StreamExt;\n        \n        let url = format!(\"{}/api/chat\", self.host);\n        let opts = options.cloned().unwrap_or_default();\n        \n        let chat_options = ChatOptions {\n            temperature: opts.temperature,\n            num_predict: opts.max_tokens.map(|t| t as i32),\n            stop: opts.stop.clone(),\n            num_ctx: Some(self.max_context_length),\n        };\n\n        let ollama_tools = if !tools.is_empty() {\n            Some(Self::convert_tools(tools))\n        } else {\n            None\n        };\n\n        // OODA-29: Enable thinking for reasoning models\n        let think = Self::is_thinking_model(&self.model);\n        \n        let request = ChatRequest {\n            model: self.model.clone(),\n            messages: Self::convert_messages(messages),\n            stream: true,\n            options: Some(chat_options),\n            tools: ollama_tools,\n            think: if think { Some(true) } else { None },\n        };\n        \n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n        \n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Ollama API error ({}): {}\",\n                status, error_text\n            )));\n        }\n        \n        let stream = response.bytes_stream();\n        \n        let mapped_stream = stream.map(|chunk_result| {\n            match chunk_result {\n                Ok(bytes) => {\n                    let text = String::from_utf8_lossy(&bytes);\n                    \n                    for line in text.lines() {\n                        if line.is_empty() {\n                            continue;\n                        }\n                        if let Ok(chunk) = serde_json::from_str::<OllamaStreamChunk>(line) {\n                            if let Some(msg) = chunk.message {\n                                // OODA-29: Check for thinking content first\n                                if let Some(thinking) = &msg.thinking {\n                                    if !thinking.is_empty() {\n                                        // Estimate thinking tokens (rough: ~4 chars per token)\n                                        let tokens_used = thinking.len() / 4;\n                                        return Ok(TraitStreamChunk::ThinkingContent {\n                                            text: thinking.clone(),\n                                            tokens_used: Some(tokens_used),\n                                            budget_total: None,\n                                        });\n                                    }\n                                }\n                                \n                                // Check for tool calls\n                                if let Some(tool_calls) = msg.tool_calls {\n                                    if let Some(tc) = tool_calls.first() {\n                                        return Ok(TraitStreamChunk::ToolCallDelta {\n                                            index: 0,\n                                            id: Some(uuid::Uuid::new_v4().to_string()),\n                                            function_name: Some(tc.function.name.clone()),\n                                            function_arguments: serde_json::to_string(&tc.function.arguments).ok(),\n                                        });\n                                    }\n                                }\n                                \n                                // Return content if not empty\n                                if !msg.content.is_empty() {\n                                    return Ok(TraitStreamChunk::Content(msg.content));\n                                }\n                            }\n                            \n                            // Check for completion\n                            if chunk.done {\n                                return Ok(TraitStreamChunk::Finished {\n                                    reason: \"stop\".to_string(),\n                                    ttft_ms: None,\n                                });\n                            }\n                        }\n                    }\n                    \n                    // Default empty content\n                    Ok(TraitStreamChunk::Content(String::new()))\n                }\n                Err(e) => Err(LlmError::NetworkError(e.to_string())),\n            }\n        });\n        \n        Ok(mapped_stream.boxed())\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for OllamaProvider {\n    fn name(&self) -> &str {\n        \"ollama\"\n    }\n\n    /// Returns the embedding model name (not completion model).\n    #[allow(clippy::misnamed_getters)]\n    fn model(&self) -> &str {\n        &self.embedding_model\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        8192 // Most Ollama embedding models support this\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        if texts.is_empty() {\n            return Ok(vec![]);\n        }\n\n        debug!(\n            \"Ollama embedding request: {} texts with model {}\",\n            texts.len(),\n            self.embedding_model\n        );\n\n        let url = format!(\"{}/api/embed\", self.host);\n\n        let request = EmbeddingRequest {\n            model: self.embedding_model.clone(),\n            input: texts.to_vec(),\n        };\n\n        let response = self\n            .client\n            .post(&url)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n        if !status.is_success() {\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Ollama API error ({}): {}\",\n                status, error_text\n            )));\n        }\n\n        let response: EmbeddingResponse = response\n            .json()\n            .await\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to parse response: {}\", e)))?;\n\n        debug!(\n            \"Ollama embedding response: {} embeddings\",\n            response.embeddings.len()\n        );\n\n        Ok(response.embeddings)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_builder_creation() {\n        let provider = OllamaProviderBuilder::new()\n            .host(\"http://localhost:11434\")\n            .model(\"mistral\")\n            .embedding_model(\"nomic-embed-text\")\n            .build()\n            .unwrap();\n\n        assert_eq!(LLMProvider::name(&provider), \"ollama\");\n        assert_eq!(LLMProvider::model(&provider), \"mistral\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"nomic-embed-text\");\n    }\n\n    #[test]\n    fn test_default_builder() {\n        let provider = OllamaProviderBuilder::new().build().unwrap();\n\n        assert_eq!(LLMProvider::name(&provider), \"ollama\");\n        assert_eq!(LLMProvider::model(&provider), \"gemma3:12b\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"embeddinggemma:latest\");\n        assert_eq!(provider.max_context_length(), 131072);\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let messages = vec![\n            ChatMessage::system(\"You are helpful\"),\n            ChatMessage::user(\"Hello\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let converted = OllamaProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 3);\n        assert_eq!(converted[0].role, \"system\");\n        assert_eq!(converted[1].role, \"user\");\n        assert_eq!(converted[2].role, \"assistant\");\n    }\n\n    #[tokio::test]\n    async fn test_embed_empty_input() {\n        let provider = OllamaProviderBuilder::new().build().unwrap();\n        let result = provider.embed(&[]).await;\n        assert!(result.is_ok());\n        assert!(result.unwrap().is_empty());\n    }\n\n    // OODA-29: Tests for thinking model detection\n    #[test]\n    fn test_is_thinking_model_deepseek_r1() {\n        assert!(OllamaProvider::is_thinking_model(\"deepseek-r1:8b\"));\n        assert!(OllamaProvider::is_thinking_model(\"deepseek-r1:70b\"));\n        assert!(OllamaProvider::is_thinking_model(\"DEEPSEEK-R1:latest\"));\n    }\n\n    #[test]\n    fn test_is_thinking_model_qwen3() {\n        assert!(OllamaProvider::is_thinking_model(\"qwen3:8b\"));\n        assert!(OllamaProvider::is_thinking_model(\"qwen3:32b\"));\n        assert!(OllamaProvider::is_thinking_model(\"QWEN3:latest\"));\n    }\n\n    #[test]\n    fn test_is_thinking_model_others() {\n        assert!(OllamaProvider::is_thinking_model(\"qwq:32b\"));\n        assert!(OllamaProvider::is_thinking_model(\"openthinker:7b\"));\n        assert!(OllamaProvider::is_thinking_model(\"phi4-reasoning:14b\"));\n        assert!(OllamaProvider::is_thinking_model(\"magistral:24b\"));\n        assert!(OllamaProvider::is_thinking_model(\"cogito:8b\"));\n        assert!(OllamaProvider::is_thinking_model(\"gpt-oss:20b\"));\n    }\n\n    #[test]\n    fn test_is_thinking_model_non_thinking() {\n        assert!(!OllamaProvider::is_thinking_model(\"llama3.2:8b\"));\n        assert!(!OllamaProvider::is_thinking_model(\"gemma3:12b\"));\n        assert!(!OllamaProvider::is_thinking_model(\"mistral:7b\"));\n        assert!(!OllamaProvider::is_thinking_model(\"codellama:34b\"));\n    }\n\n    #[test]\n    fn test_response_with_thinking_parsing() {\n        // Test that ResponseMessage can parse thinking field\n        let json = r#\"{\n            \"role\": \"assistant\",\n            \"content\": \"The answer is 3.\",\n            \"thinking\": \"Let me count the r's in strawberry: s-t-r-a-w-b-e-r-r-y. That's 3 r's.\"\n        }\"#;\n        \n        let msg: ResponseMessage = serde_json::from_str(json).unwrap();\n        assert_eq!(msg.content, \"The answer is 3.\");\n        assert!(msg.thinking.is_some());\n        assert!(msg.thinking.unwrap().contains(\"Let me count\"));\n    }\n\n    #[test]\n    fn test_response_without_thinking_parsing() {\n        // Test that ResponseMessage works without thinking field\n        let json = r#\"{\n            \"role\": \"assistant\",\n            \"content\": \"Hello, how can I help you?\"\n        }\"#;\n        \n        let msg: ResponseMessage = serde_json::from_str(json).unwrap();\n        assert_eq!(msg.content, \"Hello, how can I help you?\");\n        assert!(msg.thinking.is_none());\n    }\n\n    #[test]\n    fn test_chat_request_with_think_serialization() {\n        let request = ChatRequest {\n            model: \"deepseek-r1:8b\".to_string(),\n            messages: vec![OllamaMessage {\n                role: \"user\".to_string(),\n                content: \"How many r's in strawberry?\".to_string(),\n            }],\n            stream: false,\n            options: None,\n            tools: None,\n            think: Some(true),\n        };\n        \n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"\\\"think\\\":true\"));\n    }\n\n    #[test]\n    fn test_chat_request_without_think_serialization() {\n        let request = ChatRequest {\n            model: \"llama3.2:8b\".to_string(),\n            messages: vec![OllamaMessage {\n                role: \"user\".to_string(),\n                content: \"Hello\".to_string(),\n            }],\n            stream: false,\n            options: None,\n            tools: None,\n            think: None,\n        };\n        \n        let json = serde_json::to_string(&request).unwrap();\n        // think should not be present when None\n        assert!(!json.contains(\"think\"));\n    }\n\n    #[test]\n    fn test_chat_options_num_ctx_serialization() {\n        let options = ChatOptions {\n            temperature: None,\n            num_predict: None,\n            stop: None,\n            num_ctx: Some(65536),\n        };\n\n        let json = serde_json::to_string(&options).unwrap();\n        assert!(\n            json.contains(\"\\\"num_ctx\\\":65536\"),\n            \"num_ctx should be serialized in options: {}\",\n            json\n        );\n    }\n\n    #[test]\n    fn test_chat_options_num_ctx_omitted_when_none() {\n        let options = ChatOptions {\n            temperature: None,\n            num_predict: None,\n            stop: None,\n            num_ctx: None,\n        };\n\n        let json = serde_json::to_string(&options).unwrap();\n        assert!(\n            !json.contains(\"num_ctx\"),\n            \"num_ctx should be omitted when None: {}\",\n            json\n        );\n    }\n\n    #[test]\n    fn test_constants() {\n        assert_eq!(DEFAULT_OLLAMA_HOST, \"http://localhost:11434\");\n        assert_eq!(DEFAULT_OLLAMA_MODEL, \"gemma3:12b\");\n        assert_eq!(DEFAULT_OLLAMA_EMBEDDING_MODEL, \"embeddinggemma:latest\");\n    }\n\n    #[test]\n    fn test_builder_default_values() {\n        let builder = OllamaProviderBuilder::default();\n        \n        assert_eq!(builder.host, \"http://localhost:11434\");\n        assert_eq!(builder.model, \"gemma3:12b\");\n        assert_eq!(builder.embedding_model, \"embeddinggemma:latest\");\n        assert_eq!(builder.max_context_length, 131072);\n        assert_eq!(builder.embedding_dimension, 768);\n    }\n\n    #[test]\n    fn test_builder_custom_context_length() {\n        let provider = OllamaProviderBuilder::new()\n            .max_context_length(65536)\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.max_context_length(), 65536);\n    }\n\n    #[test]\n    fn test_builder_custom_embedding_dimension() {\n        let provider = OllamaProviderBuilder::new()\n            .embedding_dimension(1536)\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.dimension(), 1536);\n    }\n\n    #[test]\n    fn test_default_local_creation() {\n        let provider = OllamaProvider::default_local().unwrap();\n        \n        assert_eq!(LLMProvider::name(&provider), \"ollama\");\n        assert_eq!(LLMProvider::model(&provider), \"gemma3:12b\");\n        assert_eq!(provider.host, \"http://localhost:11434\");\n    }\n\n    #[test]\n    fn test_supports_streaming() {\n        let provider = OllamaProviderBuilder::new().build().unwrap();\n        assert!(provider.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_json_mode() {\n        let provider = OllamaProviderBuilder::new().build().unwrap();\n        // Ollama doesn't override supports_json_mode, so default is false\n        assert!(!provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_embedding_provider_name() {\n        let provider = OllamaProviderBuilder::new().build().unwrap();\n        assert_eq!(EmbeddingProvider::name(&provider), \"ollama\");\n    }\n\n    #[test]\n    fn test_embedding_provider_dimension() {\n        let provider = OllamaProviderBuilder::new()\n            .embedding_dimension(1024)\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.dimension(), 1024);\n    }\n\n    #[test]\n    fn test_embedding_provider_max_tokens() {\n        let provider = OllamaProviderBuilder::new().build().unwrap();\n        assert_eq!(provider.max_tokens(), 8192);\n    }\n\n    #[test]\n    fn test_message_conversion_tool_role() {\n        let messages = vec![\n            ChatMessage::tool_result(\"tool-1\", \"Tool output\"),\n        ];\n\n        let converted = OllamaProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 1);\n        // Ollama doesn't have tool role, converts to \"user\"\n        assert_eq!(converted[0].role, \"user\");\n        assert_eq!(converted[0].content, \"Tool output\");\n    }\n\n    #[test]\n    fn test_ollama_message_serialization() {\n        let msg = OllamaMessage {\n            role: \"user\".to_string(),\n            content: \"Hello world\".to_string(),\n        };\n\n        let json = serde_json::to_string(&msg).unwrap();\n        assert!(json.contains(\"\\\"role\\\":\\\"user\\\"\"));\n        assert!(json.contains(\"\\\"content\\\":\\\"Hello world\\\"\"));\n    }\n\n    #[test]\n    fn test_from_env_uses_defaults() {\n        // Clear env vars\n        std::env::remove_var(\"OLLAMA_HOST\");\n        std::env::remove_var(\"OLLAMA_MODEL\");\n        std::env::remove_var(\"OLLAMA_EMBEDDING_MODEL\");\n        std::env::remove_var(\"OLLAMA_CONTEXT_LENGTH\");\n\n        let provider = OllamaProvider::from_env().unwrap();\n        \n        assert_eq!(provider.host, \"http://localhost:11434\");\n        assert_eq!(LLMProvider::model(&provider), \"gemma3:12b\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"embeddinggemma:latest\");\n        assert_eq!(provider.max_context_length(), 131072);\n    }\n\n    #[test]\n    fn test_chat_options_temperature_serialization() {\n        let options = ChatOptions {\n            temperature: Some(0.7),\n            num_predict: Some(1024),\n            stop: Some(vec![\"END\".to_string()]),\n            num_ctx: Some(32768),\n        };\n\n        let json = serde_json::to_string(&options).unwrap();\n        assert!(json.contains(\"\\\"temperature\\\":0.7\"));\n        assert!(json.contains(\"\\\"num_predict\\\":1024\"));\n        assert!(json.contains(\"\\\"stop\\\":[\\\"END\\\"]\"));\n        assert!(json.contains(\"\\\"num_ctx\\\":32768\"));\n    }\n}\n","traces":[{"line":80,"address":[],"length":0,"stats":{"Line":27}},{"line":82,"address":[],"length":0,"stats":{"Line":81}},{"line":83,"address":[],"length":0,"stats":{"Line":81}},{"line":84,"address":[],"length":0,"stats":{"Line":27}},{"line":93,"address":[],"length":0,"stats":{"Line":26}},{"line":94,"address":[],"length":0,"stats":{"Line":26}},{"line":98,"address":[],"length":0,"stats":{"Line":16}},{"line":99,"address":[],"length":0,"stats":{"Line":48}},{"line":100,"address":[],"length":0,"stats":{"Line":16}},{"line":104,"address":[],"length":0,"stats":{"Line":16}},{"line":105,"address":[],"length":0,"stats":{"Line":48}},{"line":106,"address":[],"length":0,"stats":{"Line":16}},{"line":110,"address":[],"length":0,"stats":{"Line":16}},{"line":111,"address":[],"length":0,"stats":{"Line":48}},{"line":112,"address":[],"length":0,"stats":{"Line":16}},{"line":116,"address":[],"length":0,"stats":{"Line":16}},{"line":117,"address":[],"length":0,"stats":{"Line":16}},{"line":118,"address":[],"length":0,"stats":{"Line":16}},{"line":122,"address":[],"length":0,"stats":{"Line":2}},{"line":123,"address":[],"length":0,"stats":{"Line":2}},{"line":124,"address":[],"length":0,"stats":{"Line":2}},{"line":128,"address":[],"length":0,"stats":{"Line":26}},{"line":129,"address":[],"length":0,"stats":{"Line":52}},{"line":130,"address":[],"length":0,"stats":{"Line":52}},{"line":133,"address":[],"length":0,"stats":{"Line":26}},{"line":135,"address":[],"length":0,"stats":{"Line":26}},{"line":136,"address":[],"length":0,"stats":{"Line":52}},{"line":137,"address":[],"length":0,"stats":{"Line":52}},{"line":138,"address":[],"length":0,"stats":{"Line":52}},{"line":139,"address":[],"length":0,"stats":{"Line":52}},{"line":140,"address":[],"length":0,"stats":{"Line":26}},{"line":141,"address":[],"length":0,"stats":{"Line":26}},{"line":159,"address":[],"length":0,"stats":{"Line":15}},{"line":160,"address":[],"length":0,"stats":{"Line":67}},{"line":162,"address":[],"length":0,"stats":{"Line":15}},{"line":163,"address":[],"length":0,"stats":{"Line":60}},{"line":165,"address":[],"length":0,"stats":{"Line":30}},{"line":166,"address":[],"length":0,"stats":{"Line":45}},{"line":169,"address":[],"length":0,"stats":{"Line":30}},{"line":171,"address":[],"length":0,"stats":{"Line":15}},{"line":174,"address":[],"length":0,"stats":{"Line":15}},{"line":175,"address":[],"length":0,"stats":{"Line":30}},{"line":176,"address":[],"length":0,"stats":{"Line":30}},{"line":177,"address":[],"length":0,"stats":{"Line":30}},{"line":178,"address":[],"length":0,"stats":{"Line":30}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":1}},{"line":189,"address":[],"length":0,"stats":{"Line":2}},{"line":366,"address":[],"length":0,"stats":{"Line":4}},{"line":367,"address":[],"length":0,"stats":{"Line":4}},{"line":368,"address":[],"length":0,"stats":{"Line":1}},{"line":369,"address":[],"length":0,"stats":{"Line":1}},{"line":370,"address":[],"length":0,"stats":{"Line":1}},{"line":371,"address":[],"length":0,"stats":{"Line":1}},{"line":375,"address":[],"length":0,"stats":{"Line":2}},{"line":376,"address":[],"length":0,"stats":{"Line":2}},{"line":378,"address":[],"length":0,"stats":{"Line":2}},{"line":379,"address":[],"length":0,"stats":{"Line":8}},{"line":380,"address":[],"length":0,"stats":{"Line":8}},{"line":400,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[],"length":0,"stats":{"Line":0}},{"line":403,"address":[],"length":0,"stats":{"Line":0}},{"line":404,"address":[],"length":0,"stats":{"Line":0}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":406,"address":[],"length":0,"stats":{"Line":0}},{"line":407,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}},{"line":436,"address":[],"length":0,"stats":{"Line":0}},{"line":437,"address":[],"length":0,"stats":{"Line":0}},{"line":438,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[],"length":0,"stats":{"Line":0}},{"line":441,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":0}},{"line":444,"address":[],"length":0,"stats":{"Line":0}},{"line":445,"address":[],"length":0,"stats":{"Line":0}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":447,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":452,"address":[],"length":0,"stats":{"Line":0}},{"line":454,"address":[],"length":0,"stats":{"Line":0}},{"line":455,"address":[],"length":0,"stats":{"Line":0}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":459,"address":[],"length":0,"stats":{"Line":0}},{"line":463,"address":[],"length":0,"stats":{"Line":0}},{"line":464,"address":[],"length":0,"stats":{"Line":0}},{"line":473,"address":[],"length":0,"stats":{"Line":16}},{"line":474,"address":[],"length":0,"stats":{"Line":48}},{"line":475,"address":[],"length":0,"stats":{"Line":16}},{"line":476,"address":[],"length":0,"stats":{"Line":13}},{"line":477,"address":[],"length":0,"stats":{"Line":10}},{"line":478,"address":[],"length":0,"stats":{"Line":9}},{"line":479,"address":[],"length":0,"stats":{"Line":8}},{"line":480,"address":[],"length":0,"stats":{"Line":7}},{"line":481,"address":[],"length":0,"stats":{"Line":6}},{"line":482,"address":[],"length":0,"stats":{"Line":5}},{"line":488,"address":[],"length":0,"stats":{"Line":6}},{"line":489,"address":[],"length":0,"stats":{"Line":6}},{"line":492,"address":[],"length":0,"stats":{"Line":4}},{"line":493,"address":[],"length":0,"stats":{"Line":4}},{"line":496,"address":[],"length":0,"stats":{"Line":3}},{"line":497,"address":[],"length":0,"stats":{"Line":3}},{"line":500,"address":[],"length":0,"stats":{"Line":0}},{"line":536,"address":[],"length":0,"stats":{"Line":0}},{"line":559,"address":[],"length":0,"stats":{"Line":0}},{"line":573,"address":[],"length":0,"stats":{"Line":0}},{"line":595,"address":[],"length":0,"stats":{"Line":0}},{"line":630,"address":[],"length":0,"stats":{"Line":0}},{"line":643,"address":[],"length":0,"stats":{"Line":0}},{"line":644,"address":[],"length":0,"stats":{"Line":0}},{"line":645,"address":[],"length":0,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":648,"address":[],"length":0,"stats":{"Line":0}},{"line":649,"address":[],"length":0,"stats":{"Line":0}},{"line":650,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":0}},{"line":654,"address":[],"length":0,"stats":{"Line":0}},{"line":655,"address":[],"length":0,"stats":{"Line":0}},{"line":656,"address":[],"length":0,"stats":{"Line":0}},{"line":660,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[],"length":0,"stats":{"Line":0}},{"line":669,"address":[],"length":0,"stats":{"Line":1}},{"line":670,"address":[],"length":0,"stats":{"Line":1}},{"line":683,"address":[],"length":0,"stats":{"Line":0}},{"line":684,"address":[],"length":0,"stats":{"Line":0}},{"line":687,"address":[],"length":0,"stats":{"Line":0}},{"line":688,"address":[],"length":0,"stats":{"Line":0}},{"line":703,"address":[],"length":0,"stats":{"Line":0}},{"line":733,"address":[],"length":0,"stats":{"Line":0}},{"line":747,"address":[],"length":0,"stats":{"Line":0}},{"line":754,"address":[],"length":0,"stats":{"Line":0}},{"line":755,"address":[],"length":0,"stats":{"Line":0}},{"line":756,"address":[],"length":0,"stats":{"Line":0}},{"line":757,"address":[],"length":0,"stats":{"Line":0}},{"line":758,"address":[],"length":0,"stats":{"Line":0}},{"line":795,"address":[],"length":0,"stats":{"Line":0}},{"line":824,"address":[],"length":0,"stats":{"Line":0}},{"line":837,"address":[],"length":0,"stats":{"Line":0}},{"line":838,"address":[],"length":0,"stats":{"Line":0}},{"line":839,"address":[],"length":0,"stats":{"Line":0}},{"line":840,"address":[],"length":0,"stats":{"Line":0}},{"line":842,"address":[],"length":0,"stats":{"Line":0}},{"line":843,"address":[],"length":0,"stats":{"Line":0}},{"line":844,"address":[],"length":0,"stats":{"Line":0}},{"line":846,"address":[],"length":0,"stats":{"Line":0}},{"line":847,"address":[],"length":0,"stats":{"Line":0}},{"line":849,"address":[],"length":0,"stats":{"Line":0}},{"line":850,"address":[],"length":0,"stats":{"Line":0}},{"line":852,"address":[],"length":0,"stats":{"Line":0}},{"line":853,"address":[],"length":0,"stats":{"Line":0}},{"line":854,"address":[],"length":0,"stats":{"Line":0}},{"line":855,"address":[],"length":0,"stats":{"Line":0}},{"line":856,"address":[],"length":0,"stats":{"Line":0}},{"line":862,"address":[],"length":0,"stats":{"Line":0}},{"line":863,"address":[],"length":0,"stats":{"Line":0}},{"line":864,"address":[],"length":0,"stats":{"Line":0}},{"line":865,"address":[],"length":0,"stats":{"Line":0}},{"line":866,"address":[],"length":0,"stats":{"Line":0}},{"line":867,"address":[],"length":0,"stats":{"Line":0}},{"line":868,"address":[],"length":0,"stats":{"Line":0}},{"line":874,"address":[],"length":0,"stats":{"Line":0}},{"line":875,"address":[],"length":0,"stats":{"Line":0}},{"line":880,"address":[],"length":0,"stats":{"Line":0}},{"line":881,"address":[],"length":0,"stats":{"Line":0}},{"line":882,"address":[],"length":0,"stats":{"Line":0}},{"line":883,"address":[],"length":0,"stats":{"Line":0}},{"line":890,"address":[],"length":0,"stats":{"Line":0}},{"line":892,"address":[],"length":0,"stats":{"Line":0}},{"line":902,"address":[],"length":0,"stats":{"Line":2}},{"line":903,"address":[],"length":0,"stats":{"Line":2}},{"line":908,"address":[],"length":0,"stats":{"Line":3}},{"line":909,"address":[],"length":0,"stats":{"Line":3}},{"line":912,"address":[],"length":0,"stats":{"Line":4}},{"line":913,"address":[],"length":0,"stats":{"Line":4}},{"line":916,"address":[],"length":0,"stats":{"Line":1}},{"line":917,"address":[],"length":0,"stats":{"Line":1}},{"line":920,"address":[],"length":0,"stats":{"Line":1}},{"line":944,"address":[],"length":0,"stats":{"Line":0}},{"line":958,"address":[],"length":0,"stats":{"Line":0}}],"covered":85,"coverable":182},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","openai.rs"],"content":"//! OpenAI provider implementation.\n//!\n//! Supports OpenAI and OpenAI-compatible APIs (Ollama, LM Studio, etc.)\n\nuse async_openai::{\n    config::OpenAIConfig,\n    types::{\n        ChatCompletionRequestAssistantMessageArgs, ChatCompletionRequestMessage,\n        ChatCompletionRequestSystemMessageArgs, ChatCompletionRequestUserMessageArgs,\n        ChatCompletionTool, ChatCompletionToolChoiceOption, ChatCompletionToolType,\n        CreateChatCompletionRequestArgs, CreateEmbeddingRequestArgs, EmbeddingInput,\n        FinishReason, FunctionObjectArgs,\n    },\n    Client,\n};\nuse async_trait::async_trait;\nuse futures::StreamExt;\nuse std::collections::HashMap;\nuse tracing::debug;\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse,\n    StreamChunk, ToolChoice, ToolDefinition,\n};\n\n/// OpenAI provider for text completion and embeddings.\npub struct OpenAIProvider {\n    client: Client<OpenAIConfig>,\n    model: String,\n    embedding_model: String,\n    max_context_length: usize,\n    embedding_dimension: usize,\n}\n\nimpl OpenAIProvider {\n    /// Create a new OpenAI provider with the given API key.\n    pub fn new(api_key: impl Into<String>) -> Self {\n        let config = OpenAIConfig::new().with_api_key(api_key);\n        Self::with_config(config)\n    }\n\n    /// Create a provider with custom configuration.\n    /// Defaults to GPT-5-mini for best balance of performance and cost.\n    pub fn with_config(config: OpenAIConfig) -> Self {\n        Self {\n            client: Client::with_config(config),\n            model: \"gpt-5-mini\".to_string(), // Updated default to GPT-5-mini\n            embedding_model: \"text-embedding-3-small\".to_string(),\n            max_context_length: 200000, // GPT-5-mini context length\n            embedding_dimension: 1536,\n        }\n    }\n\n    /// Create a provider for an OpenAI-compatible API.\n    pub fn compatible(api_key: impl Into<String>, base_url: impl Into<String>) -> Self {\n        let config = OpenAIConfig::new()\n            .with_api_key(api_key)\n            .with_api_base(base_url);\n        Self::with_config(config)\n    }\n\n    /// Set the completion model.\n    pub fn with_model(mut self, model: impl Into<String>) -> Self {\n        self.model = model.into();\n        self.max_context_length = Self::context_length_for_model(&self.model);\n        self\n    }\n\n    /// Set the embedding model.\n    pub fn with_embedding_model(mut self, model: impl Into<String>) -> Self {\n        self.embedding_model = model.into();\n        self.embedding_dimension = Self::dimension_for_model(&self.embedding_model);\n        self\n    }\n\n    /// Get the context length for a model.\n    fn context_length_for_model(model: &str) -> usize {\n        match model {\n            // GPT-5 series (2026 models)\n            m if m.contains(\"gpt-5.2\") || m.contains(\"gpt-5.1\") => 200000,\n            m if m.contains(\"gpt-5-nano\") => 128000,\n            m if m.contains(\"gpt-5-mini\") || m.contains(\"gpt-5\") => 200000,\n\n            // GPT-4.1 series\n            m if m.contains(\"gpt-4.1\") => 128000,\n\n            // O-series reasoning models\n            m if m.contains(\"o4\") || m.contains(\"o3\") => 200000,\n            m if m.contains(\"o1\") => 200000,\n\n            // GPT-4 series\n            m if m.contains(\"gpt-4o\") => 128000,\n            m if m.contains(\"gpt-4-turbo\") => 128000,\n            m if m.contains(\"gpt-4-32k\") => 32768,\n            m if m.contains(\"gpt-4\") => 8192,\n\n            // GPT-3.5 series\n            m if m.contains(\"gpt-3.5-turbo-16k\") => 16384,\n            m if m.contains(\"gpt-3.5\") => 4096,\n\n            // Codex models\n            m if m.contains(\"codex\") => 200000,\n\n            // Realtime and audio models\n            m if m.contains(\"gpt-realtime\") || m.contains(\"gpt-audio\") => 128000,\n\n            _ => 128000, // Updated default for newer models\n        }\n    }\n\n    /// Get the embedding dimension for a model.\n    fn dimension_for_model(model: &str) -> usize {\n        match model {\n            m if m.contains(\"text-embedding-3-large\") => 3072,\n            m if m.contains(\"text-embedding-3-small\") => 1536,\n            m if m.contains(\"text-embedding-ada\") => 1536,\n            _ => 1536, // Default\n        }\n    }\n\n    /// Convert chat messages to OpenAI format.\n    fn convert_messages(messages: &[ChatMessage]) -> Result<Vec<ChatCompletionRequestMessage>> {\n        messages\n            .iter()\n            .map(|msg| {\n                match msg.role {\n                    ChatRole::System => ChatCompletionRequestSystemMessageArgs::default()\n                        .content(msg.content.as_str())\n                        .build()\n                        .map(Into::into)\n                        .map_err(|e| LlmError::InvalidRequest(e.to_string())),\n                    ChatRole::User => ChatCompletionRequestUserMessageArgs::default()\n                        .content(msg.content.as_str())\n                        .build()\n                        .map(Into::into)\n                        .map_err(|e| LlmError::InvalidRequest(e.to_string())),\n                    ChatRole::Assistant => ChatCompletionRequestAssistantMessageArgs::default()\n                        .content(msg.content.as_str())\n                        .build()\n                        .map(Into::into)\n                        .map_err(|e| LlmError::InvalidRequest(e.to_string())),\n                    ChatRole::Tool | ChatRole::Function => {\n                        // Tool/Function messages are handled as user messages in simplified API\n                        ChatCompletionRequestUserMessageArgs::default()\n                            .content(msg.content.as_str())\n                            .build()\n                            .map(Into::into)\n                            .map_err(|e| LlmError::InvalidRequest(e.to_string()))\n                    }\n                }\n            })\n            .collect()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OpenAIProvider {\n    fn name(&self) -> &str {\n        \"openai\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let openai_messages = Self::convert_messages(messages)?;\n        let options = options.cloned().unwrap_or_default();\n\n        let mut request_builder = CreateChatCompletionRequestArgs::default();\n        request_builder.model(&self.model).messages(openai_messages);\n\n        if let Some(max_tokens) = options.max_tokens {\n            request_builder.max_tokens(max_tokens as u32);\n        }\n\n        if let Some(temp) = options.temperature {\n            request_builder.temperature(temp);\n        }\n\n        if let Some(top_p) = options.top_p {\n            request_builder.top_p(top_p);\n        }\n\n        if let Some(stop) = options.stop {\n            request_builder.stop(stop);\n        }\n\n        if let Some(freq_penalty) = options.frequency_penalty {\n            request_builder.frequency_penalty(freq_penalty);\n        }\n\n        if let Some(pres_penalty) = options.presence_penalty {\n            request_builder.presence_penalty(pres_penalty);\n        }\n\n        let request = request_builder\n            .build()\n            .map_err(|e| LlmError::InvalidRequest(e.to_string()))?;\n\n        let response = self.client.chat().create(request).await?;\n\n        // Debug logging for token tracking\n        debug!(\n            \"OpenAI response - usage: {:?}, model: {}\",\n            response.usage, response.model\n        );\n\n        let choice = response\n            .choices\n            .first()\n            .ok_or_else(|| LlmError::ApiError(\"No choices in response\".to_string()))?;\n\n        let content = choice.message.content.clone().unwrap_or_default();\n\n        let usage = response\n            .usage\n            .clone()\n            .unwrap_or(async_openai::types::CompletionUsage {\n                prompt_tokens: 0,\n                completion_tokens: 0,\n                total_tokens: 0,\n            });\n\n        // Note: Cache hit tracking for OpenAI requires async-openai >= 0.32\n        // which adds prompt_tokens_details.cached_tokens field.\n        // Current version (0.24) does not support cache hit extraction.\n        // TODO: Upgrade async-openai when feasible for full cache tracking.\n        let cache_hit_tokens: Option<usize> = None;\n\n        // Log extracted token counts\n        debug!(\n            \"OpenAI token usage - prompt: {}, completion: {}, total: {}\",\n            usage.prompt_tokens, usage.completion_tokens, usage.total_tokens\n        );\n\n        let mut metadata = HashMap::new();\n        metadata.insert(\"response_id\".to_string(), serde_json::json!(response.id));\n\n        Ok(LLMResponse {\n            content,\n            prompt_tokens: usage.prompt_tokens as usize,\n            completion_tokens: usage.completion_tokens as usize,\n            total_tokens: usage.total_tokens as usize,\n            model: response.model,\n            finish_reason: choice.finish_reason.map(|r| format!(\"{:?}\", r)),\n            tool_calls: Vec::new(),\n            metadata,\n            cache_hit_tokens,\n            // OODA-15: Reasoning tokens not yet extracted (requires async-openai 0.32+)\n            // TODO: Extract output_tokens_details.reasoning_tokens when library upgraded\n            thinking_tokens: None,\n            thinking_content: None,\n        })\n    }\n\n    async fn stream(\n        &self,\n        prompt: &str,\n    ) -> Result<futures::stream::BoxStream<'static, Result<String>>> {\n        use futures::StreamExt;\n\n        let request = ChatCompletionRequestUserMessageArgs::default()\n            .content(prompt)\n            .build()\n            .map(Into::into)\n            .map_err(|e| LlmError::InvalidRequest(e.to_string()))?;\n\n        let request = CreateChatCompletionRequestArgs::default()\n            .model(&self.model)\n            .messages(vec![request])\n            .stream(true)\n            .build()\n            .map_err(|e| LlmError::InvalidRequest(e.to_string()))?;\n\n        let stream = self.client.chat().create_stream(request).await?;\n\n        let mapped_stream = stream.map(|res| match res {\n            Ok(response) => {\n                let content = response\n                    .choices\n                    .first()\n                    .and_then(|c| c.delta.content.clone())\n                    .unwrap_or_default();\n                Ok(content)\n            }\n            Err(e) => Err(LlmError::ApiError(e.to_string())),\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<futures::stream::BoxStream<'static, Result<StreamChunk>>> {\n        let openai_messages = Self::convert_messages(messages)?;\n        let options = options.cloned().unwrap_or_default();\n\n        // Convert tools to OpenAI format\n        let openai_tools: Vec<ChatCompletionTool> = tools\n            .iter()\n            .map(|tool| ChatCompletionTool {\n                r#type: ChatCompletionToolType::Function,\n                function: FunctionObjectArgs::default()\n                    .name(&tool.function.name)\n                    .description(&tool.function.description)\n                    .parameters(tool.function.parameters.clone())\n                    .build()\n                    .expect(\"Invalid tool definition\"),\n            })\n            .collect();\n\n        // Build request\n        let mut request_builder = CreateChatCompletionRequestArgs::default();\n        request_builder\n            .model(&self.model)\n            .messages(openai_messages)\n            .tools(openai_tools)\n            .stream(true); // Enable streaming\n\n        // Set tool choice if specified\n        if let Some(tc) = tool_choice {\n            match tc {\n                ToolChoice::Auto(_) => {\n                    request_builder.tool_choice(ChatCompletionToolChoiceOption::Auto);\n                }\n                ToolChoice::Required(_) => {\n                    request_builder.tool_choice(ChatCompletionToolChoiceOption::Required);\n                }\n                _ => {}\n            }\n        }\n\n        if let Some(temp) = options.temperature {\n            request_builder.temperature(temp);\n        }\n\n        if let Some(max_tokens) = options.max_tokens {\n            request_builder.max_tokens(max_tokens as u32);\n        }\n\n        let request = request_builder\n            .build()\n            .map_err(|e| LlmError::InvalidRequest(e.to_string()))?;\n\n        let stream = self.client.chat().create_stream(request).await?;\n\n        // Map OpenAI stream to our StreamChunk format\n        let mapped_stream = stream.map(|result| {\n            match result {\n                Ok(response) => {\n                    let choice = response.choices.first();\n                    if let Some(choice) = choice {\n                        // Stream content chunks immediately\n                        if let Some(content) = &choice.delta.content {\n                            return Ok(StreamChunk::Content(content.clone()));\n                        }\n\n                        // Return tool call delta (agent will accumulate)\n                        if let Some(tool_call_chunks) = &choice.delta.tool_calls {\n                            if let Some(chunk) = tool_call_chunks.first() {\n                                return Ok(StreamChunk::ToolCallDelta {\n                                    index: chunk.index as usize,\n                                    id: chunk.id.clone(),\n                                    function_name: chunk\n                                        .function\n                                        .as_ref()\n                                        .and_then(|f| f.name.clone()),\n                                    function_arguments: chunk\n                                        .function\n                                        .as_ref()\n                                        .and_then(|f| f.arguments.clone()),\n                                });\n                            }\n                        }\n\n                        // Check finish reason\n                        if let Some(finish_reason) = &choice.finish_reason {\n                            let reason = match finish_reason {\n                                FinishReason::Stop => \"stop\",\n                                FinishReason::Length => \"length\",\n                                FinishReason::ToolCalls => \"tool_calls\",\n                                FinishReason::ContentFilter => \"content_filter\",\n                                FinishReason::FunctionCall => \"function_call\",\n                            };\n                            return Ok(StreamChunk::Finished {\n                                reason: reason.to_string(),\n                                ttft_ms: None,\n                            });\n                        }\n                    }\n                    // Empty chunk (no content or tool calls)\n                    Ok(StreamChunk::Content(String::new()))\n                }\n                Err(e) => Err(LlmError::ApiError(e.to_string())),\n            }\n        });\n\n        Ok(mapped_stream.boxed())\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_json_mode(&self) -> bool {\n        self.model.contains(\"gpt-4\") || self.model.contains(\"gpt-3.5-turbo\")\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for OpenAIProvider {\n    fn name(&self) -> &str {\n        \"openai\"\n    }\n\n    /// Returns the embedding model name (not completion model).\n    ///\n    /// Note: `model` field refers to completion, `embedding_model` is for embeddings.\n    #[allow(clippy::misnamed_getters)]\n    fn model(&self) -> &str {\n        &self.embedding_model\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        8191 // OpenAI embedding models support 8191 tokens\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        if texts.is_empty() {\n            return Ok(Vec::new());\n        }\n\n        let input = EmbeddingInput::StringArray(texts.to_vec());\n\n        let request = CreateEmbeddingRequestArgs::default()\n            .model(&self.embedding_model)\n            .input(input)\n            .build()\n            .map_err(|e| LlmError::InvalidRequest(e.to_string()))?;\n\n        let response = self.client.embeddings().create(request).await?;\n\n        let embeddings: Vec<Vec<f32>> = response.data.into_iter().map(|e| e.embedding).collect();\n\n        Ok(embeddings)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_context_length_detection() {\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-4o\"), 128000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-4\"), 8192);\n        assert_eq!(\n            OpenAIProvider::context_length_for_model(\"gpt-3.5-turbo\"),\n            4096\n        );\n    }\n\n    #[test]\n    fn test_embedding_dimension_detection() {\n        assert_eq!(\n            OpenAIProvider::dimension_for_model(\"text-embedding-3-large\"),\n            3072\n        );\n        assert_eq!(\n            OpenAIProvider::dimension_for_model(\"text-embedding-3-small\"),\n            1536\n        );\n    }\n\n    #[test]\n    fn test_provider_builder() {\n        let provider = OpenAIProvider::new(\"test-key\")\n            .with_model(\"gpt-4\")\n            .with_embedding_model(\"text-embedding-3-large\");\n\n        assert_eq!(LLMProvider::model(&provider), \"gpt-4\");\n        assert_eq!(provider.dimension(), 3072);\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let messages = vec![\n            ChatMessage::system(\"You are helpful\"),\n            ChatMessage::user(\"Hello\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let converted = OpenAIProvider::convert_messages(&messages).unwrap();\n        assert_eq!(converted.len(), 3);\n    }\n\n    // ---- Iteration 27: Additional OpenAI tests ----\n\n    #[test]\n    fn test_context_length_gpt5_series() {\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-5.2-turbo\"), 200000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-5.1-preview\"), 200000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-5-nano\"), 128000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-5-mini\"), 200000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-5\"), 200000);\n    }\n\n    #[test]\n    fn test_context_length_o_series() {\n        assert_eq!(OpenAIProvider::context_length_for_model(\"o4-mini\"), 200000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"o3-preview\"), 200000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"o1-preview\"), 200000);\n    }\n\n    #[test]\n    fn test_context_length_gpt4_variants() {\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-4-turbo-preview\"), 128000);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-4-32k-0613\"), 32768);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-4-0613\"), 8192);\n    }\n\n    #[test]\n    fn test_context_length_gpt35_variants() {\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-3.5-turbo-16k\"), 16384);\n        assert_eq!(OpenAIProvider::context_length_for_model(\"gpt-3.5-turbo-1106\"), 4096);\n    }\n\n    #[test]\n    fn test_context_length_unknown_defaults_high() {\n        // Unknown models default to 128K (newer default)\n        assert_eq!(OpenAIProvider::context_length_for_model(\"unknown-future-model\"), 128000);\n    }\n\n    #[test]\n    fn test_dimension_ada_model() {\n        assert_eq!(OpenAIProvider::dimension_for_model(\"text-embedding-ada-002\"), 1536);\n    }\n\n    #[test]\n    fn test_dimension_unknown_defaults() {\n        assert_eq!(OpenAIProvider::dimension_for_model(\"unknown-embedding\"), 1536);\n    }\n\n    #[test]\n    fn test_provider_name() {\n        let provider = OpenAIProvider::new(\"test-key\");\n        assert_eq!(LLMProvider::name(&provider), \"openai\");\n    }\n\n    #[test]\n    fn test_provider_max_context_length() {\n        let provider = OpenAIProvider::new(\"test-key\").with_model(\"gpt-4\");\n        assert_eq!(provider.max_context_length(), 8192);\n    }\n\n    #[test]\n    fn test_provider_dimension() {\n        let provider = OpenAIProvider::new(\"test-key\")\n            .with_embedding_model(\"text-embedding-3-large\");\n        assert_eq!(provider.dimension(), 3072);\n    }\n\n    #[test]\n    fn test_provider_embedding_model() {\n        let provider = OpenAIProvider::new(\"test-key\")\n            .with_embedding_model(\"text-embedding-3-small\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"text-embedding-3-small\");\n    }\n\n    #[test]\n    fn test_message_conversion_tool_role() {\n        let messages = vec![ChatMessage::tool_result(\"call_1\", \"result data\")];\n        // Tool messages convert to user messages in simplified API\n        let converted = OpenAIProvider::convert_messages(&messages).unwrap();\n        assert_eq!(converted.len(), 1);\n    }\n\n    #[test]\n    fn test_supports_streaming() {\n        let provider = OpenAIProvider::new(\"test-key\");\n        assert!(provider.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_json_mode_gpt4() {\n        let provider = OpenAIProvider::new(\"test-key\").with_model(\"gpt-4o\");\n        assert!(provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_supports_json_mode_gpt35() {\n        let provider = OpenAIProvider::new(\"test-key\").with_model(\"gpt-3.5-turbo\");\n        assert!(provider.supports_json_mode());\n    }\n\n    #[test]\n    fn test_supports_json_mode_default_is_false() {\n        // Default model is gpt-5-mini which doesn't have \"gpt-4\" or \"gpt-3.5-turbo\" in name\n        let provider = OpenAIProvider::new(\"test-key\");\n        assert!(!provider.supports_json_mode());\n    }\n}\n","traces":[{"line":38,"address":[],"length":0,"stats":{"Line":11}},{"line":39,"address":[],"length":0,"stats":{"Line":44}},{"line":40,"address":[],"length":0,"stats":{"Line":22}},{"line":45,"address":[],"length":0,"stats":{"Line":11}},{"line":47,"address":[],"length":0,"stats":{"Line":33}},{"line":48,"address":[],"length":0,"stats":{"Line":33}},{"line":49,"address":[],"length":0,"stats":{"Line":11}},{"line":56,"address":[],"length":0,"stats":{"Line":0}},{"line":57,"address":[],"length":0,"stats":{"Line":0}},{"line":58,"address":[],"length":0,"stats":{"Line":0}},{"line":59,"address":[],"length":0,"stats":{"Line":0}},{"line":60,"address":[],"length":0,"stats":{"Line":0}},{"line":64,"address":[],"length":0,"stats":{"Line":4}},{"line":65,"address":[],"length":0,"stats":{"Line":12}},{"line":66,"address":[],"length":0,"stats":{"Line":4}},{"line":67,"address":[],"length":0,"stats":{"Line":4}},{"line":71,"address":[],"length":0,"stats":{"Line":3}},{"line":72,"address":[],"length":0,"stats":{"Line":9}},{"line":73,"address":[],"length":0,"stats":{"Line":3}},{"line":74,"address":[],"length":0,"stats":{"Line":3}},{"line":78,"address":[],"length":0,"stats":{"Line":21}},{"line":79,"address":[],"length":0,"stats":{"Line":21}},{"line":81,"address":[],"length":0,"stats":{"Line":86}},{"line":82,"address":[],"length":0,"stats":{"Line":40}},{"line":83,"address":[],"length":0,"stats":{"Line":74}},{"line":86,"address":[],"length":0,"stats":{"Line":32}},{"line":89,"address":[],"length":0,"stats":{"Line":66}},{"line":90,"address":[],"length":0,"stats":{"Line":30}},{"line":93,"address":[],"length":0,"stats":{"Line":30}},{"line":94,"address":[],"length":0,"stats":{"Line":24}},{"line":95,"address":[],"length":0,"stats":{"Line":22}},{"line":96,"address":[],"length":0,"stats":{"Line":26}},{"line":99,"address":[],"length":0,"stats":{"Line":12}},{"line":100,"address":[],"length":0,"stats":{"Line":14}},{"line":103,"address":[],"length":0,"stats":{"Line":2}},{"line":106,"address":[],"length":0,"stats":{"Line":4}},{"line":108,"address":[],"length":0,"stats":{"Line":1}},{"line":113,"address":[],"length":0,"stats":{"Line":7}},{"line":114,"address":[],"length":0,"stats":{"Line":7}},{"line":115,"address":[],"length":0,"stats":{"Line":20}},{"line":116,"address":[],"length":0,"stats":{"Line":12}},{"line":117,"address":[],"length":0,"stats":{"Line":6}},{"line":118,"address":[],"length":0,"stats":{"Line":1}},{"line":123,"address":[],"length":0,"stats":{"Line":2}},{"line":124,"address":[],"length":0,"stats":{"Line":2}},{"line":126,"address":[],"length":0,"stats":{"Line":6}},{"line":127,"address":[],"length":0,"stats":{"Line":4}},{"line":128,"address":[],"length":0,"stats":{"Line":2}},{"line":129,"address":[],"length":0,"stats":{"Line":2}},{"line":130,"address":[],"length":0,"stats":{"Line":1}},{"line":131,"address":[],"length":0,"stats":{"Line":1}},{"line":132,"address":[],"length":0,"stats":{"Line":1}},{"line":133,"address":[],"length":0,"stats":{"Line":2}},{"line":134,"address":[],"length":0,"stats":{"Line":2}},{"line":135,"address":[],"length":0,"stats":{"Line":1}},{"line":136,"address":[],"length":0,"stats":{"Line":1}},{"line":137,"address":[],"length":0,"stats":{"Line":1}},{"line":138,"address":[],"length":0,"stats":{"Line":2}},{"line":139,"address":[],"length":0,"stats":{"Line":2}},{"line":140,"address":[],"length":0,"stats":{"Line":1}},{"line":141,"address":[],"length":0,"stats":{"Line":1}},{"line":142,"address":[],"length":0,"stats":{"Line":1}},{"line":145,"address":[],"length":0,"stats":{"Line":2}},{"line":146,"address":[],"length":0,"stats":{"Line":2}},{"line":147,"address":[],"length":0,"stats":{"Line":1}},{"line":148,"address":[],"length":0,"stats":{"Line":1}},{"line":149,"address":[],"length":0,"stats":{"Line":1}},{"line":159,"address":[],"length":0,"stats":{"Line":3}},{"line":160,"address":[],"length":0,"stats":{"Line":3}},{"line":163,"address":[],"length":0,"stats":{"Line":1}},{"line":164,"address":[],"length":0,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":1}},{"line":168,"address":[],"length":0,"stats":{"Line":1}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":275,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":0}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":312,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":1}},{"line":323,"address":[],"length":0,"stats":{"Line":1}},{"line":340,"address":[],"length":0,"stats":{"Line":0}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":343,"address":[],"length":0,"stats":{"Line":0}},{"line":344,"address":[],"length":0,"stats":{"Line":0}},{"line":345,"address":[],"length":0,"stats":{"Line":0}},{"line":346,"address":[],"length":0,"stats":{"Line":0}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[],"length":0,"stats":{"Line":0}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":388,"address":[],"length":0,"stats":{"Line":0}},{"line":389,"address":[],"length":0,"stats":{"Line":0}},{"line":390,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":397,"address":[],"length":0,"stats":{"Line":0}},{"line":398,"address":[],"length":0,"stats":{"Line":0}},{"line":400,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[],"length":0,"stats":{"Line":0}},{"line":402,"address":[],"length":0,"stats":{"Line":0}},{"line":403,"address":[],"length":0,"stats":{"Line":0}},{"line":404,"address":[],"length":0,"stats":{"Line":0}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":406,"address":[],"length":0,"stats":{"Line":0}},{"line":407,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":0}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":415,"address":[],"length":0,"stats":{"Line":0}},{"line":416,"address":[],"length":0,"stats":{"Line":0}},{"line":417,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":420,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[],"length":0,"stats":{"Line":0}},{"line":425,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[],"length":0,"stats":{"Line":0}},{"line":439,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":3}},{"line":444,"address":[],"length":0,"stats":{"Line":5}},{"line":450,"address":[],"length":0,"stats":{"Line":1}},{"line":451,"address":[],"length":0,"stats":{"Line":1}},{"line":458,"address":[],"length":0,"stats":{"Line":1}},{"line":459,"address":[],"length":0,"stats":{"Line":1}},{"line":462,"address":[],"length":0,"stats":{"Line":3}},{"line":463,"address":[],"length":0,"stats":{"Line":3}},{"line":466,"address":[],"length":0,"stats":{"Line":0}},{"line":467,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":481,"address":[],"length":0,"stats":{"Line":0}}],"covered":78,"coverable":143},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","openai_compatible.rs"],"content":"//! OpenAI-compatible provider for custom LLM services.\n//!\n//! @implements OODA-200: Configurable LLM Providers\n//!\n//! This provider supports any API that follows the OpenAI chat completions format:\n//! - Z.ai (GLM models)\n//! - DeepSeek\n//! - Together AI\n//! - Groq\n//! - Any OpenAI-compatible endpoint\n//!\n//! # Configuration Example\n//!\n//! ```toml\n//! [[providers]]\n//! name = \"zai\"\n//! display_name = \"Z.AI Platform\"\n//! type = \"openai_compatible\"\n//! api_key_env = \"ZAI_API_KEY\"\n//! base_url = \"https://api.z.ai/api/paas/v4\"\n//! default_llm_model = \"glm-4.7\"\n//!\n//! [providers.headers]\n//! Accept-Language = \"en-US,en\"\n//!\n//! [[providers.models]]\n//! name = \"glm-4.7\"\n//! context_length = 128000\n//! ```\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse reqwest::header::{self, HeaderMap, HeaderName, HeaderValue};\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse std::time::Duration;\nuse tracing::{debug, warn};\n\nuse crate::error::{LlmError, Result};\nuse crate::model_config::{ModelCard, ModelType, ProviderConfig};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, FunctionCall, LLMProvider,\n    LLMResponse, StreamChunk, ToolCall, ToolChoice, ToolDefinition,\n};\n\n// ============================================================================\n// Request/Response Types (OpenAI-compatible format)\n// ============================================================================\n\n/// OpenAI-compatible chat request body.\n#[derive(Debug, Serialize)]\nstruct ChatRequest<'a> {\n    model: &'a str,\n    messages: Vec<MessageRequest>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    max_tokens: Option<usize>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stream: Option<bool>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<ToolDefinition>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_choice: Option<serde_json::Value>,\n    /// Z.ai specific: thinking mode configuration\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    thinking: Option<ThinkingConfig>,\n    /// Response format (for JSON mode)\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    response_format: Option<ResponseFormat>,\n}\n\n// ============================================================================\n// Message Content Types for Multimodal Support (OODA-52)\n// ============================================================================\n\n/// Request message content - either simple text or multipart with images.\n///\n/// WHY: Vision-capable models (GPT-4V, GPT-4o) accept content as either:\n/// - String: Simple text message\n/// - Array: Multipart with text and image_url parts\n///\n/// Using #[serde(untagged)] allows Serde to serialize the appropriate format\n/// based on the variant, maintaining backwards compatibility for text-only.\n#[derive(Debug, Serialize)]\n#[serde(untagged)]\nenum RequestContent {\n    /// Simple text content (serializes as string)\n    Text(String),\n    /// Multipart content with text and images (serializes as array)\n    Parts(Vec<ContentPart>),\n}\n\n/// Content part for multipart messages.\n///\n/// OpenAI vision API format uses tagged unions with \"type\" field.\n#[derive(Debug, Serialize)]\n#[serde(tag = \"type\")]\nenum ContentPart {\n    /// Text content part\n    #[serde(rename = \"text\")]\n    Text { text: String },\n    /// Image URL content part (supports base64 data URIs)\n    #[serde(rename = \"image_url\")]\n    ImageUrl { image_url: ImageUrlContent },\n}\n\n/// Image URL content for vision models.\n///\n/// Supports both:\n/// - URL references: \"https://example.com/image.png\"\n/// - Base64 data URIs: \"data:image/png;base64,...\"\n#[derive(Debug, Serialize)]\nstruct ImageUrlContent {\n    /// Image URL or data URI\n    url: String,\n    /// Detail level: \"auto\" (default), \"low\", or \"high\"\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    detail: Option<String>,\n}\n\n// ============================================================================\n// Original Request/Response Types\n// ============================================================================\n\n#[derive(Debug, Serialize)]\nstruct MessageRequest {\n    role: String,\n    /// Content can be string or multipart array for vision models\n    content: RequestContent,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_call_id: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_calls: Option<Vec<ToolCallRequest>>,\n}\n\n#[derive(Debug, Serialize)]\nstruct ToolCallRequest {\n    id: String,\n    #[serde(rename = \"type\")]\n    call_type: String,\n    function: FunctionCallRequest,\n}\n\n#[derive(Debug, Serialize)]\nstruct FunctionCallRequest {\n    name: String,\n    arguments: String,\n}\n\n/// Z.ai thinking mode configuration.\n#[derive(Debug, Serialize)]\nstruct ThinkingConfig {\n    #[serde(rename = \"type\")]\n    thinking_type: String, // \"enabled\" or \"disabled\"\n}\n\n#[derive(Debug, Serialize)]\nstruct ResponseFormat {\n    #[serde(rename = \"type\")]\n    format_type: String, // \"text\" or \"json_object\"\n}\n\n/// OpenAI-compatible chat response.\n#[derive(Debug, Deserialize)]\nstruct ChatResponse {\n    #[allow(dead_code)]\n    id: Option<String>,\n    #[allow(dead_code)]\n    model: Option<String>,\n    choices: Vec<Choice>,\n    usage: Option<Usage>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct Choice {\n    #[allow(dead_code)]\n    index: Option<usize>,\n    message: Option<MessageContent>,\n    // Delta is used for streaming responses (not currently implemented)\n    #[allow(dead_code)]\n    delta: Option<serde_json::Value>,\n    finish_reason: Option<String>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct MessageContent {\n    #[allow(dead_code)]\n    role: Option<String>,\n    content: Option<String>,\n    /// Z.ai specific: reasoning content from thinking mode\n    reasoning_content: Option<String>,\n    tool_calls: Option<Vec<ToolCallResponse>>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ToolCallResponse {\n    id: String,\n    #[serde(rename = \"type\")]\n    #[allow(dead_code)]\n    call_type: Option<String>,\n    function: FunctionCallResponse,\n}\n\n#[derive(Debug, Deserialize)]\nstruct FunctionCallResponse {\n    name: String,\n    arguments: String,\n}\n\n/// Completion tokens details (xAI, DeepSeek, OpenAI o-series)\n#[derive(Debug, Deserialize, Default)]\nstruct CompletionTokensDetails {\n    /// Tokens used for reasoning/thinking (OODA-28)\n    #[serde(default)]\n    reasoning_tokens: Option<usize>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct Usage {\n    prompt_tokens: usize,\n    completion_tokens: usize,\n    #[allow(dead_code)]\n    total_tokens: Option<usize>,\n    /// Details about completion tokens including reasoning (OODA-28)\n    #[serde(default)]\n    completion_tokens_details: Option<CompletionTokensDetails>,\n}\n\n/// Error response from API.\n#[derive(Debug, Deserialize)]\nstruct ErrorResponse {\n    error: ErrorDetail,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ErrorDetail {\n    message: String,\n    #[allow(dead_code)]\n    code: Option<String>,\n}\n\n/// Stream chunk for server-sent events (SSE) streaming.\n#[derive(Debug, Deserialize)]\nstruct ChatStreamChunk {\n    #[allow(dead_code)]\n    id: Option<String>,\n    #[allow(dead_code)]\n    model: Option<String>,\n    choices: Vec<StreamChoice>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct StreamChoice {\n    #[allow(dead_code)]\n    index: Option<usize>,\n    delta: Option<StreamDelta>,\n    #[allow(dead_code)]\n    finish_reason: Option<String>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct StreamDelta {\n    #[serde(default)]\n    content: Option<String>,\n    /// DeepSeek reasoning/thinking content (OODA-27)\n    /// Streamed before final content for deepseek-reasoner model\n    #[serde(default)]\n    reasoning_content: Option<String>,\n    tool_calls: Option<Vec<ToolCallDelta>>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ToolCallDelta {\n    index: Option<usize>,\n    id: Option<String>,\n    function: Option<FunctionDelta>,\n}\n\n#[derive(Debug, Deserialize)]\nstruct FunctionDelta {\n    name: Option<String>,\n    arguments: Option<String>,\n}\n\n// ============================================================================\n// OpenAI-Compatible Provider Implementation\n// ============================================================================\n\n/// A configurable provider for any OpenAI-compatible API.\n///\n/// This provider can connect to any service that implements the OpenAI\n/// chat completions API format, including:\n/// - Z.ai (GLM models)\n/// - DeepSeek\n/// - Together AI\n/// - Groq\n/// - Local services (Ollama, LM Studio)\n#[derive(Debug)]\npub struct OpenAICompatibleProvider {\n    /// HTTP client with pre-configured headers\n    client: Client,\n    /// Provider configuration from TOML\n    config: ProviderConfig,\n    /// API key (resolved from environment)\n    api_key: String,\n    /// Current model name\n    model: String,\n    /// Model capabilities from config\n    model_card: Option<ModelCard>,\n    /// Base URL for API calls\n    base_url: String,\n}\n\nimpl OpenAICompatibleProvider {\n    /// Create provider from TOML configuration.\n    ///\n    /// # Arguments\n    ///\n    /// * `config` - Provider configuration from models.toml\n    ///\n    /// # Errors\n    ///\n    /// Returns error if:\n    /// - Required API key environment variable is not set\n    /// - Base URL is not configured\n    pub fn from_config(config: ProviderConfig) -> Result<Self> {\n        // 1. Resolve API key from environment\n        let api_key = Self::resolve_api_key(&config)?;\n\n        // 2. Resolve base URL\n        let base_url = Self::resolve_base_url(&config)?;\n\n        // 3. Build HTTP client with custom headers\n        let client = Self::build_client(&config)?;\n\n        // 4. Get default model\n        let model = config\n            .default_llm_model\n            .clone()\n            .unwrap_or_else(|| \"default\".to_string());\n\n        // 5. Find model card for capabilities\n        let model_card = config.models.iter().find(|m| m.name == model).cloned();\n\n        debug!(\n            provider = config.name,\n            model = model,\n            base_url = base_url,\n            \"Created OpenAI-compatible provider\"\n        );\n\n        Ok(Self {\n            client,\n            config,\n            api_key,\n            model,\n            model_card,\n            base_url,\n        })\n    }\n\n    /// Resolve API key from environment variable.\n    fn resolve_api_key(config: &ProviderConfig) -> Result<String> {\n        if let Some(env_var) = &config.api_key_env {\n            std::env::var(env_var).map_err(|_| {\n                LlmError::ConfigError(format!(\n                    \"API key environment variable '{}' not set for provider '{}'. \\\n                     Please set it with: export {}=your-api-key\",\n                    env_var, config.name, env_var\n                ))\n            })\n        } else {\n            // Some providers don't require API key (local servers)\n            Ok(String::new())\n        }\n    }\n\n    /// Resolve base URL from config or environment.\n    fn resolve_base_url(config: &ProviderConfig) -> Result<String> {\n        // Check environment variable override first\n        if let Some(env_var) = &config.base_url_env {\n            if let Ok(url) = std::env::var(env_var) {\n                return Ok(url);\n            }\n        }\n\n        // Use config base_url\n        config\n            .base_url\n            .clone()\n            .ok_or_else(|| {\n                LlmError::ConfigError(format!(\n                    \"Provider '{}' requires 'base_url' or 'base_url_env' to be set\",\n                    config.name\n                ))\n            })\n    }\n\n    /// Build HTTP client with custom headers.\n    fn build_client(config: &ProviderConfig) -> Result<Client> {\n        let mut headers = HeaderMap::new();\n\n        // Add Content-Type header\n        headers.insert(\n            header::CONTENT_TYPE,\n            HeaderValue::from_static(\"application/json\"),\n        );\n\n        // NOTE: Not adding User-Agent here - some providers (POE) reject custom agents\n        // Reqwest will use its default User-Agent which works better\n\n        // Add custom headers from config\n        for (key, value) in &config.headers {\n            let header_name = HeaderName::from_bytes(key.as_bytes()).map_err(|e| {\n                LlmError::ConfigError(format!(\"Invalid header name '{}': {}\", key, e))\n            })?;\n            let header_value = HeaderValue::from_str(value).map_err(|e| {\n                LlmError::ConfigError(format!(\"Invalid header value for '{}': {}\", key, e))\n            })?;\n            headers.insert(header_name, header_value);\n        }\n\n        Client::builder()\n            .default_headers(headers)\n            .timeout(Duration::from_secs(config.timeout_seconds))\n            .build()\n            .map_err(|e| LlmError::ConfigError(format!(\"Failed to build HTTP client: {}\", e)))\n    }\n\n    /// Build the chat completions endpoint URL.\n    fn chat_completions_url(&self) -> String {\n        let base = self.base_url.trim_end_matches('/');\n        format!(\"{}/chat/completions\", base)\n    }\n\n    /// Extract content from message, supporting both standard and reasoning content.\n    ///\n    /// Z.AI and similar providers return content in `reasoning_content` field when\n    /// thinking mode is enabled. This function checks both fields and returns the\n    /// appropriate content.\n    fn extract_content(message: &MessageContent) -> String {\n        // Prioritize reasoning_content (Z.AI thinking mode)\n        if let Some(ref reasoning) = message.reasoning_content {\n            if !reasoning.is_empty() {\n                return reasoning.clone();\n            }\n        }\n        // Fall back to standard content field\n        message.content.clone().unwrap_or_default()\n    }\n\n    /// Convert ChatMessage to request format (OODA-52: supports multimodal).\n    ///\n    /// WHY: Vision models require content as array of parts when images present.\n    /// This function checks for images and builds appropriate format:\n    /// - No images: content = \"text\" (simple string)\n    /// - With images: content = [{type: \"text\", ...}, {type: \"image_url\", ...}]\n    fn convert_messages(messages: &[ChatMessage]) -> Vec<MessageRequest> {\n        messages\n            .iter()\n            .map(|msg| {\n                let role = match msg.role {\n                    ChatRole::System => \"system\",\n                    ChatRole::User => \"user\",\n                    ChatRole::Assistant => \"assistant\",\n                    ChatRole::Tool | ChatRole::Function => \"tool\",\n                };\n\n                // OODA-52: Check if message has images for multipart content\n                let content = if msg.has_images() {\n                    // Build multipart content with text and images\n                    let mut parts = Vec::new();\n                    \n                    // Add text part first (if not empty)\n                    if !msg.content.is_empty() {\n                        parts.push(ContentPart::Text { text: msg.content.clone() });\n                    }\n                    \n                    // Add image parts\n                    if let Some(ref images) = msg.images {\n                        for img in images {\n                            parts.push(ContentPart::ImageUrl {\n                                image_url: ImageUrlContent {\n                                    url: img.to_data_uri(),\n                                    detail: img.detail.clone(),\n                                },\n                            });\n                        }\n                    }\n                    \n                    RequestContent::Parts(parts)\n                } else {\n                    // Simple text content\n                    RequestContent::Text(msg.content.clone())\n                };\n\n                MessageRequest {\n                    role: role.to_string(),\n                    content,\n                    tool_call_id: msg.tool_call_id.clone(),\n                    tool_calls: None,\n                }\n            })\n            .collect()\n    }\n\n    /// Convert ToolChoice to JSON value.\n    fn convert_tool_choice(choice: &ToolChoice) -> serde_json::Value {\n        match choice {\n            ToolChoice::Auto(_) => serde_json::json!(\"auto\"),\n            ToolChoice::Required(_) => serde_json::json!(\"required\"),\n            ToolChoice::Function {\n                function,\n                ..\n            } => {\n                serde_json::json!({\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": function.name\n                    }\n                })\n            }\n        }\n    }\n\n    /// Make a non-streaming chat completion request.\n    async fn chat_request(&self, request: &ChatRequest<'_>) -> Result<ChatResponse> {\n        let url = self.chat_completions_url();\n        \n        debug!(\"OpenAI-compatible API Request: url={} model={} provider={}\", url, request.model, self.config.name);\n\n        let mut req_builder = self.client.post(&url);\n\n        // Add authorization if API key is set\n        if !self.api_key.is_empty() {\n            req_builder = req_builder.header(\"Authorization\", format!(\"Bearer {}\", self.api_key));\n            debug!(\"API key: {}...\", &self.api_key[..20.min(self.api_key.len())]);\n        } else {\n            warn!(\"No API key set for provider: {}\", self.config.name);\n        }\n\n        let response = req_builder\n            .json(request)\n            .send()\n            .await\n            .map_err(|e| {\n                warn!(\"Network error calling {} API: {}\", self.config.name, e);\n                LlmError::NetworkError(format!(\"Failed to connect to {}: {}\", url, e))\n            })?;\n\n        let status = response.status();\n        debug!(\"{} API Response: status={}\", self.config.name, status);\n        \n        let body = response\n            .text()\n            .await\n            .map_err(|e| {\n                warn!(\"Failed to read response body: {}\", e);\n                LlmError::NetworkError(e.to_string())\n            })?;\n\n        if !status.is_success() {\n            warn!(\"{} API error: status={} body={}\", self.config.name, status, body);\n            // Try to parse error response\n            if let Ok(error_resp) = serde_json::from_str::<ErrorResponse>(&body) {\n                return Err(LlmError::ApiError(format!(\n                    \"{} API {}: {}\",\n                    self.config.name,\n                    status.as_u16(),\n                    error_resp.error.message\n                )));\n            }\n            return Err(LlmError::ApiError(format!(\n                \"{} API error {}: {}\",\n                self.config.name,\n                status.as_u16(),\n                body\n            )));\n        }\n\n        debug!(\"{} API success, parsing response body (length: {})\", self.config.name, body.len());\n        \n        // OODA-99.3: Log raw response for GLM debugging\n        if self.model.to_lowercase().contains(\"glm\") {\n            debug!(\"GLM response body: {}\", &body[..1000.min(body.len())]);\n        }\n        \n        serde_json::from_str(&body)\n            .map_err(|e| {\n                warn!(\"Failed to parse {} response: {} | body: {}\", self.config.name, e, &body[..500.min(body.len())]);\n                LlmError::ApiError(format!(\"Failed to parse {} response: {} | body preview: {}\", self.config.name, e, &body[..500.min(body.len())]))\n            })\n    }\n\n    /// Set the model for this provider.\n    pub fn with_model(mut self, model: impl Into<String>) -> Self {\n        let model_name = model.into();\n        self.model_card = self.config.models.iter().find(|m| m.name == model_name).cloned();\n        self.model = model_name;\n        self\n    }\n\n    /// Get the current model card.\n    pub fn model_card(&self) -> Option<&ModelCard> {\n        self.model_card.as_ref()\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for OpenAICompatibleProvider {\n    fn name(&self) -> &str {\n        &self.config.name\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.model_card\n            .as_ref()\n            .map(|m| m.capabilities.context_length)\n            .unwrap_or(128000)\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let options = options.cloned().unwrap_or_default();\n        let messages_req = Self::convert_messages(messages);\n\n        // Check if JSON mode is requested via response_format\n        let use_json_mode = options\n            .response_format\n            .as_ref()\n            .map(|f| f == \"json_object\" || f == \"json\")\n            .unwrap_or(false);\n\n        // Build request (no tools for basic chat)\n        let request = ChatRequest {\n            model: &self.model,\n            messages: messages_req,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: Some(false),\n            tools: None,\n            tool_choice: None,\n            thinking: if self.config.supports_thinking {\n                Some(ThinkingConfig {\n                    thinking_type: \"enabled\".to_string(),\n                })\n            } else {\n                None\n            },\n            response_format: if use_json_mode {\n                Some(ResponseFormat {\n                    format_type: \"json_object\".to_string(),\n                })\n            } else {\n                None\n            },\n        };\n\n        let response = self.chat_request(&request).await?;\n\n        // Extract content from response\n        let choice = response.choices.first().ok_or_else(|| {\n            LlmError::ApiError(\"No choices in response\".to_string())\n        })?;\n\n        let message = choice.message.as_ref().ok_or_else(|| {\n            LlmError::ApiError(\"No message in choice\".to_string())\n        })?;\n\n        let content = message.content.clone().unwrap_or_default();\n\n        // Extract usage including reasoning tokens (OODA-28)\n        let (prompt_tokens, completion_tokens, reasoning_tokens) = response\n            .usage\n            .as_ref()\n            .map(|u| {\n                let reasoning = u.completion_tokens_details\n                    .as_ref()\n                    .and_then(|d| d.reasoning_tokens);\n                (u.prompt_tokens, u.completion_tokens, reasoning)\n            })\n            .unwrap_or((0, 0, None));\n\n        let mut llm_response = LLMResponse::new(content, &self.model)\n            .with_usage(prompt_tokens, completion_tokens)\n            .with_finish_reason(choice.finish_reason.clone().unwrap_or_else(|| \"stop\".to_string()));\n\n        // Add reasoning tokens if available (xAI, DeepSeek)\n        if let Some(tokens) = reasoning_tokens {\n            llm_response = llm_response.with_thinking_tokens(tokens);\n        }\n\n        Ok(llm_response)\n    }\n\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        debug!(\n            \"chat_with_tools called: model={} provider={} messages={} tools={}\",\n            self.model,\n            self.config.name,\n            messages.len(),\n            tools.len()\n        );\n        \n        let options = options.cloned().unwrap_or_default();\n        let messages_req = Self::convert_messages(messages);\n\n        // Build request with tools\n        let request = ChatRequest {\n            model: &self.model,\n            messages: messages_req,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: Some(false),\n            tools: if tools.is_empty() {\n                None\n            } else {\n                Some(tools.to_vec())\n            },\n            tool_choice: tool_choice.as_ref().map(Self::convert_tool_choice),\n            thinking: if self.config.supports_thinking {\n                Some(ThinkingConfig {\n                    thinking_type: \"enabled\".to_string(),\n                })\n            } else {\n                None\n            },\n            response_format: None,\n        };\n\n        let response = self.chat_request(&request).await?;\n\n        // Extract content from response\n        let choice = response.choices.first().ok_or_else(|| {\n            LlmError::ApiError(\"No choices in response\".to_string())\n        })?;\n\n        let message = choice.message.as_ref().ok_or_else(|| {\n            LlmError::ApiError(\"No message in choice\".to_string())\n        })?;\n\n        let content = Self::extract_content(message);\n\n        // OODA-99.3: Debug log message structure for GLM\n        if self.model.to_lowercase().contains(\"glm\") {\n            debug!(\n                \"GLM message structure - content_len={} tool_calls_present={} tool_calls_count={}\",\n                content.len(),\n                message.tool_calls.is_some(),\n                message.tool_calls.as_ref().map(|t| t.len()).unwrap_or(0)\n            );\n        }\n\n        // Extract tool calls if present\n        let tool_calls: Vec<ToolCall> = message\n            .tool_calls\n            .as_ref()\n            .map(|calls| {\n                calls\n                    .iter()\n                    .map(|tc| {\n                        // OODA-99.3: Debug logging for GLM empty arguments issue\n                        if tc.function.arguments.is_empty() || tc.function.arguments == \"{}\" {\n                            warn!(\n                                \"Empty tool arguments detected - tool={} id={} args='{}' (GLM model may not be providing arguments correctly)\",\n                                tc.function.name, tc.id, tc.function.arguments\n                            );\n                        } else {\n                            debug!(\n                                \"Tool call extracted - tool={} args_len={} id={}\",\n                                tc.function.name, tc.function.arguments.len(), tc.id\n                            );\n                        }\n                        \n                        ToolCall {\n                            id: tc.id.clone(),\n                            call_type: \"function\".to_string(),\n                            function: FunctionCall {\n                                name: tc.function.name.clone(),\n                                arguments: tc.function.arguments.clone(),\n                            },\n                        }\n                    })\n                    .collect()\n            })\n            .unwrap_or_default();\n\n        // Extract usage including reasoning tokens (OODA-28)\n        let (prompt_tokens, completion_tokens, reasoning_tokens) = response\n            .usage\n            .as_ref()\n            .map(|u| {\n                let reasoning = u.completion_tokens_details\n                    .as_ref()\n                    .and_then(|d| d.reasoning_tokens);\n                (u.prompt_tokens, u.completion_tokens, reasoning)\n            })\n            .unwrap_or((0, 0, None));\n\n        let mut llm_response = LLMResponse::new(content, &self.model)\n            .with_usage(prompt_tokens, completion_tokens)\n            .with_tool_calls(tool_calls)\n            .with_finish_reason(choice.finish_reason.clone().unwrap_or_else(|| \"stop\".to_string()));\n\n        // Add reasoning tokens if available (xAI, DeepSeek)\n        if let Some(tokens) = reasoning_tokens {\n            llm_response = llm_response.with_thinking_tokens(tokens);\n        }\n\n        Ok(llm_response)\n    }\n\n    fn supports_streaming(&self) -> bool {\n        self.model_card\n            .as_ref()\n            .map(|m| m.capabilities.supports_streaming)\n            .unwrap_or(true)\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        self.model_card\n            .as_ref()\n            .map(|m| m.capabilities.supports_function_calling)\n            .unwrap_or(true)\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        use futures::StreamExt;\n\n        let messages = vec![MessageRequest {\n            role: \"user\".to_string(),\n            content: RequestContent::Text(prompt.to_string()),\n            tool_call_id: None,\n            tool_calls: None,\n        }];\n\n        let request = ChatRequest {\n            model: &self.model,\n            messages,\n            temperature: None,\n            max_tokens: None,\n            stream: Some(true),\n            tools: None,\n            tool_choice: None,\n            thinking: None,\n            response_format: None,\n        };\n\n        let url = self.chat_completions_url();\n        \n        // DEBUG: Log full request details\n        debug!(\"{} Stream Request: url={} model={}\", self.config.name, url, &self.model);\n        debug!(\"{} Stream Request body: {}\", self.config.name, serde_json::to_string_pretty(&request).unwrap_or_default());\n        \n        let mut req_builder = self.client.post(&url);\n\n        if !self.api_key.is_empty() {\n            req_builder = req_builder.header(\"Authorization\", format!(\"Bearer {}\", self.api_key));\n        }\n\n        let req_builder = req_builder.json(&request);\n\n        use reqwest_eventsource::EventSource;\n        let event_source = EventSource::new(req_builder)\n            .map_err(|e| {\n                let error_msg = e.to_string();\n                warn!(\"Failed to create event source: {}\", error_msg);\n                \n                // OODA-99: Provide helpful guidance for 400 Bad Request errors\n                // OODA-100: Enhanced to also detect tool/function calling issues\n                if error_msg.contains(\"400\") && error_msg.contains(\"Bad Request\") {\n                    let error_lower = error_msg.to_lowercase();\n                    \n                    // Check if this is a tool/function calling issue\n                    if error_lower.contains(\"tool\") \n                        || error_lower.contains(\"function\")\n                        || error_msg.contains(\"not supported\")\n                        || error_msg.contains(\"No endpoints found\") {\n                        LlmError::ApiError(format!(\n                            \"stream failed: {}\\n\\n\\\n                            ðŸ’¡ Model doesn't support function calling required by EdgeCode React agent.\\n\\\n                            \\n\\\n                            Try one of these compatible models:\\n\\\n                            - anthropic/claude-3.5-sonnet (recommended)\\n\\\n                            - openai/gpt-4o\\n\\\n                            - google/gemini-2.0-flash-exp\\n\\\n                            - meta-llama/llama-3.3-70b-instruct\\n\\\n                            \\n\\\n                            Use /model to select a different model.\",\n                            error_msg\n                        ))\n                    } else {\n                        // Context window issue\n                        LlmError::ApiError(format!(\n                            \"stream failed: {}\\n\\n\\\n                            ðŸ’¡ Troubleshooting 400 Bad Request:\\n\\\n                            \\n\\\n                            If using LMStudio:\\n\\\n                            â€¢ The prompt likely exceeds your model's configured context window\\n\\\n                            â€¢ Solution 1: Increase context length in LMStudio model settings (32K+ recommended)\\n\\\n                            â€¢ Solution 2: Set LMSTUDIO_CONTEXT_LENGTH environment variable (e.g., 32768 or 65536)\\n\\\n                            â€¢ Solution 3: Use a model with larger context window\\n\\\n                            â€¢ Solution 4: Reduce task complexity or working directory size\\n\\\n                            \\n\\\n                            If using other providers:\\n\\\n                            â€¢ Check the model's context limits in the provider's documentation\\n\\\n                            â€¢ Reduce the amount of context being sent (files, history, etc.)\\n\\\n                            â€¢ Use a model with larger context window\",\n                            error_msg\n                        ))\n                    }\n                } else {\n                    LlmError::ApiError(format!(\"stream failed: {}\", error_msg))\n                }\n            })?;\n\n        use futures::stream;\n        use reqwest_eventsource::Event;\n        \n        let stream = stream::unfold(event_source, |mut es| async move {\n            match es.next().await {\n                Some(Ok(Event::Open)) => {\n                    // Connection opened, continue to next event\n                    Some((Ok(\"\".to_string()), es))\n                }\n                Some(Ok(Event::Message(msg))) => {\n                    if msg.data == \"[DONE]\" {\n                        es.close();\n                        return None;\n                    }\n\n                    // Parse SSE chunk\n                    match serde_json::from_str::<ChatStreamChunk>(&msg.data) {\n                        Ok(chunk) => {\n                            if let Some(choice) = chunk.choices.first() {\n                                if let Some(ref delta) = choice.delta {\n                                    // OODA-27: DeepSeek reasoning_content support\n                                    // Note: basic stream() returns String, so we prefix thinking\n                                    if let Some(ref reasoning) = delta.reasoning_content {\n                                        if !reasoning.is_empty() {\n                                            // For basic stream, we emit reasoning as-is\n                                            // The consumer can detect it via <think> tags if needed\n                                            return Some((Ok(reasoning.clone()), es));\n                                        }\n                                    }\n                                    if let Some(ref content) = delta.content {\n                                        if !content.is_empty() {\n                                            return Some((Ok(content.clone()), es));\n                                        }\n                                    }\n                                }\n                            }\n                            // No content in this chunk, continue to next\n                            Some((Ok(\"\".to_string()), es))\n                        }\n                        Err(e) => {\n                            warn!(\"Failed to parse stream chunk: {} | data: {}\", e, msg.data);\n                            Some((Err(LlmError::ApiError(format!(\"Parse error: {}\", e))), es))\n                        }\n                    }\n                }\n                Some(Err(e)) => {\n                    es.close();\n                    Some((Err(LlmError::ApiError(format!(\"Stream error: {}\", e))), es))\n                }\n                None => None,\n            }\n        });\n\n        Ok(stream.boxed())\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        // Tool streaming is supported if both streaming and function calling are supported\n        self.supports_streaming() && self.supports_function_calling()\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        use futures::stream::{self, StreamExt};\n        use reqwest_eventsource::{Event, EventSource};\n\n        let options = options.cloned().unwrap_or_default();\n        let messages_req = Self::convert_messages(messages);\n\n        // Build streaming request with tools\n        let request = ChatRequest {\n            model: &self.model,\n            messages: messages_req,\n            temperature: options.temperature,\n            max_tokens: options.max_tokens,\n            stream: Some(true),\n            tools: if tools.is_empty() {\n                None\n            } else {\n                Some(tools.to_vec())\n            },\n            tool_choice: tool_choice.as_ref().map(Self::convert_tool_choice),\n            thinking: if self.config.supports_thinking {\n                Some(ThinkingConfig {\n                    thinking_type: \"enabled\".to_string(),\n                })\n            } else {\n                None\n            },\n            response_format: None,\n        };\n\n        let url = self.chat_completions_url();\n        \n        debug!(\n            \"Starting streaming request: model={} url={} tools={}\",\n            self.model,\n            url,\n            tools.len()\n        );\n        \n        let mut req_builder = self.client.post(&url);\n\n        // Add authorization if API key is set\n        if !self.api_key.is_empty() {\n            req_builder = req_builder.header(\"Authorization\", format!(\"Bearer {}\", self.api_key));\n        }\n\n        let req_builder = req_builder.json(&request);\n        \n        // Create event source for SSE streaming\n        let event_source = EventSource::new(req_builder)\n            .map_err(|e| {\n                warn!(\"Failed to create event source: {}\", e);\n                LlmError::ApiError(format!(\"Failed to create event source: {}\", e))\n            })?;\n\n        let stream = stream::unfold(event_source, |mut es| async move {\n            match es.next().await {\n                Some(Ok(Event::Open)) => {\n                    // Connection opened, continue to next event\n                    Some((Ok(StreamChunk::Content(\"\".to_string())), es))\n                }\n                Some(Ok(Event::Message(msg))) => {\n                    if msg.data == \"[DONE]\" {\n                        es.close();\n                        return None;\n                    }\n\n                    // OODA-99.3: Log raw SSE message data for GLM tool debugging\n                    if msg.data.contains(\"tool_calls\") || msg.data.contains(\"write_file\") {\n                        debug!(\"RAW SSE message (len={}): {}\", msg.data.len(), &msg.data);\n                    }\n\n                    // Parse SSE chunk\n                    match serde_json::from_str::<ChatStreamChunk>(&msg.data) {\n                        Ok(chunk) => {\n                            if let Some(choice) = chunk.choices.first() {\n                                if let Some(ref delta) = choice.delta {\n                                    // OODA-27: DeepSeek reasoning_content (thinking) chunk\n                                    // Reasoning content comes first, then final content\n                                    if let Some(ref reasoning) = delta.reasoning_content {\n                                        if !reasoning.is_empty() {\n                                            return Some((Ok(StreamChunk::ThinkingContent {\n                                                text: reasoning.clone(),\n                                                tokens_used: None, // DeepSeek provides reasoning_tokens in final chunk\n                                                budget_total: None,\n                                            }), es));\n                                        }\n                                    }\n                                    \n                                    // Content chunk\n                                    if let Some(ref content) = delta.content {\n                                        if !content.is_empty() {\n                                            return Some((Ok(StreamChunk::Content(content.clone())), es));\n                                        }\n                                    }\n                                    \n                                    // Tool call deltas\n                                    if let Some(ref tool_calls) = delta.tool_calls {\n                                        // OODA-99.3: Log tool call structure for GLM debugging\n                                        for (i, tool_call) in tool_calls.iter().enumerate() {\n                                            if let Some(ref function) = tool_call.function {\n                                                debug!(\n                                                    \"GLM tool_call[{}] - id={:?} index={:?} name={:?} args_len={} args={:?}\",\n                                                    i,\n                                                    tool_call.id,\n                                                    tool_call.index,\n                                                    function.name,\n                                                    function.arguments.as_ref().map(|s| s.len()).unwrap_or(0),\n                                                    function.arguments\n                                                );\n                                            }\n                                        }\n                                        \n                                        for tool_call in tool_calls {\n                                            if let Some(ref function) = tool_call.function {\n                                                // OODA-99.3: Handle Z.AI GLM models that send name+arguments in single delta\n                                                // Must check for BOTH before returning to avoid early return bug\n                                                let has_name = function.name.is_some();\n                                                let has_args = function.arguments.is_some();\n                                                \n                                                if has_name || has_args {\n                                                    return Some((Ok(StreamChunk::ToolCallDelta {\n                                                        index: tool_call.index.unwrap_or(0),\n                                                        id: tool_call.id.clone(),\n                                                        function_name: function.name.clone(),\n                                                        function_arguments: function.arguments.clone(),\n                                                    }), es));\n                                                }\n                                            }\n                                        }\n                                    }\n                                }\n                            }\n                            // Empty chunk, continue\n                            Some((Ok(StreamChunk::Content(\"\".to_string())), es))\n                        }\n                        Err(e) => {\n                            warn!(\"Failed to parse stream chunk: {} | data: {}\", e, msg.data);\n                            es.close();\n                            Some((Err(LlmError::ApiError(format!(\"Failed to parse stream chunk: {} | data: {}\", e, msg.data))), es))\n                        }\n                    }\n                }\n                Some(Err(e)) => {\n                    // Stream error\n                    warn!(\"Stream error: {}\", e);\n                    Some((Err(LlmError::NetworkError(format!(\"Stream error: {}\", e))), es))\n                }\n                None => {\n                    // Stream ended\n                    None\n                }\n            }\n        });\n\n        Ok(Box::pin(stream))\n    }\n}\n\n// Implement EmbeddingProvider if the provider has embedding models\n#[async_trait]\nimpl EmbeddingProvider for OpenAICompatibleProvider {\n    fn name(&self) -> &str {\n        &self.config.name\n    }\n\n    fn model(&self) -> &str {\n        self.config\n            .default_embedding_model\n            .as_deref()\n            .unwrap_or(\"unknown\")\n    }\n\n    fn dimension(&self) -> usize {\n        // Find embedding model in config\n        self.config\n            .models\n            .iter()\n            .find(|m| matches!(m.model_type, ModelType::Embedding))\n            .map(|m| m.capabilities.embedding_dimension)\n            .unwrap_or(1536)\n    }\n\n    fn max_tokens(&self) -> usize {\n        // Find embedding model in config and get max tokens\n        self.config\n            .models\n            .iter()\n            .find(|m| matches!(m.model_type, ModelType::Embedding))\n            .map(|m| m.capabilities.max_output_tokens)\n            .unwrap_or(8192)\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // Check if we have an embedding model\n        let embedding_model = self\n            .config\n            .default_embedding_model\n            .as_ref()\n            .ok_or_else(|| {\n                LlmError::ConfigError(format!(\n                    \"Provider '{}' does not have an embedding model configured\",\n                    self.config.name\n                ))\n            })?;\n\n        warn!(\n            provider = self.config.name,\n            model = embedding_model,\n            text_count = texts.len(),\n            \"Embedding not fully implemented for OpenAI-compatible providers\"\n        );\n\n        Err(LlmError::NotSupported(\n            \"Embedding support for custom providers is not yet implemented. \\\n             Use a dedicated embedding provider like OpenAI.\"\n                .to_string(),\n        ))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    fn create_test_config() -> ProviderConfig {\n        ProviderConfig {\n            name: \"test-provider\".to_string(),\n            display_name: \"Test Provider\".to_string(),\n            provider_type: crate::model_config::ProviderType::OpenAICompatible,\n            api_key_env: Some(\"TEST_API_KEY\".to_string()),\n            base_url: Some(\"https://api.example.com/v1\".to_string()),\n            default_llm_model: Some(\"test-model\".to_string()),\n            models: vec![ModelCard {\n                name: \"test-model\".to_string(),\n                display_name: \"Test Model\".to_string(),\n                model_type: ModelType::Llm,\n                capabilities: crate::model_config::ModelCapabilities {\n                    context_length: 128000,\n                    max_output_tokens: 8192,\n                    supports_function_calling: true,\n                    supports_streaming: true,\n                    ..Default::default()\n                },\n                ..Default::default()\n            }],\n            ..Default::default()\n        }\n    }\n\n    #[test]\n    fn test_provider_creation_requires_api_key() {\n        let config = create_test_config();\n\n        // Should fail without API key set\n        std::env::remove_var(\"TEST_API_KEY\");\n        let result = OpenAICompatibleProvider::from_config(config);\n        assert!(result.is_err());\n        assert!(result.unwrap_err().to_string().contains(\"TEST_API_KEY\"));\n    }\n\n    #[test]\n    fn test_provider_creation_success() {\n        let config = create_test_config();\n\n        std::env::set_var(\"TEST_API_KEY\", \"test-key-12345\");\n        let provider = OpenAICompatibleProvider::from_config(config).unwrap();\n\n        assert_eq!(LLMProvider::name(&provider), \"test-provider\");\n        assert_eq!(LLMProvider::model(&provider), \"test-model\");\n        assert_eq!(provider.max_context_length(), 128000);\n        assert!(provider.supports_function_calling());\n\n        std::env::remove_var(\"TEST_API_KEY\");\n    }\n\n    #[test]\n    fn test_chat_completions_url() {\n        std::env::set_var(\"TEST_API_KEY2\", \"key\");\n\n        let mut config = create_test_config();\n        config.api_key_env = Some(\"TEST_API_KEY2\".to_string());\n        config.base_url = Some(\"https://api.z.ai/api/paas/v4\".to_string());\n\n        let provider = OpenAICompatibleProvider::from_config(config).unwrap();\n        assert_eq!(\n            provider.chat_completions_url(),\n            \"https://api.z.ai/api/paas/v4/chat/completions\"\n        );\n\n        std::env::remove_var(\"TEST_API_KEY2\");\n    }\n\n    #[test]\n    fn test_custom_headers() {\n        std::env::set_var(\"TEST_API_KEY3\", \"key\");\n\n        let mut config = create_test_config();\n        config.api_key_env = Some(\"TEST_API_KEY3\".to_string());\n        config.headers.insert(\"Accept-Language\".to_string(), \"en-US,en\".to_string());\n        config.headers.insert(\"X-Custom-Header\".to_string(), \"custom-value\".to_string());\n\n        let result = OpenAICompatibleProvider::from_config(config);\n        assert!(result.is_ok());\n\n        std::env::remove_var(\"TEST_API_KEY3\");\n    }\n\n    #[test]\n    fn test_convert_messages() {\n        let messages = vec![\n            ChatMessage::system(\"You are a helpful assistant.\"),\n            ChatMessage::user(\"Hello!\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let converted = OpenAICompatibleProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 3);\n        assert_eq!(converted[0].role, \"system\");\n        assert_eq!(converted[1].role, \"user\");\n        assert_eq!(converted[2].role, \"assistant\");\n    }\n\n    #[test]\n    fn test_base_url_env_override() {\n        std::env::set_var(\"TEST_API_KEY4\", \"key\");\n        std::env::set_var(\"CUSTOM_BASE_URL\", \"https://override.example.com/v1\");\n\n        let mut config = create_test_config();\n        config.api_key_env = Some(\"TEST_API_KEY4\".to_string());\n        config.base_url = Some(\"https://default.example.com/v1\".to_string());\n        config.base_url_env = Some(\"CUSTOM_BASE_URL\".to_string());\n\n        let provider = OpenAICompatibleProvider::from_config(config).unwrap();\n        assert_eq!(provider.base_url, \"https://override.example.com/v1\");\n\n        std::env::remove_var(\"TEST_API_KEY4\");\n        std::env::remove_var(\"CUSTOM_BASE_URL\");\n    }\n\n    #[test]\n    fn test_with_model() {\n        std::env::set_var(\"TEST_API_KEY5\", \"key\");\n\n        let mut config = create_test_config();\n        config.api_key_env = Some(\"TEST_API_KEY5\".to_string());\n        config.models.push(ModelCard {\n            name: \"another-model\".to_string(),\n            display_name: \"Another Model\".to_string(),\n            model_type: ModelType::Llm,\n            capabilities: crate::model_config::ModelCapabilities {\n                context_length: 32000,\n                ..Default::default()\n            },\n            ..Default::default()\n        });\n\n        let provider = OpenAICompatibleProvider::from_config(config)\n            .unwrap()\n            .with_model(\"another-model\");\n\n        assert_eq!(LLMProvider::model(&provider), \"another-model\");\n        assert_eq!(provider.max_context_length(), 32000);\n\n        std::env::remove_var(\"TEST_API_KEY5\");\n    }\n\n    // =========================================================================\n    // Multipart Message Tests (OODA-52)\n    // =========================================================================\n\n    #[test]\n    fn test_convert_messages_text_only() {\n        let messages = vec![ChatMessage::user(\"Hello, world!\")];\n        let converted = OpenAICompatibleProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 1);\n        assert_eq!(converted[0].role, \"user\");\n        \n        // Text-only should serialize as string\n        let json = serde_json::to_value(&converted[0]).unwrap();\n        assert_eq!(json[\"content\"], \"Hello, world!\");\n    }\n\n    #[test]\n    fn test_convert_messages_with_images() {\n        use crate::traits::ImageData;\n        \n        let images = vec![ImageData::new(\"base64data\", \"image/png\")];\n        let messages = vec![ChatMessage::user_with_images(\"What's this?\", images)];\n        let converted = OpenAICompatibleProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 1);\n        \n        // With images should serialize as array\n        let json = serde_json::to_value(&converted[0]).unwrap();\n        let content = &json[\"content\"];\n        \n        assert!(content.is_array());\n        assert_eq!(content.as_array().unwrap().len(), 2);\n        \n        // First part: text\n        assert_eq!(content[0][\"type\"], \"text\");\n        assert_eq!(content[0][\"text\"], \"What's this?\");\n        \n        // Second part: image_url\n        assert_eq!(content[1][\"type\"], \"image_url\");\n        assert!(content[1][\"image_url\"][\"url\"].as_str().unwrap().starts_with(\"data:image/png;base64,\"));\n    }\n\n    #[test]\n    fn test_convert_messages_with_image_detail() {\n        use crate::traits::ImageData;\n        \n        let images = vec![ImageData::new(\"data\", \"image/jpeg\").with_detail(\"high\")];\n        let messages = vec![ChatMessage::user_with_images(\"Analyze\", images)];\n        let converted = OpenAICompatibleProvider::convert_messages(&messages);\n\n        let json = serde_json::to_value(&converted[0]).unwrap();\n        let content = &json[\"content\"];\n        \n        assert_eq!(content[1][\"image_url\"][\"detail\"], \"high\");\n    }\n\n    // =========================================================================\n    // Reasoning Tokens Tests (OODA-28)\n    // =========================================================================\n\n    #[test]\n    fn test_usage_with_reasoning_tokens() {\n        // Test parsing xAI/DeepSeek usage with reasoning_tokens\n        let json = r#\"{\n            \"prompt_tokens\": 32,\n            \"completion_tokens\": 9,\n            \"total_tokens\": 135,\n            \"completion_tokens_details\": {\n                \"reasoning_tokens\": 94\n            }\n        }\"#;\n\n        let usage: Usage = serde_json::from_str(json).unwrap();\n        assert_eq!(usage.prompt_tokens, 32);\n        assert_eq!(usage.completion_tokens, 9);\n        assert_eq!(\n            usage.completion_tokens_details.as_ref().unwrap().reasoning_tokens,\n            Some(94)\n        );\n    }\n\n    #[test]\n    fn test_usage_without_reasoning_tokens() {\n        // Test parsing standard usage without reasoning details\n        let json = r#\"{\n            \"prompt_tokens\": 100,\n            \"completion_tokens\": 50,\n            \"total_tokens\": 150\n        }\"#;\n\n        let usage: Usage = serde_json::from_str(json).unwrap();\n        assert_eq!(usage.prompt_tokens, 100);\n        assert_eq!(usage.completion_tokens, 50);\n        assert!(usage.completion_tokens_details.is_none());\n    }\n\n    #[test]\n    fn test_stream_delta_with_reasoning_content() {\n        // Test parsing DeepSeek/xAI streaming delta with reasoning\n        let json = r#\"{\n            \"content\": null,\n            \"reasoning_content\": \"Let me think about this...\"\n        }\"#;\n\n        let delta: StreamDelta = serde_json::from_str(json).unwrap();\n        assert!(delta.content.is_none());\n        assert_eq!(\n            delta.reasoning_content,\n            Some(\"Let me think about this...\".to_string())\n        );\n    }\n\n    // =========================================================================\n    // OODA-36: Additional Unit Tests\n    // =========================================================================\n\n    #[test]\n    fn test_supports_streaming() {\n        std::env::set_var(\"TEST_API_KEY_STREAM\", \"key\");\n\n        let config = create_test_config_with_key(\"TEST_API_KEY_STREAM\");\n        let provider = OpenAICompatibleProvider::from_config(config).unwrap();\n        \n        // Default model has supports_streaming = true in test config\n        assert!(provider.supports_streaming());\n\n        std::env::remove_var(\"TEST_API_KEY_STREAM\");\n    }\n\n    #[test]\n    fn test_thinking_config_serialization() {\n        let config = ThinkingConfig {\n            thinking_type: \"enabled\".to_string(),\n        };\n        \n        let json = serde_json::to_value(&config).unwrap();\n        assert_eq!(json[\"type\"], \"enabled\");\n    }\n\n    #[test]\n    fn test_response_format_serialization() {\n        let format = ResponseFormat {\n            format_type: \"json_object\".to_string(),\n        };\n        \n        let json = serde_json::to_value(&format).unwrap();\n        assert_eq!(json[\"type\"], \"json_object\");\n    }\n\n    #[test]\n    fn test_embedding_provider_name() {\n        std::env::set_var(\"TEST_API_KEY_EMB1\", \"key\");\n\n        let config = create_test_config_with_key(\"TEST_API_KEY_EMB1\");\n        let provider = OpenAICompatibleProvider::from_config(config).unwrap();\n        \n        assert_eq!(EmbeddingProvider::name(&provider), \"test-provider\");\n\n        std::env::remove_var(\"TEST_API_KEY_EMB1\");\n    }\n\n    #[test]\n    fn test_embedding_provider_model() {\n        std::env::set_var(\"TEST_API_KEY_EMB2\", \"key\");\n\n        let mut config = create_test_config_with_key(\"TEST_API_KEY_EMB2\");\n        config.default_embedding_model = Some(\"text-embedding-ada-002\".to_string());\n        let provider = OpenAICompatibleProvider::from_config(config).unwrap();\n        \n        assert_eq!(EmbeddingProvider::model(&provider), \"text-embedding-ada-002\");\n\n        std::env::remove_var(\"TEST_API_KEY_EMB2\");\n    }\n\n    #[test]\n    fn test_tool_call_request_serialization() {\n        let tool_call = ToolCallRequest {\n            id: \"call_123\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCallRequest {\n                name: \"get_weather\".to_string(),\n                arguments: r#\"{\"location\":\"NYC\"}\"#.to_string(),\n            },\n        };\n        \n        let json = serde_json::to_value(&tool_call).unwrap();\n        assert_eq!(json[\"id\"], \"call_123\");\n        assert_eq!(json[\"type\"], \"function\");\n        assert_eq!(json[\"function\"][\"name\"], \"get_weather\");\n        assert_eq!(json[\"function\"][\"arguments\"], r#\"{\"location\":\"NYC\"}\"#);\n    }\n\n    #[test]\n    fn test_function_call_request_serialization() {\n        let func_call = FunctionCallRequest {\n            name: \"search\".to_string(),\n            arguments: r#\"{\"query\":\"rust programming\"}\"#.to_string(),\n        };\n        \n        let json = serde_json::to_value(&func_call).unwrap();\n        assert_eq!(json[\"name\"], \"search\");\n        assert_eq!(json[\"arguments\"], r#\"{\"query\":\"rust programming\"}\"#);\n    }\n\n    /// Helper to create test config with specific API key env var\n    fn create_test_config_with_key(env_var: &str) -> ProviderConfig {\n        ProviderConfig {\n            name: \"test-provider\".to_string(),\n            display_name: \"Test Provider\".to_string(),\n            provider_type: crate::model_config::ProviderType::OpenAICompatible,\n            api_key_env: Some(env_var.to_string()),\n            base_url: Some(\"https://api.test.com/v1\".to_string()),\n            default_llm_model: Some(\"test-model\".to_string()),\n            models: vec![ModelCard {\n                name: \"test-model\".to_string(),\n                display_name: \"Test Model\".to_string(),\n                model_type: ModelType::Llm,\n                capabilities: crate::model_config::ModelCapabilities {\n                    context_length: 128000,\n                    supports_streaming: true,\n                    supports_function_calling: true,\n                    ..Default::default()\n                },\n                ..Default::default()\n            }],\n            ..Default::default()\n        }\n    }\n}\n","traces":[{"line":327,"address":[],"length":0,"stats":{"Line":17}},{"line":329,"address":[],"length":0,"stats":{"Line":51}},{"line":332,"address":[],"length":0,"stats":{"Line":48}},{"line":335,"address":[],"length":0,"stats":{"Line":48}},{"line":338,"address":[],"length":0,"stats":{"Line":32}},{"line":339,"address":[],"length":0,"stats":{"Line":16}},{"line":341,"address":[],"length":0,"stats":{"Line":16}},{"line":344,"address":[],"length":0,"stats":{"Line":96}},{"line":346,"address":[],"length":0,"stats":{"Line":16}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":16}},{"line":354,"address":[],"length":0,"stats":{"Line":32}},{"line":355,"address":[],"length":0,"stats":{"Line":32}},{"line":356,"address":[],"length":0,"stats":{"Line":32}},{"line":357,"address":[],"length":0,"stats":{"Line":32}},{"line":358,"address":[],"length":0,"stats":{"Line":16}},{"line":359,"address":[],"length":0,"stats":{"Line":16}},{"line":364,"address":[],"length":0,"stats":{"Line":17}},{"line":365,"address":[],"length":0,"stats":{"Line":34}},{"line":366,"address":[],"length":0,"stats":{"Line":52}},{"line":367,"address":[],"length":0,"stats":{"Line":1}},{"line":368,"address":[],"length":0,"stats":{"Line":1}},{"line":369,"address":[],"length":0,"stats":{"Line":1}},{"line":370,"address":[],"length":0,"stats":{"Line":1}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":380,"address":[],"length":0,"stats":{"Line":16}},{"line":382,"address":[],"length":0,"stats":{"Line":25}},{"line":383,"address":[],"length":0,"stats":{"Line":10}},{"line":384,"address":[],"length":0,"stats":{"Line":1}},{"line":389,"address":[],"length":0,"stats":{"Line":15}},{"line":390,"address":[],"length":0,"stats":{"Line":15}},{"line":392,"address":[],"length":0,"stats":{"Line":15}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[],"length":0,"stats":{"Line":16}},{"line":402,"address":[],"length":0,"stats":{"Line":32}},{"line":405,"address":[],"length":0,"stats":{"Line":32}},{"line":406,"address":[],"length":0,"stats":{"Line":16}},{"line":407,"address":[],"length":0,"stats":{"Line":16}},{"line":414,"address":[],"length":0,"stats":{"Line":22}},{"line":415,"address":[],"length":0,"stats":{"Line":8}},{"line":416,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":8}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":8}},{"line":424,"address":[],"length":0,"stats":{"Line":16}},{"line":425,"address":[],"length":0,"stats":{"Line":32}},{"line":426,"address":[],"length":0,"stats":{"Line":48}},{"line":428,"address":[],"length":0,"stats":{"Line":16}},{"line":432,"address":[],"length":0,"stats":{"Line":7}},{"line":433,"address":[],"length":0,"stats":{"Line":14}},{"line":434,"address":[],"length":0,"stats":{"Line":14}},{"line":442,"address":[],"length":0,"stats":{"Line":1}},{"line":444,"address":[],"length":0,"stats":{"Line":1}},{"line":445,"address":[],"length":0,"stats":{"Line":0}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":450,"address":[],"length":0,"stats":{"Line":3}},{"line":459,"address":[],"length":0,"stats":{"Line":9}},{"line":460,"address":[],"length":0,"stats":{"Line":9}},{"line":462,"address":[],"length":0,"stats":{"Line":22}},{"line":463,"address":[],"length":0,"stats":{"Line":26}},{"line":464,"address":[],"length":0,"stats":{"Line":3}},{"line":465,"address":[],"length":0,"stats":{"Line":9}},{"line":466,"address":[],"length":0,"stats":{"Line":1}},{"line":467,"address":[],"length":0,"stats":{"Line":0}},{"line":471,"address":[],"length":0,"stats":{"Line":39}},{"line":473,"address":[],"length":0,"stats":{"Line":4}},{"line":476,"address":[],"length":0,"stats":{"Line":4}},{"line":477,"address":[],"length":0,"stats":{"Line":6}},{"line":481,"address":[],"length":0,"stats":{"Line":4}},{"line":482,"address":[],"length":0,"stats":{"Line":8}},{"line":483,"address":[],"length":0,"stats":{"Line":6}},{"line":484,"address":[],"length":0,"stats":{"Line":2}},{"line":485,"address":[],"length":0,"stats":{"Line":6}},{"line":486,"address":[],"length":0,"stats":{"Line":2}},{"line":492,"address":[],"length":0,"stats":{"Line":2}},{"line":495,"address":[],"length":0,"stats":{"Line":11}},{"line":498,"address":[],"length":0,"stats":{"Line":13}},{"line":499,"address":[],"length":0,"stats":{"Line":39}},{"line":500,"address":[],"length":0,"stats":{"Line":26}},{"line":501,"address":[],"length":0,"stats":{"Line":26}},{"line":502,"address":[],"length":0,"stats":{"Line":13}},{"line":509,"address":[],"length":0,"stats":{"Line":1}},{"line":510,"address":[],"length":0,"stats":{"Line":1}},{"line":511,"address":[],"length":0,"stats":{"Line":1}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":514,"address":[],"length":0,"stats":{"Line":0}},{"line":517,"address":[],"length":0,"stats":{"Line":0}},{"line":518,"address":[],"length":0,"stats":{"Line":0}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":520,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":10}},{"line":529,"address":[],"length":0,"stats":{"Line":15}},{"line":531,"address":[],"length":0,"stats":{"Line":5}},{"line":533,"address":[],"length":0,"stats":{"Line":20}},{"line":536,"address":[],"length":0,"stats":{"Line":5}},{"line":537,"address":[],"length":0,"stats":{"Line":25}},{"line":538,"address":[],"length":0,"stats":{"Line":5}},{"line":540,"address":[],"length":0,"stats":{"Line":0}},{"line":543,"address":[],"length":0,"stats":{"Line":15}},{"line":544,"address":[],"length":0,"stats":{"Line":15}},{"line":546,"address":[],"length":0,"stats":{"Line":5}},{"line":547,"address":[],"length":0,"stats":{"Line":5}},{"line":548,"address":[],"length":0,"stats":{"Line":0}},{"line":549,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":15}},{"line":553,"address":[],"length":0,"stats":{"Line":5}},{"line":555,"address":[],"length":0,"stats":{"Line":15}},{"line":557,"address":[],"length":0,"stats":{"Line":5}},{"line":558,"address":[],"length":0,"stats":{"Line":5}},{"line":559,"address":[],"length":0,"stats":{"Line":0}},{"line":560,"address":[],"length":0,"stats":{"Line":0}},{"line":563,"address":[],"length":0,"stats":{"Line":5}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":567,"address":[],"length":0,"stats":{"Line":0}},{"line":568,"address":[],"length":0,"stats":{"Line":0}},{"line":569,"address":[],"length":0,"stats":{"Line":0}},{"line":570,"address":[],"length":0,"stats":{"Line":0}},{"line":571,"address":[],"length":0,"stats":{"Line":0}},{"line":574,"address":[],"length":0,"stats":{"Line":0}},{"line":575,"address":[],"length":0,"stats":{"Line":0}},{"line":576,"address":[],"length":0,"stats":{"Line":0}},{"line":577,"address":[],"length":0,"stats":{"Line":0}},{"line":578,"address":[],"length":0,"stats":{"Line":0}},{"line":582,"address":[],"length":0,"stats":{"Line":5}},{"line":585,"address":[],"length":0,"stats":{"Line":5}},{"line":586,"address":[],"length":0,"stats":{"Line":0}},{"line":589,"address":[],"length":0,"stats":{"Line":10}},{"line":590,"address":[],"length":0,"stats":{"Line":5}},{"line":591,"address":[],"length":0,"stats":{"Line":0}},{"line":592,"address":[],"length":0,"stats":{"Line":0}},{"line":597,"address":[],"length":0,"stats":{"Line":2}},{"line":598,"address":[],"length":0,"stats":{"Line":6}},{"line":599,"address":[],"length":0,"stats":{"Line":30}},{"line":600,"address":[],"length":0,"stats":{"Line":4}},{"line":601,"address":[],"length":0,"stats":{"Line":2}},{"line":605,"address":[],"length":0,"stats":{"Line":0}},{"line":606,"address":[],"length":0,"stats":{"Line":0}},{"line":612,"address":[],"length":0,"stats":{"Line":1}},{"line":613,"address":[],"length":0,"stats":{"Line":1}},{"line":616,"address":[],"length":0,"stats":{"Line":2}},{"line":617,"address":[],"length":0,"stats":{"Line":2}},{"line":620,"address":[],"length":0,"stats":{"Line":2}},{"line":621,"address":[],"length":0,"stats":{"Line":2}},{"line":623,"address":[],"length":0,"stats":{"Line":2}},{"line":627,"address":[],"length":0,"stats":{"Line":2}},{"line":659,"address":[],"length":0,"stats":{"Line":1}},{"line":690,"address":[],"length":0,"stats":{"Line":0}},{"line":691,"address":[],"length":0,"stats":{"Line":0}},{"line":694,"address":[],"length":0,"stats":{"Line":0}},{"line":695,"address":[],"length":0,"stats":{"Line":0}},{"line":704,"address":[],"length":0,"stats":{"Line":4}},{"line":705,"address":[],"length":0,"stats":{"Line":8}},{"line":706,"address":[],"length":0,"stats":{"Line":4}},{"line":707,"address":[],"length":0,"stats":{"Line":4}},{"line":708,"address":[],"length":0,"stats":{"Line":8}},{"line":714,"address":[],"length":0,"stats":{"Line":0}},{"line":768,"address":[],"length":0,"stats":{"Line":0}},{"line":769,"address":[],"length":0,"stats":{"Line":0}},{"line":772,"address":[],"length":0,"stats":{"Line":0}},{"line":773,"address":[],"length":0,"stats":{"Line":0}},{"line":784,"address":[],"length":0,"stats":{"Line":0}},{"line":792,"address":[],"length":0,"stats":{"Line":1}},{"line":793,"address":[],"length":0,"stats":{"Line":1}},{"line":794,"address":[],"length":0,"stats":{"Line":1}},{"line":795,"address":[],"length":0,"stats":{"Line":2}},{"line":797,"address":[],"length":0,"stats":{"Line":3}},{"line":798,"address":[],"length":0,"stats":{"Line":0}},{"line":799,"address":[],"length":0,"stats":{"Line":0}},{"line":803,"address":[],"length":0,"stats":{"Line":1}},{"line":804,"address":[],"length":0,"stats":{"Line":0}},{"line":805,"address":[],"length":0,"stats":{"Line":0}},{"line":809,"address":[],"length":0,"stats":{"Line":1}},{"line":810,"address":[],"length":0,"stats":{"Line":3}},{"line":811,"address":[],"length":0,"stats":{"Line":3}},{"line":812,"address":[],"length":0,"stats":{"Line":1}},{"line":813,"address":[],"length":0,"stats":{"Line":3}},{"line":814,"address":[],"length":0,"stats":{"Line":1}},{"line":818,"address":[],"length":0,"stats":{"Line":1}},{"line":826,"address":[],"length":0,"stats":{"Line":1}},{"line":827,"address":[],"length":0,"stats":{"Line":2}},{"line":828,"address":[],"length":0,"stats":{"Line":1}},{"line":829,"address":[],"length":0,"stats":{"Line":1}},{"line":830,"address":[],"length":0,"stats":{"Line":2}},{"line":837,"address":[],"length":0,"stats":{"Line":0}},{"line":847,"address":[],"length":0,"stats":{"Line":1}},{"line":848,"address":[],"length":0,"stats":{"Line":1}},{"line":850,"address":[],"length":0,"stats":{"Line":1}},{"line":854,"address":[],"length":0,"stats":{"Line":1}},{"line":855,"address":[],"length":0,"stats":{"Line":1}},{"line":857,"address":[],"length":0,"stats":{"Line":1}},{"line":861,"address":[],"length":0,"stats":{"Line":1}},{"line":899,"address":[],"length":0,"stats":{"Line":0}},{"line":900,"address":[],"length":0,"stats":{"Line":0}},{"line":901,"address":[],"length":0,"stats":{"Line":0}},{"line":905,"address":[],"length":0,"stats":{"Line":0}},{"line":906,"address":[],"length":0,"stats":{"Line":0}},{"line":909,"address":[],"length":0,"stats":{"Line":0}},{"line":910,"address":[],"length":0,"stats":{"Line":0}},{"line":911,"address":[],"length":0,"stats":{"Line":0}},{"line":912,"address":[],"length":0,"stats":{"Line":0}},{"line":913,"address":[],"length":0,"stats":{"Line":0}},{"line":914,"address":[],"length":0,"stats":{"Line":0}},{"line":915,"address":[],"length":0,"stats":{"Line":0}},{"line":916,"address":[],"length":0,"stats":{"Line":0}},{"line":917,"address":[],"length":0,"stats":{"Line":0}},{"line":918,"address":[],"length":0,"stats":{"Line":0}},{"line":919,"address":[],"length":0,"stats":{"Line":0}},{"line":920,"address":[],"length":0,"stats":{"Line":0}},{"line":921,"address":[],"length":0,"stats":{"Line":0}},{"line":922,"address":[],"length":0,"stats":{"Line":0}},{"line":923,"address":[],"length":0,"stats":{"Line":0}},{"line":924,"address":[],"length":0,"stats":{"Line":0}},{"line":928,"address":[],"length":0,"stats":{"Line":0}},{"line":929,"address":[],"length":0,"stats":{"Line":0}},{"line":930,"address":[],"length":0,"stats":{"Line":0}},{"line":931,"address":[],"length":0,"stats":{"Line":0}},{"line":932,"address":[],"length":0,"stats":{"Line":0}},{"line":933,"address":[],"length":0,"stats":{"Line":0}},{"line":934,"address":[],"length":0,"stats":{"Line":0}},{"line":935,"address":[],"length":0,"stats":{"Line":0}},{"line":936,"address":[],"length":0,"stats":{"Line":0}},{"line":937,"address":[],"length":0,"stats":{"Line":0}},{"line":938,"address":[],"length":0,"stats":{"Line":0}},{"line":939,"address":[],"length":0,"stats":{"Line":0}},{"line":940,"address":[],"length":0,"stats":{"Line":0}},{"line":941,"address":[],"length":0,"stats":{"Line":0}},{"line":942,"address":[],"length":0,"stats":{"Line":0}},{"line":943,"address":[],"length":0,"stats":{"Line":0}},{"line":947,"address":[],"length":0,"stats":{"Line":0}},{"line":954,"address":[],"length":0,"stats":{"Line":12}},{"line":955,"address":[],"length":0,"stats":{"Line":36}},{"line":958,"address":[],"length":0,"stats":{"Line":1}},{"line":960,"address":[],"length":0,"stats":{"Line":11}},{"line":961,"address":[],"length":0,"stats":{"Line":11}},{"line":962,"address":[],"length":0,"stats":{"Line":2}},{"line":963,"address":[],"length":0,"stats":{"Line":1}},{"line":967,"address":[],"length":0,"stats":{"Line":10}},{"line":968,"address":[],"length":0,"stats":{"Line":10}},{"line":969,"address":[],"length":0,"stats":{"Line":20}},{"line":970,"address":[],"length":0,"stats":{"Line":20}},{"line":973,"address":[],"length":0,"stats":{"Line":10}},{"line":974,"address":[],"length":0,"stats":{"Line":0}},{"line":977,"address":[],"length":0,"stats":{"Line":0}},{"line":980,"address":[],"length":0,"stats":{"Line":19}},{"line":981,"address":[],"length":0,"stats":{"Line":9}},{"line":982,"address":[],"length":0,"stats":{"Line":9}},{"line":988,"address":[],"length":0,"stats":{"Line":1}},{"line":990,"address":[],"length":0,"stats":{"Line":0}},{"line":991,"address":[],"length":0,"stats":{"Line":0}},{"line":992,"address":[],"length":0,"stats":{"Line":0}},{"line":996,"address":[],"length":0,"stats":{"Line":0}},{"line":997,"address":[],"length":0,"stats":{"Line":0}},{"line":998,"address":[],"length":0,"stats":{"Line":0}},{"line":1000,"address":[],"length":0,"stats":{"Line":0}},{"line":1007,"address":[],"length":0,"stats":{"Line":0}},{"line":1009,"address":[],"length":0,"stats":{"Line":0}},{"line":1068,"address":[],"length":0,"stats":{"Line":0}},{"line":1069,"address":[],"length":0,"stats":{"Line":0}},{"line":1070,"address":[],"length":0,"stats":{"Line":0}},{"line":1073,"address":[],"length":0,"stats":{"Line":0}},{"line":1074,"address":[],"length":0,"stats":{"Line":0}},{"line":1077,"address":[],"length":0,"stats":{"Line":0}},{"line":1079,"address":[],"length":0,"stats":{"Line":0}},{"line":1080,"address":[],"length":0,"stats":{"Line":0}},{"line":1081,"address":[],"length":0,"stats":{"Line":0}},{"line":1082,"address":[],"length":0,"stats":{"Line":0}},{"line":1086,"address":[],"length":0,"stats":{"Line":0}},{"line":1087,"address":[],"length":0,"stats":{"Line":0}},{"line":1091,"address":[],"length":0,"stats":{"Line":0}},{"line":1092,"address":[],"length":0,"stats":{"Line":0}},{"line":1093,"address":[],"length":0,"stats":{"Line":0}},{"line":1094,"address":[],"length":0,"stats":{"Line":0}},{"line":1097,"address":[],"length":0,"stats":{"Line":0}},{"line":1098,"address":[],"length":0,"stats":{"Line":0}},{"line":1099,"address":[],"length":0,"stats":{"Line":0}},{"line":1100,"address":[],"length":0,"stats":{"Line":0}},{"line":1101,"address":[],"length":0,"stats":{"Line":0}},{"line":1102,"address":[],"length":0,"stats":{"Line":0}},{"line":1103,"address":[],"length":0,"stats":{"Line":0}},{"line":1108,"address":[],"length":0,"stats":{"Line":0}},{"line":1109,"address":[],"length":0,"stats":{"Line":0}},{"line":1110,"address":[],"length":0,"stats":{"Line":0}},{"line":1115,"address":[],"length":0,"stats":{"Line":0}},{"line":1117,"address":[],"length":0,"stats":{"Line":0}},{"line":1118,"address":[],"length":0,"stats":{"Line":0}},{"line":1119,"address":[],"length":0,"stats":{"Line":0}},{"line":1120,"address":[],"length":0,"stats":{"Line":0}},{"line":1125,"address":[],"length":0,"stats":{"Line":0}},{"line":1131,"address":[],"length":0,"stats":{"Line":0}},{"line":1132,"address":[],"length":0,"stats":{"Line":0}},{"line":1135,"address":[],"length":0,"stats":{"Line":0}},{"line":1136,"address":[],"length":0,"stats":{"Line":0}},{"line":1138,"address":[],"length":0,"stats":{"Line":0}},{"line":1139,"address":[],"length":0,"stats":{"Line":0}},{"line":1140,"address":[],"length":0,"stats":{"Line":0}},{"line":1141,"address":[],"length":0,"stats":{"Line":0}},{"line":1142,"address":[],"length":0,"stats":{"Line":0}},{"line":1143,"address":[],"length":0,"stats":{"Line":0}},{"line":1144,"address":[],"length":0,"stats":{"Line":0}},{"line":1152,"address":[],"length":0,"stats":{"Line":0}},{"line":1154,"address":[],"length":0,"stats":{"Line":0}},{"line":1155,"address":[],"length":0,"stats":{"Line":0}},{"line":1156,"address":[],"length":0,"stats":{"Line":0}},{"line":1157,"address":[],"length":0,"stats":{"Line":0}},{"line":1161,"address":[],"length":0,"stats":{"Line":0}},{"line":1163,"address":[],"length":0,"stats":{"Line":0}},{"line":1164,"address":[],"length":0,"stats":{"Line":0}},{"line":1168,"address":[],"length":0,"stats":{"Line":0}},{"line":1180,"address":[],"length":0,"stats":{"Line":1}},{"line":1181,"address":[],"length":0,"stats":{"Line":1}},{"line":1184,"address":[],"length":0,"stats":{"Line":1}},{"line":1185,"address":[],"length":0,"stats":{"Line":2}},{"line":1186,"address":[],"length":0,"stats":{"Line":2}},{"line":1191,"address":[],"length":0,"stats":{"Line":0}},{"line":1193,"address":[],"length":0,"stats":{"Line":0}},{"line":1194,"address":[],"length":0,"stats":{"Line":0}},{"line":1196,"address":[],"length":0,"stats":{"Line":0}},{"line":1197,"address":[],"length":0,"stats":{"Line":0}},{"line":1201,"address":[],"length":0,"stats":{"Line":0}},{"line":1203,"address":[],"length":0,"stats":{"Line":0}},{"line":1204,"address":[],"length":0,"stats":{"Line":0}},{"line":1206,"address":[],"length":0,"stats":{"Line":0}},{"line":1207,"address":[],"length":0,"stats":{"Line":0}},{"line":1211,"address":[],"length":0,"stats":{"Line":0}},{"line":1217,"address":[],"length":0,"stats":{"Line":0}},{"line":1218,"address":[],"length":0,"stats":{"Line":0}},{"line":1219,"address":[],"length":0,"stats":{"Line":0}},{"line":1220,"address":[],"length":0,"stats":{"Line":0}}],"covered":162,"coverable":331},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","openrouter.rs"],"content":"//! OpenRouter provider for access to 200+ LLM models.\n//!\n//! @implements OODA Iteration 02: OpenRouter Provider\n//! @implements OODA-72: Dynamic Model Discovery with Caching\n//!\n//! OpenRouter provides a unified API to access models from:\n//! - OpenAI (GPT-4, GPT-4.5, GPT-5)\n//! - Anthropic (Claude 3.5, 4, 4.5)\n//! - Google (Gemini Pro, Flash)\n//! - Meta (LLaMA 3.1, 3.2)\n//! - Mistral (Mixtral, Mistral Large)\n//! - And 200+ more models\n//!\n//! # Example\n//!\n//! ```rust,ignore\n//! use edgequake_llm::{OpenRouterProvider, ChatMessage, LLMProvider};\n//!\n//! // From environment variable\n//! let provider = OpenRouterProvider::from_env()?;\n//!\n//! // With specific model\n//! let provider = OpenRouterProvider::new(\"sk-or-...\")\n//!     .with_model(\"anthropic/claude-3.5-sonnet\");\n//!\n//! let response = provider.chat(&[ChatMessage::user(\"Hello!\")], None).await?;\n//!\n//! // List available models (with caching)\n//! let models = provider.list_models_cached(Duration::from_secs(3600)).await?;\n//! for model in models.iter().take(5) {\n//!     println!(\"{}: {} ({}K context)\", model.id, model.name, model.context_length / 1000);\n//! }\n//! ```\n\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse futures::StreamExt;\nuse reqwest::header::{HeaderMap, HeaderValue, AUTHORIZATION, CONTENT_TYPE};\nuse reqwest::Client;\nuse serde::{Deserialize, Serialize};\nuse tokio::sync::RwLock;\nuse tracing::{debug, instrument, warn};\n\nuse crate::error::{LlmError, Result};\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, FunctionCall, LLMProvider,\n    LLMResponse, StreamChunk, ToolCall, ToolChoice, ToolDefinition,\n};\n\n// ============================================================================\n// Constants\n// ============================================================================\n\n/// OpenRouter API base URL.\nconst OPENROUTER_BASE_URL: &str = \"https://openrouter.ai/api/v1\";\n\n/// Default model (Claude 3.5 Sonnet via OpenRouter).\nconst DEFAULT_MODEL: &str = \"anthropic/claude-3.5-sonnet\";\n\n/// Default max tokens.\nconst DEFAULT_MAX_TOKENS: u32 = 4096;\n\n/// Default max context length\nconst DEFAULT_MAX_CONTEXT_LENGTH: usize = 128_000;\n\n// ============================================================================\n// Request/Response Types (OpenAI-compatible)\n// ============================================================================\n\n/// Chat completion request body.\n#[derive(Debug, Serialize)]\nstruct ChatRequest<'a> {\n    model: &'a str,\n    messages: Vec<RequestMessage>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stream: Option<bool>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    max_tokens: Option<u32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    temperature: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    top_p: Option<f32>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    stop: Option<Vec<String>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tools: Option<Vec<RequestTool>>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_choice: Option<serde_json::Value>,\n}\n\n#[derive(Debug, Serialize)]\nstruct RequestMessage {\n    role: String,\n    content: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_call_id: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    tool_calls: Option<Vec<RequestToolCall>>,\n}\n\n#[derive(Debug, Serialize)]\nstruct RequestTool {\n    #[serde(rename = \"type\")]\n    tool_type: String,\n    function: RequestFunction,\n}\n\n#[derive(Debug, Serialize)]\nstruct RequestFunction {\n    name: String,\n    description: String,\n    parameters: serde_json::Value,\n}\n\n#[derive(Debug, Serialize)]\nstruct RequestToolCall {\n    id: String,\n    #[serde(rename = \"type\")]\n    call_type: String,\n    function: RequestFunctionCall,\n}\n\n#[derive(Debug, Serialize)]\nstruct RequestFunctionCall {\n    name: String,\n    arguments: String,\n}\n\n/// Chat completion response.\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ChatResponse {\n    id: String,\n    model: String,\n    choices: Vec<Choice>,\n    usage: Option<Usage>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct Choice {\n    index: usize,\n    message: Option<ResponseMessage>,\n    finish_reason: Option<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ResponseMessage {\n    role: String,\n    content: Option<String>,\n    tool_calls: Option<Vec<ResponseToolCall>>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ResponseToolCall {\n    id: String,\n    #[serde(rename = \"type\")]\n    call_type: Option<String>,\n    function: ResponseFunctionCall,\n}\n\n#[derive(Debug, Deserialize)]\nstruct ResponseFunctionCall {\n    name: String,\n    arguments: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct Usage {\n    prompt_tokens: u32,\n    completion_tokens: u32,\n    total_tokens: Option<u32>,\n}\n\n/// Error response from OpenRouter.\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ErrorResponse {\n    error: ErrorDetail,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct ErrorDetail {\n    message: String,\n    code: Option<i32>,\n}\n\n/// Stream chunk for SSE responses.\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct StreamChunkResponse {\n    id: Option<String>,\n    model: Option<String>,\n    choices: Vec<StreamChoice>,\n    usage: Option<Usage>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct StreamChoice {\n    index: Option<usize>,\n    delta: Option<StreamDelta>,\n    finish_reason: Option<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct StreamDelta {\n    role: Option<String>,\n    content: Option<String>,\n    /// OODA-05: Extended thinking/reasoning content from models like Claude, DeepSeek R1/V3\n    /// Uses serde aliases to handle different provider field names\n    #[serde(alias = \"thinking\", alias = \"reasoning_content\")]\n    reasoning: Option<String>,\n    tool_calls: Option<Vec<StreamToolCall>>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct StreamToolCall {\n    index: Option<usize>,\n    id: Option<String>,\n    #[serde(rename = \"type\")]\n    call_type: Option<String>,\n    function: Option<StreamFunction>,\n}\n\n#[derive(Debug, Deserialize)]\n#[allow(dead_code)]\nstruct StreamFunction {\n    name: Option<String>,\n    arguments: Option<String>,\n}\n\n// ============================================================================\n// Model Discovery Types (OODA-72)\n// ============================================================================\n\n/// Response from the /api/v1/models endpoint.\n///\n/// Contains a list of all available models with their metadata.\n#[derive(Debug, Deserialize, Clone)]\npub struct ModelsResponse {\n    pub data: Vec<ModelInfo>,\n}\n\n/// Information about a single model available through OpenRouter.\n///\n/// Includes pricing, capabilities, and context length.\n#[derive(Debug, Deserialize, Clone)]\npub struct ModelInfo {\n    /// Unique model identifier (e.g., \"openai/gpt-4o\")\n    pub id: String,\n    /// Human-readable name (e.g., \"GPT-4o\")\n    pub name: String,\n    /// Maximum context window in tokens\n    #[serde(default)]\n    pub context_length: usize,\n    /// Pricing information\n    #[serde(default)]\n    pub pricing: ModelPricing,\n    /// Model architecture details\n    #[serde(default)]\n    pub architecture: ModelArchitecture,\n    /// Supported API parameters\n    #[serde(default)]\n    pub supported_parameters: Vec<String>,\n    /// Model description\n    #[serde(default)]\n    pub description: Option<String>,\n    /// Unix timestamp when model was added\n    #[serde(default)]\n    pub created: Option<u64>,\n}\n\n/// Pricing information for an OpenRouter model.\n///\n/// All values are in USD per token/request.\n#[derive(Debug, Deserialize, Clone, Default)]\npub struct ModelPricing {\n    /// Cost per input token (as string, e.g., \"0.00003\")\n    #[serde(default)]\n    pub prompt: String,\n    /// Cost per output token\n    #[serde(default)]\n    pub completion: String,\n    /// Fixed cost per request\n    #[serde(default)]\n    pub request: Option<String>,\n    /// Cost per image input\n    #[serde(default)]\n    pub image: Option<String>,\n}\n\n/// Architecture details for an OpenRouter model.\n#[derive(Debug, Deserialize, Clone, Default)]\npub struct ModelArchitecture {\n    /// Input modalities (e.g., [\"text\", \"image\"])\n    #[serde(default)]\n    pub input_modalities: Vec<String>,\n    /// Output modalities (e.g., [\"text\"])\n    #[serde(default)]\n    pub output_modalities: Vec<String>,\n    /// Tokenizer used (e.g., \"GPT\", \"Claude\")\n    #[serde(default)]\n    pub tokenizer: Option<String>,\n    /// Instruction format type\n    #[serde(default)]\n    pub instruct_type: Option<String>,\n}\n\n/// Internal cache for model list.\n///\n/// Uses timestamp to determine cache freshness.\n#[derive(Debug)]\nstruct ModelCache {\n    models: Vec<ModelInfo>,\n    fetched_at: Instant,\n}\n\n// ============================================================================\n// OpenRouterProvider Implementation\n// ============================================================================\n\n/// OpenRouter LLM provider.\n///\n/// Provides access to 200+ models through a single API.\n/// Supports dynamic model discovery with caching.\n#[derive(Debug)]\npub struct OpenRouterProvider {\n    client: Client,\n    api_key: String,\n    base_url: String,\n    model: String,\n    max_tokens: u32,\n    max_context_length: usize,\n    site_url: Option<String>,\n    site_name: Option<String>,\n    /// Cached model list (thread-safe with interior mutability)\n    model_cache: Arc<RwLock<Option<ModelCache>>>,\n}\n\n// Manual Clone implementation since RwLock<Option<ModelCache>> doesn't derive Clone\nimpl Clone for OpenRouterProvider {\n    fn clone(&self) -> Self {\n        Self {\n            client: self.client.clone(),\n            api_key: self.api_key.clone(),\n            base_url: self.base_url.clone(),\n            model: self.model.clone(),\n            max_tokens: self.max_tokens,\n            max_context_length: self.max_context_length,\n            site_url: self.site_url.clone(),\n            site_name: self.site_name.clone(),\n            // Share the same cache across clones\n            model_cache: Arc::clone(&self.model_cache),\n        }\n    }\n}\n\nimpl OpenRouterProvider {\n    /// Create a new OpenRouter provider with the given API key.\n    ///\n    /// # Arguments\n    ///\n    /// * `api_key` - OpenRouter API key (starts with `sk-or-`)\n    pub fn new(api_key: impl Into<String>) -> Self {\n        Self {\n            client: Client::builder()\n                .timeout(std::time::Duration::from_secs(300))\n                .build()\n                .expect(\"Failed to create HTTP client\"),\n            api_key: api_key.into(),\n            base_url: OPENROUTER_BASE_URL.to_string(),\n            model: DEFAULT_MODEL.to_string(),\n            max_tokens: DEFAULT_MAX_TOKENS,\n            max_context_length: DEFAULT_MAX_CONTEXT_LENGTH,\n            site_url: None,\n            site_name: None,\n            model_cache: Arc::new(RwLock::new(None)),\n        }\n    }\n\n    /// Create from environment variable `OPENROUTER_API_KEY`.\n    ///\n    /// # Errors\n    ///\n    /// Returns error if `OPENROUTER_API_KEY` is not set.\n    pub fn from_env() -> Result<Self> {\n        let api_key = std::env::var(\"OPENROUTER_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\n                \"OPENROUTER_API_KEY environment variable not set. \\\n                 Get your API key at https://openrouter.ai/keys\"\n                    .to_string(),\n            )\n        })?;\n\n        let mut provider = Self::new(api_key);\n\n        // Optional model override\n        if let Ok(model) = std::env::var(\"OPENROUTER_MODEL\") {\n            provider.model = model;\n        }\n\n        // Optional site URL and name\n        if let Ok(url) = std::env::var(\"OPENROUTER_SITE_URL\") {\n            provider.site_url = Some(url);\n        }\n        if let Ok(name) = std::env::var(\"OPENROUTER_SITE_NAME\") {\n            provider.site_name = Some(name);\n        }\n\n        Ok(provider)\n    }\n\n    /// Set the model to use.\n    pub fn with_model(mut self, model: impl Into<String>) -> Self {\n        self.model = model.into();\n        self\n    }\n\n    /// Set the base URL (for testing or proxies).\n    pub fn with_base_url(mut self, base_url: impl Into<String>) -> Self {\n        self.base_url = base_url.into();\n        self\n    }\n\n    /// Set the maximum tokens for responses.\n    pub fn with_max_tokens(mut self, max_tokens: u32) -> Self {\n        self.max_tokens = max_tokens;\n        self\n    }\n\n    /// Set the site URL for OpenRouter dashboard tracking.\n    pub fn with_site_url(mut self, url: impl Into<String>) -> Self {\n        self.site_url = Some(url.into());\n        self\n    }\n\n    /// Set the site name for OpenRouter dashboard tracking.\n    pub fn with_site_name(mut self, name: impl Into<String>) -> Self {\n        self.site_name = Some(name.into());\n        self\n    }\n\n    /// Get the chat completions endpoint URL.\n    fn endpoint(&self) -> String {\n        format!(\"{}/chat/completions\", self.base_url)\n    }\n\n    /// Build headers for API requests.\n    fn headers(&self) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(\n            AUTHORIZATION,\n            HeaderValue::from_str(&format!(\"Bearer {}\", self.api_key))\n                .expect(\"Invalid API key format\"),\n        );\n        headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n\n        // Optional tracking headers\n        if let Some(ref url) = self.site_url {\n            if let Ok(value) = HeaderValue::from_str(url) {\n                headers.insert(\"HTTP-Referer\", value);\n            }\n        }\n        if let Some(ref name) = self.site_name {\n            if let Ok(value) = HeaderValue::from_str(name) {\n                headers.insert(\"X-Title\", value);\n            }\n        }\n\n        headers\n    }\n\n    /// Convert EdgeCode messages to OpenRouter format.\n    fn convert_messages(messages: &[ChatMessage]) -> Vec<RequestMessage> {\n        messages\n            .iter()\n            .map(|msg| {\n                let role = match msg.role {\n                    ChatRole::System => \"system\",\n                    ChatRole::User => \"user\",\n                    ChatRole::Assistant => \"assistant\",\n                    ChatRole::Tool => \"tool\",\n                    ChatRole::Function => \"function\",\n                };\n\n                RequestMessage {\n                    role: role.to_string(),\n                    content: msg.content.clone(),\n                    tool_call_id: msg.tool_call_id.clone(),\n                    tool_calls: msg.tool_calls.as_ref().map(|calls| {\n                        calls\n                            .iter()\n                            .map(|tc| RequestToolCall {\n                                id: tc.id.clone(),\n                                call_type: \"function\".to_string(),\n                                function: RequestFunctionCall {\n                                    name: tc.function.name.clone(),\n                                    arguments: tc.function.arguments.clone(),\n                                },\n                            })\n                            .collect()\n                    }),\n                }\n            })\n            .collect()\n    }\n\n    /// Convert EdgeCode tools to OpenRouter format.\n    fn convert_tools(tools: &[ToolDefinition]) -> Vec<RequestTool> {\n        tools\n            .iter()\n            .map(|tool| RequestTool {\n                tool_type: \"function\".to_string(),\n                function: RequestFunction {\n                    name: tool.function.name.clone(),\n                    description: tool.function.description.clone(),\n                    parameters: tool.function.parameters.clone(),\n                },\n            })\n            .collect()\n    }\n\n    /// Convert EdgeCode ToolChoice to OpenRouter format.\n    fn convert_tool_choice(choice: &ToolChoice) -> serde_json::Value {\n        match choice {\n            ToolChoice::Auto(_) => serde_json::json!(\"auto\"),\n            ToolChoice::Required(_) => serde_json::json!(\"required\"),\n            ToolChoice::Function {\n                choice_type: _,\n                function,\n            } => serde_json::json!({\n                \"type\": \"function\",\n                \"function\": { \"name\": function.name }\n            }),\n        }\n    }\n\n    /// Parse response into LLMResponse.\n    fn parse_response(response: ChatResponse) -> LLMResponse {\n        let choice = response.choices.first();\n        let message = choice.and_then(|c| c.message.as_ref());\n\n        let content = message\n            .and_then(|m| m.content.as_ref())\n            .cloned()\n            .unwrap_or_default();\n\n        let tool_calls = message\n            .and_then(|m| m.tool_calls.as_ref())\n            .map(|tcs| {\n                tcs.iter()\n                    .map(|tc| ToolCall {\n                        id: tc.id.clone(),\n                        call_type: \"function\".to_string(),\n                        function: FunctionCall {\n                            name: tc.function.name.clone(),\n                            arguments: tc.function.arguments.clone(),\n                        },\n                    })\n                    .collect()\n            })\n            .unwrap_or_default();\n\n        let usage = response.usage.as_ref();\n        let prompt_tokens = usage.map(|u| u.prompt_tokens as usize).unwrap_or(0);\n        let completion_tokens = usage.map(|u| u.completion_tokens as usize).unwrap_or(0);\n\n        LLMResponse {\n            content,\n            tool_calls,\n            prompt_tokens,\n            completion_tokens,\n            total_tokens: prompt_tokens + completion_tokens,\n            model: response.model,\n            finish_reason: choice.and_then(|c| c.finish_reason.clone()),\n            metadata: HashMap::new(),\n            cache_hit_tokens: None,\n            thinking_tokens: None,\n            thinking_content: None,\n        }\n    }\n\n    /// Check if an error is retryable.\n    /// \n    /// OODA-15: Retry logic for rate limits and transient errors.\n    #[allow(dead_code)]\n    fn is_retryable_error(error: &LlmError) -> bool {\n        match error {\n            LlmError::RateLimited(_) | LlmError::NetworkError(_) => true,\n            LlmError::ApiError(msg) => {\n                msg.contains(\"502\") || msg.contains(\"503\") || msg.contains(\"504\")\n            }\n            _ => false,\n        }\n    }\n\n    /// Check if HTTP status is retryable.\n    fn is_retryable_status(status: reqwest::StatusCode) -> bool {\n        matches!(status.as_u16(), 429 | 500 | 502 | 503 | 504)\n    }\n\n    /// Handle API error responses with helpful guidance.\n    fn handle_error(status: reqwest::StatusCode, body: &str) -> LlmError {\n        // Try to parse error response\n        if let Ok(error_response) = serde_json::from_str::<ErrorResponse>(body) {\n            let message = error_response.error.message;\n            match status.as_u16() {\n                400 => {\n                    // Bad Request - check for common issues\n                    if message.to_lowercase().contains(\"tool\") \n                        || message.to_lowercase().contains(\"function\")\n                        || message.contains(\"not supported\")\n                        || message.contains(\"No endpoints found\") {\n                        LlmError::InvalidRequest(format!(\n                            \"Model doesn't support function calling: {}.\\n\\\n                             \\n\\\n                             ðŸ’¡ EdgeCode React agent requires function calling support.\\n\\\n                             \\n\\\n                             Try one of these compatible models:\\n\\\n                             - anthropic/claude-3.5-sonnet (recommended)\\n\\\n                             - openai/gpt-4o\\n\\\n                             - google/gemini-2.0-flash-exp\\n\\\n                             - meta-llama/llama-3.3-70b-instruct\\n\\\n                             \\n\\\n                             Use /model to select a different model.\",\n                            message\n                        ))\n                    } else {\n                        LlmError::InvalidRequest(format!(\n                            \"{}. Check that the model name is correct and the request format is valid.\",\n                            message\n                        ))\n                    }\n                }\n                401 => LlmError::AuthError(message),\n                402 => {\n                    // Insufficient credits\n                    LlmError::ApiError(format!(\n                        \"Insufficient credits: {}. Add credits at https://openrouter.ai/credits\",\n                        message\n                    ))\n                }\n                403 => {\n                    // Forbidden - usually regional restrictions or model not available\n                    if message.contains(\"not available in your region\") || message.contains(\"region\") {\n                        LlmError::ApiError(format!(\n                            \"Regional restriction: {}. This model is not available in your geographic region. Try selecting a different model with /model or check OpenRouter's model availability at https://openrouter.ai/docs/models\",\n                            message\n                        ))\n                    } else if message.contains(\"moderation\") {\n                        LlmError::ApiError(format!(\n                            \"Content policy violation: {}. Your request was blocked by content moderation. Review OpenRouter's policies or try a different model.\",\n                            message\n                        ))\n                    } else {\n                        LlmError::ApiError(format!(\n                            \"Access forbidden: {}. This may be due to model availability, account restrictions, or content policy. Try a different model with /model\",\n                            message\n                        ))\n                    }\n                }\n                404 => {\n                    // Not Found - usually incorrect model name\n                    LlmError::ApiError(format!(\n                        \"Model not found: {}. The model name may be incorrect or the model may have been removed. Use /model to see available models.\",\n                        message\n                    ))\n                }\n                429 => LlmError::RateLimited(message),\n                _ => LlmError::ApiError(format!(\"{}: {}\", status, message)),\n            }\n        } else {\n            // Couldn't parse error response - provide basic status code info\n            match status.as_u16() {\n                403 => LlmError::ApiError(format!(\n                    \"403 Forbidden: {}. This model may not be available in your region or due to account restrictions. Try selecting a different model with /model\",\n                    body\n                )),\n                404 => LlmError::ApiError(format!(\n                    \"404 Not Found: {}. The model may not exist or has been removed. Use /model to see available models.\",\n                    body\n                )),\n                _ => LlmError::ApiError(format!(\"{}: {}\", status, body)),\n            }\n        }\n    }\n\n    /// Send a chat request (non-streaming) with automatic retry.\n    /// \n    /// OODA-15: Implements exponential backoff for rate limits and transient errors.\n    /// Retries up to 3 times with delays: 1s, 2s, 4s.\n    #[instrument(skip(self, request))]\n    async fn send_request(&self, request: &ChatRequest<'_>) -> Result<ChatResponse> {\n        const MAX_RETRIES: u32 = 3;\n        const BASE_DELAY_MS: u64 = 1000;\n\n        let mut last_error = None;\n\n        for attempt in 0..=MAX_RETRIES {\n            if attempt > 0 {\n                let delay_ms = BASE_DELAY_MS * (1 << (attempt - 1)); // 1s, 2s, 4s\n                debug!(\n                    \"Retry attempt {}/{} after {}ms delay\",\n                    attempt, MAX_RETRIES, delay_ms\n                );\n                tokio::time::sleep(Duration::from_millis(delay_ms)).await;\n            }\n\n            // OODA-16: Enhanced request logging with message count and tools\n            debug!(\n                \"OpenRouter request: model={}, messages={}, tools={}, max_tokens={:?}, stream={:?}\",\n                request.model,\n                request.messages.len(),\n                request.tools.as_ref().map(|t| t.len()).unwrap_or(0),\n                request.max_tokens,\n                request.stream\n            );\n\n            let start = std::time::Instant::now();\n            let response = match self\n                .client\n                .post(self.endpoint())\n                .headers(self.headers())\n                .json(request)\n                .send()\n                .await\n            {\n                Ok(resp) => resp,\n                Err(e) => {\n                    let error = LlmError::NetworkError(e.to_string());\n                    if attempt < MAX_RETRIES {\n                        warn!(\"Network error (attempt {}/{}): {}\", attempt + 1, MAX_RETRIES + 1, e);\n                        last_error = Some(error);\n                        continue;\n                    }\n                    return Err(error);\n                }\n            };\n\n            let status = response.status();\n            let body = match response.text().await {\n                Ok(b) => b,\n                Err(e) => {\n                    let error = LlmError::NetworkError(e.to_string());\n                    if attempt < MAX_RETRIES {\n                        last_error = Some(error);\n                        continue;\n                    }\n                    return Err(error);\n                }\n            };\n\n            if !status.is_success() {\n                let error = Self::handle_error(status, &body);\n                \n                // Only retry on retryable errors\n                if Self::is_retryable_status(status) && attempt < MAX_RETRIES {\n                    warn!(\n                        \"Retryable error (attempt {}/{}): {:?}\",\n                        attempt + 1, MAX_RETRIES + 1, error\n                    );\n                    last_error = Some(error);\n                    continue;\n                }\n                return Err(error);\n            }\n\n            // OODA-16: Parse response and log success with timing and token usage\n            let elapsed = start.elapsed();\n            let response: ChatResponse = serde_json::from_str(&body)\n                .map_err(|e| LlmError::ApiError(format!(\"Failed to parse response: {}\", e)))?;\n            \n            debug!(\n                \"OpenRouter response: elapsed={}ms, prompt_tokens={}, completion_tokens={}, total_tokens={}, finish_reason={:?}\",\n                elapsed.as_millis(),\n                response.usage.as_ref().map(|u| u.prompt_tokens).unwrap_or(0),\n                response.usage.as_ref().map(|u| u.completion_tokens).unwrap_or(0),\n                response.usage.as_ref().and_then(|u| u.total_tokens).unwrap_or(0),\n                response.choices.first().and_then(|c| c.finish_reason.as_deref()).unwrap_or(\"none\")\n            );\n            \n            return Ok(response);\n        }\n\n        // Should not reach here, but just in case\n        Err(last_error.unwrap_or_else(|| LlmError::ApiError(\"Max retries exceeded\".to_string())))\n    }\n\n    // ========================================================================\n    // Model Discovery Methods (OODA-72)\n    // ========================================================================\n\n    /// List all available models from the OpenRouter API.\n    ///\n    /// This method always fetches fresh data from the API.\n    /// For cached access, use `list_models_cached()` instead.\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let provider = OpenRouterProvider::from_env()?;\n    /// let models = provider.list_models().await?;\n    /// for model in models.iter().take(10) {\n    ///     println!(\"{}: {}\", model.id, model.name);\n    /// }\n    /// ```\n    #[instrument(skip(self))]\n    pub async fn list_models(&self) -> Result<Vec<ModelInfo>> {\n        debug!(\"Fetching models from OpenRouter API\");\n\n        let url = format!(\"{}/models\", self.base_url);\n\n        let response = self\n            .client\n            .get(&url)\n            .headers(self.headers())\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        let status = response.status();\n        let body = response\n            .text()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !status.is_success() {\n            return Err(Self::handle_error(status, &body));\n        }\n\n        let models_response: ModelsResponse = serde_json::from_str(&body)\n            .map_err(|e| LlmError::ApiError(format!(\"Failed to parse models response: {}\", e)))?;\n\n        debug!(\n            \"Fetched {} models from OpenRouter\",\n            models_response.data.len()\n        );\n\n        // Update cache\n        {\n            let mut cache = self.model_cache.write().await;\n            *cache = Some(ModelCache {\n                models: models_response.data.clone(),\n                fetched_at: Instant::now(),\n            });\n        }\n\n        Ok(models_response.data)\n    }\n\n    /// List models with caching.\n    ///\n    /// Returns cached models if cache is valid (within `max_age`).\n    /// Otherwise fetches fresh data from the API.\n    ///\n    /// # Arguments\n    ///\n    /// * `max_age` - Maximum age of cached data before refresh\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// use std::time::Duration;\n    ///\n    /// let provider = OpenRouterProvider::from_env()?;\n    ///\n    /// // Use cached data up to 1 hour old\n    /// let models = provider.list_models_cached(Duration::from_secs(3600)).await?;\n    /// ```\n    pub async fn list_models_cached(&self, max_age: Duration) -> Result<Vec<ModelInfo>> {\n        // Check cache first\n        {\n            let cache = self.model_cache.read().await;\n            if let Some(ref cached) = *cache {\n                if cached.fetched_at.elapsed() < max_age {\n                    debug!(\n                        \"Using cached models ({} models, age: {:?})\",\n                        cached.models.len(),\n                        cached.fetched_at.elapsed()\n                    );\n                    return Ok(cached.models.clone());\n                }\n            }\n        }\n\n        // Cache miss or expired, fetch fresh\n        self.list_models().await\n    }\n\n    /// Invalidate the model cache.\n    ///\n    /// Next call to `list_models_cached()` will fetch fresh data.\n    pub async fn invalidate_model_cache(&self) {\n        let mut cache = self.model_cache.write().await;\n        *cache = None;\n        debug!(\"Model cache invalidated\");\n    }\n\n    /// Get a specific model by ID.\n    ///\n    /// Uses cached data if available (1 hour TTL).\n    ///\n    /// # Arguments\n    ///\n    /// * `model_id` - Model identifier (e.g., \"openai/gpt-4o\")\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let provider = OpenRouterProvider::from_env()?;\n    /// if let Some(model) = provider.get_model(\"openai/gpt-4o\").await? {\n    ///     println!(\"Context length: {}\", model.context_length);\n    /// }\n    /// ```\n    pub async fn get_model(&self, model_id: &str) -> Result<Option<ModelInfo>> {\n        let models = self\n            .list_models_cached(Duration::from_secs(3600))\n            .await?;\n        Ok(models.into_iter().find(|m| m.id == model_id))\n    }\n\n    /// Get models filtered by input modality.\n    ///\n    /// # Arguments\n    ///\n    /// * `modality` - Input modality (e.g., \"text\", \"image\")\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let provider = OpenRouterProvider::from_env()?;\n    /// let vision_models = provider.get_models_by_modality(\"image\").await?;\n    /// ```\n    pub async fn get_models_by_modality(&self, modality: &str) -> Result<Vec<ModelInfo>> {\n        let models = self\n            .list_models_cached(Duration::from_secs(3600))\n            .await?;\n        Ok(models\n            .into_iter()\n            .filter(|m| m.architecture.input_modalities.contains(&modality.to_string()))\n            .collect())\n    }\n\n    /// Get the number of cached models.\n    pub async fn cached_model_count(&self) -> usize {\n        let cache = self.model_cache.read().await;\n        cache.as_ref().map(|c| c.models.len()).unwrap_or(0)\n    }\n\n    /// Check if the model cache is valid.\n    pub async fn is_cache_valid(&self, max_age: Duration) -> bool {\n        let cache = self.model_cache.read().await;\n        cache\n            .as_ref()\n            .map(|c| c.fetched_at.elapsed() < max_age)\n            .unwrap_or(false)\n    }\n}\n\n// ============================================================================\n// LLMProvider Implementation\n// ============================================================================\n\n#[async_trait]\nimpl LLMProvider for OpenRouterProvider {\n    fn name(&self) -> &str {\n        \"openrouter\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    #[instrument(skip(self, prompt))]\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    #[instrument(skip(self, prompt, options))]\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    #[instrument(skip(self, messages, options))]\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let options = options.cloned().unwrap_or_default();\n\n        let request = ChatRequest {\n            model: &self.model,\n            messages: Self::convert_messages(messages),\n            stream: Some(false),\n            max_tokens: Some(options.max_tokens.unwrap_or(self.max_tokens as usize) as u32),\n            temperature: options.temperature,\n            top_p: options.top_p,\n            stop: options.stop.clone(),\n            tools: None,\n            tool_choice: None,\n        };\n\n        let response = self.send_request(&request).await?;\n        Ok(Self::parse_response(response))\n    }\n\n    #[instrument(skip(self, messages, tools, options))]\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let options = options.cloned().unwrap_or_default();\n\n        let request = ChatRequest {\n            model: &self.model,\n            messages: Self::convert_messages(messages),\n            stream: Some(false),\n            max_tokens: Some(options.max_tokens.unwrap_or(self.max_tokens as usize) as u32),\n            temperature: options.temperature,\n            top_p: options.top_p,\n            stop: options.stop.clone(),\n            tools: Some(Self::convert_tools(tools)),\n            tool_choice: tool_choice.map(|tc| Self::convert_tool_choice(&tc)),\n        };\n\n        let response = self.send_request(&request).await?;\n        Ok(Self::parse_response(response))\n    }\n\n    #[instrument(skip(self, prompt))]\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        let messages = vec![ChatMessage::user(prompt)];\n\n        let request = ChatRequest {\n            model: &self.model,\n            messages: Self::convert_messages(&messages),\n            stream: Some(true),\n            max_tokens: Some(self.max_tokens),\n            temperature: None,\n            top_p: None,\n            stop: None,\n            tools: None,\n            tool_choice: None,\n        };\n\n        let request_body = serde_json::to_string(&request)\n            .map_err(|e| LlmError::InvalidRequest(format!(\"Failed to serialize request: {}\", e)))?;\n\n        let client = self.client.clone();\n        let endpoint = self.endpoint();\n        let headers = self.headers();\n\n        let response = client\n            .post(&endpoint)\n            .headers(headers)\n            .body(request_body)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            return Err(Self::handle_error(status, &body));\n        }\n\n        // OODA-94: Use proper SSE line buffering (same fix as chat_with_tools_stream)\n        let mut line_buffer = String::new();\n        \n        let stream = response.bytes_stream().map(move |chunk| {\n            let chunk = chunk.map_err(|e| LlmError::NetworkError(e.to_string()))?;\n            let text = String::from_utf8_lossy(&chunk);\n            \n            // OODA-94: Append new bytes to persistent buffer\n            line_buffer.push_str(&text);\n\n            let mut content = String::new();\n            \n            // OODA-94: Extract only complete lines (ending with \\n)\n            while let Some(newline_idx) = line_buffer.find('\\n') {\n                let line = line_buffer[..newline_idx].trim().to_string();\n                line_buffer.drain(..=newline_idx);\n                \n                if line.is_empty() || line.starts_with(':') {\n                    continue;\n                }\n\n                if let Some(data) = line.strip_prefix(\"data: \") {\n                    if data == \"[DONE]\" {\n                        continue;\n                    }\n\n                    if let Ok(chunk) = serde_json::from_str::<StreamChunkResponse>(data) {\n                        for choice in chunk.choices {\n                            if let Some(delta) = choice.delta {\n                                if let Some(c) = delta.content {\n                                    content.push_str(&c);\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n\n            Ok(content)\n        });\n\n        Ok(stream.boxed())\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        let options = options.cloned().unwrap_or_default();\n\n        let request = ChatRequest {\n            model: &self.model,\n            messages: Self::convert_messages(messages),\n            stream: Some(true),\n            max_tokens: Some(options.max_tokens.unwrap_or(self.max_tokens as usize) as u32),\n            temperature: options.temperature,\n            top_p: options.top_p,\n            stop: options.stop.clone(),\n            tools: Some(Self::convert_tools(tools)),\n            tool_choice: tool_choice.map(|tc| Self::convert_tool_choice(&tc)),\n        };\n\n        let request_body = serde_json::to_string(&request)\n            .map_err(|e| LlmError::InvalidRequest(format!(\"Failed to serialize request: {}\", e)))?;\n\n        let client = self.client.clone();\n        let endpoint = self.endpoint();\n        let headers = self.headers();\n\n        let response = client\n            .post(&endpoint)\n            .headers(headers)\n            .body(request_body)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            return Err(Self::handle_error(status, &body));\n        }\n\n        // OODA-94: Use proper SSE line buffering to prevent argument truncation\n        // WHY: Network chunks may split SSE lines at arbitrary byte boundaries.\n        // Without buffering, incomplete lines like \"data: {\\\"tool\\\":\\\"mkdir\" get lost\n        // when the next chunk contains the rest: \" -p ./demo/snake\\\"}\"\n        // HOW: Buffer incomplete lines across chunks, only process complete lines (ending in \\n)\n        let mut line_buffer = String::new();\n        \n        let stream = response\n            .bytes_stream()\n            .map(move |chunk| -> Result<Vec<StreamChunk>> {\n                let chunk = chunk.map_err(|e| LlmError::NetworkError(e.to_string()))?;\n                let text = String::from_utf8_lossy(&chunk);\n                \n                // OODA-94: Append new bytes to persistent buffer\n                line_buffer.push_str(&text);\n\n                let mut chunks = Vec::new();\n                \n                // OODA-94: Extract and process only complete lines (ending with \\n)\n                // Keep incomplete lines in buffer for next chunk\n                while let Some(newline_idx) = line_buffer.find('\\n') {\n                    let line = line_buffer[..newline_idx].trim().to_string();\n                    line_buffer.drain(..=newline_idx);\n                    \n                    if line.is_empty() || line.starts_with(':') {\n                        continue;\n                    }\n\n                    if let Some(data) = line.strip_prefix(\"data: \") {\n                        if data == \"[DONE]\" {\n                            chunks.push(StreamChunk::Finished {\n                                reason: \"stop\".to_string(),\n                                ttft_ms: None,\n                            });\n                            continue;\n                        }\n\n                        if let Ok(chunk_response) = serde_json::from_str::<StreamChunkResponse>(data)\n                        {\n                            for choice in chunk_response.choices {\n                                if let Some(delta) = choice.delta {\n                                    // OODA-05: Check for reasoning/thinking content first\n                                    // Models like Claude Extended Thinking, DeepSeek R1/V3\n                                    // OODA-10: Added budget_total field\n                                    if let Some(reasoning) = delta.reasoning {\n                                        if !reasoning.is_empty() {\n                                            chunks.push(StreamChunk::ThinkingContent {\n                                                text: reasoning,\n                                                tokens_used: None, // Not available in stream\n                                                budget_total: None, // OODA-10: Provider-specific\n                                            });\n                                        }\n                                    }\n                                    if let Some(content) = delta.content {\n                                        if !content.is_empty() {\n                                            chunks.push(StreamChunk::Content(content));\n                                        }\n                                    }\n                                    if let Some(tool_calls) = delta.tool_calls {\n                                        for tc in tool_calls {\n                                            if let Some(func) = tc.function {\n                                                let args = func.arguments.unwrap_or_default();\n                                                if !args.is_empty() || tc.id.is_some() {\n                                                    chunks.push(StreamChunk::ToolCallDelta {\n                                                        index: tc.index.unwrap_or(0),\n                                                        id: tc.id,\n                                                        function_name: func.name,\n                                                        function_arguments: if args.is_empty() {\n                                                            None\n                                                        } else {\n                                                            Some(args)\n                                                        },\n                                                    });\n                                                }\n                                            }\n                                        }\n                                    }\n                                }\n                                if let Some(reason) = choice.finish_reason {\n                                    chunks.push(StreamChunk::Finished { reason, ttft_ms: None });\n                                }\n                            }\n                        }\n                    }\n                }\n                Ok(chunks)\n            })\n            .flat_map(|result: Result<Vec<StreamChunk>>| {\n                futures::stream::iter(match result {\n                    Ok(chunks) => chunks.into_iter().map(Ok).collect::<Vec<_>>(),\n                    Err(e) => vec![Err(e)],\n                })\n            });\n\n        Ok(stream.boxed())\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        true\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n}\n\n// OpenRouter doesn't provide embeddings\n#[async_trait]\nimpl EmbeddingProvider for OpenRouterProvider {\n    fn name(&self) -> &str {\n        \"openrouter\"\n    }\n\n    fn model(&self) -> &str {\n        \"none\"\n    }\n\n    fn dimension(&self) -> usize {\n        0\n    }\n\n    fn max_tokens(&self) -> usize {\n        0\n    }\n\n    async fn embed(&self, _texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        Err(LlmError::InvalidRequest(\n            \"OpenRouter does not support embeddings. Use a dedicated embedding provider.\"\n                .to_string(),\n        ))\n    }\n}\n\n// ============================================================================\n// Unit Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_new_provider() {\n        let provider = OpenRouterProvider::new(\"test-key\");\n        assert_eq!(provider.api_key, \"test-key\");\n        assert_eq!(provider.model, DEFAULT_MODEL);\n        assert_eq!(provider.base_url, OPENROUTER_BASE_URL);\n    }\n\n    #[test]\n    fn test_with_model() {\n        let provider = OpenRouterProvider::new(\"test-key\").with_model(\"openai/gpt-4o\");\n        assert_eq!(provider.model, \"openai/gpt-4o\");\n    }\n\n    #[test]\n    fn test_with_base_url() {\n        let provider =\n            OpenRouterProvider::new(\"test-key\").with_base_url(\"https://custom.openrouter.ai\");\n        assert_eq!(provider.base_url, \"https://custom.openrouter.ai\");\n    }\n\n    #[test]\n    fn test_with_site_url() {\n        let provider = OpenRouterProvider::new(\"test-key\").with_site_url(\"https://myapp.com\");\n        assert_eq!(provider.site_url, Some(\"https://myapp.com\".to_string()));\n    }\n\n    #[test]\n    fn test_with_site_name() {\n        let provider = OpenRouterProvider::new(\"test-key\").with_site_name(\"My App\");\n        assert_eq!(provider.site_name, Some(\"My App\".to_string()));\n    }\n\n    #[test]\n    fn test_from_env_missing_key() {\n        // Make sure the env var is not set\n        std::env::remove_var(\"OPENROUTER_API_KEY\");\n        let result = OpenRouterProvider::from_env();\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_endpoint() {\n        let provider = OpenRouterProvider::new(\"test-key\");\n        assert_eq!(\n            provider.endpoint(),\n            format!(\"{}/chat/completions\", OPENROUTER_BASE_URL)\n        );\n    }\n\n    #[test]\n    fn test_headers() {\n        let provider = OpenRouterProvider::new(\"test-key\")\n            .with_site_url(\"https://example.com\")\n            .with_site_name(\"Example\");\n        let headers = provider.headers();\n\n        assert!(headers.contains_key(AUTHORIZATION));\n        assert!(headers.contains_key(CONTENT_TYPE));\n        assert!(headers.contains_key(\"HTTP-Referer\"));\n        assert!(headers.contains_key(\"X-Title\"));\n    }\n\n    #[test]\n    fn test_convert_messages() {\n        let messages = vec![\n            ChatMessage::system(\"You are helpful.\"),\n            ChatMessage::user(\"Hello!\"),\n        ];\n        let converted = OpenRouterProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 2);\n        assert_eq!(converted[0].role, \"system\");\n        assert_eq!(converted[0].content, \"You are helpful.\");\n        assert_eq!(converted[1].role, \"user\");\n        assert_eq!(converted[1].content, \"Hello!\");\n    }\n\n    #[test]\n    fn test_convert_tool_choice() {\n        assert_eq!(\n            OpenRouterProvider::convert_tool_choice(&ToolChoice::auto()),\n            serde_json::json!(\"auto\")\n        );\n        assert_eq!(\n            OpenRouterProvider::convert_tool_choice(&ToolChoice::required()),\n            serde_json::json!(\"required\")\n        );\n        assert_eq!(\n            OpenRouterProvider::convert_tool_choice(&ToolChoice::function(\"my_func\")),\n            serde_json::json!({\"type\": \"function\", \"function\": {\"name\": \"my_func\"}})\n        );\n    }\n\n    #[test]\n    fn test_parse_response() {\n        let response = ChatResponse {\n            id: \"gen-123\".to_string(),\n            model: \"anthropic/claude-3.5-sonnet\".to_string(),\n            choices: vec![Choice {\n                index: 0,\n                message: Some(ResponseMessage {\n                    role: \"assistant\".to_string(),\n                    content: Some(\"Hello!\".to_string()),\n                    tool_calls: None,\n                }),\n                finish_reason: Some(\"stop\".to_string()),\n            }],\n            usage: Some(Usage {\n                prompt_tokens: 10,\n                completion_tokens: 5,\n                total_tokens: Some(15),\n            }),\n        };\n\n        let llm_response = OpenRouterProvider::parse_response(response);\n\n        assert_eq!(llm_response.content, \"Hello!\");\n        assert_eq!(llm_response.prompt_tokens, 10);\n        assert_eq!(llm_response.completion_tokens, 5);\n        assert_eq!(llm_response.total_tokens, 15);\n        assert_eq!(llm_response.model, \"anthropic/claude-3.5-sonnet\");\n        assert_eq!(llm_response.finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[test]\n    fn test_parse_response_with_tool_calls() {\n        let response = ChatResponse {\n            id: \"gen-456\".to_string(),\n            model: \"openai/gpt-4o\".to_string(),\n            choices: vec![Choice {\n                index: 0,\n                message: Some(ResponseMessage {\n                    role: \"assistant\".to_string(),\n                    content: None,\n                    tool_calls: Some(vec![ResponseToolCall {\n                        id: \"call_1\".to_string(),\n                        call_type: Some(\"function\".to_string()),\n                        function: ResponseFunctionCall {\n                            name: \"get_weather\".to_string(),\n                            arguments: r#\"{\"location\":\"Paris\"}\"#.to_string(),\n                        },\n                    }]),\n                }),\n                finish_reason: Some(\"tool_calls\".to_string()),\n            }],\n            usage: Some(Usage {\n                prompt_tokens: 20,\n                completion_tokens: 10,\n                total_tokens: Some(30),\n            }),\n        };\n\n        let llm_response = OpenRouterProvider::parse_response(response);\n\n        assert_eq!(llm_response.tool_calls.len(), 1);\n        assert_eq!(llm_response.tool_calls[0].id, \"call_1\");\n        assert_eq!(llm_response.tool_calls[0].name(), \"get_weather\");\n        assert!(llm_response.tool_calls[0].arguments().contains(\"Paris\"));\n    }\n\n    #[test]\n    fn test_provider_trait() {\n        let provider = OpenRouterProvider::new(\"test-key\");\n        assert_eq!(LLMProvider::name(&provider), \"openrouter\");\n        assert_eq!(LLMProvider::model(&provider), DEFAULT_MODEL);\n        assert!(provider.supports_streaming());\n        assert!(provider.supports_function_calling());\n    }\n\n    // Integration test - requires API key\n    #[tokio::test]\n    #[ignore]\n    async fn test_chat_completion_live() {\n        let provider = OpenRouterProvider::from_env().expect(\"OPENROUTER_API_KEY not set\");\n        let messages = vec![ChatMessage::user(\"Say 'hello' and nothing else.\")];\n\n        let response = provider.chat(&messages, None).await;\n        assert!(response.is_ok());\n\n        let response = response.unwrap();\n        assert!(!response.content.is_empty());\n        assert!(response.prompt_tokens > 0);\n        assert!(response.completion_tokens > 0);\n    }\n\n    // ========================================================================\n    // Model Discovery Tests (OODA-72)\n    // ========================================================================\n\n    #[test]\n    fn test_model_info_deserialization() {\n        let json = r#\"{\n            \"id\": \"openai/gpt-4o\",\n            \"name\": \"GPT-4o\",\n            \"context_length\": 128000,\n            \"pricing\": {\n                \"prompt\": \"0.000005\",\n                \"completion\": \"0.000015\"\n            },\n            \"architecture\": {\n                \"input_modalities\": [\"text\", \"image\"],\n                \"output_modalities\": [\"text\"]\n            },\n            \"supported_parameters\": [\"temperature\", \"max_tokens\"]\n        }\"#;\n\n        let model: ModelInfo = serde_json::from_str(json).unwrap();\n        assert_eq!(model.id, \"openai/gpt-4o\");\n        assert_eq!(model.name, \"GPT-4o\");\n        assert_eq!(model.context_length, 128000);\n        assert_eq!(model.pricing.prompt, \"0.000005\");\n        assert_eq!(model.architecture.input_modalities.len(), 2);\n        assert!(model\n            .architecture\n            .input_modalities\n            .contains(&\"image\".to_string()));\n    }\n\n    #[test]\n    fn test_models_response_deserialization() {\n        let json = r#\"{\n            \"data\": [\n                {\n                    \"id\": \"openai/gpt-4o\",\n                    \"name\": \"GPT-4o\",\n                    \"context_length\": 128000,\n                    \"pricing\": {\"prompt\": \"0.000005\", \"completion\": \"0.000015\"}\n                },\n                {\n                    \"id\": \"anthropic/claude-3.5-sonnet\",\n                    \"name\": \"Claude 3.5 Sonnet\",\n                    \"context_length\": 200000,\n                    \"pricing\": {\"prompt\": \"0.000003\", \"completion\": \"0.000015\"}\n                }\n            ]\n        }\"#;\n\n        let response: ModelsResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.data.len(), 2);\n        assert_eq!(response.data[0].id, \"openai/gpt-4o\");\n        assert_eq!(response.data[1].id, \"anthropic/claude-3.5-sonnet\");\n    }\n\n    #[tokio::test]\n    async fn test_cache_initially_empty() {\n        let provider = OpenRouterProvider::new(\"test-key\");\n        assert_eq!(provider.cached_model_count().await, 0);\n        assert!(!provider\n            .is_cache_valid(std::time::Duration::from_secs(3600))\n            .await);\n    }\n\n    #[tokio::test]\n    async fn test_provider_clone_shares_cache() {\n        let provider1 = OpenRouterProvider::new(\"test-key\");\n        let provider2 = provider1.clone();\n\n        // Both should start with empty cache\n        assert_eq!(provider1.cached_model_count().await, 0);\n        assert_eq!(provider2.cached_model_count().await, 0);\n\n        // Since they share the Arc, they share the cache\n        // (We can't test cache population without mocking, but we verify structure)\n    }\n\n    // Integration test - requires API key\n    #[tokio::test]\n    #[ignore]\n    async fn test_list_models_live() {\n        let provider = OpenRouterProvider::from_env().expect(\"OPENROUTER_API_KEY not set\");\n\n        let models = provider.list_models().await;\n        assert!(models.is_ok(), \"Failed to list models: {:?}\", models);\n\n        let models = models.unwrap();\n        assert!(!models.is_empty(), \"Should have at least one model\");\n\n        // Verify cache was populated\n        assert!(provider.cached_model_count().await > 0);\n        assert!(provider\n            .is_cache_valid(std::time::Duration::from_secs(3600))\n            .await);\n\n        // Find a known model\n        let gpt4 = models.iter().find(|m| m.id.contains(\"gpt-4\"));\n        assert!(gpt4.is_some(), \"Should have a GPT-4 model\");\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_list_models_cached_live() {\n        use std::time::Duration;\n\n        let provider = OpenRouterProvider::from_env().expect(\"OPENROUTER_API_KEY not set\");\n\n        // First call should fetch\n        let models1 = provider\n            .list_models_cached(Duration::from_secs(3600))\n            .await\n            .unwrap();\n        let count1 = models1.len();\n\n        // Second call should use cache\n        let models2 = provider\n            .list_models_cached(Duration::from_secs(3600))\n            .await\n            .unwrap();\n        assert_eq!(models2.len(), count1, \"Cache should return same models\");\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_get_model_live() {\n        let provider = OpenRouterProvider::from_env().expect(\"OPENROUTER_API_KEY not set\");\n\n        let model = provider.get_model(\"openai/gpt-4o\").await;\n        assert!(model.is_ok());\n\n        let model = model.unwrap();\n        assert!(model.is_some(), \"Should find gpt-4o model\");\n\n        let model = model.unwrap();\n        assert_eq!(model.id, \"openai/gpt-4o\");\n        assert!(model.context_length > 0);\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_get_models_by_modality_live() {\n        let provider = OpenRouterProvider::from_env().expect(\"OPENROUTER_API_KEY not set\");\n\n        let vision_models = provider.get_models_by_modality(\"image\").await;\n        assert!(vision_models.is_ok());\n\n        let vision_models = vision_models.unwrap();\n        // Should have at least some vision models\n        assert!(!vision_models.is_empty(), \"Should have vision models\");\n\n        // All returned models should support image input\n        for model in &vision_models {\n            assert!(\n                model\n                    .architecture\n                    .input_modalities\n                    .contains(&\"image\".to_string()),\n                \"Model {} should support image input\",\n                model.id\n            );\n        }\n    }\n\n    // OODA-94: Test SSE line buffering logic\n    // This tests the line buffering algorithm that prevents argument truncation\n    // when SSE data is split across network chunks\n    #[test]\n    fn test_sse_line_buffering_algorithm() {\n        // Simulate the line buffering logic used in stream() and chat_with_tools_stream()\n        fn process_chunks(chunks: &[&str]) -> Vec<String> {\n            let mut line_buffer = String::new();\n            let mut complete_lines = Vec::new();\n\n            for chunk in chunks {\n                // Append chunk to buffer (simulates: line_buffer.push_str(&text))\n                line_buffer.push_str(chunk);\n\n                // Extract complete lines (simulates the while loop)\n                while let Some(newline_idx) = line_buffer.find('\\n') {\n                    let line = line_buffer[..newline_idx].trim().to_string();\n                    line_buffer.drain(..=newline_idx);\n                    if !line.is_empty() {\n                        complete_lines.push(line);\n                    }\n                }\n            }\n\n            complete_lines\n        }\n\n        // Test 1: Data split in middle of JSON\n        // This is the exact scenario that caused \"mkdir ake_gemini406\" truncation\n        let chunks = vec![\n            \"data: {\\\"function\\\":{\\\"name\\\":\\\"run_command\\\",\\\"arguments\\\":\\\"{\\\\\\\"command\\\\\\\":\\\\\\\"mkdir\",\n            \" -p ./demo/snake_gemini406\\\\\\\"}\\\"}}\",\n            \"\\n\",\n        ];\n        let lines = process_chunks(&chunks);\n        assert_eq!(lines.len(), 1);\n        assert!(lines[0].contains(\"mkdir -p ./demo/snake_gemini406\"));\n\n        // Test 2: Multiple complete lines in one chunk\n        let chunks = vec![\n            \"data: {\\\"content\\\":\\\"Hello\\\"}\\n\",\n            \"data: {\\\"content\\\":\\\"World\\\"}\\n\",\n        ];\n        let lines = process_chunks(&chunks);\n        assert_eq!(lines.len(), 2);\n        assert!(lines[0].contains(\"Hello\"));\n        assert!(lines[1].contains(\"World\"));\n\n        // Test 3: Partial line at end (should NOT be processed yet)\n        let chunks = vec![\n            \"data: {\\\"content\\\":\\\"Complete\\\"}\\n\",\n            \"data: {\\\"content\\\":\\\"Incomplete\",\n        ];\n        let lines = process_chunks(&chunks);\n        assert_eq!(lines.len(), 1);\n        assert!(lines[0].contains(\"Complete\"));\n\n        // Test 4: Empty lines and comments should be filtered\n        let chunks = vec![\n            \"\\n\",\n            \": comment\\n\",\n            \"data: {\\\"content\\\":\\\"Real\\\"}\\n\",\n            \"\\n\",\n        ];\n        let lines = process_chunks(&chunks);\n        assert_eq!(lines.len(), 2); // comment line and data line\n        assert!(lines[1].contains(\"Real\"));\n\n        // Test 5: Split exactly at newline character\n        let chunks = vec![\n            \"data: {\\\"content\\\":\\\"First\\\"}\",\n            \"\\ndata: {\\\"content\\\":\\\"Second\\\"}\\n\",\n        ];\n        let lines = process_chunks(&chunks);\n        assert_eq!(lines.len(), 2);\n        assert!(lines[0].contains(\"First\"));\n        assert!(lines[1].contains(\"Second\"));\n    }\n}\n","traces":[{"line":353,"address":[],"length":0,"stats":{"Line":1}},{"line":355,"address":[],"length":0,"stats":{"Line":3}},{"line":356,"address":[],"length":0,"stats":{"Line":3}},{"line":357,"address":[],"length":0,"stats":{"Line":3}},{"line":358,"address":[],"length":0,"stats":{"Line":3}},{"line":359,"address":[],"length":0,"stats":{"Line":2}},{"line":360,"address":[],"length":0,"stats":{"Line":2}},{"line":361,"address":[],"length":0,"stats":{"Line":3}},{"line":362,"address":[],"length":0,"stats":{"Line":3}},{"line":364,"address":[],"length":0,"stats":{"Line":1}},{"line":375,"address":[],"length":0,"stats":{"Line":10}},{"line":377,"address":[],"length":0,"stats":{"Line":20}},{"line":381,"address":[],"length":0,"stats":{"Line":30}},{"line":382,"address":[],"length":0,"stats":{"Line":30}},{"line":383,"address":[],"length":0,"stats":{"Line":30}},{"line":388,"address":[],"length":0,"stats":{"Line":20}},{"line":397,"address":[],"length":0,"stats":{"Line":1}},{"line":398,"address":[],"length":0,"stats":{"Line":3}},{"line":399,"address":[],"length":0,"stats":{"Line":1}},{"line":400,"address":[],"length":0,"stats":{"Line":1}},{"line":401,"address":[],"length":0,"stats":{"Line":1}},{"line":402,"address":[],"length":0,"stats":{"Line":1}},{"line":406,"address":[],"length":0,"stats":{"Line":0}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":410,"address":[],"length":0,"stats":{"Line":0}},{"line":414,"address":[],"length":0,"stats":{"Line":0}},{"line":415,"address":[],"length":0,"stats":{"Line":0}},{"line":417,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":425,"address":[],"length":0,"stats":{"Line":1}},{"line":426,"address":[],"length":0,"stats":{"Line":3}},{"line":427,"address":[],"length":0,"stats":{"Line":1}},{"line":431,"address":[],"length":0,"stats":{"Line":1}},{"line":432,"address":[],"length":0,"stats":{"Line":3}},{"line":433,"address":[],"length":0,"stats":{"Line":1}},{"line":437,"address":[],"length":0,"stats":{"Line":0}},{"line":438,"address":[],"length":0,"stats":{"Line":0}},{"line":439,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":2}},{"line":444,"address":[],"length":0,"stats":{"Line":4}},{"line":445,"address":[],"length":0,"stats":{"Line":2}},{"line":449,"address":[],"length":0,"stats":{"Line":2}},{"line":450,"address":[],"length":0,"stats":{"Line":4}},{"line":451,"address":[],"length":0,"stats":{"Line":2}},{"line":455,"address":[],"length":0,"stats":{"Line":1}},{"line":456,"address":[],"length":0,"stats":{"Line":2}},{"line":460,"address":[],"length":0,"stats":{"Line":1}},{"line":461,"address":[],"length":0,"stats":{"Line":2}},{"line":462,"address":[],"length":0,"stats":{"Line":2}},{"line":463,"address":[],"length":0,"stats":{"Line":1}},{"line":464,"address":[],"length":0,"stats":{"Line":2}},{"line":465,"address":[],"length":0,"stats":{"Line":2}},{"line":467,"address":[],"length":0,"stats":{"Line":4}},{"line":470,"address":[],"length":0,"stats":{"Line":2}},{"line":471,"address":[],"length":0,"stats":{"Line":3}},{"line":472,"address":[],"length":0,"stats":{"Line":2}},{"line":475,"address":[],"length":0,"stats":{"Line":2}},{"line":476,"address":[],"length":0,"stats":{"Line":3}},{"line":477,"address":[],"length":0,"stats":{"Line":2}},{"line":481,"address":[],"length":0,"stats":{"Line":1}},{"line":485,"address":[],"length":0,"stats":{"Line":1}},{"line":486,"address":[],"length":0,"stats":{"Line":1}},{"line":488,"address":[],"length":0,"stats":{"Line":3}},{"line":489,"address":[],"length":0,"stats":{"Line":4}},{"line":490,"address":[],"length":0,"stats":{"Line":1}},{"line":491,"address":[],"length":0,"stats":{"Line":1}},{"line":492,"address":[],"length":0,"stats":{"Line":0}},{"line":493,"address":[],"length":0,"stats":{"Line":0}},{"line":494,"address":[],"length":0,"stats":{"Line":0}},{"line":498,"address":[],"length":0,"stats":{"Line":4}},{"line":499,"address":[],"length":0,"stats":{"Line":4}},{"line":500,"address":[],"length":0,"stats":{"Line":4}},{"line":501,"address":[],"length":0,"stats":{"Line":6}},{"line":502,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[],"length":0,"stats":{"Line":0}},{"line":505,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":508,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":520,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":523,"address":[],"length":0,"stats":{"Line":0}},{"line":524,"address":[],"length":0,"stats":{"Line":0}},{"line":525,"address":[],"length":0,"stats":{"Line":0}},{"line":526,"address":[],"length":0,"stats":{"Line":0}},{"line":527,"address":[],"length":0,"stats":{"Line":0}},{"line":528,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":3}},{"line":536,"address":[],"length":0,"stats":{"Line":3}},{"line":537,"address":[],"length":0,"stats":{"Line":1}},{"line":538,"address":[],"length":0,"stats":{"Line":1}},{"line":541,"address":[],"length":0,"stats":{"Line":1}},{"line":542,"address":[],"length":0,"stats":{"Line":1}},{"line":543,"address":[],"length":0,"stats":{"Line":1}},{"line":544,"address":[],"length":0,"stats":{"Line":2}},{"line":550,"address":[],"length":0,"stats":{"Line":2}},{"line":551,"address":[],"length":0,"stats":{"Line":4}},{"line":552,"address":[],"length":0,"stats":{"Line":10}},{"line":554,"address":[],"length":0,"stats":{"Line":4}},{"line":555,"address":[],"length":0,"stats":{"Line":6}},{"line":559,"address":[],"length":0,"stats":{"Line":4}},{"line":560,"address":[],"length":0,"stats":{"Line":6}},{"line":561,"address":[],"length":0,"stats":{"Line":3}},{"line":562,"address":[],"length":0,"stats":{"Line":1}},{"line":563,"address":[],"length":0,"stats":{"Line":1}},{"line":564,"address":[],"length":0,"stats":{"Line":2}},{"line":565,"address":[],"length":0,"stats":{"Line":2}},{"line":566,"address":[],"length":0,"stats":{"Line":1}},{"line":567,"address":[],"length":0,"stats":{"Line":3}},{"line":568,"address":[],"length":0,"stats":{"Line":1}},{"line":571,"address":[],"length":0,"stats":{"Line":1}},{"line":575,"address":[],"length":0,"stats":{"Line":6}},{"line":576,"address":[],"length":0,"stats":{"Line":10}},{"line":577,"address":[],"length":0,"stats":{"Line":10}},{"line":584,"address":[],"length":0,"stats":{"Line":2}},{"line":585,"address":[],"length":0,"stats":{"Line":2}},{"line":586,"address":[],"length":0,"stats":{"Line":8}},{"line":587,"address":[],"length":0,"stats":{"Line":2}},{"line":598,"address":[],"length":0,"stats":{"Line":0}},{"line":599,"address":[],"length":0,"stats":{"Line":0}},{"line":600,"address":[],"length":0,"stats":{"Line":0}},{"line":601,"address":[],"length":0,"stats":{"Line":0}},{"line":602,"address":[],"length":0,"stats":{"Line":0}},{"line":604,"address":[],"length":0,"stats":{"Line":0}},{"line":609,"address":[],"length":0,"stats":{"Line":0}},{"line":610,"address":[],"length":0,"stats":{"Line":0}},{"line":614,"address":[],"length":0,"stats":{"Line":0}},{"line":616,"address":[],"length":0,"stats":{"Line":0}},{"line":617,"address":[],"length":0,"stats":{"Line":0}},{"line":618,"address":[],"length":0,"stats":{"Line":0}},{"line":621,"address":[],"length":0,"stats":{"Line":0}},{"line":622,"address":[],"length":0,"stats":{"Line":0}},{"line":623,"address":[],"length":0,"stats":{"Line":0}},{"line":624,"address":[],"length":0,"stats":{"Line":0}},{"line":625,"address":[],"length":0,"stats":{"Line":0}},{"line":626,"address":[],"length":0,"stats":{"Line":0}},{"line":627,"address":[],"length":0,"stats":{"Line":0}},{"line":628,"address":[],"length":0,"stats":{"Line":0}},{"line":629,"address":[],"length":0,"stats":{"Line":0}},{"line":630,"address":[],"length":0,"stats":{"Line":0}},{"line":631,"address":[],"length":0,"stats":{"Line":0}},{"line":632,"address":[],"length":0,"stats":{"Line":0}},{"line":633,"address":[],"length":0,"stats":{"Line":0}},{"line":634,"address":[],"length":0,"stats":{"Line":0}},{"line":635,"address":[],"length":0,"stats":{"Line":0}},{"line":636,"address":[],"length":0,"stats":{"Line":0}},{"line":637,"address":[],"length":0,"stats":{"Line":0}},{"line":640,"address":[],"length":0,"stats":{"Line":0}},{"line":641,"address":[],"length":0,"stats":{"Line":0}},{"line":642,"address":[],"length":0,"stats":{"Line":0}},{"line":646,"address":[],"length":0,"stats":{"Line":0}},{"line":649,"address":[],"length":0,"stats":{"Line":0}},{"line":650,"address":[],"length":0,"stats":{"Line":0}},{"line":651,"address":[],"length":0,"stats":{"Line":0}},{"line":656,"address":[],"length":0,"stats":{"Line":0}},{"line":657,"address":[],"length":0,"stats":{"Line":0}},{"line":658,"address":[],"length":0,"stats":{"Line":0}},{"line":659,"address":[],"length":0,"stats":{"Line":0}},{"line":661,"address":[],"length":0,"stats":{"Line":0}},{"line":662,"address":[],"length":0,"stats":{"Line":0}},{"line":663,"address":[],"length":0,"stats":{"Line":0}},{"line":664,"address":[],"length":0,"stats":{"Line":0}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[],"length":0,"stats":{"Line":0}},{"line":669,"address":[],"length":0,"stats":{"Line":0}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":676,"address":[],"length":0,"stats":{"Line":0}},{"line":677,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":0}},{"line":681,"address":[],"length":0,"stats":{"Line":0}},{"line":685,"address":[],"length":0,"stats":{"Line":0}},{"line":686,"address":[],"length":0,"stats":{"Line":0}},{"line":687,"address":[],"length":0,"stats":{"Line":0}},{"line":688,"address":[],"length":0,"stats":{"Line":0}},{"line":690,"address":[],"length":0,"stats":{"Line":0}},{"line":691,"address":[],"length":0,"stats":{"Line":0}},{"line":692,"address":[],"length":0,"stats":{"Line":0}},{"line":694,"address":[],"length":0,"stats":{"Line":0}},{"line":704,"address":[],"length":0,"stats":{"Line":0}},{"line":725,"address":[],"length":0,"stats":{"Line":0}},{"line":782,"address":[],"length":0,"stats":{"Line":0}},{"line":790,"address":[],"length":0,"stats":{"Line":0}},{"line":797,"address":[],"length":0,"stats":{"Line":0}},{"line":819,"address":[],"length":0,"stats":{"Line":0}},{"line":830,"address":[],"length":0,"stats":{"Line":0}},{"line":836,"address":[],"length":0,"stats":{"Line":0}},{"line":843,"address":[],"length":0,"stats":{"Line":0}},{"line":881,"address":[],"length":0,"stats":{"Line":0}},{"line":884,"address":[],"length":0,"stats":{"Line":0}},{"line":885,"address":[],"length":0,"stats":{"Line":0}},{"line":886,"address":[],"length":0,"stats":{"Line":0}},{"line":887,"address":[],"length":0,"stats":{"Line":0}},{"line":888,"address":[],"length":0,"stats":{"Line":0}},{"line":889,"address":[],"length":0,"stats":{"Line":0}},{"line":890,"address":[],"length":0,"stats":{"Line":0}},{"line":892,"address":[],"length":0,"stats":{"Line":0}},{"line":898,"address":[],"length":0,"stats":{"Line":0}},{"line":904,"address":[],"length":0,"stats":{"Line":0}},{"line":905,"address":[],"length":0,"stats":{"Line":0}},{"line":906,"address":[],"length":0,"stats":{"Line":0}},{"line":907,"address":[],"length":0,"stats":{"Line":0}},{"line":926,"address":[],"length":0,"stats":{"Line":0}},{"line":927,"address":[],"length":0,"stats":{"Line":0}},{"line":928,"address":[],"length":0,"stats":{"Line":0}},{"line":929,"address":[],"length":0,"stats":{"Line":0}},{"line":930,"address":[],"length":0,"stats":{"Line":0}},{"line":945,"address":[],"length":0,"stats":{"Line":0}},{"line":946,"address":[],"length":0,"stats":{"Line":0}},{"line":947,"address":[],"length":0,"stats":{"Line":0}},{"line":948,"address":[],"length":0,"stats":{"Line":0}},{"line":949,"address":[],"length":0,"stats":{"Line":0}},{"line":950,"address":[],"length":0,"stats":{"Line":0}},{"line":951,"address":[],"length":0,"stats":{"Line":0}},{"line":952,"address":[],"length":0,"stats":{"Line":0}},{"line":956,"address":[],"length":0,"stats":{"Line":6}},{"line":957,"address":[],"length":0,"stats":{"Line":6}},{"line":958,"address":[],"length":0,"stats":{"Line":9}},{"line":962,"address":[],"length":0,"stats":{"Line":2}},{"line":963,"address":[],"length":0,"stats":{"Line":2}},{"line":964,"address":[],"length":0,"stats":{"Line":1}},{"line":966,"address":[],"length":0,"stats":{"Line":1}},{"line":977,"address":[],"length":0,"stats":{"Line":1}},{"line":978,"address":[],"length":0,"stats":{"Line":1}},{"line":981,"address":[],"length":0,"stats":{"Line":1}},{"line":982,"address":[],"length":0,"stats":{"Line":1}},{"line":985,"address":[],"length":0,"stats":{"Line":0}},{"line":986,"address":[],"length":0,"stats":{"Line":0}},{"line":1054,"address":[],"length":0,"stats":{"Line":0}},{"line":1078,"address":[],"length":0,"stats":{"Line":0}},{"line":1090,"address":[],"length":0,"stats":{"Line":0}},{"line":1101,"address":[],"length":0,"stats":{"Line":0}},{"line":1102,"address":[],"length":0,"stats":{"Line":0}},{"line":1103,"address":[],"length":0,"stats":{"Line":0}},{"line":1106,"address":[],"length":0,"stats":{"Line":0}},{"line":1108,"address":[],"length":0,"stats":{"Line":0}},{"line":1111,"address":[],"length":0,"stats":{"Line":0}},{"line":1112,"address":[],"length":0,"stats":{"Line":0}},{"line":1113,"address":[],"length":0,"stats":{"Line":0}},{"line":1115,"address":[],"length":0,"stats":{"Line":0}},{"line":1116,"address":[],"length":0,"stats":{"Line":0}},{"line":1119,"address":[],"length":0,"stats":{"Line":0}},{"line":1120,"address":[],"length":0,"stats":{"Line":0}},{"line":1121,"address":[],"length":0,"stats":{"Line":0}},{"line":1124,"address":[],"length":0,"stats":{"Line":0}},{"line":1125,"address":[],"length":0,"stats":{"Line":0}},{"line":1126,"address":[],"length":0,"stats":{"Line":0}},{"line":1127,"address":[],"length":0,"stats":{"Line":0}},{"line":1128,"address":[],"length":0,"stats":{"Line":0}},{"line":1136,"address":[],"length":0,"stats":{"Line":0}},{"line":1160,"address":[],"length":0,"stats":{"Line":0}},{"line":1164,"address":[],"length":0,"stats":{"Line":0}},{"line":1176,"address":[],"length":0,"stats":{"Line":0}},{"line":1193,"address":[],"length":0,"stats":{"Line":0}},{"line":1194,"address":[],"length":0,"stats":{"Line":0}},{"line":1195,"address":[],"length":0,"stats":{"Line":0}},{"line":1198,"address":[],"length":0,"stats":{"Line":0}},{"line":1200,"address":[],"length":0,"stats":{"Line":0}},{"line":1204,"address":[],"length":0,"stats":{"Line":0}},{"line":1205,"address":[],"length":0,"stats":{"Line":0}},{"line":1206,"address":[],"length":0,"stats":{"Line":0}},{"line":1208,"address":[],"length":0,"stats":{"Line":0}},{"line":1209,"address":[],"length":0,"stats":{"Line":0}},{"line":1212,"address":[],"length":0,"stats":{"Line":0}},{"line":1213,"address":[],"length":0,"stats":{"Line":0}},{"line":1214,"address":[],"length":0,"stats":{"Line":0}},{"line":1215,"address":[],"length":0,"stats":{"Line":0}},{"line":1216,"address":[],"length":0,"stats":{"Line":0}},{"line":1218,"address":[],"length":0,"stats":{"Line":0}},{"line":1221,"address":[],"length":0,"stats":{"Line":0}},{"line":1223,"address":[],"length":0,"stats":{"Line":0}},{"line":1224,"address":[],"length":0,"stats":{"Line":0}},{"line":1228,"address":[],"length":0,"stats":{"Line":0}},{"line":1229,"address":[],"length":0,"stats":{"Line":0}},{"line":1230,"address":[],"length":0,"stats":{"Line":0}},{"line":1231,"address":[],"length":0,"stats":{"Line":0}},{"line":1232,"address":[],"length":0,"stats":{"Line":0}},{"line":1233,"address":[],"length":0,"stats":{"Line":0}},{"line":1237,"address":[],"length":0,"stats":{"Line":0}},{"line":1238,"address":[],"length":0,"stats":{"Line":0}},{"line":1239,"address":[],"length":0,"stats":{"Line":0}},{"line":1242,"address":[],"length":0,"stats":{"Line":0}},{"line":1243,"address":[],"length":0,"stats":{"Line":0}},{"line":1244,"address":[],"length":0,"stats":{"Line":0}},{"line":1245,"address":[],"length":0,"stats":{"Line":0}},{"line":1246,"address":[],"length":0,"stats":{"Line":0}},{"line":1247,"address":[],"length":0,"stats":{"Line":0}},{"line":1248,"address":[],"length":0,"stats":{"Line":0}},{"line":1249,"address":[],"length":0,"stats":{"Line":0}},{"line":1250,"address":[],"length":0,"stats":{"Line":0}},{"line":1251,"address":[],"length":0,"stats":{"Line":0}},{"line":1252,"address":[],"length":0,"stats":{"Line":0}},{"line":1254,"address":[],"length":0,"stats":{"Line":0}},{"line":1262,"address":[],"length":0,"stats":{"Line":0}},{"line":1263,"address":[],"length":0,"stats":{"Line":0}},{"line":1269,"address":[],"length":0,"stats":{"Line":0}},{"line":1271,"address":[],"length":0,"stats":{"Line":0}},{"line":1272,"address":[],"length":0,"stats":{"Line":0}},{"line":1273,"address":[],"length":0,"stats":{"Line":0}},{"line":1274,"address":[],"length":0,"stats":{"Line":0}},{"line":1281,"address":[],"length":0,"stats":{"Line":1}},{"line":1282,"address":[],"length":0,"stats":{"Line":1}},{"line":1285,"address":[],"length":0,"stats":{"Line":1}},{"line":1286,"address":[],"length":0,"stats":{"Line":1}},{"line":1289,"address":[],"length":0,"stats":{"Line":0}},{"line":1290,"address":[],"length":0,"stats":{"Line":0}},{"line":1297,"address":[],"length":0,"stats":{"Line":0}},{"line":1298,"address":[],"length":0,"stats":{"Line":0}},{"line":1301,"address":[],"length":0,"stats":{"Line":0}},{"line":1302,"address":[],"length":0,"stats":{"Line":0}},{"line":1305,"address":[],"length":0,"stats":{"Line":0}},{"line":1306,"address":[],"length":0,"stats":{"Line":0}},{"line":1309,"address":[],"length":0,"stats":{"Line":0}},{"line":1310,"address":[],"length":0,"stats":{"Line":0}},{"line":1313,"address":[],"length":0,"stats":{"Line":0}}],"covered":106,"coverable":318},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","tracing.rs"],"content":"//! Tracing Provider Wrapper for OpenTelemetry GenAI Observability\n//!\n//! OODA-LOG-03: Provides distributed tracing for all LLM provider calls\n//! following OpenTelemetry GenAI semantic conventions.\n//!\n//! # Architecture\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚  Agent / CLI        â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//!           â”‚ chat(), complete()\n//!           â–¼\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚  TracingProvider    â”‚  â† Creates spans with GenAI attributes\n//! â”‚  â€¢ gen_ai.operation â”‚\n//! â”‚  â€¢ gen_ai.system    â”‚\n//! â”‚  â€¢ gen_ai.request.* â”‚\n//! â”‚  â€¢ gen_ai.usage.*   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//!           â”‚ delegates\n//!           â–¼\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚  Inner Provider     â”‚\n//! â”‚  (OpenAI, Azure,    â”‚\n//! â”‚   Gemini, etc.)     â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # GenAI Semantic Conventions\n//!\n//! Follows: <https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/>\n//!\n//! | Attribute | Description |\n//! |-----------|-------------|\n//! | gen_ai.operation.name | \"chat\", \"embeddings\", etc. |\n//! | gen_ai.system | Provider name (openai, azure, gemini) |\n//! | gen_ai.request.model | Requested model name |\n//! | gen_ai.request.max_tokens | Max tokens requested |\n//! | gen_ai.request.temperature | Temperature setting |\n//! | gen_ai.response.model | Actual model used |\n//! | gen_ai.usage.input_tokens | Prompt tokens |\n//! | gen_ai.usage.output_tokens | Completion tokens |\n//! | gen_ai.response.finish_reasons | Finish reason array |\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse tracing::{info_span, Instrument};\n\nuse crate::error::Result;\nuse crate::providers::genai_events;\nuse crate::traits::{\n    ChatMessage, CompletionOptions, LLMProvider, LLMResponse, StreamChunk, ToolChoice,\n    ToolDefinition,\n};\n\n/// GenAI semantic convention attribute names.\n///\n/// From: <https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/>\npub mod genai_attrs {\n    /// The name of the operation being performed.\n    pub const OPERATION_NAME: &str = \"gen_ai.operation.name\";\n    /// The name of the GenAI system (provider).\n    pub const SYSTEM: &str = \"gen_ai.system\";\n    /// The requested model name.\n    pub const REQUEST_MODEL: &str = \"gen_ai.request.model\";\n    /// Maximum tokens to generate.\n    pub const REQUEST_MAX_TOKENS: &str = \"gen_ai.request.max_tokens\";\n    /// Temperature for generation.\n    pub const REQUEST_TEMPERATURE: &str = \"gen_ai.request.temperature\";\n    /// Top-p sampling parameter.\n    pub const REQUEST_TOP_P: &str = \"gen_ai.request.top_p\";\n    /// The actual model used in response.\n    pub const RESPONSE_MODEL: &str = \"gen_ai.response.model\";\n    /// Input/prompt token count.\n    pub const USAGE_INPUT_TOKENS: &str = \"gen_ai.usage.input_tokens\";\n    /// Output/completion token count.\n    pub const USAGE_OUTPUT_TOKENS: &str = \"gen_ai.usage.output_tokens\";\n    /// Finish reasons array.\n    pub const RESPONSE_FINISH_REASONS: &str = \"gen_ai.response.finish_reasons\";\n\n    // OODA-15: Reasoning/Thinking token attributes\n    /// Number of reasoning/thinking tokens used by the model.\n    ///\n    /// Custom extension following OTel GenAI naming conventions.\n    /// OpenAI o-series: extracted from output_tokens_details.reasoning_tokens\n    /// Anthropic Claude: derived from thinking block token count\n    pub const USAGE_REASONING_TOKENS: &str = \"gen_ai.usage.reasoning_tokens\";\n\n    /// Reasoning/thinking content (opt-in, may be large).\n    ///\n    /// Contains the model's internal reasoning text if available\n    /// and content capture is enabled.\n    pub const REASONING_CONTENT: &str = \"gen_ai.reasoning.content\";\n}\n\n/// Check if content capture is enabled via environment variable.\n///\n/// Returns true if EDGECODE_CAPTURE_CONTENT is set to \"true\" or \"1\".\n/// This is privacy-by-default - content is NOT captured unless explicitly enabled.\nfn should_capture_content() -> bool {\n    std::env::var(\"EDGECODE_CAPTURE_CONTENT\")\n        .map(|v| v == \"true\" || v == \"1\")\n        .unwrap_or(false)\n}\n\n/// A wrapper that adds OpenTelemetry tracing to any LLM provider.\n///\n/// Implements the decorator pattern to add GenAI semantic convention\n/// spans around all provider calls without modifying the inner provider.\n///\n/// # Example\n///\n/// ```ignore\n/// use edgequake_llm::providers::{OpenAIProvider, TracingProvider};\n///\n/// let provider = OpenAIProvider::new(\"api-key\");\n/// let traced = TracingProvider::new(provider);\n///\n/// // All calls now generate spans with GenAI attributes\n/// let response = traced.chat(&messages, None).await?;\n/// ```\npub struct TracingProvider<P: LLMProvider> {\n    inner: P,\n}\n\nimpl<P: LLMProvider> TracingProvider<P> {\n    /// Create a new tracing wrapper around the given provider.\n    pub fn new(inner: P) -> Self {\n        Self { inner }\n    }\n\n    /// Get a reference to the inner provider.\n    pub fn inner(&self) -> &P {\n        &self.inner\n    }\n\n    /// Consume the wrapper and return the inner provider.\n    pub fn into_inner(self) -> P {\n        self.inner\n    }\n}\n\n#[async_trait]\nimpl<P: LLMProvider> LLMProvider for TracingProvider<P> {\n    fn name(&self) -> &str {\n        self.inner.name()\n    }\n\n    fn model(&self) -> &str {\n        self.inner.model()\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.inner.max_context_length()\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        let span = info_span!(\n            \"gen_ai.complete\",\n            { genai_attrs::OPERATION_NAME } = \"complete\",\n            { genai_attrs::SYSTEM } = self.inner.name(),\n            { genai_attrs::REQUEST_MODEL } = self.inner.model(),\n            prompt_length = prompt.len(),\n            \"gen_ai.prompt\" = tracing::field::Empty,\n            \"gen_ai.completion.content\" = tracing::field::Empty,\n            \"langfuse.observation.input\" = tracing::field::Empty,\n            \"langfuse.observation.output\" = tracing::field::Empty,\n        );\n\n        // OODA-LOG-09: Capture prompt content if enabled\n        if should_capture_content() {\n            span.record(\"gen_ai.prompt\", prompt);\n            span.record(\"langfuse.observation.input\", prompt);\n        }\n\n        let response = self.inner.complete(prompt).instrument(span.clone()).await?;\n\n        // Record response attributes\n        span.record(genai_attrs::RESPONSE_MODEL, &response.model);\n        span.record(\n            genai_attrs::USAGE_INPUT_TOKENS,\n            response.prompt_tokens as i64,\n        );\n        span.record(\n            genai_attrs::USAGE_OUTPUT_TOKENS,\n            response.completion_tokens as i64,\n        );\n        if let Some(reason) = &response.finish_reason {\n            span.record(genai_attrs::RESPONSE_FINISH_REASONS, reason.as_str());\n        }\n\n        // OODA-LOG-09: Capture response content if enabled\n        if should_capture_content() {\n            span.record(\"gen_ai.completion.content\", response.content.as_str());\n            span.record(\"langfuse.observation.output\", response.content.as_str());\n        }\n\n        Ok(response)\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let span = info_span!(\n            \"gen_ai.complete\",\n            { genai_attrs::OPERATION_NAME } = \"complete\",\n            { genai_attrs::SYSTEM } = self.inner.name(),\n            { genai_attrs::REQUEST_MODEL } = self.inner.model(),\n            { genai_attrs::REQUEST_MAX_TOKENS } = options.max_tokens.map(|t| t as i64),\n            { genai_attrs::REQUEST_TEMPERATURE } = options.temperature.map(|t| t as f64),\n            { genai_attrs::REQUEST_TOP_P } = options.top_p.map(|t| t as f64),\n            prompt_length = prompt.len(),\n            \"gen_ai.prompt\" = tracing::field::Empty,\n            \"gen_ai.completion.content\" = tracing::field::Empty,\n            \"langfuse.observation.input\" = tracing::field::Empty,\n            \"langfuse.observation.output\" = tracing::field::Empty,\n        );\n\n        // OODA-LOG-09: Capture prompt content if enabled\n        if should_capture_content() {\n            span.record(\"gen_ai.prompt\", prompt);\n            span.record(\"langfuse.observation.input\", prompt);\n        }\n\n        let response = self\n            .inner\n            .complete_with_options(prompt, options)\n            .instrument(span.clone())\n            .await?;\n\n        span.record(genai_attrs::RESPONSE_MODEL, &response.model);\n        span.record(\n            genai_attrs::USAGE_INPUT_TOKENS,\n            response.prompt_tokens as i64,\n        );\n        span.record(\n            genai_attrs::USAGE_OUTPUT_TOKENS,\n            response.completion_tokens as i64,\n        );\n        if let Some(reason) = &response.finish_reason {\n            span.record(genai_attrs::RESPONSE_FINISH_REASONS, reason.as_str());\n        }\n\n        // OODA-LOG-09: Capture response content if enabled\n        if should_capture_content() {\n            span.record(\"gen_ai.completion.content\", response.content.as_str());\n            span.record(\"langfuse.observation.output\", response.content.as_str());\n        }\n\n        // OODA-LOG-11: Emit GenAI event with prompt and completion content\n        let input_messages = vec![ChatMessage {\n            role: crate::traits::ChatRole::User,\n            content: prompt.to_string(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n        let output_messages = vec![ChatMessage {\n            role: crate::traits::ChatRole::Assistant,\n            content: response.content.clone(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n\n        // Emit GenAI event - use async block with instrument to ensure it's in span context\n        async {\n            genai_events::emit_inference_event(\n                &input_messages,\n                &output_messages,\n                &response,\n                Some(options),\n            );\n        }\n        .instrument(span)\n        .await;\n\n        Ok(response)\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let max_tokens = options.and_then(|o| o.max_tokens).map(|t| t as i64);\n        let temperature = options.and_then(|o| o.temperature).map(|t| t as f64);\n        let top_p = options.and_then(|o| o.top_p).map(|t| t as f64);\n\n        let span = info_span!(\n            \"gen_ai.chat\",\n            { genai_attrs::OPERATION_NAME } = \"chat\",\n            { genai_attrs::SYSTEM } = self.inner.name(),\n            { genai_attrs::REQUEST_MODEL } = self.inner.model(),\n            { genai_attrs::REQUEST_MAX_TOKENS } = max_tokens,\n            { genai_attrs::REQUEST_TEMPERATURE } = temperature,\n            { genai_attrs::REQUEST_TOP_P } = top_p,\n            message_count = messages.len(),\n            // Response fields - to be filled after completion\n            { genai_attrs::RESPONSE_MODEL } = tracing::field::Empty,\n            { genai_attrs::USAGE_INPUT_TOKENS } = tracing::field::Empty,\n            { genai_attrs::USAGE_OUTPUT_TOKENS } = tracing::field::Empty,\n            { genai_attrs::USAGE_REASONING_TOKENS } = tracing::field::Empty,\n            { genai_attrs::RESPONSE_FINISH_REASONS } = tracing::field::Empty,\n            \"gen_ai.prompt\" = tracing::field::Empty,\n            \"gen_ai.completion.content\" = tracing::field::Empty,\n            { genai_attrs::REASONING_CONTENT } = tracing::field::Empty,\n            \"langfuse.observation.input\" = tracing::field::Empty,\n            \"langfuse.observation.output\" = tracing::field::Empty,\n        );\n\n        // OODA-LOG-09: Capture message content if enabled (opt-in for privacy)\n        if should_capture_content() {\n            // Serialize messages as JSON for easier viewing in Jaeger\n            if let Ok(messages_json) = serde_json::to_string(messages) {\n                span.record(\"gen_ai.prompt\", messages_json.as_str());\n                span.record(\"langfuse.observation.input\", messages_json.as_str());\n            }\n        }\n\n        let response = self\n            .inner\n            .chat(messages, options)\n            .instrument(span.clone())\n            .await?;\n\n        // Record response attributes after completion\n        span.record(genai_attrs::RESPONSE_MODEL, &response.model);\n        span.record(\n            genai_attrs::USAGE_INPUT_TOKENS,\n            response.prompt_tokens as i64,\n        );\n        span.record(\n            genai_attrs::USAGE_OUTPUT_TOKENS,\n            response.completion_tokens as i64,\n        );\n\n        // OODA-15: Record reasoning/thinking tokens if present\n        if let Some(thinking_tokens) = response.thinking_tokens {\n            span.record(genai_attrs::USAGE_REASONING_TOKENS, thinking_tokens as i64);\n        }\n\n        if let Some(reason) = &response.finish_reason {\n            span.record(genai_attrs::RESPONSE_FINISH_REASONS, reason.as_str());\n        }\n\n        // OODA-LOG-09: Capture response content if enabled\n        if should_capture_content() {\n            span.record(\"gen_ai.completion.content\", response.content.as_str());\n            span.record(\"langfuse.observation.output\", response.content.as_str());\n\n            // OODA-15: Capture thinking content if available and content capture enabled\n            if let Some(thinking) = &response.thinking_content {\n                // Truncate to 10KB to prevent trace bloat\n                let truncated = if thinking.len() > 10240 {\n                    // Find valid UTF-8 boundary near 10240 bytes\n                    let truncate_at = thinking\n                        .char_indices()\n                        .take_while(|(idx, _)| *idx < 10240)\n                        .last()\n                        .map(|(idx, c)| idx + c.len_utf8())\n                        .unwrap_or(0);\n                    format!(\"{}...[truncated]\", &thinking[..truncate_at])\n                } else {\n                    thinking.clone()\n                };\n                span.record(genai_attrs::REASONING_CONTENT, truncated.as_str());\n            }\n        }\n\n        // OODA-LOG-11: Emit GenAI event with input and output messages\n        let output_messages = vec![ChatMessage {\n            role: crate::traits::ChatRole::Assistant,\n            content: response.content.clone(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }];\n\n        // Emit GenAI event - use async block with instrument to ensure it's in span context\n        async {\n            genai_events::emit_inference_event(messages, &output_messages, &response, options);\n        }\n        .instrument(span)\n        .await;\n\n        tracing::info!(\n            target: \"gen_ai.usage\",\n            input_tokens = response.prompt_tokens,\n            output_tokens = response.completion_tokens,\n            model = %response.model,\n            \"LLM chat completion\"\n        );\n\n        Ok(response)\n    }\n\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let max_tokens = options.and_then(|o| o.max_tokens).map(|t| t as i64);\n        let temperature = options.and_then(|o| o.temperature).map(|t| t as f64);\n\n        let span = info_span!(\n            \"gen_ai.chat_with_tools\",\n            { genai_attrs::OPERATION_NAME } = \"chat_with_tools\",\n            { genai_attrs::SYSTEM } = self.inner.name(),\n            { genai_attrs::REQUEST_MODEL } = self.inner.model(),\n            { genai_attrs::REQUEST_MAX_TOKENS } = max_tokens,\n            { genai_attrs::REQUEST_TEMPERATURE } = temperature,\n            message_count = messages.len(),\n            tool_count = tools.len(),\n            { genai_attrs::RESPONSE_MODEL } = tracing::field::Empty,\n            { genai_attrs::USAGE_INPUT_TOKENS } = tracing::field::Empty,\n            { genai_attrs::USAGE_OUTPUT_TOKENS } = tracing::field::Empty,\n            { genai_attrs::USAGE_REASONING_TOKENS } = tracing::field::Empty,\n            { genai_attrs::RESPONSE_FINISH_REASONS } = tracing::field::Empty,\n            \"gen_ai.prompt\" = tracing::field::Empty,\n            \"gen_ai.completion.content\" = tracing::field::Empty,\n            { genai_attrs::REASONING_CONTENT } = tracing::field::Empty,\n            \"langfuse.observation.input\" = tracing::field::Empty,\n            \"langfuse.observation.output\" = tracing::field::Empty,\n            \"langfuse.observation.type\" = \"generation\",\n        );\n\n        // OODA-LOG-09: Capture message content if enabled\n        if should_capture_content() {\n            // Serialize messages as JSON for easier viewing in Jaeger\n            if let Ok(messages_json) = serde_json::to_string(messages) {\n                span.record(\"gen_ai.prompt\", messages_json.as_str());\n                span.record(\"langfuse.observation.input\", messages_json.as_str());\n            }\n        }\n\n        let response = self\n            .inner\n            .chat_with_tools(messages, tools, tool_choice, options)\n            .instrument(span.clone())\n            .await?;\n\n        span.record(genai_attrs::RESPONSE_MODEL, &response.model);\n        span.record(\n            genai_attrs::USAGE_INPUT_TOKENS,\n            response.prompt_tokens as i64,\n        );\n        span.record(\n            genai_attrs::USAGE_OUTPUT_TOKENS,\n            response.completion_tokens as i64,\n        );\n\n        // OODA-15: Record reasoning/thinking tokens if present\n        if let Some(thinking_tokens) = response.thinking_tokens {\n            span.record(genai_attrs::USAGE_REASONING_TOKENS, thinking_tokens as i64);\n        }\n\n        if let Some(reason) = &response.finish_reason {\n            span.record(genai_attrs::RESPONSE_FINISH_REASONS, reason.as_str());\n        }\n\n        // OODA-LOG-09: Capture response content if enabled\n        if should_capture_content() {\n            // OODA-22: For tool call responses, serialize tool calls as output\n            let output_content = if !response.tool_calls.is_empty() {\n                // When the response contains tool calls, serialize them as JSON output\n                serde_json::to_string(&response.tool_calls)\n                    .unwrap_or_else(|_| format!(\"{} tool call(s)\", response.tool_calls.len()))\n            } else {\n                // Normal text content\n                response.content.clone()\n            };\n\n            span.record(\"gen_ai.completion.content\", output_content.as_str());\n            span.record(\"langfuse.observation.output\", output_content.as_str());\n\n            // OODA-15: Capture thinking content if available\n            if let Some(thinking) = &response.thinking_content {\n                let truncated = if thinking.len() > 10240 {\n                    // Find valid UTF-8 boundary near 10240 bytes\n                    let truncate_at = thinking\n                        .char_indices()\n                        .take_while(|(idx, _)| *idx < 10240)\n                        .last()\n                        .map(|(idx, c)| idx + c.len_utf8())\n                        .unwrap_or(0);\n                    format!(\"{}...[truncated]\", &thinking[..truncate_at])\n                } else {\n                    thinking.clone()\n                };\n                span.record(genai_attrs::REASONING_CONTENT, truncated.as_str());\n            }\n        }\n\n        tracing::info!(\n            target: \"gen_ai.usage\",\n            input_tokens = response.prompt_tokens,\n            output_tokens = response.completion_tokens,\n            model = %response.model,\n            tool_calls = response.tool_calls.len(),\n            \"LLM chat with tools completion\"\n        );\n\n        Ok(response)\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        let span = info_span!(\n            \"gen_ai.stream\",\n            { genai_attrs::OPERATION_NAME } = \"stream\",\n            { genai_attrs::SYSTEM } = self.inner.name(),\n            { genai_attrs::REQUEST_MODEL } = self.inner.model(),\n            prompt_length = prompt.len(),\n            \"gen_ai.prompt\" = tracing::field::Empty,\n        );\n\n        // OODA-LOG-09: Capture prompt content if enabled\n        if should_capture_content() {\n            span.record(\"gen_ai.prompt\", prompt);\n        }\n\n        // Note: Streaming doesn't give us token counts until the end,\n        // so we can't record usage in the span for now\n        self.inner.stream(prompt).instrument(span).await\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        let max_tokens = options.and_then(|o| o.max_tokens).map(|t| t as i64);\n        let temperature = options.and_then(|o| o.temperature).map(|t| t as f64);\n\n        let span = info_span!(\n            \"gen_ai.stream_with_tools\",\n            { genai_attrs::OPERATION_NAME } = \"stream_with_tools\",\n            { genai_attrs::SYSTEM } = self.inner.name(),\n            { genai_attrs::REQUEST_MODEL } = self.inner.model(),\n            { genai_attrs::REQUEST_MAX_TOKENS } = max_tokens,\n            { genai_attrs::REQUEST_TEMPERATURE } = temperature,\n            message_count = messages.len(),\n            tool_count = tools.len(),\n            \"gen_ai.prompt\" = tracing::field::Empty,\n            \"langfuse.observation.input\" = tracing::field::Empty,\n            \"langfuse.observation.type\" = \"generation\",\n        );\n\n        // OODA-LOG-09: Capture message content if enabled\n        if should_capture_content() {\n            // Serialize messages as JSON for easier viewing in Jaeger\n            if let Ok(messages_json) = serde_json::to_string(messages) {\n                span.record(\"gen_ai.prompt\", messages_json.as_str());\n                span.record(\"langfuse.observation.input\", messages_json.as_str());\n            }\n        }\n\n        // DEBUG: Log span creation\n        tracing::info!(\n            span_id = ?span.id(),\n            span_name = \"gen_ai.stream_with_tools\",\n            \"Created LLM streaming span\"\n        );\n\n        // NOTE: VsCodeCopilotProvider does not support tool streaming (supports_tool_streaming returns false)\n        // so this path is currently unused by the React agent. The non-streaming chat_with_tools is used instead.\n        // Keeping this implementation for future providers that support tool streaming.\n        self.inner\n            .chat_with_tools_stream(messages, tools, tool_choice, options)\n            .instrument(span)\n            .await\n    }\n\n    fn supports_streaming(&self) -> bool {\n        self.inner.supports_streaming()\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        self.inner.supports_tool_streaming()\n    }\n\n    fn supports_json_mode(&self) -> bool {\n        self.inner.supports_json_mode()\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        self.inner.supports_function_calling()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::providers::mock::MockProvider;\n\n    #[test]\n    fn test_tracing_provider_delegates_name() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert_eq!(traced.name(), \"mock\");\n    }\n\n    #[test]\n    fn test_tracing_provider_delegates_model() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert_eq!(traced.model(), \"mock-model\");\n    }\n\n    #[test]\n    fn test_tracing_provider_delegates_max_context_length() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert_eq!(traced.max_context_length(), 4096);\n    }\n\n    #[test]\n    fn test_into_inner() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        let inner = traced.into_inner();\n        assert_eq!(inner.name(), \"mock\");\n    }\n\n    #[tokio::test]\n    async fn test_complete_creates_span() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n\n        let response = traced.complete(\"Hello\").await.unwrap();\n        assert_eq!(response.content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_chat_creates_span() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n\n        let messages = vec![ChatMessage::user(\"Hello\")];\n        let response = traced.chat(&messages, None).await.unwrap();\n        assert_eq!(response.content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_complete_with_options_creates_span() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n\n        let options = CompletionOptions {\n            max_tokens: Some(100),\n            temperature: Some(0.7),\n            top_p: Some(0.9),\n            ..Default::default()\n        };\n\n        let response = traced\n            .complete_with_options(\"Prompt\", &options)\n            .await\n            .unwrap();\n        assert_eq!(response.content, \"Mock response\");\n    }\n\n    // ---- Iteration 26: Additional tracing provider tests ----\n\n    #[test]\n    fn test_inner_accessor() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert_eq!(traced.inner().name(), \"mock\");\n    }\n\n    #[test]\n    fn test_supports_streaming_delegation() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        // MockProvider default: false\n        assert!(!traced.supports_streaming());\n    }\n\n    #[test]\n    fn test_supports_json_mode_delegation() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert!(!traced.supports_json_mode());\n    }\n\n    #[test]\n    fn test_supports_function_calling_delegation() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert!(!traced.supports_function_calling());\n    }\n\n    #[test]\n    fn test_supports_tool_streaming_delegation() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n        assert!(!traced.supports_tool_streaming());\n    }\n\n    #[test]\n    fn test_genai_attrs_constants() {\n        assert_eq!(genai_attrs::OPERATION_NAME, \"gen_ai.operation.name\");\n        assert_eq!(genai_attrs::SYSTEM, \"gen_ai.system\");\n        assert_eq!(genai_attrs::REQUEST_MODEL, \"gen_ai.request.model\");\n        assert_eq!(genai_attrs::REQUEST_MAX_TOKENS, \"gen_ai.request.max_tokens\");\n        assert_eq!(genai_attrs::REQUEST_TEMPERATURE, \"gen_ai.request.temperature\");\n        assert_eq!(genai_attrs::REQUEST_TOP_P, \"gen_ai.request.top_p\");\n        assert_eq!(genai_attrs::RESPONSE_MODEL, \"gen_ai.response.model\");\n        assert_eq!(genai_attrs::USAGE_INPUT_TOKENS, \"gen_ai.usage.input_tokens\");\n        assert_eq!(genai_attrs::USAGE_OUTPUT_TOKENS, \"gen_ai.usage.output_tokens\");\n        assert_eq!(genai_attrs::RESPONSE_FINISH_REASONS, \"gen_ai.response.finish_reasons\");\n        assert_eq!(genai_attrs::USAGE_REASONING_TOKENS, \"gen_ai.usage.reasoning_tokens\");\n        assert_eq!(genai_attrs::REASONING_CONTENT, \"gen_ai.reasoning.content\");\n    }\n\n    #[test]\n    fn test_should_capture_content_default_false() {\n        // When env var is not set, should be false\n        std::env::remove_var(\"EDGECODE_CAPTURE_CONTENT\");\n        assert!(!should_capture_content());\n    }\n\n    #[tokio::test]\n    async fn test_chat_with_options() {\n        let mock = MockProvider::new();\n        let traced = TracingProvider::new(mock);\n\n        let messages = vec![ChatMessage::user(\"Hello\")];\n        let options = CompletionOptions {\n            max_tokens: Some(50),\n            temperature: Some(0.5),\n            ..Default::default()\n        };\n\n        let response = traced.chat(&messages, Some(&options)).await.unwrap();\n        assert_eq!(response.content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_chat_with_tools_delegation() {\n        use crate::providers::mock::MockAgentProvider;\n\n        let mock = MockAgentProvider::new();\n        mock.add_response(\"tool response\").await;\n\n        let traced = TracingProvider::new(mock);\n        let messages = vec![ChatMessage::user(\"use tools\")];\n        let response = traced.chat_with_tools(&messages, &[], None, None).await.unwrap();\n        assert_eq!(response.content, \"tool response\");\n    }\n\n    #[tokio::test]\n    async fn test_stream_delegation() {\n        use futures::StreamExt;\n\n        let mock = MockProvider::new();\n        mock.add_response(\"streamed\").await;\n        let traced = TracingProvider::new(mock);\n\n        let mut stream = traced.stream(\"prompt\").await.unwrap();\n        let chunk = stream.next().await.unwrap().unwrap();\n        assert_eq!(chunk, \"streamed\");\n    }\n\n    #[tokio::test]\n    async fn test_complete_with_queued_response() {\n        let mock = MockProvider::new();\n        mock.add_response(\"custom traced\").await;\n        let traced = TracingProvider::new(mock);\n\n        let response = traced.complete(\"Hi\").await.unwrap();\n        assert_eq!(response.content, \"custom traced\");\n    }\n}\n","traces":[{"line":101,"address":[],"length":0,"stats":{"Line":14}},{"line":102,"address":[],"length":0,"stats":{"Line":14}},{"line":103,"address":[],"length":0,"stats":{"Line":14}},{"line":129,"address":[],"length":0,"stats":{"Line":16}},{"line":134,"address":[],"length":0,"stats":{"Line":1}},{"line":135,"address":[],"length":0,"stats":{"Line":1}},{"line":139,"address":[],"length":0,"stats":{"Line":1}},{"line":140,"address":[],"length":0,"stats":{"Line":1}},{"line":146,"address":[],"length":0,"stats":{"Line":1}},{"line":147,"address":[],"length":0,"stats":{"Line":1}},{"line":150,"address":[],"length":0,"stats":{"Line":1}},{"line":151,"address":[],"length":0,"stats":{"Line":1}},{"line":154,"address":[],"length":0,"stats":{"Line":1}},{"line":155,"address":[],"length":0,"stats":{"Line":2}},{"line":158,"address":[],"length":0,"stats":{"Line":2}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}},{"line":163,"address":[],"length":0,"stats":{"Line":0}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":168,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":181,"address":[],"length":0,"stats":{"Line":0}},{"line":182,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":189,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[],"length":0,"stats":{"Line":0}},{"line":196,"address":[],"length":0,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":207,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":211,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[],"length":0,"stats":{"Line":0}},{"line":214,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":223,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":230,"address":[],"length":0,"stats":{"Line":0}},{"line":231,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":235,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":244,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":249,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":254,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":260,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":1}},{"line":276,"address":[],"length":0,"stats":{"Line":1}},{"line":277,"address":[],"length":0,"stats":{"Line":1}},{"line":278,"address":[],"length":0,"stats":{"Line":1}},{"line":279,"address":[],"length":0,"stats":{"Line":1}},{"line":282,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":1}},{"line":294,"address":[],"length":0,"stats":{"Line":1}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":302,"address":[],"length":0,"stats":{"Line":0}},{"line":303,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[],"length":0,"stats":{"Line":0}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":307,"address":[],"length":0,"stats":{"Line":0}},{"line":308,"address":[],"length":0,"stats":{"Line":0}},{"line":309,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":0}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":312,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":315,"address":[],"length":0,"stats":{"Line":0}},{"line":316,"address":[],"length":0,"stats":{"Line":0}},{"line":320,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":323,"address":[],"length":0,"stats":{"Line":0}},{"line":324,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":331,"address":[],"length":0,"stats":{"Line":0}},{"line":332,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":0}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":338,"address":[],"length":0,"stats":{"Line":0}},{"line":340,"address":[],"length":0,"stats":{"Line":0}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":346,"address":[],"length":0,"stats":{"Line":0}},{"line":347,"address":[],"length":0,"stats":{"Line":0}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":351,"address":[],"length":0,"stats":{"Line":0}},{"line":355,"address":[],"length":0,"stats":{"Line":0}},{"line":356,"address":[],"length":0,"stats":{"Line":0}},{"line":357,"address":[],"length":0,"stats":{"Line":0}},{"line":360,"address":[],"length":0,"stats":{"Line":0}},{"line":362,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[],"length":0,"stats":{"Line":0}},{"line":370,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[],"length":0,"stats":{"Line":0}},{"line":380,"address":[],"length":0,"stats":{"Line":0}},{"line":381,"address":[],"length":0,"stats":{"Line":0}},{"line":382,"address":[],"length":0,"stats":{"Line":0}},{"line":383,"address":[],"length":0,"stats":{"Line":0}},{"line":384,"address":[],"length":0,"stats":{"Line":0}},{"line":385,"address":[],"length":0,"stats":{"Line":0}},{"line":386,"address":[],"length":0,"stats":{"Line":0}},{"line":390,"address":[],"length":0,"stats":{"Line":2}},{"line":391,"address":[],"length":0,"stats":{"Line":10}},{"line":393,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":396,"address":[],"length":0,"stats":{"Line":0}},{"line":397,"address":[],"length":0,"stats":{"Line":0}},{"line":398,"address":[],"length":0,"stats":{"Line":0}},{"line":399,"address":[],"length":0,"stats":{"Line":0}},{"line":400,"address":[],"length":0,"stats":{"Line":0}},{"line":401,"address":[],"length":0,"stats":{"Line":0}},{"line":404,"address":[],"length":0,"stats":{"Line":0}},{"line":414,"address":[],"length":0,"stats":{"Line":0}},{"line":415,"address":[],"length":0,"stats":{"Line":0}},{"line":417,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":420,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":422,"address":[],"length":0,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[],"length":0,"stats":{"Line":0}},{"line":425,"address":[],"length":0,"stats":{"Line":0}},{"line":426,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":428,"address":[],"length":0,"stats":{"Line":0}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[],"length":0,"stats":{"Line":0}},{"line":433,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":440,"address":[],"length":0,"stats":{"Line":0}},{"line":442,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":0}},{"line":444,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":449,"address":[],"length":0,"stats":{"Line":0}},{"line":450,"address":[],"length":0,"stats":{"Line":0}},{"line":451,"address":[],"length":0,"stats":{"Line":0}},{"line":452,"address":[],"length":0,"stats":{"Line":0}},{"line":454,"address":[],"length":0,"stats":{"Line":0}},{"line":455,"address":[],"length":0,"stats":{"Line":0}},{"line":456,"address":[],"length":0,"stats":{"Line":0}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":459,"address":[],"length":0,"stats":{"Line":0}},{"line":460,"address":[],"length":0,"stats":{"Line":0}},{"line":461,"address":[],"length":0,"stats":{"Line":0}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":466,"address":[],"length":0,"stats":{"Line":0}},{"line":469,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":474,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":479,"address":[],"length":0,"stats":{"Line":0}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":485,"address":[],"length":0,"stats":{"Line":0}},{"line":486,"address":[],"length":0,"stats":{"Line":0}},{"line":489,"address":[],"length":0,"stats":{"Line":0}},{"line":490,"address":[],"length":0,"stats":{"Line":0}},{"line":492,"address":[],"length":0,"stats":{"Line":0}},{"line":494,"address":[],"length":0,"stats":{"Line":0}},{"line":496,"address":[],"length":0,"stats":{"Line":0}},{"line":498,"address":[],"length":0,"stats":{"Line":0}},{"line":500,"address":[],"length":0,"stats":{"Line":0}},{"line":502,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":508,"address":[],"length":0,"stats":{"Line":0}},{"line":509,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":511,"address":[],"length":0,"stats":{"Line":0}},{"line":512,"address":[],"length":0,"stats":{"Line":0}},{"line":515,"address":[],"length":0,"stats":{"Line":0}},{"line":518,"address":[],"length":0,"stats":{"Line":1}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":522,"address":[],"length":0,"stats":{"Line":0}},{"line":523,"address":[],"length":0,"stats":{"Line":0}},{"line":524,"address":[],"length":0,"stats":{"Line":0}},{"line":525,"address":[],"length":0,"stats":{"Line":0}},{"line":529,"address":[],"length":0,"stats":{"Line":0}},{"line":530,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":545,"address":[],"length":0,"stats":{"Line":0}},{"line":546,"address":[],"length":0,"stats":{"Line":0}},{"line":548,"address":[],"length":0,"stats":{"Line":0}},{"line":550,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":552,"address":[],"length":0,"stats":{"Line":0}},{"line":553,"address":[],"length":0,"stats":{"Line":0}},{"line":554,"address":[],"length":0,"stats":{"Line":0}},{"line":555,"address":[],"length":0,"stats":{"Line":0}},{"line":556,"address":[],"length":0,"stats":{"Line":0}},{"line":557,"address":[],"length":0,"stats":{"Line":0}},{"line":558,"address":[],"length":0,"stats":{"Line":0}},{"line":563,"address":[],"length":0,"stats":{"Line":0}},{"line":565,"address":[],"length":0,"stats":{"Line":0}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":567,"address":[],"length":0,"stats":{"Line":0}},{"line":572,"address":[],"length":0,"stats":{"Line":0}},{"line":573,"address":[],"length":0,"stats":{"Line":0}},{"line":574,"address":[],"length":0,"stats":{"Line":0}},{"line":575,"address":[],"length":0,"stats":{"Line":0}},{"line":581,"address":[],"length":0,"stats":{"Line":0}},{"line":582,"address":[],"length":0,"stats":{"Line":0}},{"line":583,"address":[],"length":0,"stats":{"Line":0}},{"line":584,"address":[],"length":0,"stats":{"Line":0}},{"line":587,"address":[],"length":0,"stats":{"Line":1}},{"line":588,"address":[],"length":0,"stats":{"Line":2}},{"line":591,"address":[],"length":0,"stats":{"Line":1}},{"line":592,"address":[],"length":0,"stats":{"Line":2}},{"line":595,"address":[],"length":0,"stats":{"Line":1}},{"line":596,"address":[],"length":0,"stats":{"Line":2}},{"line":599,"address":[],"length":0,"stats":{"Line":1}},{"line":600,"address":[],"length":0,"stats":{"Line":2}}],"covered":33,"coverable":273},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","auth.rs"],"content":"//! GitHub device code authentication flow for Copilot.\n//!\n//! Implements the OAuth device code flow to authenticate with GitHub\n//! and obtain access tokens for GitHub Copilot.\n//!\n//! # Device Code Flow\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                  GitHub OAuth Device Code Flow                   â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                   â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n//! â”‚  â”‚ 1. Request     â”‚  POST /login/device/code                     â”‚\n//! â”‚  â”‚    Device Code â”‚  â†’ Returns user_code + device_code           â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n//! â”‚          â”‚                                                        â”‚\n//! â”‚          â–¼                                                        â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n//! â”‚  â”‚ 2. Display URL â”‚  User visits: https://github.com/login/deviceâ”‚\n//! â”‚  â”‚    to User     â”‚  User enters: ABC-123 (user_code)            â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n//! â”‚          â”‚                                                        â”‚\n//! â”‚          â–¼                                                        â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n//! â”‚  â”‚ 3. Poll for    â”‚â”€â”€â”€â–¶â”‚ authorization_   â”‚ (waiting)            â”‚\n//! â”‚  â”‚    Access Tokenâ”‚    â”‚ pending          â”‚                      â”‚\n//! â”‚  â”‚                â”‚â—€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n//! â”‚  â”‚   (every N sec)â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n//! â”‚  â”‚                â”‚â”€â”€â”€â–¶â”‚ access_token     â”‚ (success!)           â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n//! â”‚          â”‚                                                        â”‚\n//! â”‚          â–¼                                                        â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n//! â”‚  â”‚ 4. Store Token â”‚  Save to ~/.config/gh/github_token.json      â”‚\n//! â”‚  â”‚    for Later   â”‚  Token used for Copilot API auth             â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚\n//! â”‚                                                                   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Security Notes\n//!\n//! - Client ID is public (Iv1.b507a08c87ecfe98) - this is by design for device flow\n//! - User must authorize in browser - no secrets embedded\n//! - Tokens stored locally with restrictive permissions\n//! - Scopes: `read:user` - minimal permissions needed\n//!\n//! # Error Handling\n//!\n//! Polling responses:\n//! - `authorization_pending` - User hasn't authorized yet, keep polling\n//! - `slow_down` - Polling too fast, increase interval\n//! - `expired_token` - Device code expired, restart flow\n//! - `access_denied` - User denied authorization\n\nuse anyhow::{Context, Result};\nuse serde::Deserialize;\nuse std::time::Duration;\n\nconst GITHUB_CLIENT_ID: &str = \"Iv1.b507a08c87ecfe98\";\nconst GITHUB_SCOPES: &str = \"read:user\";\nconst DEVICE_CODE_URL: &str = \"https://github.com/login/device/code\";\nconst ACCESS_TOKEN_URL: &str = \"https://github.com/login/oauth/access_token\";\n\n/// Response from device code request.\n#[derive(Debug, Clone, Deserialize)]\npub struct DeviceCodeResponse {\n    pub device_code: String,\n    pub user_code: String,\n    pub verification_uri: String,\n    pub expires_in: u64,\n    pub interval: u64,\n}\n\n/// Response from access token polling.\n#[derive(Debug, Clone, Deserialize)]\n#[serde(untagged)]\npub enum AccessTokenResponse {\n    Success {\n        access_token: String,\n        token_type: String,\n        scope: String,\n    },\n    Pending {\n        error: String,\n    },\n}\n\n/// GitHub authentication manager.\npub struct GitHubAuth {\n    client: reqwest::Client,\n}\n\nimpl GitHubAuth {\n    /// Create a new GitHub auth manager.\n    pub fn new() -> Result<Self> {\n        let client = reqwest::Client::builder()\n            .timeout(Duration::from_secs(30))\n            .build()\n            .context(\"Failed to create HTTP client\")?;\n\n        Ok(Self { client })\n    }\n\n    /// Request a device code from GitHub.\n    pub async fn request_device_code(&self) -> Result<DeviceCodeResponse> {\n        let response = self\n            .client\n            .post(DEVICE_CODE_URL)\n            .header(\"Accept\", \"application/json\")\n            .header(\"Content-Type\", \"application/json\")\n            .json(&serde_json::json!({\n                \"client_id\": GITHUB_CLIENT_ID,\n                \"scope\": GITHUB_SCOPES,\n            }))\n            .send()\n            .await\n            .context(\"Failed to request device code\")?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            anyhow::bail!(\"Device code request failed: {} - {}\", status, body);\n        }\n\n        response\n            .json()\n            .await\n            .context(\"Failed to parse device code response\")\n    }\n\n    /// Poll for access token after user authorization.\n    pub async fn poll_access_token(&self, device_code: &str, interval: u64) -> Result<String> {\n        let poll_interval = Duration::from_secs(interval);\n\n        loop {\n            tokio::time::sleep(poll_interval).await;\n\n            let response = self\n                .client\n                .post(ACCESS_TOKEN_URL)\n                .header(\"Accept\", \"application/json\")\n                .header(\"Content-Type\", \"application/json\")\n                .json(&serde_json::json!({\n                    \"client_id\": GITHUB_CLIENT_ID,\n                    \"device_code\": device_code,\n                    \"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n                }))\n                .send()\n                .await\n                .context(\"Failed to poll for access token\")?;\n\n            if !response.status().is_success() {\n                let status = response.status();\n                let body = response.text().await.unwrap_or_default();\n                anyhow::bail!(\"Access token poll failed: {} - {}\", status, body);\n            }\n\n            let result: AccessTokenResponse = response\n                .json()\n                .await\n                .context(\"Failed to parse access token response\")?;\n\n            match result {\n                AccessTokenResponse::Success { access_token, .. } => {\n                    return Ok(access_token);\n                }\n                AccessTokenResponse::Pending { error } => {\n                    if error == \"authorization_pending\" {\n                        // Continue polling\n                        continue;\n                    } else if error == \"slow_down\" {\n                        // Slow down polling\n                        tokio::time::sleep(Duration::from_secs(5)).await;\n                        continue;\n                    } else {\n                        anyhow::bail!(\"Authorization failed: {}\", error);\n                    }\n                }\n            }\n        }\n    }\n\n    /// Complete device code flow: request code and poll for token.\n    pub async fn device_code_flow<F>(&self, on_code: F) -> Result<String>\n    where\n        F: FnOnce(&DeviceCodeResponse),\n    {\n        // Request device code\n        let device_code_response = self.request_device_code().await?;\n\n        // Notify callback with user code\n        on_code(&device_code_response);\n\n        // Poll for access token\n        self.poll_access_token(\n            &device_code_response.device_code,\n            device_code_response.interval,\n        )\n        .await\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    #[ignore] // Requires user interaction\n    async fn test_device_code_flow() {\n        let auth = GitHubAuth::new().unwrap();\n\n        let token = auth\n            .device_code_flow(|code| {\n                println!(\"\\nPlease visit: {}\", code.verification_uri);\n                println!(\"And enter code: {}\", code.user_code);\n                println!(\"\\nWaiting for authorization...\\n\");\n            })\n            .await\n            .unwrap();\n\n        println!(\"Got token: {}...\", &token[..20]);\n        assert!(!token.is_empty());\n    }\n\n    // ========================================================================\n    // Response Parsing Tests (No Network Required)\n    // ========================================================================\n\n    #[test]\n    fn test_device_code_response_deserialization() {\n        let json = r#\"{\n            \"device_code\": \"abc123\",\n            \"user_code\": \"ABCD-1234\",\n            \"verification_uri\": \"https://github.com/login/device\",\n            \"expires_in\": 900,\n            \"interval\": 5\n        }\"#;\n\n        let response: DeviceCodeResponse = serde_json::from_str(json).unwrap();\n\n        assert_eq!(response.device_code, \"abc123\");\n        assert_eq!(response.user_code, \"ABCD-1234\");\n        assert_eq!(response.verification_uri, \"https://github.com/login/device\");\n        assert_eq!(response.expires_in, 900);\n        assert_eq!(response.interval, 5);\n    }\n\n    #[test]\n    fn test_device_code_response_with_extra_fields() {\n        // GitHub may return additional fields\n        let json = r#\"{\n            \"device_code\": \"abc123\",\n            \"user_code\": \"EFGH-5678\",\n            \"verification_uri\": \"https://github.com/login/device\",\n            \"expires_in\": 600,\n            \"interval\": 10,\n            \"verification_uri_complete\": \"https://github.com/login/device?user_code=EFGH-5678\"\n        }\"#;\n\n        // Should parse without error despite extra field\n        let response: DeviceCodeResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.user_code, \"EFGH-5678\");\n    }\n\n    #[test]\n    fn test_access_token_success_response() {\n        let json = r#\"{\n            \"access_token\": \"gho_test_token_abc123\",\n            \"token_type\": \"bearer\",\n            \"scope\": \"read:user\"\n        }\"#;\n\n        let response: AccessTokenResponse = serde_json::from_str(json).unwrap();\n\n        match response {\n            AccessTokenResponse::Success {\n                access_token,\n                token_type,\n                scope,\n            } => {\n                assert_eq!(access_token, \"gho_test_token_abc123\");\n                assert_eq!(token_type, \"bearer\");\n                assert_eq!(scope, \"read:user\");\n            }\n            AccessTokenResponse::Pending { .. } => {\n                panic!(\"Expected Success variant\");\n            }\n        }\n    }\n\n    #[test]\n    fn test_access_token_pending_authorization() {\n        let json = r#\"{\n            \"error\": \"authorization_pending\"\n        }\"#;\n\n        let response: AccessTokenResponse = serde_json::from_str(json).unwrap();\n\n        match response {\n            AccessTokenResponse::Pending { error } => {\n                assert_eq!(error, \"authorization_pending\");\n            }\n            AccessTokenResponse::Success { .. } => {\n                panic!(\"Expected Pending variant\");\n            }\n        }\n    }\n\n    #[test]\n    fn test_access_token_slow_down_error() {\n        let json = r#\"{\n            \"error\": \"slow_down\"\n        }\"#;\n\n        let response: AccessTokenResponse = serde_json::from_str(json).unwrap();\n\n        match response {\n            AccessTokenResponse::Pending { error } => {\n                assert_eq!(error, \"slow_down\");\n            }\n            AccessTokenResponse::Success { .. } => {\n                panic!(\"Expected Pending variant\");\n            }\n        }\n    }\n\n    #[test]\n    fn test_access_token_expired_token_error() {\n        let json = r#\"{\n            \"error\": \"expired_token\"\n        }\"#;\n\n        let response: AccessTokenResponse = serde_json::from_str(json).unwrap();\n\n        match response {\n            AccessTokenResponse::Pending { error } => {\n                assert_eq!(error, \"expired_token\");\n            }\n            AccessTokenResponse::Success { .. } => {\n                panic!(\"Expected Pending variant\");\n            }\n        }\n    }\n\n    #[test]\n    fn test_access_token_access_denied_error() {\n        let json = r#\"{\n            \"error\": \"access_denied\"\n        }\"#;\n\n        let response: AccessTokenResponse = serde_json::from_str(json).unwrap();\n\n        match response {\n            AccessTokenResponse::Pending { error } => {\n                assert_eq!(error, \"access_denied\");\n            }\n            AccessTokenResponse::Success { .. } => {\n                panic!(\"Expected Pending variant\");\n            }\n        }\n    }\n\n    // ========================================================================\n    // Client Creation Tests\n    // ========================================================================\n\n    #[test]\n    fn test_github_auth_creation() {\n        let auth = GitHubAuth::new();\n        assert!(auth.is_ok());\n    }\n\n    // ========================================================================\n    // Constants Tests\n    // ========================================================================\n\n    #[test]\n    fn test_github_client_id_format() {\n        // Client ID should be in Iv1. format\n        assert!(GITHUB_CLIENT_ID.starts_with(\"Iv1.\"));\n        assert!(GITHUB_CLIENT_ID.len() > 5);\n    }\n\n    #[test]\n    fn test_device_code_url_is_https() {\n        assert!(DEVICE_CODE_URL.starts_with(\"https://\"));\n        assert!(DEVICE_CODE_URL.contains(\"github.com\"));\n    }\n\n    #[test]\n    fn test_access_token_url_is_https() {\n        assert!(ACCESS_TOKEN_URL.starts_with(\"https://\"));\n        assert!(ACCESS_TOKEN_URL.contains(\"github.com\"));\n    }\n}\n","traces":[{"line":97,"address":[],"length":0,"stats":{"Line":1}},{"line":98,"address":[],"length":0,"stats":{"Line":2}},{"line":99,"address":[],"length":0,"stats":{"Line":2}},{"line":103,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":146,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":162,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":194,"address":[],"length":0,"stats":{"Line":0}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":0}}],"covered":4,"coverable":52},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","client.rs"],"content":"//! HTTP client for GitHub Copilot API.\n//!\n//! # Direct vs Proxy Mode\n//!\n//! This client supports two modes of operation:\n//!\n//! ## Direct Mode (Default)\n//! Connects directly to `api.githubcopilot.com` using native Rust HTTP.\n//! No external dependencies required after initial GitHub authentication.\n//!\n//! ## Proxy Mode (Legacy)\n//! Connects to a local copilot-api proxy for backward compatibility.\n//! Set `VSCODE_COPILOT_DIRECT=false` to use proxy mode.\n//!\n//! # Request Flow\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                      REQUEST FLOW                                â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                  â”‚\n//! â”‚  Application                      VsCodeCopilotClient           â”‚\n//! â”‚       â”‚                                  â”‚                       â”‚\n//! â”‚       â”‚  complete(\"prompt\")              â”‚                       â”‚\n//! â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                       â”‚\n//! â”‚       â”‚                                  â”‚                       â”‚\n//! â”‚       â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n//! â”‚       â”‚                          â”‚ 1. get_token()â”‚               â”‚\n//! â”‚       â”‚                          â”‚   â””â”€â–¶ TokenMgrâ”‚               â”‚\n//! â”‚       â”‚                          â”‚      (refresh â”‚               â”‚\n//! â”‚       â”‚                          â”‚       if exp) â”‚               â”‚\n//! â”‚       â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n//! â”‚       â”‚                                  â”‚                       â”‚\n//! â”‚       â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n//! â”‚       â”‚                          â”‚2. build_hdrs()â”‚               â”‚\n//! â”‚       â”‚                          â”‚ - Authorizationâ”‚              â”‚\n//! â”‚       â”‚                          â”‚ - x-request-id â”‚              â”‚\n//! â”‚       â”‚                          â”‚ - openai-intentâ”‚              â”‚\n//! â”‚       â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n//! â”‚       â”‚                                  â”‚                       â”‚\n//! â”‚       â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n//! â”‚       â”‚                          â”‚ 3. POST to APIâ”‚               â”‚\n//! â”‚       â”‚                          â”‚  Direct: /chatâ”‚               â”‚\n//! â”‚       â”‚                          â”‚  Proxy: /v1/  â”‚               â”‚\n//! â”‚       â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n//! â”‚       â”‚                                  â”‚                       â”‚\n//! â”‚       â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                       â”‚\n//! â”‚       â”‚  Response (parsed JSON)          â”‚                       â”‚\n//! â”‚                                                                  â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Account Types\n//!\n//! Supports different GitHub Copilot account types:\n//! - `individual` â†’ `api.githubcopilot.com`\n//! - `business` â†’ `api.business.githubcopilot.com`\n//! - `enterprise` â†’ `api.enterprise.githubcopilot.com`\n//!\n//! # Header Requirements\n//!\n//! Direct mode sends these headers (matching TypeScript copilot-api):\n//! - `Authorization: Bearer <token>`\n//! - `x-github-api-version: 2025-04-01`\n//! - `copilot-integration-id: vscode-chat`\n//! - `openai-intent: conversation-panel`\n//! - `x-request-id: <uuid>`\n//! - `editor-version: vscode/1.95.0`\n//! - `editor-plugin-version: copilot-chat/0.26.7`\n//! - `x-vscode-user-agent-library-version: electron-fetch`\n\nuse reqwest::{Client as ReqwestClient, Response, StatusCode};\nuse serde::de::DeserializeOwned;\nuse std::time::Duration;\nuse tracing::{debug, error, warn};\nuse uuid::Uuid;\n\nuse super::error::{Result, VsCodeError};\nuse super::token::TokenManager;\nuse super::types::*;\n\n// API Constants - Match TypeScript copilot-api implementation\nconst COPILOT_API_VERSION: &str = \"2025-04-01\";\n#[allow(dead_code)]\nconst COPILOT_VERSION: &str = \"0.26.7\";\nconst EDITOR_VERSION: &str = \"vscode/1.95.0\";\nconst EDITOR_PLUGIN_VERSION: &str = \"copilot-chat/0.26.7\";\nconst USER_AGENT: &str = \"GitHubCopilotChat/0.26.7\";\nconst MAX_RETRIES: u32 = 3; // Maximum retry attempts for transient errors\nconst INITIAL_RETRY_DELAY_MS: u64 = 1000; // Initial retry delay (1 second)\n\n/// Account type for Copilot API endpoint selection.\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]\npub enum AccountType {\n    /// Individual GitHub Copilot subscription\n    #[default]\n    Individual,\n    /// GitHub Copilot Business\n    Business,\n    /// GitHub Copilot Enterprise\n    Enterprise,\n}\n\nimpl AccountType {\n    /// Get the base URL for this account type.\n    pub fn base_url(&self) -> &'static str {\n        match self {\n            AccountType::Individual => \"https://api.githubcopilot.com\",\n            AccountType::Business => \"https://api.business.githubcopilot.com\",\n            AccountType::Enterprise => \"https://api.enterprise.githubcopilot.com\",\n        }\n    }\n\n    /// Parse account type from string.\n    #[allow(clippy::should_implement_trait)]\n    pub fn from_str(s: &str) -> Option<Self> {\n        match s.to_lowercase().as_str() {\n            \"individual\" => Some(AccountType::Individual),\n            \"business\" => Some(AccountType::Business),\n            \"enterprise\" => Some(AccountType::Enterprise),\n            _ => None,\n        }\n    }\n}\n\n/// HTTP client for GitHub Copilot API.\n///\n/// Supports both direct API access and proxy mode.\n#[derive(Clone)]\npub struct VsCodeCopilotClient {\n    client: ReqwestClient,\n    base_url: String,\n    token_manager: TokenManager,\n    /// Whether to use direct API mode (true) or proxy mode (false).\n    direct_mode: bool,\n    /// Account type for API endpoint selection.\n    #[allow(dead_code)]\n    account_type: AccountType,\n    /// Whether vision mode is enabled for requests.\n    vision_enabled: bool,\n}\n\nimpl VsCodeCopilotClient {\n    /// Create a new client with direct API mode (default).\n    ///\n    /// Uses `api.githubcopilot.com` directly without a proxy.\n    pub fn new(timeout: Duration) -> Result<Self> {\n        Self::new_with_options(timeout, true, AccountType::Individual)\n    }\n\n    /// Create a new client with a custom base URL (for proxy mode).\n    ///\n    /// This is the legacy mode that connects to a local copilot-api proxy.\n    pub fn with_base_url(base_url: impl Into<String>, timeout: Duration) -> Result<Self> {\n        let base_url = base_url.into();\n        let is_direct = base_url.contains(\"githubcopilot.com\");\n\n        let client = ReqwestClient::builder()\n            .timeout(timeout)\n            .pool_max_idle_per_host(10)\n            .pool_idle_timeout(Duration::from_secs(90))\n            .build()\n            .map_err(|e| VsCodeError::ClientInit(e.to_string()))?;\n\n        let token_manager =\n            TokenManager::new().map_err(|e| VsCodeError::ClientInit(e.to_string()))?;\n\n        debug!(\n            base_url = %base_url,\n            timeout_secs = timeout.as_secs(),\n            direct_mode = is_direct,\n            \"VSCode Copilot client initialized\"\n        );\n\n        Ok(Self {\n            client,\n            base_url,\n            token_manager,\n            direct_mode: is_direct,\n            account_type: AccountType::Individual,\n            vision_enabled: false,\n        })\n    }\n\n    /// Create a new client with specified options.\n    ///\n    /// # Arguments\n    ///\n    /// * `timeout` - Request timeout duration\n    /// * `direct_mode` - If true, use direct API; if false, use proxy\n    /// * `account_type` - Account type for API endpoint selection\n    pub fn new_with_options(\n        timeout: Duration,\n        direct_mode: bool,\n        account_type: AccountType,\n    ) -> Result<Self> {\n        let base_url = if direct_mode {\n            account_type.base_url().to_string()\n        } else {\n            std::env::var(\"VSCODE_COPILOT_PROXY_URL\")\n                .unwrap_or_else(|_| \"http://localhost:4141\".to_string())\n        };\n\n        let client = ReqwestClient::builder()\n            .timeout(timeout)\n            .pool_max_idle_per_host(10)\n            .pool_idle_timeout(Duration::from_secs(90))\n            .build()\n            .map_err(|e| VsCodeError::ClientInit(e.to_string()))?;\n\n        let token_manager =\n            TokenManager::new().map_err(|e| VsCodeError::ClientInit(e.to_string()))?;\n\n        debug!(\n            base_url = %base_url,\n            timeout_secs = timeout.as_secs(),\n            direct_mode = direct_mode,\n            account_type = ?account_type,\n            \"VSCode Copilot client initialized\"\n        );\n\n        Ok(Self {\n            client,\n            base_url,\n            token_manager,\n            direct_mode,\n            account_type,\n            vision_enabled: false,\n        })\n    }\n\n    /// Enable vision mode for image processing.\n    pub fn with_vision(mut self, enabled: bool) -> Self {\n        self.vision_enabled = enabled;\n        self\n    }\n\n    /// Get a valid Copilot token, refreshing if needed.\n    async fn get_token(&self) -> Result<String> {\n        self.token_manager\n            .get_valid_copilot_token()\n            .await\n            .map_err(|e| VsCodeError::Authentication(e.to_string()))\n    }\n\n    /// Build request headers with authentication.\n    ///\n    /// For direct mode, includes all headers required by GitHub Copilot API.\n    /// For proxy mode, includes minimal headers (proxy adds the rest).\n    async fn build_headers(&self) -> Result<reqwest::header::HeaderMap> {\n        let token = self.get_token().await?;\n\n        let mut headers = reqwest::header::HeaderMap::new();\n\n        // Authorization - required for both modes\n        headers.insert(\n            reqwest::header::AUTHORIZATION,\n            format!(\"Bearer {}\", token).parse().unwrap(),\n        );\n\n        // Content-Type\n        headers.insert(\n            reqwest::header::CONTENT_TYPE,\n            \"application/json\".parse().unwrap(),\n        );\n\n        // User-Agent\n        headers.insert(reqwest::header::USER_AGENT, USER_AGENT.parse().unwrap());\n\n        // Editor version headers\n        headers.insert(\"editor-version\", EDITOR_VERSION.parse().unwrap());\n        headers.insert(\n            \"editor-plugin-version\",\n            EDITOR_PLUGIN_VERSION.parse().unwrap(),\n        );\n\n        // Direct mode requires additional headers to match TypeScript implementation\n        if self.direct_mode {\n            // Copilot-specific headers\n            headers.insert(\"copilot-integration-id\", \"vscode-chat\".parse().unwrap());\n            headers.insert(\"openai-intent\", \"conversation-panel\".parse().unwrap());\n\n            // GitHub API version\n            headers.insert(\"x-github-api-version\", COPILOT_API_VERSION.parse().unwrap());\n\n            // Request ID for tracing\n            headers.insert(\"x-request-id\", Uuid::new_v4().to_string().parse().unwrap());\n\n            // VSCode user agent library\n            headers.insert(\n                \"x-vscode-user-agent-library-version\",\n                \"electron-fetch\".parse().unwrap(),\n            );\n\n            // Vision mode header\n            if self.vision_enabled {\n                headers.insert(\"copilot-vision-request\", \"true\".parse().unwrap());\n            }\n        }\n\n        Ok(headers)\n    }\n\n    /// Retry an async operation with exponential backoff for retryable errors.\n    ///\n    /// This method implements automatic retry logic for transient failures:\n    /// - Network errors (timeouts, connection refused)\n    /// - Rate limiting (429)\n    /// - Service unavailable (503)\n    /// - Bad gateway (502)\n    ///\n    /// Non-retryable errors (authentication, invalid request) are returned immediately.\n    async fn retry_with_backoff<F, Fut, T>(\n        &self,\n        operation: F,\n        operation_name: &str,\n    ) -> Result<T>\n    where\n        F: Fn() -> Fut,\n        Fut: std::future::Future<Output = Result<T>>,\n    {\n        let mut last_error = None;\n        \n        for attempt in 0..=MAX_RETRIES {\n            match operation().await {\n                Ok(result) => return Ok(result),\n                Err(e) => {\n                    // Check if error is retryable\n                    if !e.is_retryable() || attempt == MAX_RETRIES {\n                        return Err(e);\n                    }\n                    \n                    // Calculate exponential backoff delay\n                    let delay = Duration::from_millis(INITIAL_RETRY_DELAY_MS * 2_u64.pow(attempt));\n                    \n                    warn!(\n                        operation = operation_name,\n                        attempt = attempt + 1,\n                        max_retries = MAX_RETRIES,\n                        delay_ms = delay.as_millis(),\n                        error = %e,\n                        \"Retrying after retryable error\"\n                    );\n                    \n                    tokio::time::sleep(delay).await;\n                    last_error = Some(e);\n                }\n            }\n        }\n        \n        Err(last_error.unwrap_or_else(|| VsCodeError::ApiError(\"Operation failed after retries\".to_string())))\n    }\n\n    /// Determine if this is an agent call (multi-turn conversation).\n    ///\n    /// # WHY: X-Initiator Header\n    ///\n    /// The Copilot API uses the X-Initiator header to distinguish between:\n    /// - `\"user\"`: Initial user message (first turn, no prior assistant/tool messages)\n    /// - `\"agent\"`: Follow-up from coding agent (has assistant or tool messages)\n    ///\n    /// This matches the TypeScript proxy behavior:\n    /// `copilot-api/src/services/copilot/create-chat-completions.ts:22-29`\n    ///\n    /// ```text\n    /// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    /// â”‚                   X-INITIATOR LOGIC                          â”‚\n    /// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    /// â”‚                                                              â”‚\n    /// â”‚  messages: [system, user]          â†’ X-Initiator: \"user\"    â”‚\n    /// â”‚                                                              â”‚\n    /// â”‚  messages: [system, user,          â†’ X-Initiator: \"agent\"   â”‚\n    /// â”‚             assistant, user]                                 â”‚\n    /// â”‚                                                              â”‚\n    /// â”‚  messages: [system, user,          â†’ X-Initiator: \"agent\"   â”‚\n    /// â”‚             assistant, tool, user]                           â”‚\n    /// â”‚                                                              â”‚\n    /// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    /// ```\n    fn is_agent_call(messages: &[RequestMessage]) -> bool {\n        messages\n            .iter()\n            .any(|m| matches!(m.role.as_str(), \"assistant\" | \"tool\"))\n    }\n\n    /// Send a chat completion request (non-streaming).\n    pub async fn chat_completion(\n        &self,\n        request: ChatCompletionRequest,\n    ) -> Result<ChatCompletionResponse> {\n        let request_clone = request.clone();\n        \n        // Wrap the request in retry logic\n        self.retry_with_backoff(\n            || async {\n                // Direct mode uses /chat/completions, proxy mode uses /v1/chat/completions\n                let url = if self.direct_mode {\n                    format!(\"{}/chat/completions\", self.base_url)\n                } else {\n                    format!(\"{}/v1/chat/completions\", self.base_url)\n                };\n                let mut headers = self.build_headers().await?;\n\n                // Add X-Initiator header for agent/user distinction (direct mode only)\n                if self.direct_mode {\n                    let initiator = if Self::is_agent_call(&request_clone.messages) {\n                        \"agent\"\n                    } else {\n                        \"user\"\n                    };\n                    headers.insert(\"X-Initiator\", initiator.parse().unwrap());\n                }\n\n                debug!(\n                    url = %url,\n                    model = %request_clone.model,\n                    message_count = request_clone.messages.len(),\n                    direct_mode = self.direct_mode,\n                    \"Sending chat completion request\"\n                );\n\n                let response = self\n                    .client\n                    .post(&url)\n                    .headers(headers)\n                    .json(&request_clone)\n                    .send()\n                    .await\n                    .map_err(|e| VsCodeError::Network(e.to_string()))?;\n\n                let mut response: ChatCompletionResponse = Self::handle_response(response).await?;\n\n                // OODA-07.2: Normalize Anthropic-style split choices\n                response = Self::normalize_choices(response);\n\n                Ok(response)\n            },\n            \"chat_completion\",\n        ).await\n    }\n\n    /// Send a streaming chat completion request.\n    pub async fn chat_completion_stream(&self, request: ChatCompletionRequest) -> Result<Response> {\n        // Direct mode uses /chat/completions, proxy mode uses /v1/chat/completions\n        let url = if self.direct_mode {\n            format!(\"{}/chat/completions\", self.base_url)\n        } else {\n            format!(\"{}/v1/chat/completions\", self.base_url)\n        };\n        let mut headers = self.build_headers().await?;\n\n        // Add X-Initiator header for agent/user distinction (direct mode only)\n        if self.direct_mode {\n            let initiator = if Self::is_agent_call(&request.messages) {\n                \"agent\"\n            } else {\n                \"user\"\n            };\n            headers.insert(\"X-Initiator\", initiator.parse().unwrap());\n        }\n\n        debug!(\n            url = %url,\n            model = %request.model,\n            message_count = request.messages.len(),\n            \"Sending streaming chat completion request\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .headers(headers)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| VsCodeError::Network(e.to_string()))?;\n\n        if response.status().is_success() {\n            Ok(response)\n        } else {\n            let status = response.status();\n            let error_body = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            warn!(\n                status = %status,\n                error = %error_body,\n                \"Streaming request failed\"\n            );\n\n            Err(Self::map_error_status(status, error_body))\n        }\n    }\n\n    /// List available models.\n    ///\n    /// Returns a list of models available for the authenticated user.\n    /// Includes model capabilities, limits, and supported features.\n    pub async fn list_models(&self) -> Result<ModelsResponse> {\n        // Direct mode uses /models, proxy mode uses /v1/models\n        let url = if self.direct_mode {\n            format!(\"{}/models\", self.base_url)\n        } else {\n            format!(\"{}/v1/models\", self.base_url)\n        };\n        let headers = self.build_headers().await?;\n\n        debug!(\n            url = %url,\n            direct_mode = self.direct_mode,\n            \"Fetching models list\"\n        );\n\n        let response = self\n            .client\n            .get(&url)\n            .headers(headers)\n            .send()\n            .await\n            .map_err(|e| VsCodeError::Network(e.to_string()))?;\n\n        Self::handle_response(response).await\n    }\n\n    /// Create embeddings for the given input.\n    ///\n    /// # Arguments\n    ///\n    /// * `request` - The embedding request containing input text(s) and model\n    ///\n    /// # Returns\n    ///\n    /// Returns an `EmbeddingResponse` containing the embedding vectors.\n    ///\n    /// # Example\n    ///\n    /// ```rust,ignore\n    /// let request = EmbeddingRequest::new(\"Hello, world!\", \"text-embedding-3-small\");\n    /// let response = client.create_embeddings(request).await?;\n    /// let embedding = response.first_embedding().unwrap();\n    /// ```\n    pub async fn create_embeddings(&self, request: EmbeddingRequest) -> Result<EmbeddingResponse> {\n        // Direct mode uses /embeddings, proxy mode uses /v1/embeddings\n        let url = if self.direct_mode {\n            format!(\"{}/embeddings\", self.base_url)\n        } else {\n            format!(\"{}/v1/embeddings\", self.base_url)\n        };\n        let headers = self.build_headers().await?;\n\n        debug!(\n            url = %url,\n            model = %request.model,\n            direct_mode = self.direct_mode,\n            \"Sending embedding request\"\n        );\n\n        let response = self\n            .client\n            .post(&url)\n            .headers(headers)\n            .json(&request)\n            .send()\n            .await\n            .map_err(|e| VsCodeError::Network(e.to_string()))?;\n\n        Self::handle_response(response).await\n    }\n\n    /// Normalize Anthropic-style split choices into a single choice.\n    ///\n    /// # WHY: Anthropic Model Response Format\n    ///\n    /// Claude models (Haiku 4.5, Sonnet 4.5) via Copilot API return TWO separate choices:\n    /// - Choice 1: Contains only `content` (the model's thinking/reasoning)\n    /// - Choice 2: Contains only `tool_calls` (the function calls to execute)\n    ///\n    /// Neither choice includes an `index` field. This function merges them into a single\n    /// choice to match the expected format where a single message contains both content\n    /// and tool_calls.\n    ///\n    /// # Example Anthropic Response\n    ///\n    /// ```json\n    /// {\n    ///   \"choices\": [\n    ///     {\n    ///       \"finish_reason\": \"tool_calls\",\n    ///       \"message\": {\n    ///         \"content\": \"I'll examine the file to understand its structure\",\n    ///         \"role\": \"assistant\"\n    ///       }\n    ///     },\n    ///     {\n    ///       \"finish_reason\": \"tool_calls\",\n    ///       \"message\": {\n    ///         \"role\": \"assistant\",\n    ///         \"tool_calls\": [...]\n    ///       }\n    ///     }\n    ///   ]\n    /// }\n    /// ```\n    ///\n    /// # WHEN TO MERGE\n    ///\n    /// Merge only when:\n    /// 1. Multiple choices exist\n    /// 2. All choices have `index == None` or `index == Some(0)` (Anthropic pattern)\n    /// 3. Each choice has partial data (some have content, others have tool_calls)\n    ///\n    /// # Arguments\n    ///\n    /// * `response` - The raw ChatCompletionResponse from the API\n    ///\n    /// # Returns\n    ///\n    /// Normalized response with merged choices\n    fn normalize_choices(mut response: ChatCompletionResponse) -> ChatCompletionResponse {\n        // Only normalize if multiple choices exist\n        if response.choices.len() <= 1 {\n            return response;\n        }\n\n        // Check if all choices have None or 0 index (Anthropic pattern)\n        let needs_merge = response\n            .choices\n            .iter()\n            .all(|c| c.index.is_none() || c.index == Some(0));\n\n        if !needs_merge {\n            return response;\n        }\n\n        debug!(\n            choice_count = response.choices.len(),\n            model = %response.model,\n            \"OODA-07.2: Normalizing Anthropic-style split choices\"\n        );\n\n        // Take ownership of choices vector\n        let mut choices_iter = response.choices.into_iter();\n\n        // Take first choice as base\n        let mut merged = choices_iter.next().unwrap();\n\n        // Merge data from remaining choices\n        for choice in choices_iter {\n            // Merge content (prefer non-empty)\n            if merged.message.content.is_none()\n                || merged\n                    .message\n                    .content\n                    .as_ref()\n                    .map(|s| s.is_empty())\n                    .unwrap_or(true)\n            {\n                merged.message.content = choice.message.content;\n            }\n\n            // Merge tool_calls (prefer non-empty, extend if both present)\n            if merged.message.tool_calls.is_none() {\n                merged.message.tool_calls = choice.message.tool_calls;\n            } else if let Some(mut existing) = merged.message.tool_calls.take() {\n                if let Some(new_calls) = choice.message.tool_calls {\n                    existing.extend(new_calls);\n                }\n                merged.message.tool_calls = Some(existing);\n            }\n\n            // Keep first non-None finish_reason\n            if merged.finish_reason.is_none() {\n                merged.finish_reason = choice.finish_reason;\n            }\n        }\n\n        // Set index explicitly\n        merged.index = Some(0);\n\n        response.choices = vec![merged];\n        response\n    }\n\n    /// Handle HTTP response with error mapping.\n    async fn handle_response<T: DeserializeOwned>(response: Response) -> Result<T> {\n        let status = response.status();\n\n        if status.is_success() {\n            // Get the response text first for debugging\n            let body_text = response\n                .text()\n                .await\n                .map_err(|e| VsCodeError::Decode(format!(\"Failed to read response body: {}\", e)))?;\n\n            // Log the raw response for debugging\n            debug!(\n                status = %status,\n                body_length = body_text.len(),\n                body_preview = &body_text[..body_text.len().min(500)],\n                \"Raw API response\"\n            );\n\n            // Try to deserialize\n            serde_json::from_str(&body_text).map_err(|e| {\n                error!(\n                    error = %e,\n                    body = %body_text,\n                    \"Failed to deserialize response\"\n                );\n                VsCodeError::Decode(format!(\n                    \"Deserialization failed: {} | Body: {}\",\n                    e, body_text\n                ))\n            })\n        } else {\n            let error_body = response\n                .text()\n                .await\n                .unwrap_or_else(|_| \"Unknown error\".to_string());\n\n            warn!(\n                status = %status,\n                error = %error_body,\n                \"Request failed\"\n            );\n\n            Err(Self::map_error_status(status, error_body))\n        }\n    }\n\n    /// Map HTTP status to VsCodeError.\n    fn map_error_status(status: StatusCode, body: String) -> VsCodeError {\n        match status {\n            StatusCode::UNAUTHORIZED | StatusCode::FORBIDDEN => {\n                VsCodeError::Authentication(format!(\"Copilot authentication failed: {}\", body))\n            }\n            StatusCode::TOO_MANY_REQUESTS => VsCodeError::RateLimited,\n            StatusCode::BAD_REQUEST => VsCodeError::InvalidRequest(body),\n            StatusCode::SERVICE_UNAVAILABLE => VsCodeError::ServiceUnavailable,\n            StatusCode::BAD_GATEWAY => {\n                VsCodeError::Network(format!(\"Upstream error (502): {}\", body))\n            }\n            StatusCode::GATEWAY_TIMEOUT | StatusCode::REQUEST_TIMEOUT => {\n                VsCodeError::Network(format!(\"Timeout: {}\", body))\n            }\n            _ => VsCodeError::ApiError(format!(\"HTTP {}: {}\", status, body)),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    // ========================================================================\n    // AccountType Tests\n    // ========================================================================\n\n    #[test]\n    fn test_account_type_base_url_individual() {\n        assert_eq!(\n            AccountType::Individual.base_url(),\n            \"https://api.githubcopilot.com\"\n        );\n    }\n\n    #[test]\n    fn test_account_type_base_url_business() {\n        assert_eq!(\n            AccountType::Business.base_url(),\n            \"https://api.business.githubcopilot.com\"\n        );\n    }\n\n    #[test]\n    fn test_account_type_base_url_enterprise() {\n        assert_eq!(\n            AccountType::Enterprise.base_url(),\n            \"https://api.enterprise.githubcopilot.com\"\n        );\n    }\n\n    #[test]\n    fn test_account_type_from_str_individual() {\n        assert_eq!(\n            AccountType::from_str(\"individual\"),\n            Some(AccountType::Individual)\n        );\n        // Case insensitive\n        assert_eq!(\n            AccountType::from_str(\"INDIVIDUAL\"),\n            Some(AccountType::Individual)\n        );\n        assert_eq!(\n            AccountType::from_str(\"Individual\"),\n            Some(AccountType::Individual)\n        );\n    }\n\n    #[test]\n    fn test_account_type_from_str_business() {\n        assert_eq!(\n            AccountType::from_str(\"business\"),\n            Some(AccountType::Business)\n        );\n        assert_eq!(\n            AccountType::from_str(\"BUSINESS\"),\n            Some(AccountType::Business)\n        );\n    }\n\n    #[test]\n    fn test_account_type_from_str_enterprise() {\n        assert_eq!(\n            AccountType::from_str(\"enterprise\"),\n            Some(AccountType::Enterprise)\n        );\n        assert_eq!(\n            AccountType::from_str(\"Enterprise\"),\n            Some(AccountType::Enterprise)\n        );\n    }\n\n    #[test]\n    fn test_account_type_from_str_unknown_returns_none() {\n        assert_eq!(AccountType::from_str(\"unknown\"), None);\n        assert_eq!(AccountType::from_str(\"\"), None);\n        assert_eq!(AccountType::from_str(\"personal\"), None);\n        assert_eq!(AccountType::from_str(\"team\"), None);\n    }\n\n    #[test]\n    fn test_account_type_default_is_individual() {\n        let default: AccountType = Default::default();\n        assert_eq!(default, AccountType::Individual);\n    }\n\n    // ========================================================================\n    // Error Status Mapping Tests\n    // ========================================================================\n\n    #[test]\n    fn test_map_error_status_unauthorized() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::UNAUTHORIZED,\n            \"Invalid token\".to_string(),\n        );\n        match err {\n            VsCodeError::Authentication(msg) => {\n                assert!(msg.contains(\"authentication failed\"));\n                assert!(msg.contains(\"Invalid token\"));\n            }\n            other => panic!(\"Expected Authentication error, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_forbidden() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::FORBIDDEN,\n            \"Access denied\".to_string(),\n        );\n        match err {\n            VsCodeError::Authentication(msg) => {\n                assert!(msg.contains(\"Access denied\"));\n            }\n            other => panic!(\"Expected Authentication error, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_rate_limited() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::TOO_MANY_REQUESTS,\n            \"Rate limit exceeded\".to_string(),\n        );\n        assert!(matches!(err, VsCodeError::RateLimited));\n    }\n\n    #[test]\n    fn test_map_error_status_bad_request() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::BAD_REQUEST,\n            \"Invalid JSON\".to_string(),\n        );\n        match err {\n            VsCodeError::InvalidRequest(msg) => assert_eq!(msg, \"Invalid JSON\"),\n            other => panic!(\"Expected InvalidRequest error, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_service_unavailable() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::SERVICE_UNAVAILABLE,\n            \"Maintenance\".to_string(),\n        );\n        assert!(matches!(err, VsCodeError::ServiceUnavailable));\n    }\n\n    #[test]\n    fn test_map_error_status_timeout() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::GATEWAY_TIMEOUT,\n            \"Upstream timeout\".to_string(),\n        );\n        match err {\n            VsCodeError::Network(msg) => {\n                assert!(msg.contains(\"Timeout\"));\n                assert!(msg.contains(\"Upstream timeout\"));\n            }\n            other => panic!(\"Expected Network error, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_request_timeout() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::REQUEST_TIMEOUT,\n            \"Request took too long\".to_string(),\n        );\n        match err {\n            VsCodeError::Network(msg) => assert!(msg.contains(\"Timeout\")),\n            other => panic!(\"Expected Network error, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_internal_server_error() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::INTERNAL_SERVER_ERROR,\n            \"Something went wrong\".to_string(),\n        );\n        match err {\n            VsCodeError::ApiError(msg) => {\n                assert!(msg.contains(\"500\"));\n                assert!(msg.contains(\"Something went wrong\"));\n            }\n            other => panic!(\"Expected ApiError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_not_found() {\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::NOT_FOUND,\n            \"Endpoint not found\".to_string(),\n        );\n        match err {\n            VsCodeError::ApiError(msg) => {\n                assert!(msg.contains(\"404\"));\n                assert!(msg.contains(\"not found\"));\n            }\n            other => panic!(\"Expected ApiError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_map_error_status_bad_gateway() {\n        // WHY: 502 indicates upstream server issue - retryable\n        let err = VsCodeCopilotClient::map_error_status(\n            StatusCode::BAD_GATEWAY,\n            \"Upstream server error\".to_string(),\n        );\n        match err {\n            VsCodeError::Network(msg) => {\n                assert!(msg.contains(\"Upstream\") || msg.contains(\"502\"));\n            }\n            other => panic!(\"Expected Network, got {:?}\", other),\n        }\n    }\n\n    // ========================================================================\n    // API Constants Tests - Verify TypeScript Parity\n    // ========================================================================\n\n    #[test]\n    fn test_header_constants_match_typescript() {\n        // These must match copilot-api/src/lib/api-config.ts\n        assert_eq!(COPILOT_API_VERSION, \"2025-04-01\");\n        assert_eq!(EDITOR_VERSION, \"vscode/1.95.0\");\n        assert!(EDITOR_PLUGIN_VERSION.contains(\"copilot\"));\n        assert!(USER_AGENT.contains(\"Copilot\"));\n    }\n\n    #[test]\n    fn test_api_version_format() {\n        // Verify API version is in expected format YYYY-MM-DD\n        assert!(COPILOT_API_VERSION.len() == 10);\n        assert!(COPILOT_API_VERSION.starts_with(\"2025\"));\n    }\n\n    #[test]\n    fn test_editor_version_format() {\n        assert!(EDITOR_VERSION.starts_with(\"vscode/\"));\n    }\n\n    #[test]\n    fn test_user_agent_contains_copilot() {\n        assert!(USER_AGENT.contains(\"Copilot\"));\n    }\n\n    // ========================================================================\n    // X-Initiator Header Tests\n    // ========================================================================\n    //\n    // WHY: These tests verify parity with TypeScript proxy behavior.\n    // See: copilot-api/src/services/copilot/create-chat-completions.ts:22-29\n    //\n    // The is_agent_call function determines the X-Initiator header value:\n    // - \"user\" for initial user queries (no assistant/tool messages)\n    // - \"agent\" for multi-turn conversations (has assistant/tool messages)\n\n    /// Helper to create a test message with a role.\n    fn make_message(role: &str) -> RequestMessage {\n        RequestMessage {\n            role: role.to_string(),\n            content: Some(RequestContent::Text(\"test\".to_string())),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n        }\n    }\n\n    #[test]\n    fn test_is_agent_call_empty_messages() {\n        // Empty messages â†’ false (no agent messages)\n        let messages: Vec<RequestMessage> = vec![];\n        assert!(!VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_user_only() {\n        // Only user messages â†’ false (initial query)\n        let messages = vec![make_message(\"user\")];\n        assert!(!VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_system_and_user() {\n        // System + user â†’ false (initial query with system prompt)\n        let messages = vec![make_message(\"system\"), make_message(\"user\")];\n        assert!(!VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_with_assistant() {\n        // Has assistant message â†’ true (multi-turn)\n        let messages = vec![\n            make_message(\"system\"),\n            make_message(\"user\"),\n            make_message(\"assistant\"),\n            make_message(\"user\"),\n        ];\n        assert!(VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_with_tool() {\n        // Has tool message â†’ true (tool response)\n        let messages = vec![\n            make_message(\"system\"),\n            make_message(\"user\"),\n            make_message(\"assistant\"),\n            make_message(\"tool\"),\n            make_message(\"user\"),\n        ];\n        assert!(VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_assistant_only() {\n        // Assistant only â†’ true\n        let messages = vec![make_message(\"assistant\")];\n        assert!(VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_tool_only() {\n        // Tool only â†’ true\n        let messages = vec![make_message(\"tool\")];\n        assert!(VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    #[test]\n    fn test_is_agent_call_developer_role() {\n        // Developer role (newer) â†’ false (not agent/tool)\n        let messages = vec![make_message(\"developer\"), make_message(\"user\")];\n        assert!(!VsCodeCopilotClient::is_agent_call(&messages));\n    }\n\n    // ========================================================================\n    // Vision Mode Tests\n    // ========================================================================\n    //\n    // WHY: The Copilot API requires the `copilot-vision-request: true` header\n    // for requests that include image content.\n    //\n    // NOTE: The TypeScript proxy auto-detects vision mode by checking if any\n    // message content contains `image_url` type. Our Rust implementation\n    // currently uses manual `with_vision(true)` - auto-detection is a future\n    // enhancement. See: copilot-api/src/services/copilot/create-chat-completions.ts:15-17\n\n    #[test]\n    fn test_client_vision_disabled_by_default() {\n        use std::time::Duration;\n\n        // New client should have vision disabled\n        let client = VsCodeCopilotClient::new(Duration::from_secs(30));\n        assert!(client.is_ok(), \"Client should be created successfully\");\n\n        // We can't directly test the internal field, but we verify\n        // with_vision returns a valid client\n        let client = client.unwrap().with_vision(false);\n        // Should compile and not panic\n        let _ = client;\n    }\n\n    #[test]\n    fn test_client_with_vision_enables_mode() {\n        use std::time::Duration;\n\n        let client = VsCodeCopilotClient::new(Duration::from_secs(30))\n            .unwrap()\n            .with_vision(true);\n\n        // Method should compile and return self\n        let _ = client;\n    }\n\n    #[test]\n    fn test_client_with_vision_chain() {\n        use std::time::Duration;\n\n        // Vision mode should be chainable\n        let client = VsCodeCopilotClient::new(Duration::from_secs(30))\n            .unwrap()\n            .with_vision(true)\n            .with_vision(false)\n            .with_vision(true);\n\n        // Should compile and not panic\n        let _ = client;\n    }\n\n    #[test]\n    fn test_client_with_base_url_vision() {\n        use std::time::Duration;\n\n        let client =\n            VsCodeCopilotClient::with_base_url(\"http://localhost:4141\", Duration::from_secs(30))\n                .unwrap()\n                .with_vision(true);\n\n        // Proxy mode with vision should work\n        let _ = client;\n    }\n\n    #[test]\n    fn test_client_with_options_vision() {\n        use std::time::Duration;\n\n        // All account types with vision\n        for account_type in [\n            AccountType::Individual,\n            AccountType::Business,\n            AccountType::Enterprise,\n        ] {\n            let client = VsCodeCopilotClient::new_with_options(\n                Duration::from_secs(30),\n                true, // direct mode\n                account_type,\n            )\n            .unwrap()\n            .with_vision(true);\n\n            // All should work\n            let _ = client;\n        }\n    }\n\n    // =========================================================================\n    // Embedding Client Tests\n    // WHY: Verify embedding URL construction and request serialization\n    // =========================================================================\n\n    #[test]\n    fn test_embedding_url_direct_mode() {\n        // WHY: Direct mode should use /embeddings (not /v1/embeddings)\n        // This matches the TypeScript proxy behavior for GitHub Copilot API\n        use std::time::Duration;\n\n        let client =\n            VsCodeCopilotClient::new(Duration::from_secs(30)).expect(\"Failed to create client\");\n\n        // Direct mode is the default\n        assert!(client.direct_mode, \"Default should be direct mode\");\n\n        // URL should be constructed as {base_url}/embeddings\n        // In direct mode, create_embeddings appends /embeddings to base_url\n        let base_url = &client.base_url;\n        assert!(\n            !base_url.ends_with(\"/v1\"),\n            \"Direct mode base URL should not end with /v1\"\n        );\n\n        // The create_embeddings method appends /embeddings in direct mode\n        // We verify the base URL is set up correctly for this\n        assert!(\n            base_url.starts_with(\"https://api\"),\n            \"Direct mode should use GitHub API: {}\",\n            base_url\n        );\n    }\n\n    #[test]\n    fn test_embedding_url_proxy_mode() {\n        // WHY: Proxy mode should use /v1/embeddings (OpenAI compatible)\n        use std::time::Duration;\n\n        let client =\n            VsCodeCopilotClient::with_base_url(\"http://localhost:1337\", Duration::from_secs(30))\n                .expect(\"Failed to create proxy client\");\n\n        // Proxy mode when using with_base_url\n        assert!(!client.direct_mode, \"Should be proxy mode\");\n\n        // The create_embeddings method appends /v1/embeddings in proxy mode\n        assert_eq!(\n            client.base_url, \"http://localhost:1337\",\n            \"Proxy base URL should be preserved\"\n        );\n    }\n\n    #[test]\n    fn test_embedding_single_input_format() {\n        // WHY: Single string input should serialize as JSON string, not array\n        let input = EmbeddingInput::Single(\"Hello, world!\".to_string());\n        let request = EmbeddingRequest::new(input, \"text-embedding-3-small\");\n\n        let json = serde_json::to_value(&request).expect(\"Failed to serialize\");\n\n        assert_eq!(\n            json[\"input\"],\n            serde_json::json!(\"Hello, world!\"),\n            \"Single input should serialize as string\"\n        );\n        assert_eq!(json[\"model\"], \"text-embedding-3-small\");\n    }\n\n    #[test]\n    fn test_embedding_multiple_inputs_format() {\n        // WHY: Multiple inputs should serialize as JSON array\n        let inputs = vec![\n            \"First\".to_string(),\n            \"Second\".to_string(),\n            \"Third\".to_string(),\n        ];\n        let input = EmbeddingInput::Multiple(inputs);\n        let request = EmbeddingRequest::new(input, \"text-embedding-3-small\");\n\n        let json = serde_json::to_value(&request).expect(\"Failed to serialize\");\n\n        assert_eq!(\n            json[\"input\"],\n            serde_json::json!([\"First\", \"Second\", \"Third\"]),\n            \"Multiple inputs should serialize as array\"\n        );\n    }\n\n    #[test]\n    fn test_embedding_model_in_request() {\n        // WHY: Model name must be correctly included in request JSON\n        let request = EmbeddingRequest::new(\"test\", \"text-embedding-ada-002\");\n\n        let json = serde_json::to_value(&request).expect(\"Failed to serialize\");\n\n        assert_eq!(\n            json[\"model\"], \"text-embedding-ada-002\",\n            \"Model name should be in request\"\n        );\n\n        // Also test text-embedding-3-large\n        let request2 = EmbeddingRequest::new(\"test\", \"text-embedding-3-large\");\n        let json2 = serde_json::to_value(&request2).expect(\"Failed to serialize\");\n\n        assert_eq!(json2[\"model\"], \"text-embedding-3-large\");\n    }\n\n    // =========================================================================\n    // List Models Client Tests\n    // WHY: Verify models endpoint URL construction for both modes\n    // =========================================================================\n\n    #[test]\n    fn test_list_models_url_direct_mode() {\n        // WHY: Direct mode should use /models (not /v1/models)\n        // This matches the TypeScript proxy behavior for GitHub Copilot API\n        use std::time::Duration;\n\n        let client =\n            VsCodeCopilotClient::new(Duration::from_secs(30)).expect(\"Failed to create client\");\n\n        // Direct mode is the default\n        assert!(client.direct_mode, \"Default should be direct mode\");\n\n        // In direct mode, list_models appends /models to base_url\n        // We verify the base URL setup is correct for this\n        let base_url = &client.base_url;\n        assert!(\n            !base_url.ends_with(\"/v1\"),\n            \"Direct mode base URL should not end with /v1\"\n        );\n\n        // Verify it's a GitHub API URL\n        assert!(\n            base_url.starts_with(\"https://api\"),\n            \"Direct mode should use GitHub API: {}\",\n            base_url\n        );\n    }\n\n    #[test]\n    fn test_list_models_url_proxy_mode() {\n        // WHY: Proxy mode should use /v1/models (OpenAI compatible)\n        use std::time::Duration;\n\n        let client =\n            VsCodeCopilotClient::with_base_url(\"http://localhost:1337\", Duration::from_secs(30))\n                .expect(\"Failed to create proxy client\");\n\n        // Proxy mode when using with_base_url\n        assert!(!client.direct_mode, \"Should be proxy mode\");\n\n        // The list_models method appends /v1/models in proxy mode\n        assert_eq!(\n            client.base_url, \"http://localhost:1337\",\n            \"Proxy base URL should be preserved\"\n        );\n    }\n\n    // =========================================================================\n    // Timeout Configuration Tests\n    // WHY: Verify timeout configuration is accepted and client is created\n    // =========================================================================\n\n    #[test]\n    fn test_client_timeout_short() {\n        // WHY: Short timeout (5s) for quick failure detection\n        use std::time::Duration;\n\n        let timeout = Duration::from_secs(5);\n        let client = VsCodeCopilotClient::new(timeout);\n\n        assert!(client.is_ok(), \"Client should accept short timeout\");\n    }\n\n    #[test]\n    fn test_client_timeout_long() {\n        // WHY: Long timeout (300s) for slow LLM responses\n        use std::time::Duration;\n\n        let timeout = Duration::from_secs(300);\n        let client = VsCodeCopilotClient::new(timeout);\n\n        assert!(client.is_ok(), \"Client should accept long timeout\");\n    }\n\n    // =========================================================================\n    // Chat URL Construction Tests (Iteration 33)\n    // WHY: Chat completion endpoint URLs must match Copilot API expectations.\n    // Different account types use different base URLs.\n    // =========================================================================\n\n    #[test]\n    fn test_chat_url_direct_mode() {\n        // WHY: Individual accounts use api.githubcopilot.com\n        // This is the default endpoint for most users\n        let base = AccountType::Individual.base_url();\n        let url = format!(\"{}/chat/completions\", base);\n\n        assert_eq!(\n            url, \"https://api.githubcopilot.com/chat/completions\",\n            \"Individual account chat URL should use main Copilot API\"\n        );\n    }\n\n    #[test]\n    fn test_chat_url_business_mode() {\n        // WHY: Business accounts have a separate endpoint for\n        // compliance and billing separation\n        let base = AccountType::Business.base_url();\n        let url = format!(\"{}/chat/completions\", base);\n\n        assert_eq!(\n            url, \"https://api.business.githubcopilot.com/chat/completions\",\n            \"Business account chat URL should use business subdomain\"\n        );\n    }\n\n    #[test]\n    fn test_chat_url_enterprise_mode() {\n        // WHY: Enterprise accounts have their own endpoint for\n        // dedicated infrastructure and data isolation\n        let base = AccountType::Enterprise.base_url();\n        let url = format!(\"{}/chat/completions\", base);\n\n        assert_eq!(\n            url, \"https://api.enterprise.githubcopilot.com/chat/completions\",\n            \"Enterprise account chat URL should use enterprise subdomain\"\n        );\n    }\n\n    #[test]\n    fn test_chat_url_proxy_mode() {\n        // WHY: Proxy mode uses the configured proxy URL\n        // Used when TypeScript proxy handles auth and headers\n        let proxy_url = \"http://localhost:1337\";\n        let url = format!(\"{}/chat/completions\", proxy_url);\n\n        assert_eq!(\n            url, \"http://localhost:1337/chat/completions\",\n            \"Proxy mode chat URL should use configured proxy base\"\n        );\n    }\n\n    // =========================================================================\n    // OODA-07.2: Choice Normalization Tests\n    // WHY: Anthropic models return split choices that need merging\n    // =========================================================================\n\n    #[test]\n    fn test_normalize_choices_single_choice() {\n        // WHY: Single choice should pass through unchanged\n        use crate::providers::vscode::types::*;\n\n        let response = ChatCompletionResponse {\n            id: \"test1\".to_string(),\n            object: None,\n            created: None,\n            model: \"gpt-4.1\".to_string(),\n            choices: vec![Choice {\n                index: Some(0),\n                message: ResponseMessage {\n                    role: \"assistant\".to_string(),\n                    content: Some(\"Hello\".to_string()),\n                    tool_calls: None,\n                    extra: None,\n                },\n                finish_reason: Some(\"stop\".to_string()),\n                extra: None,\n            }],\n            usage: None,\n            extra: None,\n        };\n\n        let normalized = VsCodeCopilotClient::normalize_choices(response.clone());\n\n        assert_eq!(normalized.choices.len(), 1);\n        assert_eq!(normalized.choices[0].index, Some(0));\n        assert_eq!(\n            normalized.choices[0].message.content,\n            Some(\"Hello\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_normalize_choices_anthropic_split() {\n        // WHY: Anthropic returns TWO choices - one with content, one with tool_calls\n        // These must be merged into a single choice\n        use crate::providers::vscode::types::*;\n\n        let response = ChatCompletionResponse {\n            id: \"msg_haiku\".to_string(),\n            object: None,\n            created: Some(1768984171),\n            model: \"claude-haiku-4.5\".to_string(),\n            choices: vec![\n                Choice {\n                    index: None,\n                    message: ResponseMessage {\n                        role: \"assistant\".to_string(),\n                        content: Some(\"I'll examine the file\".to_string()),\n                        tool_calls: None,\n                        extra: None,\n                    },\n                    finish_reason: Some(\"tool_calls\".to_string()),\n                    extra: None,\n                },\n                Choice {\n                    index: None,\n                    message: ResponseMessage {\n                        role: \"assistant\".to_string(),\n                        content: None,\n                        tool_calls: Some(vec![ResponseToolCall {\n                            id: \"toolu_123\".to_string(),\n                            function: ResponseFunctionCall {\n                                name: \"read_file\".to_string(),\n                                arguments: \"{\\\"path\\\":\\\"test.js\\\"}\".to_string(),\n                            },\n                            call_type: \"function\".to_string(),\n                        }]),\n                        extra: None,\n                    },\n                    finish_reason: Some(\"tool_calls\".to_string()),\n                    extra: None,\n                },\n            ],\n            usage: Some(Usage {\n                prompt_tokens: 100,\n                completion_tokens: 50,\n                total_tokens: 150,\n                prompt_tokens_details: None,\n                extra: None,\n            }),\n            extra: None,\n        };\n\n        let normalized = VsCodeCopilotClient::normalize_choices(response);\n\n        // Should merge into single choice\n        assert_eq!(normalized.choices.len(), 1);\n\n        let choice = &normalized.choices[0];\n        assert_eq!(choice.index, Some(0));\n\n        // Should have both content and tool_calls merged\n        assert_eq!(\n            choice.message.content,\n            Some(\"I'll examine the file\".to_string())\n        );\n        assert!(choice.message.tool_calls.is_some());\n        assert_eq!(choice.message.tool_calls.as_ref().unwrap().len(), 1);\n        assert_eq!(\n            choice.message.tool_calls.as_ref().unwrap()[0].function.name,\n            \"read_file\"\n        );\n    }\n\n    #[test]\n    fn test_normalize_choices_no_merge_with_indices() {\n        // WHY: Choices with different indices should NOT be merged\n        use crate::providers::vscode::types::*;\n\n        let response = ChatCompletionResponse {\n            id: \"test_multiple\".to_string(),\n            object: None,\n            created: None,\n            model: \"gpt-4.1\".to_string(),\n            choices: vec![\n                Choice {\n                    index: Some(0),\n                    message: ResponseMessage {\n                        role: \"assistant\".to_string(),\n                        content: Some(\"First\".to_string()),\n                        tool_calls: None,\n                        extra: None,\n                    },\n                    finish_reason: Some(\"stop\".to_string()),\n                    extra: None,\n                },\n                Choice {\n                    index: Some(1),\n                    message: ResponseMessage {\n                        role: \"assistant\".to_string(),\n                        content: Some(\"Second\".to_string()),\n                        tool_calls: None,\n                        extra: None,\n                    },\n                    finish_reason: Some(\"stop\".to_string()),\n                    extra: None,\n                },\n            ],\n            usage: None,\n            extra: None,\n        };\n\n        let normalized = VsCodeCopilotClient::normalize_choices(response.clone());\n\n        // Should NOT merge - keep both choices\n        assert_eq!(normalized.choices.len(), 2);\n        assert_eq!(normalized.choices[0].index, Some(0));\n        assert_eq!(normalized.choices[1].index, Some(1));\n    }\n}\n","traces":[{"line":106,"address":[],"length":0,"stats":{"Line":37}},{"line":107,"address":[],"length":0,"stats":{"Line":37}},{"line":108,"address":[],"length":0,"stats":{"Line":29}},{"line":109,"address":[],"length":0,"stats":{"Line":4}},{"line":110,"address":[],"length":0,"stats":{"Line":4}},{"line":116,"address":[],"length":0,"stats":{"Line":11}},{"line":117,"address":[],"length":0,"stats":{"Line":11}},{"line":118,"address":[],"length":0,"stats":{"Line":14}},{"line":119,"address":[],"length":0,"stats":{"Line":10}},{"line":120,"address":[],"length":0,"stats":{"Line":8}},{"line":121,"address":[],"length":0,"stats":{"Line":4}},{"line":147,"address":[],"length":0,"stats":{"Line":7}},{"line":148,"address":[],"length":0,"stats":{"Line":21}},{"line":154,"address":[],"length":0,"stats":{"Line":4}},{"line":155,"address":[],"length":0,"stats":{"Line":12}},{"line":156,"address":[],"length":0,"stats":{"Line":8}},{"line":158,"address":[],"length":0,"stats":{"Line":8}},{"line":159,"address":[],"length":0,"stats":{"Line":8}},{"line":161,"address":[],"length":0,"stats":{"Line":8}},{"line":163,"address":[],"length":0,"stats":{"Line":4}},{"line":165,"address":[],"length":0,"stats":{"Line":4}},{"line":166,"address":[],"length":0,"stats":{"Line":8}},{"line":168,"address":[],"length":0,"stats":{"Line":4}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":175,"address":[],"length":0,"stats":{"Line":4}},{"line":176,"address":[],"length":0,"stats":{"Line":8}},{"line":177,"address":[],"length":0,"stats":{"Line":8}},{"line":178,"address":[],"length":0,"stats":{"Line":8}},{"line":179,"address":[],"length":0,"stats":{"Line":4}},{"line":180,"address":[],"length":0,"stats":{"Line":4}},{"line":181,"address":[],"length":0,"stats":{"Line":4}},{"line":192,"address":[],"length":0,"stats":{"Line":28}},{"line":197,"address":[],"length":0,"stats":{"Line":56}},{"line":198,"address":[],"length":0,"stats":{"Line":56}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":201,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":56}},{"line":205,"address":[],"length":0,"stats":{"Line":56}},{"line":207,"address":[],"length":0,"stats":{"Line":56}},{"line":209,"address":[],"length":0,"stats":{"Line":28}},{"line":211,"address":[],"length":0,"stats":{"Line":28}},{"line":212,"address":[],"length":0,"stats":{"Line":56}},{"line":214,"address":[],"length":0,"stats":{"Line":28}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":28}},{"line":223,"address":[],"length":0,"stats":{"Line":56}},{"line":224,"address":[],"length":0,"stats":{"Line":56}},{"line":225,"address":[],"length":0,"stats":{"Line":56}},{"line":226,"address":[],"length":0,"stats":{"Line":28}},{"line":227,"address":[],"length":0,"stats":{"Line":28}},{"line":228,"address":[],"length":0,"stats":{"Line":28}},{"line":233,"address":[],"length":0,"stats":{"Line":27}},{"line":234,"address":[],"length":0,"stats":{"Line":27}},{"line":235,"address":[],"length":0,"stats":{"Line":27}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":256,"address":[],"length":0,"stats":{"Line":0}},{"line":257,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":263,"address":[],"length":0,"stats":{"Line":0}},{"line":264,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":281,"address":[],"length":0,"stats":{"Line":0}},{"line":284,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":290,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":301,"address":[],"length":0,"stats":{"Line":0}},{"line":313,"address":[],"length":0,"stats":{"Line":0}},{"line":322,"address":[],"length":0,"stats":{"Line":0}},{"line":324,"address":[],"length":0,"stats":{"Line":0}},{"line":325,"address":[],"length":0,"stats":{"Line":0}},{"line":326,"address":[],"length":0,"stats":{"Line":0}},{"line":327,"address":[],"length":0,"stats":{"Line":0}},{"line":329,"address":[],"length":0,"stats":{"Line":0}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":334,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":0}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":338,"address":[],"length":0,"stats":{"Line":0}},{"line":339,"address":[],"length":0,"stats":{"Line":0}},{"line":340,"address":[],"length":0,"stats":{"Line":0}},{"line":341,"address":[],"length":0,"stats":{"Line":0}},{"line":342,"address":[],"length":0,"stats":{"Line":0}},{"line":345,"address":[],"length":0,"stats":{"Line":0}},{"line":346,"address":[],"length":0,"stats":{"Line":0}},{"line":351,"address":[],"length":0,"stats":{"Line":0}},{"line":380,"address":[],"length":0,"stats":{"Line":8}},{"line":381,"address":[],"length":0,"stats":{"Line":8}},{"line":383,"address":[],"length":0,"stats":{"Line":44}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":397,"address":[],"length":0,"stats":{"Line":0}},{"line":398,"address":[],"length":0,"stats":{"Line":0}},{"line":400,"address":[],"length":0,"stats":{"Line":0}},{"line":402,"address":[],"length":0,"stats":{"Line":0}},{"line":405,"address":[],"length":0,"stats":{"Line":0}},{"line":406,"address":[],"length":0,"stats":{"Line":0}},{"line":407,"address":[],"length":0,"stats":{"Line":0}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":411,"address":[],"length":0,"stats":{"Line":0}},{"line":414,"address":[],"length":0,"stats":{"Line":0}},{"line":417,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":422,"address":[],"length":0,"stats":{"Line":0}},{"line":423,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[],"length":0,"stats":{"Line":0}},{"line":425,"address":[],"length":0,"stats":{"Line":0}},{"line":426,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":428,"address":[],"length":0,"stats":{"Line":0}},{"line":429,"address":[],"length":0,"stats":{"Line":0}},{"line":431,"address":[],"length":0,"stats":{"Line":0}},{"line":434,"address":[],"length":0,"stats":{"Line":0}},{"line":436,"address":[],"length":0,"stats":{"Line":0}},{"line":443,"address":[],"length":0,"stats":{"Line":0}},{"line":445,"address":[],"length":0,"stats":{"Line":0}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":450,"address":[],"length":0,"stats":{"Line":0}},{"line":453,"address":[],"length":0,"stats":{"Line":0}},{"line":454,"address":[],"length":0,"stats":{"Line":0}},{"line":455,"address":[],"length":0,"stats":{"Line":0}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":459,"address":[],"length":0,"stats":{"Line":0}},{"line":462,"address":[],"length":0,"stats":{"Line":0}},{"line":465,"address":[],"length":0,"stats":{"Line":0}},{"line":466,"address":[],"length":0,"stats":{"Line":0}},{"line":469,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":471,"address":[],"length":0,"stats":{"Line":0}},{"line":472,"address":[],"length":0,"stats":{"Line":0}},{"line":473,"address":[],"length":0,"stats":{"Line":0}},{"line":475,"address":[],"length":0,"stats":{"Line":0}},{"line":476,"address":[],"length":0,"stats":{"Line":0}},{"line":478,"address":[],"length":0,"stats":{"Line":0}},{"line":479,"address":[],"length":0,"stats":{"Line":0}},{"line":481,"address":[],"length":0,"stats":{"Line":0}},{"line":482,"address":[],"length":0,"stats":{"Line":0}},{"line":484,"address":[],"length":0,"stats":{"Line":0}},{"line":485,"address":[],"length":0,"stats":{"Line":0}},{"line":487,"address":[],"length":0,"stats":{"Line":0}},{"line":490,"address":[],"length":0,"stats":{"Line":0}},{"line":493,"address":[],"length":0,"stats":{"Line":0}},{"line":501,"address":[],"length":0,"stats":{"Line":0}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[],"length":0,"stats":{"Line":0}},{"line":506,"address":[],"length":0,"stats":{"Line":0}},{"line":508,"address":[],"length":0,"stats":{"Line":0}},{"line":510,"address":[],"length":0,"stats":{"Line":0}},{"line":513,"address":[],"length":0,"stats":{"Line":0}},{"line":516,"address":[],"length":0,"stats":{"Line":0}},{"line":517,"address":[],"length":0,"stats":{"Line":0}},{"line":518,"address":[],"length":0,"stats":{"Line":0}},{"line":519,"address":[],"length":0,"stats":{"Line":0}},{"line":521,"address":[],"length":0,"stats":{"Line":0}},{"line":522,"address":[],"length":0,"stats":{"Line":0}},{"line":524,"address":[],"length":0,"stats":{"Line":0}},{"line":544,"address":[],"length":0,"stats":{"Line":0}},{"line":546,"address":[],"length":0,"stats":{"Line":0}},{"line":547,"address":[],"length":0,"stats":{"Line":0}},{"line":549,"address":[],"length":0,"stats":{"Line":0}},{"line":551,"address":[],"length":0,"stats":{"Line":0}},{"line":553,"address":[],"length":0,"stats":{"Line":0}},{"line":557,"address":[],"length":0,"stats":{"Line":0}},{"line":560,"address":[],"length":0,"stats":{"Line":0}},{"line":561,"address":[],"length":0,"stats":{"Line":0}},{"line":562,"address":[],"length":0,"stats":{"Line":0}},{"line":563,"address":[],"length":0,"stats":{"Line":0}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":566,"address":[],"length":0,"stats":{"Line":0}},{"line":567,"address":[],"length":0,"stats":{"Line":0}},{"line":569,"address":[],"length":0,"stats":{"Line":0}},{"line":621,"address":[],"length":0,"stats":{"Line":3}},{"line":623,"address":[],"length":0,"stats":{"Line":3}},{"line":624,"address":[],"length":0,"stats":{"Line":1}},{"line":628,"address":[],"length":0,"stats":{"Line":4}},{"line":629,"address":[],"length":0,"stats":{"Line":2}},{"line":631,"address":[],"length":0,"stats":{"Line":12}},{"line":633,"address":[],"length":0,"stats":{"Line":2}},{"line":634,"address":[],"length":0,"stats":{"Line":1}},{"line":637,"address":[],"length":0,"stats":{"Line":1}},{"line":638,"address":[],"length":0,"stats":{"Line":0}},{"line":640,"address":[],"length":0,"stats":{"Line":0}},{"line":644,"address":[],"length":0,"stats":{"Line":3}},{"line":647,"address":[],"length":0,"stats":{"Line":4}},{"line":650,"address":[],"length":0,"stats":{"Line":3}},{"line":652,"address":[],"length":0,"stats":{"Line":2}},{"line":653,"address":[],"length":0,"stats":{"Line":1}},{"line":654,"address":[],"length":0,"stats":{"Line":1}},{"line":655,"address":[],"length":0,"stats":{"Line":1}},{"line":656,"address":[],"length":0,"stats":{"Line":1}},{"line":657,"address":[],"length":0,"stats":{"Line":3}},{"line":658,"address":[],"length":0,"stats":{"Line":1}},{"line":660,"address":[],"length":0,"stats":{"Line":0}},{"line":664,"address":[],"length":0,"stats":{"Line":3}},{"line":665,"address":[],"length":0,"stats":{"Line":1}},{"line":666,"address":[],"length":0,"stats":{"Line":1}},{"line":667,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[],"length":0,"stats":{"Line":0}},{"line":670,"address":[],"length":0,"stats":{"Line":0}},{"line":674,"address":[],"length":0,"stats":{"Line":2}},{"line":675,"address":[],"length":0,"stats":{"Line":0}},{"line":680,"address":[],"length":0,"stats":{"Line":1}},{"line":682,"address":[],"length":0,"stats":{"Line":3}},{"line":683,"address":[],"length":0,"stats":{"Line":1}},{"line":687,"address":[],"length":0,"stats":{"Line":0}},{"line":688,"address":[],"length":0,"stats":{"Line":0}},{"line":690,"address":[],"length":0,"stats":{"Line":0}},{"line":692,"address":[],"length":0,"stats":{"Line":0}},{"line":694,"address":[],"length":0,"stats":{"Line":0}},{"line":695,"address":[],"length":0,"stats":{"Line":0}},{"line":698,"address":[],"length":0,"stats":{"Line":0}},{"line":699,"address":[],"length":0,"stats":{"Line":0}},{"line":700,"address":[],"length":0,"stats":{"Line":0}},{"line":701,"address":[],"length":0,"stats":{"Line":0}},{"line":702,"address":[],"length":0,"stats":{"Line":0}},{"line":706,"address":[],"length":0,"stats":{"Line":0}},{"line":707,"address":[],"length":0,"stats":{"Line":0}},{"line":708,"address":[],"length":0,"stats":{"Line":0}},{"line":709,"address":[],"length":0,"stats":{"Line":0}},{"line":710,"address":[],"length":0,"stats":{"Line":0}},{"line":712,"address":[],"length":0,"stats":{"Line":0}},{"line":713,"address":[],"length":0,"stats":{"Line":0}},{"line":714,"address":[],"length":0,"stats":{"Line":0}},{"line":718,"address":[],"length":0,"stats":{"Line":0}},{"line":720,"address":[],"length":0,"stats":{"Line":0}},{"line":721,"address":[],"length":0,"stats":{"Line":0}},{"line":723,"address":[],"length":0,"stats":{"Line":0}},{"line":724,"address":[],"length":0,"stats":{"Line":0}},{"line":725,"address":[],"length":0,"stats":{"Line":0}},{"line":726,"address":[],"length":0,"stats":{"Line":0}},{"line":729,"address":[],"length":0,"stats":{"Line":0}},{"line":734,"address":[],"length":0,"stats":{"Line":10}},{"line":735,"address":[],"length":0,"stats":{"Line":10}},{"line":737,"address":[],"length":0,"stats":{"Line":2}},{"line":739,"address":[],"length":0,"stats":{"Line":1}},{"line":740,"address":[],"length":0,"stats":{"Line":1}},{"line":741,"address":[],"length":0,"stats":{"Line":1}},{"line":743,"address":[],"length":0,"stats":{"Line":1}},{"line":746,"address":[],"length":0,"stats":{"Line":2}},{"line":748,"address":[],"length":0,"stats":{"Line":2}}],"covered":88,"coverable":261},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","error.rs"],"content":"//! Error types for VSCode Copilot provider.\n//!\n//! # Design Rationale\n//!\n//! WHY: We use a dedicated `VsCodeError` type rather than the generic `LlmError` because:\n//! 1. VSCode/Copilot has specific error conditions (proxy unavailable, rate limiting)\n//! 2. Error messages can include actionable hints (e.g., \"Is copilot-api running?\")\n//! 3. Conversion to `LlmError` is automatic via `From` trait\n//!\n//! # Error Categories\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    VsCodeError Types                             â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                   â”‚\n//! â”‚  Initialization Errors                                           â”‚\n//! â”‚  â”œâ”€â”€ ClientInit      â†’ HTTP client TLS/config issues            â”‚\n//! â”‚  â””â”€â”€ ProxyUnavailable â†’ Proxy server not running                â”‚\n//! â”‚                                                                   â”‚\n//! â”‚  Runtime Errors                                                   â”‚\n//! â”‚  â”œâ”€â”€ Network         â†’ DNS, timeout, connection refused         â”‚\n//! â”‚  â”œâ”€â”€ Authentication  â†’ Invalid/expired token                    â”‚\n//! â”‚  â”œâ”€â”€ RateLimited     â†’ 429 Too Many Requests                    â”‚\n//! â”‚  â”œâ”€â”€ InvalidRequest  â†’ 400 Bad Request                          â”‚\n//! â”‚  â””â”€â”€ ServiceUnavailable â†’ 503 Service Unavailable               â”‚\n//! â”‚                                                                   â”‚\n//! â”‚  Response Errors                                                  â”‚\n//! â”‚  â”œâ”€â”€ ApiError        â†’ Generic API error (500, etc.)            â”‚\n//! â”‚  â”œâ”€â”€ Decode          â†’ JSON deserialization failed              â”‚\n//! â”‚  â””â”€â”€ Stream          â†’ SSE parsing error                        â”‚\n//! â”‚                                                                   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Error Recovery\n//!\n//! | Error | Retryable | Action |\n//! |-------|-----------|--------|\n//! | `RateLimited` | Yes | Wait with exponential backoff |\n//! | `Network` | Yes | Retry after delay |\n//! | `ServiceUnavailable` | Yes | Retry after delay |\n//! | `Authentication` | No | Re-authenticate |\n//! | `InvalidRequest` | No | Fix request parameters |\n//! | `ClientInit` | No | Fix configuration |\n\nuse thiserror::Error;\n\npub type Result<T> = std::result::Result<T, VsCodeError>;\n\n/// VSCode Copilot provider errors.\n#[derive(Error, Debug)]\npub enum VsCodeError {\n    /// Failed to initialize HTTP client.\n    #[error(\"Failed to initialize client: {0}\")]\n    ClientInit(String),\n\n    /// Proxy server is unavailable or not responding.\n    #[error(\"Proxy unavailable: {0}. Is copilot-api running on localhost:4141?\")]\n    ProxyUnavailable(String),\n\n    /// Network communication error.\n    #[error(\"Network error: {0}\")]\n    Network(String),\n\n    /// Authentication or authorization failed.\n    #[error(\"Authentication failed: {0}\")]\n    Authentication(String),\n\n    /// Rate limit exceeded.\n    #[error(\"Rate limited. Try again later.\")]\n    RateLimited,\n\n    /// Invalid request format or parameters.\n    #[error(\"Invalid request: {0}\")]\n    InvalidRequest(String),\n\n    /// Service temporarily unavailable.\n    #[error(\"Service unavailable\")]\n    ServiceUnavailable,\n\n    /// Generic API error.\n    #[error(\"API error: {0}\")]\n    ApiError(String),\n\n    /// Failed to decode response.\n    #[error(\"Failed to decode response: {0}\")]\n    Decode(String),\n\n    /// Streaming error.\n    #[error(\"Stream error: {0}\")]\n    Stream(String),\n}\n\nimpl VsCodeError {\n    /// Returns true if this error is retryable.\n    ///\n    /// # WHY\n    ///\n    /// Consumers of this API need to know which errors warrant retry attempts\n    /// versus which errors indicate permanent failures. This method encapsulates\n    /// that knowledge so callers don't need to match on error variants.\n    ///\n    /// # Retryable Errors\n    ///\n    /// - `Network`: Temporary connectivity issues (DNS, timeout, connection refused)\n    /// - `RateLimited`: 429 response - will succeed after backoff\n    /// - `ServiceUnavailable`: 503 response - server temporarily down\n    ///\n    /// # Non-Retryable Errors\n    ///\n    /// - `ClientInit`: Configuration issue - won't resolve with retry\n    /// - `ProxyUnavailable`: Proxy needs to be started\n    /// - `Authentication`: Token invalid - need new token\n    /// - `InvalidRequest`: Request parameters wrong - fix before retry\n    /// - `ApiError`: Permanent server-side failure\n    /// - `Decode`: Response format issue - server bug or version mismatch\n    /// - `Stream`: SSE parsing error - unlikely to resolve\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// use edgequake_llm::providers::vscode::VsCodeError;\n    ///\n    /// let err = VsCodeError::RateLimited;\n    /// if err.is_retryable() {\n    ///     // Apply exponential backoff and retry\n    /// }\n    /// ```\n    pub fn is_retryable(&self) -> bool {\n        matches!(\n            self,\n            VsCodeError::Network(_) | VsCodeError::RateLimited | VsCodeError::ServiceUnavailable\n        )\n    }\n}\n\n// Convert VsCodeError to LlmError\nimpl From<VsCodeError> for crate::error::LlmError {\n    fn from(err: VsCodeError) -> Self {\n        match err {\n            VsCodeError::ClientInit(msg) => Self::ConfigError(msg),\n            VsCodeError::ProxyUnavailable(msg) => Self::NetworkError(msg),\n            VsCodeError::Network(msg) => Self::NetworkError(msg),\n            VsCodeError::Authentication(msg) => Self::AuthError(msg),\n            VsCodeError::RateLimited => Self::RateLimited(\"Rate limit exceeded\".to_string()),\n            VsCodeError::InvalidRequest(msg) => Self::InvalidRequest(msg),\n            VsCodeError::ServiceUnavailable => {\n                Self::NetworkError(\"Service unavailable\".to_string())\n            }\n            VsCodeError::ApiError(msg) => Self::ApiError(msg),\n            VsCodeError::Decode(msg) => Self::ApiError(format!(\"Decode: {}\", msg)),\n            VsCodeError::Stream(msg) => Self::ApiError(format!(\"Stream: {}\", msg)),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use crate::error::LlmError;\n\n    // ========================================================================\n    // Display Trait Tests - Verify Error Messages\n    // ========================================================================\n\n    #[test]\n    fn test_vscode_error_display_client_init() {\n        let err = VsCodeError::ClientInit(\"TLS handshake failed\".to_string());\n        let msg = err.to_string();\n        assert!(msg.contains(\"Failed to initialize client\"));\n        assert!(msg.contains(\"TLS handshake failed\"));\n    }\n\n    #[test]\n    fn test_vscode_error_display_proxy_unavailable() {\n        let err = VsCodeError::ProxyUnavailable(\"connection refused\".to_string());\n        let msg = err.to_string();\n        assert!(msg.contains(\"Proxy unavailable\"));\n        assert!(msg.contains(\"connection refused\"));\n        assert!(msg.contains(\"localhost:4141\")); // Helpful hint\n    }\n\n    #[test]\n    fn test_vscode_error_display_network() {\n        let err = VsCodeError::Network(\"timeout after 30s\".to_string());\n        assert_eq!(err.to_string(), \"Network error: timeout after 30s\");\n    }\n\n    #[test]\n    fn test_vscode_error_display_authentication() {\n        let err = VsCodeError::Authentication(\"token expired\".to_string());\n        assert_eq!(err.to_string(), \"Authentication failed: token expired\");\n    }\n\n    #[test]\n    fn test_vscode_error_display_rate_limited() {\n        let err = VsCodeError::RateLimited;\n        assert_eq!(err.to_string(), \"Rate limited. Try again later.\");\n    }\n\n    #[test]\n    fn test_vscode_error_display_service_unavailable() {\n        let err = VsCodeError::ServiceUnavailable;\n        assert_eq!(err.to_string(), \"Service unavailable\");\n    }\n\n    // ========================================================================\n    // From<VsCodeError> for LlmError Conversion Tests\n    // ========================================================================\n\n    #[test]\n    fn test_conversion_client_init_to_config_error() {\n        let vscode_err = VsCodeError::ClientInit(\"init failed\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::ConfigError(msg) => assert_eq!(msg, \"init failed\"),\n            other => panic!(\"Expected ConfigError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_proxy_unavailable_to_network_error() {\n        let vscode_err = VsCodeError::ProxyUnavailable(\"refused\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::NetworkError(msg) => assert_eq!(msg, \"refused\"),\n            other => panic!(\"Expected NetworkError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_network_to_network_error() {\n        let vscode_err = VsCodeError::Network(\"dns lookup failed\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::NetworkError(msg) => assert_eq!(msg, \"dns lookup failed\"),\n            other => panic!(\"Expected NetworkError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_authentication_to_auth_error() {\n        let vscode_err = VsCodeError::Authentication(\"invalid token\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::AuthError(msg) => assert_eq!(msg, \"invalid token\"),\n            other => panic!(\"Expected AuthError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_rate_limited() {\n        let vscode_err = VsCodeError::RateLimited;\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::RateLimited(msg) => assert!(msg.contains(\"Rate limit\")),\n            other => panic!(\"Expected RateLimited, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_invalid_request() {\n        let vscode_err = VsCodeError::InvalidRequest(\"missing model\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::InvalidRequest(msg) => assert_eq!(msg, \"missing model\"),\n            other => panic!(\"Expected InvalidRequest, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_service_unavailable() {\n        let vscode_err = VsCodeError::ServiceUnavailable;\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::NetworkError(msg) => assert!(msg.contains(\"unavailable\")),\n            other => panic!(\"Expected NetworkError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_api_error() {\n        let vscode_err = VsCodeError::ApiError(\"internal server error\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::ApiError(msg) => assert_eq!(msg, \"internal server error\"),\n            other => panic!(\"Expected ApiError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_decode_error() {\n        let vscode_err = VsCodeError::Decode(\"invalid JSON\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::ApiError(msg) => {\n                assert!(msg.contains(\"Decode\"));\n                assert!(msg.contains(\"invalid JSON\"));\n            }\n            other => panic!(\"Expected ApiError, got {:?}\", other),\n        }\n    }\n\n    #[test]\n    fn test_conversion_stream_error() {\n        let vscode_err = VsCodeError::Stream(\"connection reset\".to_string());\n        let llm_err: LlmError = vscode_err.into();\n\n        match llm_err {\n            LlmError::ApiError(msg) => {\n                assert!(msg.contains(\"Stream\"));\n                assert!(msg.contains(\"connection reset\"));\n            }\n            other => panic!(\"Expected ApiError, got {:?}\", other),\n        }\n    }\n\n    // ========================================================================\n    // is_retryable() Tests\n    // WHY: Verify correct categorization of retryable vs non-retryable errors\n    // ========================================================================\n\n    #[test]\n    fn test_is_retryable_network_error() {\n        // WHY: Network errors are temporary and should be retried\n        let err = VsCodeError::Network(\"connection timeout\".to_string());\n        assert!(err.is_retryable(), \"Network errors should be retryable\");\n    }\n\n    #[test]\n    fn test_is_retryable_rate_limited() {\n        // WHY: 429 response means we should wait and retry\n        let err = VsCodeError::RateLimited;\n        assert!(\n            err.is_retryable(),\n            \"Rate limited errors should be retryable\"\n        );\n    }\n\n    #[test]\n    fn test_is_retryable_service_unavailable() {\n        // WHY: 503 means server is temporarily down\n        let err = VsCodeError::ServiceUnavailable;\n        assert!(\n            err.is_retryable(),\n            \"Service unavailable should be retryable\"\n        );\n    }\n\n    #[test]\n    fn test_is_not_retryable_auth_error() {\n        // WHY: Auth errors need new credentials, not retry\n        let err = VsCodeError::Authentication(\"token expired\".to_string());\n        assert!(!err.is_retryable(), \"Auth errors should not be retryable\");\n    }\n\n    #[test]\n    fn test_is_not_retryable_invalid_request() {\n        // WHY: Invalid request needs to be fixed, not retried\n        let err = VsCodeError::InvalidRequest(\"missing model\".to_string());\n        assert!(\n            !err.is_retryable(),\n            \"Invalid request should not be retryable\"\n        );\n    }\n\n    #[test]\n    fn test_is_not_retryable_client_init() {\n        // WHY: Client init errors are configuration issues\n        let err = VsCodeError::ClientInit(\"TLS failed\".to_string());\n        assert!(\n            !err.is_retryable(),\n            \"Client init errors should not be retryable\"\n        );\n    }\n\n    #[test]\n    fn test_is_not_retryable_api_error() {\n        // WHY: Generic API errors (500) are typically permanent\n        let err = VsCodeError::ApiError(\"internal error\".to_string());\n        assert!(!err.is_retryable(), \"API errors should not be retryable\");\n    }\n\n    #[test]\n    fn test_is_not_retryable_decode_error() {\n        // WHY: Decode errors indicate server response format issues\n        let err = VsCodeError::Decode(\"invalid JSON\".to_string());\n        assert!(!err.is_retryable(), \"Decode errors should not be retryable\");\n    }\n}\n","traces":[{"line":130,"address":[],"length":0,"stats":{"Line":8}},{"line":131,"address":[],"length":0,"stats":{"Line":5}},{"line":132,"address":[],"length":0,"stats":{"Line":8}},{"line":140,"address":[],"length":0,"stats":{"Line":10}},{"line":141,"address":[],"length":0,"stats":{"Line":10}},{"line":142,"address":[],"length":0,"stats":{"Line":2}},{"line":143,"address":[],"length":0,"stats":{"Line":2}},{"line":144,"address":[],"length":0,"stats":{"Line":2}},{"line":145,"address":[],"length":0,"stats":{"Line":2}},{"line":146,"address":[],"length":0,"stats":{"Line":1}},{"line":147,"address":[],"length":0,"stats":{"Line":2}},{"line":149,"address":[],"length":0,"stats":{"Line":1}},{"line":151,"address":[],"length":0,"stats":{"Line":2}},{"line":152,"address":[],"length":0,"stats":{"Line":2}},{"line":153,"address":[],"length":0,"stats":{"Line":2}}],"covered":15,"coverable":15},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","mod.rs"],"content":"//! VSCode Copilot LLM provider.\n//!\n//! This provider integrates with GitHub Copilot via the copilot-api proxy.\n//!\n//! # Phase 1: Proxy-Based Integration\n//!\n//! This implementation connects to a local copilot-api proxy server running\n//! on localhost:4141 (by default). The proxy handles GitHub authentication\n//! and token management.\n//!\n//! # Examples\n//!\n//! ```no_run\n//! use edgequake_llm::{VsCodeCopilotProvider, LLMProvider};\n//!\n//! # async fn example() -> Result<(), Box<dyn std::error::Error>> {\n//! let provider = VsCodeCopilotProvider::new()\n//!     .model(\"gpt-4o-mini\")\n//!     .build()?;\n//!\n//! let response = provider.complete(\"Hello, world!\").await?;\n//! println!(\"Response: {}\", response.content);\n//! # Ok(())\n//! # }\n//! ```\n//!\n//! # Setup\n//!\n//! 1. Install and authenticate with copilot-api:\n//!    ```bash\n//!    cd copilot-api\n//!    bun install\n//!    bun run auth\n//!    ```\n//!\n//! 2. Start the proxy server:\n//!    ```bash\n//!    bun run start\n//!    ```\n//!\n//! 3. Use the provider in your Rust code.\n//!\n//! # See Also\n//!\n//! - [copilot-api repository](https://github.com/ericc-ch/copilot-api)\n//! - [LLMProvider trait](../../traits/trait.LLMProvider.html)\n\npub mod auth;\nmod client;\nmod error;\nmod stream;\npub mod token;\npub mod types;\n\nuse async_trait::async_trait;\nuse futures::stream::{BoxStream, StreamExt};\nuse std::time::Duration;\nuse tracing::debug;\n\npub use client::{AccountType, VsCodeCopilotClient};\npub use error::{Result, VsCodeError};\npub use types::{Model, ModelsResponse};\n\nuse crate::error::Result as LlmResult;\nuse crate::traits::{\n    ChatMessage, ChatRole, CompletionOptions, EmbeddingProvider, FunctionCall, LLMProvider,\n    LLMResponse, StreamChunk, ToolCall, ToolChoice, ToolDefinition,\n};\nuse types::{\n    ChatCompletionRequest, ContentPart, EmbeddingInput, EmbeddingRequest, ImageUrlContent,\n    RequestContent, RequestFunction, RequestMessage, RequestTool, ResponseFormat,\n};\n\n/// VSCode Copilot LLM provider (proxy-based).\n///\n/// Connects to copilot-api proxy for GitHub Copilot access.\n#[derive(Clone)]\npub struct VsCodeCopilotProvider {\n    /// HTTP client for proxy communication.\n    client: VsCodeCopilotClient,\n\n    /// Model identifier (e.g., \"gpt-4o-mini\", \"gpt-4o\").\n    model: String,\n\n    /// Maximum context window size.\n    max_context_length: usize,\n\n    /// Whether vision mode is supported/enabled.\n    #[allow(dead_code)]\n    supports_vision: bool,\n\n    /// Embedding model to use.\n    embedding_model: String,\n\n    /// Embedding dimension for the selected model.\n    embedding_dimension: usize,\n}\n\nimpl VsCodeCopilotProvider {\n    /// Create a new provider builder with default settings.\n    #[allow(clippy::new_ret_no_self)]\n    pub fn new() -> VsCodeCopilotProviderBuilder {\n        VsCodeCopilotProviderBuilder::default()\n    }\n\n    /// Create a provider builder with custom proxy URL.\n    pub fn with_proxy(proxy_url: impl Into<String>) -> VsCodeCopilotProviderBuilder {\n        VsCodeCopilotProviderBuilder::new().proxy_url(proxy_url)\n    }\n\n    /// Get a reference to the HTTP client for advanced operations.\n    pub fn get_client(&self) -> &VsCodeCopilotClient {\n        &self.client\n    }\n\n    /// List available models from the Copilot API.\n    ///\n    /// # OODA-79: Dynamic Model Discovery\n    ///\n    /// Delegates to the underlying client to fetch available models.\n    /// Returns models that are available for the authenticated user.\n    pub async fn list_models(&self) -> Result<types::ModelsResponse> {\n        self.client.list_models().await\n    }\n\n    /// Convert internal messages to API format.\n    ///\n    /// # OODA-55: Multipart Image Support\n    ///\n    /// Handles messages with images using multipart content format:\n    /// ```text\n    /// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    /// â”‚ RequestMessage                              â”‚\n    /// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    /// â”‚ content: RequestContent                     â”‚\n    /// â”‚   â”œâ”€â”€ Text(String)        â† text-only      â”‚\n    /// â”‚   â””â”€â”€ Parts(Vec<ContentPart>)              â”‚\n    /// â”‚         â”œâ”€â”€ Text { text }                  â”‚\n    /// â”‚         â””â”€â”€ ImageUrl { image_url: {        â”‚\n    /// â”‚               url: \"data:image/png;...\"    â”‚\n    /// â”‚               detail: \"auto\"               â”‚\n    /// â”‚             }}                             â”‚\n    /// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    /// ```\n    fn convert_messages(messages: &[ChatMessage]) -> Vec<RequestMessage> {\n        messages\n            .iter()\n            .map(|msg| {\n                // Convert tool calls if present (for assistant messages)\n                let tool_calls = msg.tool_calls.as_ref().map(|calls| {\n                    calls\n                        .iter()\n                        .map(|tc| types::ResponseToolCall {\n                            id: tc.id.clone(),\n                            call_type: \"function\".to_string(),\n                            function: types::ResponseFunctionCall {\n                                name: tc.name().to_string(),\n                                arguments: tc.arguments().to_string(),\n                            },\n                        })\n                        .collect()\n                });\n\n                // Convert cache control if present\n                let cache_control =\n                    msg.cache_control\n                        .as_ref()\n                        .map(|cc| types::RequestCacheControl {\n                            cache_type: cc.cache_type.clone(),\n                        });\n\n                // OODA-55: Build content based on whether images are present\n                // User messages with images use multipart format, others use simple text\n                let content = if msg.content.is_empty() && tool_calls.is_some() {\n                    None // OpenAI API expects no content when there are only tool calls\n                } else if msg.has_images() {\n                    // Build multipart content with text + images\n                    let mut parts: Vec<ContentPart> = Vec::new();\n\n                    // Add text part first (if not empty)\n                    if !msg.content.is_empty() {\n                        parts.push(ContentPart::Text {\n                            text: msg.content.clone(),\n                        });\n                    }\n\n                    // Add image parts\n                    if let Some(images) = &msg.images {\n                        for img in images {\n                            // Build data URI: data:<mime_type>;base64,<data>\n                            let data_uri = format!(\n                                \"data:{};base64,{}\",\n                                img.mime_type, img.data\n                            );\n                            parts.push(ContentPart::ImageUrl {\n                                image_url: ImageUrlContent {\n                                    url: data_uri,\n                                    detail: img.detail.clone(),\n                                },\n                            });\n                        }\n                    }\n\n                    Some(RequestContent::Parts(parts))\n                } else {\n                    // Simple text content\n                    Some(RequestContent::Text(msg.content.clone()))\n                };\n\n                RequestMessage {\n                    role: match msg.role {\n                        ChatRole::System => \"system\".to_string(),\n                        ChatRole::User => \"user\".to_string(),\n                        ChatRole::Assistant => \"assistant\".to_string(),\n                        ChatRole::Tool => \"tool\".to_string(),\n                        ChatRole::Function => \"tool\".to_string(),\n                    },\n                    content,\n                    name: msg.name.clone(),\n                    tool_calls,\n                    tool_call_id: msg.tool_call_id.clone(),\n                    cache_control,\n                }\n            })\n            .collect()\n    }\n\n    /// Convert tool definitions to API format.\n    fn convert_tools(tools: &[ToolDefinition]) -> Vec<RequestTool> {\n        tools\n            .iter()\n            .map(|tool| RequestTool {\n                tool_type: \"function\".to_string(),\n                function: RequestFunction {\n                    name: tool.function.name.clone(),\n                    description: tool.function.description.clone(),\n                    parameters: tool.function.parameters.clone(),\n                    strict: tool.function.strict,\n                },\n            })\n            .collect()\n    }\n\n    /// Convert tool choice to API format.\n    fn convert_tool_choice(choice: Option<ToolChoice>) -> Option<serde_json::Value> {\n        choice.map(|c| match c {\n            ToolChoice::Auto(s) | ToolChoice::Required(s) => serde_json::Value::String(s),\n            ToolChoice::Function { function, .. } => {\n                serde_json::json!({\n                    \"type\": \"function\",\n                    \"function\": {\n                        \"name\": function.name\n                    }\n                })\n            }\n        })\n    }\n\n    /// Convert response tool calls to internal format.\n    fn convert_response_tool_calls(calls: Option<Vec<types::ResponseToolCall>>) -> Vec<ToolCall> {\n        calls\n            .unwrap_or_default()\n            .into_iter()\n            .map(|tc| ToolCall {\n                id: tc.id,\n                call_type: tc.call_type,\n                function: FunctionCall {\n                    name: tc.function.name,\n                    arguments: tc.function.arguments,\n                },\n            })\n            .collect()\n    }\n}\n\nimpl Default for VsCodeCopilotProvider {\n    fn default() -> Self {\n        Self::new()\n            .build()\n            .expect(\"Failed to build default VsCodeCopilotProvider\")\n    }\n}\n\n/// Builder for VsCodeCopilotProvider.\n///\n/// # Example\n///\n/// ```rust,no_run\n/// use edgequake_llm::VsCodeCopilotProvider;\n///\n/// // Direct mode (default - recommended)\n/// let provider = VsCodeCopilotProvider::new()\n///     .direct()  // Use direct API (default)\n///     .model(\"gpt-4o\")\n///     .build()?;\n///\n/// // Proxy mode (legacy)\n/// let provider = VsCodeCopilotProvider::new()\n///     .proxy_url(\"http://localhost:4141\")\n///     .model(\"gpt-4o-mini\")\n///     .build()?;\n/// # Ok::<(), Box<dyn std::error::Error>>(())\n/// ```\n#[derive(Clone)]\npub struct VsCodeCopilotProviderBuilder {\n    /// Base URL for the API (proxy URL or direct API URL).\n    base_url: Option<String>,\n    /// Model name.\n    model: String,\n    /// Maximum context length.\n    max_context_length: usize,\n    /// Whether vision is supported.\n    supports_vision: bool,\n    /// Request timeout.\n    timeout: Duration,\n    /// Whether to use direct API mode.\n    direct_mode: bool,\n    /// Account type for direct mode.\n    account_type: client::AccountType,\n    /// Embedding model to use.\n    embedding_model: String,\n    /// Embedding dimension.\n    embedding_dimension: usize,\n}\n\nimpl Default for VsCodeCopilotProviderBuilder {\n    fn default() -> Self {\n        // Check environment variable for direct mode preference\n        let direct_mode = std::env::var(\"VSCODE_COPILOT_DIRECT\")\n            .map(|v| v.to_lowercase() != \"false\" && v != \"0\")\n            .unwrap_or(true); // Default to direct mode\n\n        // Check environment for account type\n        let account_type = std::env::var(\"VSCODE_COPILOT_ACCOUNT_TYPE\")\n            .ok()\n            .and_then(|s| client::AccountType::from_str(&s))\n            .unwrap_or_default();\n\n        // Check environment for embedding model\n        let embedding_model = std::env::var(\"VSCODE_COPILOT_EMBEDDING_MODEL\")\n            .unwrap_or_else(|_| \"text-embedding-3-small\".to_string());\n\n        // Set dimension based on model\n        let embedding_dimension = Self::dimension_for_embedding_model(&embedding_model);\n\n        Self {\n            base_url: None,\n            model: \"gpt-4o-mini\".to_string(),\n            max_context_length: 128_000,\n            supports_vision: false,\n            timeout: Duration::from_secs(120),\n            direct_mode,\n            account_type,\n            embedding_model,\n            embedding_dimension,\n        }\n    }\n}\n\nimpl VsCodeCopilotProviderBuilder {\n    /// Create a new builder with default settings.\n    pub fn new() -> Self {\n        Self::default()\n    }\n\n    /// Set the proxy URL (enables proxy mode).\n    ///\n    /// This disables direct mode and connects through a local copilot-api proxy.\n    pub fn proxy_url(mut self, url: impl Into<String>) -> Self {\n        self.base_url = Some(url.into());\n        self.direct_mode = false;\n        self\n    }\n\n    /// Enable direct API mode (default).\n    ///\n    /// Connects directly to api.githubcopilot.com without a proxy.\n    pub fn direct(mut self) -> Self {\n        self.direct_mode = true;\n        self.base_url = None;\n        self\n    }\n\n    /// Set the account type for direct mode.\n    ///\n    /// - `Individual` - Personal GitHub Copilot subscription\n    /// - `Business` - GitHub Copilot Business\n    /// - `Enterprise` - GitHub Copilot Enterprise\n    pub fn account_type(mut self, account_type: client::AccountType) -> Self {\n        self.account_type = account_type;\n        self\n    }\n\n    /// Set the model to use.\n    pub fn model(mut self, model: impl Into<String>) -> Self {\n        let model_str = model.into();\n        self.max_context_length = Self::context_length_for_model(&model_str);\n\n        // Increase timeout for Grok models (known to be slower)\n        if model_str.contains(\"grok\") {\n            self.timeout = Duration::from_secs(300); // 5 minutes for Grok\n        }\n\n        self.model = model_str;\n        self\n    }\n\n    /// Set the embedding model to use.\n    ///\n    /// Supported models include:\n    /// - `text-embedding-3-small` (default, 1536 dimensions)\n    /// - `text-embedding-3-large` (3072 dimensions)\n    /// - `text-embedding-ada-002` (1536 dimensions)\n    pub fn embedding_model(mut self, model: impl Into<String>) -> Self {\n        let model_str = model.into();\n        self.embedding_dimension = Self::dimension_for_embedding_model(&model_str);\n        self.embedding_model = model_str;\n        self\n    }\n\n    /// Enable or disable vision support.\n    pub fn with_vision(mut self, enabled: bool) -> Self {\n        self.supports_vision = enabled;\n        self\n    }\n\n    /// Set the request timeout.\n    pub fn timeout(mut self, duration: Duration) -> Self {\n        self.timeout = duration;\n        self\n    }\n\n    /// Build the provider.\n    pub fn build(self) -> Result<VsCodeCopilotProvider> {\n        let client = if let Some(url) = &self.base_url {\n            // Proxy mode with custom URL\n            VsCodeCopilotClient::with_base_url(url, self.timeout)?\n        } else if self.direct_mode {\n            // Direct mode\n            VsCodeCopilotClient::new_with_options(self.timeout, true, self.account_type)?\n                .with_vision(self.supports_vision)\n        } else {\n            // Proxy mode with default URL\n            let proxy_url = std::env::var(\"VSCODE_COPILOT_PROXY_URL\")\n                .unwrap_or_else(|_| \"http://localhost:4141\".to_string());\n            VsCodeCopilotClient::with_base_url(&proxy_url, self.timeout)?\n        };\n\n        let mode_str = if self.direct_mode { \"direct\" } else { \"proxy\" };\n\n        debug!(\n            model = %self.model,\n            max_context = self.max_context_length,\n            mode = mode_str,\n            account_type = ?self.account_type,\n            embedding_model = %self.embedding_model,\n            \"Built VsCodeCopilotProvider\"\n        );\n\n        Ok(VsCodeCopilotProvider {\n            client,\n            model: self.model,\n            max_context_length: self.max_context_length,\n            supports_vision: self.supports_vision,\n            embedding_model: self.embedding_model,\n            embedding_dimension: self.embedding_dimension,\n        })\n    }\n\n    /// Get context length for a model.\n    fn context_length_for_model(model: &str) -> usize {\n        match model {\n            m if m.contains(\"grok\") => 131_072, // Grok models have 131K context window\n            m if m.contains(\"gpt-4o\") => 128_000,\n            m if m.contains(\"gpt-4-turbo\") => 128_000,\n            m if m.contains(\"gpt-4-32k\") => 32_768,\n            m if m.contains(\"gpt-4\") => 8_192,\n            m if m.contains(\"gpt-3.5-turbo-16k\") => 16_384,\n            m if m.contains(\"gpt-3.5\") => 4_096,\n            m if m.contains(\"o1\") || m.contains(\"o3\") => 200_000,\n            _ => 128_000, // Conservative default\n        }\n    }\n\n    /// Get embedding dimension for a model.\n    fn dimension_for_embedding_model(model: &str) -> usize {\n        match model {\n            m if m.contains(\"text-embedding-3-large\") => 3072,\n            m if m.contains(\"text-embedding-3-small\") => 1536,\n            m if m.contains(\"text-embedding-ada\") => 1536,\n            m if m.contains(\"copilot-text-embedding\") => 1536,\n            _ => 1536, // Conservative default\n        }\n    }\n}\n\n#[async_trait]\nimpl LLMProvider for VsCodeCopilotProvider {\n    fn name(&self) -> &str {\n        \"vscode-copilot\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.max_context_length\n    }\n\n    async fn complete(&self, prompt: &str) -> LlmResult<LLMResponse> {\n        self.complete_with_options(prompt, &CompletionOptions::default())\n            .await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> LlmResult<LLMResponse> {\n        let mut messages = Vec::new();\n\n        if let Some(system) = &options.system_prompt {\n            messages.push(ChatMessage::system(system));\n        }\n        messages.push(ChatMessage::user(prompt));\n\n        self.chat(&messages, Some(options)).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> LlmResult<LLMResponse> {\n        // Convert messages\n        let request_messages = Self::convert_messages(messages);\n\n        // Build request\n        let opts = options.cloned().unwrap_or_default();\n        let request = ChatCompletionRequest {\n            messages: request_messages,\n            model: self.model.clone(),\n            temperature: opts.temperature,\n            top_p: opts.top_p,\n            max_tokens: opts.max_tokens,\n            stop: opts.stop,\n            stream: Some(false),\n            frequency_penalty: opts.frequency_penalty,\n            presence_penalty: opts.presence_penalty,\n            response_format: opts\n                .response_format\n                .map(|fmt| ResponseFormat { format_type: fmt }),\n            tools: None,\n            tool_choice: None,\n            parallel_tool_calls: None,\n        };\n\n        debug!(\n            model = %self.model,\n            message_count = messages.len(),\n            \"Sending chat request\"\n        );\n\n        // Send request\n        let response = self.client.chat_completion(request).await?;\n\n        // Extract result\n        let choice = response\n            .choices\n            .first()\n            .ok_or_else(|| crate::error::LlmError::ApiError(\"No choices in response\".into()))?;\n\n        let content = choice.message.content.clone().unwrap_or_default();\n\n        let usage = response.usage.unwrap_or(types::Usage {\n            prompt_tokens: 0,\n            completion_tokens: 0,\n            total_tokens: 0,\n            prompt_tokens_details: None,\n            extra: None,\n        });\n\n        debug!(\n            prompt_tokens = usage.prompt_tokens,\n            completion_tokens = usage.completion_tokens,\n            \"Chat request completed\"\n        );\n\n        // Convert any tool calls in the response\n        let tool_calls = Self::convert_response_tool_calls(choice.message.tool_calls.clone());\n\n        // OODA-24: Extract cached_tokens for KV cache hit tracking\n        let cache_hit_tokens = usage\n            .prompt_tokens_details\n            .as_ref()\n            .and_then(|d| d.cached_tokens);\n\n        // OODA-13: Capture response ID for OpenTelemetry GenAI semantic conventions\n        let mut response_builder = LLMResponse::new(content, response.model.clone())\n            .with_usage(usage.prompt_tokens, usage.completion_tokens)\n            .with_finish_reason(choice.finish_reason.clone().unwrap_or_default())\n            .with_tool_calls(tool_calls)\n            .with_metadata(\"id\", serde_json::json!(response.id));\n\n        // Add cache hit tokens if available (OODA-24)\n        if let Some(cached) = cache_hit_tokens {\n            response_builder = response_builder.with_cache_hit_tokens(cached);\n        }\n\n        Ok(response_builder)\n    }\n\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> LlmResult<LLMResponse> {\n        // Convert messages\n        let request_messages = Self::convert_messages(messages);\n\n        // Convert tools\n        let request_tools = if tools.is_empty() {\n            None\n        } else {\n            Some(Self::convert_tools(tools))\n        };\n\n        // Convert tool choice\n        let request_tool_choice = Self::convert_tool_choice(tool_choice);\n\n        // Build request\n        let opts = options.cloned().unwrap_or_default();\n        let request = ChatCompletionRequest {\n            messages: request_messages,\n            model: self.model.clone(),\n            temperature: opts.temperature,\n            top_p: opts.top_p,\n            max_tokens: opts.max_tokens,\n            stop: opts.stop,\n            stream: Some(false),\n            frequency_penalty: opts.frequency_penalty,\n            presence_penalty: opts.presence_penalty,\n            response_format: opts\n                .response_format\n                .map(|fmt| ResponseFormat { format_type: fmt }),\n            tools: request_tools,\n            tool_choice: request_tool_choice,\n            parallel_tool_calls: Some(true),\n        };\n\n        debug!(\n            model = %self.model,\n            message_count = messages.len(),\n            tool_count = tools.len(),\n            \"Sending chat request with tools\"\n        );\n\n        // Send request\n        let response = self.client.chat_completion(request).await?;\n\n        // Extract result\n        let choice = response\n            .choices\n            .first()\n            .ok_or_else(|| crate::error::LlmError::ApiError(\"No choices in response\".into()))?;\n\n        let content = choice.message.content.clone().unwrap_or_default();\n        let tool_calls = Self::convert_response_tool_calls(choice.message.tool_calls.clone());\n\n        let usage = response.usage.unwrap_or(types::Usage {\n            prompt_tokens: 0,\n            completion_tokens: 0,\n            total_tokens: 0,\n            prompt_tokens_details: None,\n            extra: None,\n        });\n\n        debug!(\n            prompt_tokens = usage.prompt_tokens,\n            completion_tokens = usage.completion_tokens,\n            tool_call_count = tool_calls.len(),\n            \"Chat with tools request completed\"\n        );\n\n        // OODA-24: Extract cached_tokens for KV cache hit tracking\n        let cache_hit_tokens = usage\n            .prompt_tokens_details\n            .as_ref()\n            .and_then(|d| d.cached_tokens);\n\n        // OODA-13: Capture response ID for OpenTelemetry GenAI semantic conventions\n        let mut response_builder = LLMResponse::new(content, response.model.clone())\n            .with_usage(usage.prompt_tokens, usage.completion_tokens)\n            .with_finish_reason(choice.finish_reason.clone().unwrap_or_default())\n            .with_tool_calls(tool_calls)\n            .with_metadata(\"id\", serde_json::json!(response.id));\n\n        // Add cache hit tokens if available (OODA-24)\n        if let Some(cached) = cache_hit_tokens {\n            response_builder = response_builder.with_cache_hit_tokens(cached);\n        }\n\n        Ok(response_builder)\n    }\n\n    async fn stream(&self, prompt: &str) -> LlmResult<BoxStream<'static, LlmResult<String>>> {\n        let request_messages = vec![RequestMessage {\n            role: \"user\".to_string(),\n            content: Some(RequestContent::Text(prompt.to_string())),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n        }];\n\n        let request = ChatCompletionRequest {\n            messages: request_messages,\n            model: self.model.clone(),\n            stream: Some(true),\n            ..Default::default()\n        };\n\n        debug!(model = %self.model, \"Sending streaming request\");\n\n        let response = self.client.chat_completion_stream(request).await?;\n        let stream = stream::parse_sse_stream(response);\n\n        // Map VsCodeError to LlmError\n        let mapped = stream.map(|result| result.map_err(|e| e.into()));\n\n        Ok(Box::pin(mapped))\n    }\n\n    fn supports_streaming(&self) -> bool {\n        true\n    }\n\n    fn supports_json_mode(&self) -> bool {\n        true\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        true\n    }\n    \n    /// OODA-05: Enable streaming with tool calls for real-time token display.\n    fn supports_tool_streaming(&self) -> bool {\n        true\n    }\n    \n    /// Stream LLM response with tool calls (OODA-05).\n    ///\n    /// Returns a stream of `StreamChunk` events for real-time:\n    /// - Content display\n    /// - Tool call progress\n    /// - Token counting and rate display\n    ///\n    /// This enables the React agent to use `StreamingProgress` instead of\n    /// `SpinnerGuard`, providing `âš¡ N tokens (M t/s)` display.\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> LlmResult<BoxStream<'static, LlmResult<StreamChunk>>> {\n        // Convert messages\n        let request_messages = Self::convert_messages(messages);\n\n        // Convert tools\n        let request_tools = if tools.is_empty() {\n            None\n        } else {\n            Some(Self::convert_tools(tools))\n        };\n\n        // Convert tool choice\n        let request_tool_choice = Self::convert_tool_choice(tool_choice);\n\n        // Build request with streaming enabled\n        let opts = options.cloned().unwrap_or_default();\n        let request = ChatCompletionRequest {\n            messages: request_messages,\n            model: self.model.clone(),\n            temperature: opts.temperature,\n            top_p: opts.top_p,\n            max_tokens: opts.max_tokens,\n            stop: opts.stop,\n            stream: Some(true), // Enable streaming\n            frequency_penalty: opts.frequency_penalty,\n            presence_penalty: opts.presence_penalty,\n            response_format: opts\n                .response_format\n                .map(|fmt| ResponseFormat { format_type: fmt }),\n            tools: request_tools,\n            tool_choice: request_tool_choice,\n            parallel_tool_calls: Some(true),\n        };\n\n        debug!(\n            model = %self.model,\n            message_count = messages.len(),\n            tool_count = tools.len(),\n            \"Sending streaming chat request with tools (OODA-05)\"\n        );\n\n        // Send streaming request\n        let response = self.client.chat_completion_stream(request).await?;\n        \n        // Parse SSE stream with tool call support\n        let stream = stream::parse_sse_stream_with_tools(response);\n\n        // Map VsCodeError to LlmError\n        let mapped = stream.map(|result| result.map_err(|e| e.into()));\n\n        Ok(Box::pin(mapped))\n    }\n}\n\n#[async_trait]\nimpl EmbeddingProvider for VsCodeCopilotProvider {\n    fn name(&self) -> &str {\n        \"vscode-copilot\"\n    }\n\n    #[allow(clippy::misnamed_getters)]\n    fn model(&self) -> &str {\n        // Note: Returns embedding_model, not the chat model - this is intentional\n        // as per EmbeddingProvider trait contract\n        &self.embedding_model\n    }\n\n    fn dimension(&self) -> usize {\n        self.embedding_dimension\n    }\n\n    fn max_tokens(&self) -> usize {\n        8192 // OpenAI embedding models support up to 8192 tokens\n    }\n\n    async fn embed(&self, texts: &[String]) -> LlmResult<Vec<Vec<f32>>> {\n        let input = if texts.len() == 1 {\n            EmbeddingInput::Single(texts[0].clone())\n        } else {\n            EmbeddingInput::Multiple(texts.to_vec())\n        };\n\n        let request = EmbeddingRequest::new(input, &self.embedding_model);\n\n        debug!(\n            model = %self.embedding_model,\n            input_count = texts.len(),\n            \"Sending embedding request\"\n        );\n\n        let response = self.client.create_embeddings(request).await?;\n\n        debug!(\n            prompt_tokens = response.usage.prompt_tokens,\n            total_tokens = response.usage.total_tokens,\n            embedding_count = response.data.len(),\n            \"Embedding request completed\"\n        );\n\n        // Return embeddings in order\n        let embeddings: Vec<Vec<f32>> = response\n            .data\n            .into_iter()\n            .map(|e| (e.index, e.embedding))\n            .collect::<Vec<_>>()\n            .into_iter()\n            .map(|(_, e)| e)\n            .collect();\n\n        // Sort by index to maintain order (API should return in order, but be safe)\n        if embeddings.len() != texts.len() {\n            return Err(crate::error::LlmError::ApiError(format!(\n                \"Expected {} embeddings, got {}\",\n                texts.len(),\n                embeddings.len()\n            )));\n        }\n\n        Ok(embeddings)\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use types::{ResponseFunctionCall, ResponseToolCall};\n\n    // =========================================================================\n    // Tool Conversion Tests\n    // WHY: Tool calling is core to coding agent functionality\n    // =========================================================================\n\n    #[test]\n    fn test_convert_single_tool() {\n        // WHY: Verify basic tool definition conversion\n        let tools = vec![ToolDefinition::function(\n            \"read_file\",\n            \"Read contents of a file\",\n            serde_json::json!({\n                \"type\": \"object\",\n                \"properties\": {\n                    \"path\": {\"type\": \"string\"}\n                },\n                \"required\": [\"path\"]\n            }),\n        )];\n\n        let converted = VsCodeCopilotProvider::convert_tools(&tools);\n\n        assert_eq!(converted.len(), 1);\n        assert_eq!(converted[0].tool_type, \"function\");\n        assert_eq!(converted[0].function.name, \"read_file\");\n        assert_eq!(converted[0].function.description, \"Read contents of a file\");\n        assert!(converted[0].function.strict.is_some());\n    }\n\n    #[test]\n    fn test_convert_multiple_tools() {\n        // WHY: Agent uses multiple tools - order must be preserved\n        let tools = vec![\n            ToolDefinition::function(\"tool_a\", \"First tool\", serde_json::json!({})),\n            ToolDefinition::function(\"tool_b\", \"Second tool\", serde_json::json!({})),\n            ToolDefinition::function(\"tool_c\", \"Third tool\", serde_json::json!({})),\n        ];\n\n        let converted = VsCodeCopilotProvider::convert_tools(&tools);\n\n        assert_eq!(converted.len(), 3);\n        assert_eq!(converted[0].function.name, \"tool_a\");\n        assert_eq!(converted[1].function.name, \"tool_b\");\n        assert_eq!(converted[2].function.name, \"tool_c\");\n    }\n\n    #[test]\n    fn test_convert_tool_with_complex_parameters() {\n        // WHY: Real tools have nested parameter schemas\n        let params = serde_json::json!({\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n                \"options\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"regex\": {\"type\": \"boolean\"},\n                        \"case_sensitive\": {\"type\": \"boolean\"}\n                    }\n                }\n            },\n            \"required\": [\"query\"]\n        });\n\n        let tools = vec![ToolDefinition::function(\n            \"grep_search\",\n            \"Search codebase\",\n            params.clone(),\n        )];\n\n        let converted = VsCodeCopilotProvider::convert_tools(&tools);\n\n        assert_eq!(converted[0].function.parameters, params);\n    }\n\n    // =========================================================================\n    // Tool Choice Tests\n    // WHY: Tool choice controls model's tool usage behavior\n    // =========================================================================\n\n    #[test]\n    fn test_tool_choice_none() {\n        // WHY: None means let API use defaults\n        let result = VsCodeCopilotProvider::convert_tool_choice(None);\n        assert!(result.is_none());\n    }\n\n    #[test]\n    fn test_tool_choice_auto() {\n        // WHY: Auto lets model decide when to use tools\n        let choice = ToolChoice::auto();\n        let result = VsCodeCopilotProvider::convert_tool_choice(Some(choice));\n\n        assert_eq!(result, Some(serde_json::Value::String(\"auto\".to_string())));\n    }\n\n    #[test]\n    fn test_tool_choice_required() {\n        // WHY: Required forces model to use at least one tool\n        let choice = ToolChoice::required();\n        let result = VsCodeCopilotProvider::convert_tool_choice(Some(choice));\n\n        assert_eq!(\n            result,\n            Some(serde_json::Value::String(\"required\".to_string()))\n        );\n    }\n\n    #[test]\n    fn test_tool_choice_function() {\n        // WHY: Can force model to call specific function\n        let choice = ToolChoice::function(\"read_file\");\n        let result = VsCodeCopilotProvider::convert_tool_choice(Some(choice));\n\n        let expected = serde_json::json!({\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"read_file\"\n            }\n        });\n\n        assert_eq!(result, Some(expected));\n    }\n\n    #[test]\n    fn test_tool_choice_none_value() {\n        // WHY: \"none\" string disables tool calling entirely\n        let choice = ToolChoice::none();\n        let result = VsCodeCopilotProvider::convert_tool_choice(Some(choice));\n\n        assert_eq!(result, Some(serde_json::Value::String(\"none\".to_string())));\n    }\n\n    // =========================================================================\n    // Response Tool Call Conversion Tests\n    // WHY: Must correctly parse tool calls from API responses\n    // =========================================================================\n\n    #[test]\n    fn test_response_tool_calls_none() {\n        // WHY: Response may have no tool calls\n        let result = VsCodeCopilotProvider::convert_response_tool_calls(None);\n        assert!(result.is_empty());\n    }\n\n    #[test]\n    fn test_response_tool_calls_single() {\n        // WHY: Most common case - one tool call\n        let calls = vec![ResponseToolCall {\n            id: \"call_123\".to_string(),\n            call_type: \"function\".to_string(),\n            function: ResponseFunctionCall {\n                name: \"read_file\".to_string(),\n                arguments: r#\"{\"path\":\"src/main.rs\"}\"#.to_string(),\n            },\n        }];\n\n        let result = VsCodeCopilotProvider::convert_response_tool_calls(Some(calls));\n\n        assert_eq!(result.len(), 1);\n        assert_eq!(result[0].id, \"call_123\");\n        assert_eq!(result[0].call_type, \"function\");\n        assert_eq!(result[0].function.name, \"read_file\");\n        assert_eq!(result[0].function.arguments, r#\"{\"path\":\"src/main.rs\"}\"#);\n    }\n\n    #[test]\n    fn test_response_tool_calls_multiple() {\n        // WHY: Model can request multiple tool calls in parallel\n        let calls = vec![\n            ResponseToolCall {\n                id: \"call_1\".to_string(),\n                call_type: \"function\".to_string(),\n                function: ResponseFunctionCall {\n                    name: \"read_file\".to_string(),\n                    arguments: \"{}\".to_string(),\n                },\n            },\n            ResponseToolCall {\n                id: \"call_2\".to_string(),\n                call_type: \"function\".to_string(),\n                function: ResponseFunctionCall {\n                    name: \"search_code\".to_string(),\n                    arguments: \"{}\".to_string(),\n                },\n            },\n        ];\n\n        let result = VsCodeCopilotProvider::convert_response_tool_calls(Some(calls));\n\n        assert_eq!(result.len(), 2);\n        assert_eq!(result[0].id, \"call_1\");\n        assert_eq!(result[1].id, \"call_2\");\n    }\n\n    // =========================================================================\n    // Message Conversion with Tool Calls Tests\n    // WHY: Assistant messages can include tool calls\n    // =========================================================================\n\n    #[test]\n    fn test_message_with_tool_calls() {\n        // WHY: Assistant can respond with tool calls\n        let mut msg = ChatMessage::assistant(\"I'll read that file for you.\");\n        msg.tool_calls = Some(vec![ToolCall {\n            id: \"call_abc\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"read_file\".to_string(),\n                arguments: r#\"{\"path\":\"Cargo.toml\"}\"#.to_string(),\n            },\n        }]);\n\n        let converted = VsCodeCopilotProvider::convert_messages(&[msg]);\n\n        assert_eq!(converted.len(), 1);\n        assert!(converted[0].tool_calls.is_some());\n\n        let tool_calls = converted[0].tool_calls.as_ref().unwrap();\n        assert_eq!(tool_calls.len(), 1);\n        assert_eq!(tool_calls[0].id, \"call_abc\");\n        assert_eq!(tool_calls[0].function.name, \"read_file\");\n    }\n\n    #[test]\n    fn test_tool_message_conversion() {\n        // WHY: Tool results are sent as \"tool\" role messages\n        let msg = ChatMessage {\n            role: ChatRole::Tool,\n            content: \"File contents: ...\".to_string(),\n            name: Some(\"read_file\".to_string()),\n            tool_calls: None,\n            tool_call_id: Some(\"call_xyz\".to_string()),\n            cache_control: None,\n            images: None,\n        };\n\n        let converted = VsCodeCopilotProvider::convert_messages(&[msg]);\n\n        assert_eq!(converted.len(), 1);\n        assert_eq!(converted[0].role, \"tool\");\n        // OODA-55: content is now RequestContent::Text\n        assert_eq!(\n            converted[0].content,\n            Some(RequestContent::Text(\"File contents: ...\".to_string()))\n        );\n        assert_eq!(converted[0].tool_call_id, Some(\"call_xyz\".to_string()));\n    }\n\n    #[test]\n    fn test_assistant_message_with_only_tool_calls() {\n        // WHY: OpenAI API expects null content when only tool calls present\n        let mut msg = ChatMessage::assistant(\"\");\n        msg.tool_calls = Some(vec![ToolCall {\n            id: \"call_1\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"list_files\".to_string(),\n                arguments: \"{}\".to_string(),\n            },\n        }]);\n\n        let converted = VsCodeCopilotProvider::convert_messages(&[msg]);\n\n        // Content should be None (not empty string) when only tool calls\n        assert!(converted[0].content.is_none());\n        assert!(converted[0].tool_calls.is_some());\n    }\n\n    // =========================================================================\n    // OODA-55: Image Serialization Tests\n    // =========================================================================\n    // WHY: VS Code Copilot uses OpenAI-compatible multipart format for images.\n    // These tests verify the correct serialization of image data URIs.\n\n    #[test]\n    fn test_convert_messages_text_only() {\n        // WHY: Text-only messages should use simple RequestContent::Text\n        let messages = vec![ChatMessage::user(\"Hello, world!\")];\n        let converted = VsCodeCopilotProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 1);\n        assert_eq!(converted[0].role, \"user\");\n        match &converted[0].content {\n            Some(RequestContent::Text(text)) => {\n                assert_eq!(text, \"Hello, world!\");\n            }\n            _ => panic!(\"Expected RequestContent::Text\"),\n        }\n    }\n\n    #[test]\n    fn test_convert_messages_with_images() {\n        // WHY: Messages with images must use multipart format\n        use crate::traits::ImageData;\n\n        let msg = ChatMessage::user_with_images(\n            \"What's in this image?\",\n            vec![ImageData {\n                data: \"iVBORw0KGgo=\".to_string(),\n                mime_type: \"image/png\".to_string(),\n                detail: None,\n            }],\n        );\n\n        let converted = VsCodeCopilotProvider::convert_messages(&[msg]);\n\n        assert_eq!(converted.len(), 1);\n        match &converted[0].content {\n            Some(RequestContent::Parts(parts)) => {\n                assert_eq!(parts.len(), 2); // text + image\n\n                // Verify text part\n                match &parts[0] {\n                    ContentPart::Text { text } => {\n                        assert_eq!(text, \"What's in this image?\");\n                    }\n                    _ => panic!(\"First part should be text\"),\n                }\n\n                // Verify image part\n                match &parts[1] {\n                    ContentPart::ImageUrl { image_url } => {\n                        assert!(image_url.url.starts_with(\"data:image/png;base64,\"));\n                        assert!(image_url.url.contains(\"iVBORw0KGgo=\"));\n                    }\n                    _ => panic!(\"Second part should be image_url\"),\n                }\n            }\n            _ => panic!(\"Expected RequestContent::Parts for image message\"),\n        }\n    }\n\n    #[test]\n    fn test_convert_messages_with_image_detail() {\n        // WHY: Detail level must be preserved for vision API control\n        use crate::traits::ImageData;\n\n        let msg = ChatMessage::user_with_images(\n            \"Describe in detail\",\n            vec![ImageData {\n                data: \"base64data\".to_string(),\n                mime_type: \"image/jpeg\".to_string(),\n                detail: Some(\"high\".to_string()),\n            }],\n        );\n\n        let converted = VsCodeCopilotProvider::convert_messages(&[msg]);\n\n        match &converted[0].content {\n            Some(RequestContent::Parts(parts)) => {\n                assert_eq!(parts.len(), 2);\n\n                match &parts[1] {\n                    ContentPart::ImageUrl { image_url } => {\n                        assert_eq!(image_url.detail, Some(\"high\".to_string()));\n                    }\n                    _ => panic!(\"Expected ImageUrl part\"),\n                }\n            }\n            _ => panic!(\"Expected Parts content\"),\n        }\n    }\n\n    // =========================================================================\n    // Original Tests\n    // =========================================================================\n\n    #[test]\n    fn test_context_length_detection() {\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::context_length_for_model(\"gpt-4o\"),\n            128_000\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::context_length_for_model(\"gpt-4o-mini\"),\n            128_000\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::context_length_for_model(\"gpt-4\"),\n            8_192\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::context_length_for_model(\"gpt-3.5-turbo\"),\n            4_096\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::context_length_for_model(\"o1-preview\"),\n            200_000\n        );\n    }\n\n    #[test]\n    fn test_message_conversion() {\n        let messages = vec![\n            ChatMessage::system(\"You are helpful.\"),\n            ChatMessage::user(\"Hello!\"),\n            ChatMessage::assistant(\"Hi there!\"),\n        ];\n\n        let converted = VsCodeCopilotProvider::convert_messages(&messages);\n\n        assert_eq!(converted.len(), 3);\n        assert_eq!(converted[0].role, \"system\");\n        // OODA-55: content is now RequestContent::Text\n        assert_eq!(\n            converted[0].content,\n            Some(RequestContent::Text(\"You are helpful.\".to_string()))\n        );\n        assert_eq!(converted[1].role, \"user\");\n        assert_eq!(converted[2].role, \"assistant\");\n    }\n\n    #[test]\n    fn test_builder_defaults() {\n        // Set env to ensure consistent test behavior\n        std::env::set_var(\"VSCODE_COPILOT_DIRECT\", \"true\");\n        let builder = VsCodeCopilotProviderBuilder::default();\n        assert_eq!(builder.model, \"gpt-4o-mini\");\n        assert_eq!(builder.max_context_length, 128_000);\n        assert!(!builder.supports_vision);\n        assert!(builder.direct_mode); // Direct mode is now default\n        std::env::remove_var(\"VSCODE_COPILOT_DIRECT\");\n    }\n\n    #[test]\n    fn test_builder_proxy_mode() {\n        let provider = VsCodeCopilotProvider::new()\n            .proxy_url(\"http://localhost:8080\")\n            .model(\"gpt-4\")\n            .with_vision(true)\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.model, \"gpt-4\");\n        assert_eq!(provider.max_context_length, 8_192);\n        assert!(provider.supports_vision);\n    }\n\n    #[test]\n    fn test_builder_direct_mode() {\n        let provider = VsCodeCopilotProvider::new()\n            .direct()\n            .model(\"gpt-4o\")\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.model, \"gpt-4o\");\n        assert_eq!(provider.max_context_length, 128_000);\n    }\n\n    #[test]\n    fn test_account_type_base_url() {\n        assert_eq!(\n            client::AccountType::Individual.base_url(),\n            \"https://api.githubcopilot.com\"\n        );\n        assert_eq!(\n            client::AccountType::Business.base_url(),\n            \"https://api.business.githubcopilot.com\"\n        );\n        assert_eq!(\n            client::AccountType::Enterprise.base_url(),\n            \"https://api.enterprise.githubcopilot.com\"\n        );\n    }\n\n    #[test]\n    fn test_embedding_dimension_detection() {\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::dimension_for_embedding_model(\"text-embedding-3-small\"),\n            1536\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::dimension_for_embedding_model(\"text-embedding-3-large\"),\n            3072\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::dimension_for_embedding_model(\"text-embedding-ada-002\"),\n            1536\n        );\n        assert_eq!(\n            VsCodeCopilotProviderBuilder::dimension_for_embedding_model(\"unknown-model\"),\n            1536 // Default\n        );\n    }\n\n    #[test]\n    fn test_builder_embedding_model() {\n        let provider = VsCodeCopilotProvider::new()\n            .direct()\n            .embedding_model(\"text-embedding-3-large\")\n            .build()\n            .unwrap();\n\n        assert_eq!(provider.embedding_model, \"text-embedding-3-large\");\n        assert_eq!(provider.embedding_dimension, 3072);\n    }\n\n    // =========================================================================\n    // Vision Mode Tests\n    // =========================================================================\n    //\n    // WHY: Vision mode enables image processing via the `copilot-vision-request`\n    // header. The TypeScript proxy auto-detects image content in messages,\n    // but our Rust implementation uses explicit `with_vision(true)`.\n    //\n    // Future: Auto-detect vision based on message content type checking.\n    // See: copilot-api/src/services/copilot/create-chat-completions.ts:15-17\n\n    #[test]\n    fn test_builder_vision_disabled_by_default() {\n        // WHY: Vision should be opt-in to avoid header overhead\n        let builder = VsCodeCopilotProvider::new().direct();\n\n        // We can build and it should work\n        let provider = builder.build();\n        assert!(provider.is_ok());\n\n        // Vision is off by default (checked via internal state)\n        let provider = provider.unwrap();\n        assert!(!provider.supports_vision);\n    }\n\n    #[test]\n    fn test_builder_with_vision_true() {\n        // WHY: User explicitly enables vision for image-containing requests\n        let provider = VsCodeCopilotProvider::new()\n            .direct()\n            .with_vision(true)\n            .build()\n            .unwrap();\n\n        assert!(provider.supports_vision);\n    }\n\n    #[test]\n    fn test_builder_with_vision_false() {\n        // WHY: User can explicitly disable vision\n        let provider = VsCodeCopilotProvider::new()\n            .direct()\n            .with_vision(true)\n            .with_vision(false) // Disable after enabling\n            .build()\n            .unwrap();\n\n        assert!(!provider.supports_vision);\n    }\n\n    #[test]\n    fn test_builder_vision_with_model() {\n        // WHY: Vision should work with any compatible model\n        let provider = VsCodeCopilotProvider::new()\n            .direct()\n            .model(\"gpt-4o\") // Vision-capable model\n            .with_vision(true)\n            .build()\n            .unwrap();\n\n        assert!(provider.supports_vision);\n        assert_eq!(provider.model, \"gpt-4o\");\n    }\n\n    #[test]\n    fn test_builder_vision_with_proxy_mode() {\n        // WHY: Vision should also work in proxy mode\n        let builder = VsCodeCopilotProvider::new()\n            .proxy_url(\"http://localhost:4141\")\n            .with_vision(true);\n\n        // Build would fail without proper auth, but builder pattern works\n        assert!(builder.supports_vision);\n    }\n\n    // =========================================================================\n    // Builder Chain Tests\n    // WHY: Verify all builder options can be chained together\n    // =========================================================================\n\n    #[test]\n    fn test_builder_chain_all_options() {\n        // WHY: Full chain should work without panicking\n        use std::time::Duration;\n\n        let builder = VsCodeCopilotProvider::new()\n            .model(\"claude-3.5-sonnet\")\n            .embedding_model(\"text-embedding-3-large\")\n            .with_vision(true)\n            .timeout(Duration::from_secs(120));\n\n        // Verify all options were set correctly\n        assert_eq!(builder.model, \"claude-3.5-sonnet\");\n        assert_eq!(builder.embedding_model, \"text-embedding-3-large\");\n        assert!(builder.supports_vision);\n        assert_eq!(builder.timeout.as_secs(), 120);\n    }\n\n    #[test]\n    fn test_builder_account_type_business() {\n        // WHY: Business accounts use different API endpoint\n        use client::AccountType;\n\n        let builder = VsCodeCopilotProvider::new().account_type(AccountType::Business);\n\n        assert!(matches!(builder.account_type, AccountType::Business));\n    }\n\n    #[test]\n    fn test_builder_account_type_enterprise() {\n        // WHY: Enterprise accounts use different API endpoint\n        use client::AccountType;\n\n        let builder = VsCodeCopilotProvider::new().account_type(AccountType::Enterprise);\n\n        assert!(matches!(builder.account_type, AccountType::Enterprise));\n    }\n\n    // ========================================================================\n    // Default Configuration Tests (Iteration 32)\n    // ========================================================================\n    // WHY: Default values are critical for user experience and must be\n    // documented through tests. Changes to defaults affect all users.\n\n    #[test]\n    fn test_builder_default_embedding_model() {\n        // WHY: Default embedding model affects dimension calculations\n        // and compatibility with existing embeddings databases.\n        // Value: text-embedding-3-small (matches OpenAI default)\n        std::env::remove_var(\"VSCODE_COPILOT_EMBEDDING_MODEL\"); // Clear env override\n        let builder = VsCodeCopilotProviderBuilder::default();\n        assert_eq!(builder.embedding_model, \"text-embedding-3-small\");\n        assert_eq!(builder.embedding_dimension, 1536);\n    }\n\n    #[test]\n    fn test_builder_default_timeout() {\n        // WHY: Default timeout must be long enough for model responses\n        // but not so long that failures hang indefinitely.\n        // Value: 120 seconds (2 minutes)\n        let builder = VsCodeCopilotProviderBuilder::default();\n        assert_eq!(builder.timeout.as_secs(), 120);\n    }\n\n    #[test]\n    fn test_builder_default_context_length() {\n        // WHY: Context length determines how many tokens can be sent.\n        // Default 128k matches modern models like gpt-4o-mini.\n        std::env::set_var(\"VSCODE_COPILOT_DIRECT\", \"true\");\n        let builder = VsCodeCopilotProviderBuilder::default();\n        assert_eq!(builder.max_context_length, 128_000);\n        std::env::remove_var(\"VSCODE_COPILOT_DIRECT\");\n    }\n}\n","traces":[{"line":102,"address":[],"length":0,"stats":{"Line":23}},{"line":103,"address":[],"length":0,"stats":{"Line":23}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":7}},{"line":146,"address":[],"length":0,"stats":{"Line":7}},{"line":148,"address":[],"length":0,"stats":{"Line":16}},{"line":150,"address":[],"length":0,"stats":{"Line":38}},{"line":151,"address":[],"length":0,"stats":{"Line":2}},{"line":152,"address":[],"length":0,"stats":{"Line":2}},{"line":153,"address":[],"length":0,"stats":{"Line":2}},{"line":154,"address":[],"length":0,"stats":{"Line":4}},{"line":155,"address":[],"length":0,"stats":{"Line":4}},{"line":156,"address":[],"length":0,"stats":{"Line":2}},{"line":157,"address":[],"length":0,"stats":{"Line":6}},{"line":158,"address":[],"length":0,"stats":{"Line":2}},{"line":161,"address":[],"length":0,"stats":{"Line":2}},{"line":165,"address":[],"length":0,"stats":{"Line":9}},{"line":166,"address":[],"length":0,"stats":{"Line":9}},{"line":167,"address":[],"length":0,"stats":{"Line":9}},{"line":168,"address":[],"length":0,"stats":{"Line":9}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":29}},{"line":175,"address":[],"length":0,"stats":{"Line":1}},{"line":176,"address":[],"length":0,"stats":{"Line":16}},{"line":178,"address":[],"length":0,"stats":{"Line":6}},{"line":181,"address":[],"length":0,"stats":{"Line":4}},{"line":182,"address":[],"length":0,"stats":{"Line":6}},{"line":183,"address":[],"length":0,"stats":{"Line":2}},{"line":188,"address":[],"length":0,"stats":{"Line":4}},{"line":189,"address":[],"length":0,"stats":{"Line":8}},{"line":191,"address":[],"length":0,"stats":{"Line":6}},{"line":192,"address":[],"length":0,"stats":{"Line":4}},{"line":193,"address":[],"length":0,"stats":{"Line":2}},{"line":195,"address":[],"length":0,"stats":{"Line":6}},{"line":196,"address":[],"length":0,"stats":{"Line":2}},{"line":197,"address":[],"length":0,"stats":{"Line":4}},{"line":198,"address":[],"length":0,"stats":{"Line":2}},{"line":204,"address":[],"length":0,"stats":{"Line":2}},{"line":207,"address":[],"length":0,"stats":{"Line":6}},{"line":211,"address":[],"length":0,"stats":{"Line":9}},{"line":212,"address":[],"length":0,"stats":{"Line":2}},{"line":213,"address":[],"length":0,"stats":{"Line":8}},{"line":214,"address":[],"length":0,"stats":{"Line":6}},{"line":215,"address":[],"length":0,"stats":{"Line":2}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":9}},{"line":219,"address":[],"length":0,"stats":{"Line":18}},{"line":220,"address":[],"length":0,"stats":{"Line":9}},{"line":221,"address":[],"length":0,"stats":{"Line":18}},{"line":222,"address":[],"length":0,"stats":{"Line":9}},{"line":229,"address":[],"length":0,"stats":{"Line":3}},{"line":230,"address":[],"length":0,"stats":{"Line":3}},{"line":232,"address":[],"length":0,"stats":{"Line":3}},{"line":233,"address":[],"length":0,"stats":{"Line":10}},{"line":234,"address":[],"length":0,"stats":{"Line":5}},{"line":235,"address":[],"length":0,"stats":{"Line":15}},{"line":236,"address":[],"length":0,"stats":{"Line":15}},{"line":237,"address":[],"length":0,"stats":{"Line":10}},{"line":238,"address":[],"length":0,"stats":{"Line":5}},{"line":245,"address":[],"length":0,"stats":{"Line":5}},{"line":246,"address":[],"length":0,"stats":{"Line":14}},{"line":247,"address":[],"length":0,"stats":{"Line":6}},{"line":248,"address":[],"length":0,"stats":{"Line":1}},{"line":249,"address":[],"length":0,"stats":{"Line":1}},{"line":250,"address":[],"length":0,"stats":{"Line":1}},{"line":251,"address":[],"length":0,"stats":{"Line":1}},{"line":252,"address":[],"length":0,"stats":{"Line":1}},{"line":260,"address":[],"length":0,"stats":{"Line":3}},{"line":261,"address":[],"length":0,"stats":{"Line":3}},{"line":264,"address":[],"length":0,"stats":{"Line":3}},{"line":265,"address":[],"length":0,"stats":{"Line":3}},{"line":266,"address":[],"length":0,"stats":{"Line":3}},{"line":267,"address":[],"length":0,"stats":{"Line":3}},{"line":268,"address":[],"length":0,"stats":{"Line":3}},{"line":269,"address":[],"length":0,"stats":{"Line":3}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":327,"address":[],"length":0,"stats":{"Line":27}},{"line":329,"address":[],"length":0,"stats":{"Line":54}},{"line":330,"address":[],"length":0,"stats":{"Line":31}},{"line":334,"address":[],"length":0,"stats":{"Line":54}},{"line":336,"address":[],"length":0,"stats":{"Line":27}},{"line":340,"address":[],"length":0,"stats":{"Line":54}},{"line":341,"address":[],"length":0,"stats":{"Line":81}},{"line":344,"address":[],"length":0,"stats":{"Line":81}},{"line":348,"address":[],"length":0,"stats":{"Line":81}},{"line":351,"address":[],"length":0,"stats":{"Line":54}},{"line":362,"address":[],"length":0,"stats":{"Line":0}},{"line":363,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":2}},{"line":370,"address":[],"length":0,"stats":{"Line":4}},{"line":371,"address":[],"length":0,"stats":{"Line":2}},{"line":372,"address":[],"length":0,"stats":{"Line":2}},{"line":378,"address":[],"length":0,"stats":{"Line":6}},{"line":379,"address":[],"length":0,"stats":{"Line":6}},{"line":380,"address":[],"length":0,"stats":{"Line":12}},{"line":381,"address":[],"length":0,"stats":{"Line":6}},{"line":389,"address":[],"length":0,"stats":{"Line":2}},{"line":390,"address":[],"length":0,"stats":{"Line":2}},{"line":391,"address":[],"length":0,"stats":{"Line":2}},{"line":395,"address":[],"length":0,"stats":{"Line":14}},{"line":396,"address":[],"length":0,"stats":{"Line":42}},{"line":397,"address":[],"length":0,"stats":{"Line":14}},{"line":400,"address":[],"length":0,"stats":{"Line":14}},{"line":401,"address":[],"length":0,"stats":{"Line":0}},{"line":404,"address":[],"length":0,"stats":{"Line":28}},{"line":405,"address":[],"length":0,"stats":{"Line":14}},{"line":414,"address":[],"length":0,"stats":{"Line":4}},{"line":415,"address":[],"length":0,"stats":{"Line":12}},{"line":416,"address":[],"length":0,"stats":{"Line":4}},{"line":417,"address":[],"length":0,"stats":{"Line":8}},{"line":418,"address":[],"length":0,"stats":{"Line":4}},{"line":422,"address":[],"length":0,"stats":{"Line":7}},{"line":423,"address":[],"length":0,"stats":{"Line":7}},{"line":424,"address":[],"length":0,"stats":{"Line":7}},{"line":428,"address":[],"length":0,"stats":{"Line":1}},{"line":429,"address":[],"length":0,"stats":{"Line":1}},{"line":430,"address":[],"length":0,"stats":{"Line":1}},{"line":434,"address":[],"length":0,"stats":{"Line":19}},{"line":435,"address":[],"length":0,"stats":{"Line":39}},{"line":437,"address":[],"length":0,"stats":{"Line":3}},{"line":438,"address":[],"length":0,"stats":{"Line":18}},{"line":440,"address":[],"length":0,"stats":{"Line":54}},{"line":441,"address":[],"length":0,"stats":{"Line":36}},{"line":444,"address":[],"length":0,"stats":{"Line":0}},{"line":445,"address":[],"length":0,"stats":{"Line":0}},{"line":446,"address":[],"length":0,"stats":{"Line":0}},{"line":449,"address":[],"length":0,"stats":{"Line":57}},{"line":451,"address":[],"length":0,"stats":{"Line":19}},{"line":457,"address":[],"length":0,"stats":{"Line":0}},{"line":460,"address":[],"length":0,"stats":{"Line":19}},{"line":461,"address":[],"length":0,"stats":{"Line":38}},{"line":462,"address":[],"length":0,"stats":{"Line":38}},{"line":463,"address":[],"length":0,"stats":{"Line":38}},{"line":464,"address":[],"length":0,"stats":{"Line":38}},{"line":465,"address":[],"length":0,"stats":{"Line":19}},{"line":466,"address":[],"length":0,"stats":{"Line":19}},{"line":471,"address":[],"length":0,"stats":{"Line":19}},{"line":472,"address":[],"length":0,"stats":{"Line":19}},{"line":473,"address":[],"length":0,"stats":{"Line":38}},{"line":474,"address":[],"length":0,"stats":{"Line":66}},{"line":475,"address":[],"length":0,"stats":{"Line":10}},{"line":476,"address":[],"length":0,"stats":{"Line":10}},{"line":477,"address":[],"length":0,"stats":{"Line":14}},{"line":478,"address":[],"length":0,"stats":{"Line":6}},{"line":479,"address":[],"length":0,"stats":{"Line":8}},{"line":480,"address":[],"length":0,"stats":{"Line":8}},{"line":481,"address":[],"length":0,"stats":{"Line":1}},{"line":486,"address":[],"length":0,"stats":{"Line":35}},{"line":487,"address":[],"length":0,"stats":{"Line":35}},{"line":488,"address":[],"length":0,"stats":{"Line":78}},{"line":489,"address":[],"length":0,"stats":{"Line":120}},{"line":490,"address":[],"length":0,"stats":{"Line":6}},{"line":491,"address":[],"length":0,"stats":{"Line":2}},{"line":492,"address":[],"length":0,"stats":{"Line":1}},{"line":499,"address":[],"length":0,"stats":{"Line":1}},{"line":500,"address":[],"length":0,"stats":{"Line":1}},{"line":503,"address":[],"length":0,"stats":{"Line":0}},{"line":504,"address":[],"length":0,"stats":{"Line":0}},{"line":507,"address":[],"length":0,"stats":{"Line":0}},{"line":508,"address":[],"length":0,"stats":{"Line":0}},{"line":511,"address":[],"length":0,"stats":{"Line":0}},{"line":572,"address":[],"length":0,"stats":{"Line":0}},{"line":668,"address":[],"length":0,"stats":{"Line":0}},{"line":709,"address":[],"length":0,"stats":{"Line":0}},{"line":732,"address":[],"length":0,"stats":{"Line":0}},{"line":737,"address":[],"length":0,"stats":{"Line":0}},{"line":738,"address":[],"length":0,"stats":{"Line":0}},{"line":741,"address":[],"length":0,"stats":{"Line":0}},{"line":742,"address":[],"length":0,"stats":{"Line":0}},{"line":745,"address":[],"length":0,"stats":{"Line":0}},{"line":746,"address":[],"length":0,"stats":{"Line":0}},{"line":750,"address":[],"length":0,"stats":{"Line":0}},{"line":751,"address":[],"length":0,"stats":{"Line":0}},{"line":817,"address":[],"length":0,"stats":{"Line":0}},{"line":825,"address":[],"length":0,"stats":{"Line":3}},{"line":826,"address":[],"length":0,"stats":{"Line":3}},{"line":830,"address":[],"length":0,"stats":{"Line":2}},{"line":833,"address":[],"length":0,"stats":{"Line":2}},{"line":836,"address":[],"length":0,"stats":{"Line":3}},{"line":837,"address":[],"length":0,"stats":{"Line":3}},{"line":840,"address":[],"length":0,"stats":{"Line":0}},{"line":841,"address":[],"length":0,"stats":{"Line":0}},{"line":844,"address":[],"length":0,"stats":{"Line":0}},{"line":872,"address":[],"length":0,"stats":{"Line":0}}],"covered":151,"coverable":190},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","stream.rs"],"content":"//! Server-Sent Events (SSE) streaming support for VSCode Copilot.\n//!\n//! # Architecture\n//!\n//! This module handles parsing of Server-Sent Events (SSE) from the Copilot API.\n//! SSE is a streaming protocol where the server pushes data to the client in\n//! a line-oriented format.\n//!\n//! ## SSE Parsing Flow\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                     SSE Stream Parsing                           â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                   â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚\n//! â”‚  â”‚ HTTP Response   â”‚  Chunked bytes from reqwest                 â”‚\n//! â”‚  â”‚ bytes_stream()  â”‚                                             â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚\n//! â”‚           â”‚                                                       â”‚\n//! â”‚           â–¼                                                       â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚\n//! â”‚  â”‚ String Buffer   â”‚  WHY: HTTP chunks may split lines           â”‚\n//! â”‚  â”‚ Accumulate      â”‚  Buffer until newline found                 â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚\n//! â”‚           â”‚                                                       â”‚\n//! â”‚           â–¼                                                       â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Recognized Prefixes:                   â”‚\n//! â”‚  â”‚ Parse SSE Line  â”‚      - data: â†’ JSON content or [DONE]       â”‚\n//! â”‚  â”‚ strip_prefix()  â”‚      - event: â†’ ignored                     â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      - id: â†’ ignored                        â”‚\n//! â”‚           â”‚               - : â†’ comment, ignored                  â”‚\n//! â”‚           â”‚                                                       â”‚\n//! â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n//! â”‚           â”‚                   â”‚                   â”‚               â”‚\n//! â”‚           â–¼                   â–¼                   â–¼               â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n//! â”‚  â”‚ data: JSON  â”‚     â”‚ data: [DONE]â”‚     â”‚ Other Lines â”‚         â”‚\n//! â”‚  â”‚ Deserialize â”‚     â”‚ End stream  â”‚     â”‚ Ignore/warn â”‚         â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n//! â”‚         â”‚                                                         â”‚\n//! â”‚         â–¼                                                         â”‚\n//! â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚\n//! â”‚  â”‚ Extract Content â”‚  choices[0].delta.content                   â”‚\n//! â”‚  â”‚ Yield String    â”‚  Empty content â†’ skip                       â”‚\n//! â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚\n//! â”‚                                                                   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! ## Buffer Strategy\n//!\n//! WHY: HTTP chunked transfer can split SSE lines at arbitrary byte boundaries.\n//! For example, a chunk might end in the middle of a JSON object:\n//!\n//! ```text\n//! Chunk 1: \"data: {\\\"id\\\":\\\"abc\\\",\\\"content\\\":\"\n//! Chunk 2: \"\\\"Hello\\\"}\\n\"\n//! ```\n//!\n//! The buffer accumulates bytes until a complete line (ending in `\\n`) is found,\n//! then processes complete lines while retaining partial data for the next chunk.\n//!\n//! ## Error Handling\n//!\n//! - Network errors â†’ `VsCodeError::Stream`\n//! - JSON parse errors â†’ `VsCodeError::Stream` with context\n//! - Unknown line formats â†’ warning logged, line ignored\n//!\n//! ## References\n//!\n//! - [SSE Specification](https://html.spec.whatwg.org/multipage/server-sent-events.html)\n//! - [OpenAI Streaming](https://platform.openai.com/docs/api-reference/chat/create#stream)\n\nuse futures::stream::{BoxStream, TryStreamExt};\nuse reqwest::Response;\nuse tracing::{debug, warn};\n\nuse super::error::{Result, VsCodeError};\nuse super::types::ChatCompletionChunk;\n\n/// Parse SSE stream from HTTP response.\n///\n/// The Copilot API returns Server-Sent Events in the format:\n/// ```text\n/// data: {\"id\":\"...\",\"object\":\"chat.completion.chunk\",...}\n///\n/// data: {\"id\":\"...\",\"object\":\"chat.completion.chunk\",...}\n///\n/// data: [DONE]\n/// ```\n///\n/// This function parses the stream and extracts content deltas.\npub(super) fn parse_sse_stream(response: Response) -> BoxStream<'static, Result<String>> {\n    // Convert bytes stream to lines, buffering partial lines\n    let mut buffer = String::new();\n\n    let stream = response\n        .bytes_stream()\n        .map_err(|e| VsCodeError::Stream(e.to_string()))\n        .try_filter_map(move |chunk| {\n            // Add new bytes to buffer\n            buffer.push_str(&String::from_utf8_lossy(&chunk));\n\n            // Extract complete lines\n            let mut lines = Vec::new();\n            while let Some(idx) = buffer.find('\\n') {\n                let line = buffer[..idx].trim().to_string();\n                buffer.drain(..=idx);\n\n                if !line.is_empty() {\n                    lines.push(line);\n                }\n            }\n\n            futures::future::ready(Ok(if lines.is_empty() {\n                None\n            } else {\n                Some(futures::stream::iter(lines.into_iter().map(Ok)))\n            }))\n        })\n        .try_flatten()\n        .try_filter_map(|line| async move {\n            // Parse SSE data lines\n            if let Some(data) = line.strip_prefix(\"data: \") {\n                // Check for [DONE] signal\n                if data.trim() == \"[DONE]\" {\n                    debug!(\"Received [DONE] signal, ending stream\");\n                    return Ok(None);\n                }\n\n                // Parse JSON chunk\n                match serde_json::from_str::<ChatCompletionChunk>(data) {\n                    Ok(chunk) => {\n                        // Extract content from first choice\n                        if let Some(choice) = chunk.choices.first() {\n                            if let Some(content) = &choice.delta.content {\n                                if !content.is_empty() {\n                                    debug!(content_len = content.len(), \"Received content delta\");\n                                    return Ok(Some(content.clone()));\n                                }\n                            }\n                        }\n                        // Empty delta, skip\n                        Ok(None)\n                    }\n                    Err(e) => {\n                        warn!(error = %e, data = %data, \"Failed to parse SSE chunk\");\n                        Err(VsCodeError::Stream(format!(\"Failed to parse chunk: {}\", e)))\n                    }\n                }\n            } else if line.starts_with(\"event: \") || line.starts_with(\"id: \") {\n                // Ignore event type and id lines\n                Ok(None)\n            } else if line.starts_with(':') {\n                // Ignore comments\n                Ok(None)\n            } else {\n                // Unknown line format\n                warn!(line = %line, \"Unexpected SSE line format\");\n                Ok(None)\n            }\n        });\n\n    Box::pin(stream)\n}\n\n/// Parse SSE stream with tool call support (OODA-05).\n///\n/// Unlike `parse_sse_stream` which returns String content, this function\n/// returns `StreamChunk` to support the full range of streaming events:\n/// - Content chunks (`StreamChunk::Content`)\n/// - Tool call deltas (`StreamChunk::ToolCallDelta`)\n/// - Finish reason (`StreamChunk::Finished`)\n///\n/// This enables the React agent to use the streaming path with real-time\n/// token counting and progress display.\n///\n/// # Flow\n///\n/// ```text\n/// SSE bytes â†’ buffer â†’ parse line â†’ match delta type â†’ StreamChunk\n///\n/// delta.content â†’ StreamChunk::Content(text)\n/// delta.tool_calls â†’ StreamChunk::ToolCallDelta {index, id, name, args}\n/// finish_reason â†’ StreamChunk::Finished {reason}\n/// ```\npub(super) fn parse_sse_stream_with_tools(\n    response: Response,\n) -> BoxStream<'static, Result<crate::traits::StreamChunk>> {\n    use crate::traits::StreamChunk;\n    \n    let mut buffer = String::new();\n\n    let stream = response\n        .bytes_stream()\n        .map_err(|e| VsCodeError::Stream(e.to_string()))\n        .try_filter_map(move |chunk| {\n            // Add new bytes to buffer\n            buffer.push_str(&String::from_utf8_lossy(&chunk));\n\n            // Extract complete lines\n            let mut lines = Vec::new();\n            while let Some(idx) = buffer.find('\\n') {\n                let line = buffer[..idx].trim().to_string();\n                buffer.drain(..=idx);\n\n                if !line.is_empty() {\n                    lines.push(line);\n                }\n            }\n\n            futures::future::ready(Ok(if lines.is_empty() {\n                None\n            } else {\n                Some(futures::stream::iter(lines.into_iter().map(Ok)))\n            }))\n        })\n        .try_flatten()\n        .try_filter_map(|line| async move {\n            // Parse SSE data lines\n            if let Some(data) = line.strip_prefix(\"data: \") {\n                // Check for [DONE] signal\n                if data.trim() == \"[DONE]\" {\n                    debug!(\"Received [DONE] signal, ending stream\");\n                    return Ok(Some(StreamChunk::Finished {\n                        reason: \"stop\".to_string(),\n                        ttft_ms: None,\n                    }));\n                }\n\n                // Parse JSON chunk\n                match serde_json::from_str::<ChatCompletionChunk>(data) {\n                    Ok(chunk) => {\n                        if let Some(choice) = chunk.choices.first() {\n                            // Check for finish reason first\n                            if let Some(ref finish_reason) = choice.finish_reason {\n                                debug!(reason = %finish_reason, \"Stream finished\");\n                                return Ok(Some(StreamChunk::Finished {\n                                    reason: finish_reason.clone(),\n                                    ttft_ms: None,\n                                }));\n                            }\n                            \n                            // Check for tool calls (OODA-05)\n                            if let Some(ref tool_calls) = choice.delta.tool_calls {\n                                if let Some(tc) = tool_calls.first() {\n                                    let function_name = tc.function.as_ref().and_then(|f| f.name.clone());\n                                    let function_arguments = tc.function.as_ref().and_then(|f| f.arguments.clone());\n                                    \n                                    debug!(\n                                        index = tc.index,\n                                        id = ?tc.id,\n                                        name = ?function_name,\n                                        \"Received tool call delta\"\n                                    );\n                                    \n                                    return Ok(Some(StreamChunk::ToolCallDelta {\n                                        index: tc.index,\n                                        id: tc.id.clone(),\n                                        function_name,\n                                        function_arguments,\n                                    }));\n                                }\n                            }\n                            \n                            // Check for content\n                            if let Some(ref content) = choice.delta.content {\n                                if !content.is_empty() {\n                                    debug!(content_len = content.len(), \"Received content delta\");\n                                    return Ok(Some(StreamChunk::Content(content.clone())));\n                                }\n                            }\n                        }\n                        // Empty delta, skip\n                        Ok(None)\n                    }\n                    Err(e) => {\n                        warn!(error = %e, data = %data, \"Failed to parse SSE chunk\");\n                        Err(VsCodeError::Stream(format!(\"Failed to parse chunk: {}\", e)))\n                    }\n                }\n            } else if line.starts_with(\"event: \") || line.starts_with(\"id: \") {\n                // Ignore event type and id lines\n                Ok(None)\n            } else if line.starts_with(':') {\n                // Ignore comments\n                Ok(None)\n            } else {\n                // Unknown line format\n                warn!(line = %line, \"Unexpected SSE line format\");\n                Ok(None)\n            }\n        });\n\n    Box::pin(stream)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    // =========================================================================\n    // SSE Line Recognition Tests\n    // WHY: Verify that all SSE line types are correctly identified and handled\n    // =========================================================================\n\n    #[test]\n    fn test_parse_done_signal() {\n        // Test that [DONE] signal is recognized\n        let data = \"data: [DONE]\";\n        assert!(data.starts_with(\"data: \"));\n        let content = &data[6..];\n        assert_eq!(content.trim(), \"[DONE]\");\n    }\n\n    #[test]\n    fn test_done_signal_with_whitespace() {\n        // WHY: The API may include trailing whitespace\n        let variations = [\n            \"data: [DONE]\",\n            \"data: [DONE] \",\n            \"data: [DONE]\\r\",\n            \"data:  [DONE]\",\n        ];\n\n        for data in variations {\n            assert!(data.starts_with(\"data:\"), \"Should start with data:\");\n            let content = data.strip_prefix(\"data:\").unwrap().trim();\n            assert_eq!(content, \"[DONE]\", \"Failed for: {:?}\", data);\n        }\n    }\n\n    #[test]\n    fn test_done_signal_is_case_sensitive() {\n        // WHY: [DONE] must be uppercase per OpenAI spec\n        let invalid = [\"data: [done]\", \"data: [Done]\", \"data: done\"];\n\n        for data in invalid {\n            let content = data.strip_prefix(\"data: \").unwrap_or(\"\").trim();\n            assert_ne!(content, \"[DONE]\", \"[DONE] check should be case-sensitive\");\n        }\n    }\n\n    #[test]\n    fn test_sse_event_line_prefix() {\n        // WHY: SSE can include event type lines which we ignore\n        let line = \"event: message\";\n        assert!(line.starts_with(\"event: \"), \"Should recognize event prefix\");\n    }\n\n    #[test]\n    fn test_sse_id_line_prefix() {\n        // WHY: SSE can include message ID lines which we ignore\n        let line = \"id: 12345\";\n        assert!(line.starts_with(\"id: \"), \"Should recognize id prefix\");\n    }\n\n    #[test]\n    fn test_sse_comment_line_prefix() {\n        // WHY: SSE comments start with colon - used for keep-alive\n        let comment = \": this is a comment\";\n        assert!(comment.starts_with(':'), \"Should recognize comment prefix\");\n    }\n\n    #[test]\n    fn test_sse_data_line_prefix() {\n        // WHY: Content lines start with \"data: \"\n        let data_line = \"data: {\\\"content\\\":\\\"hello\\\"}\";\n        assert!(data_line.starts_with(\"data: \"));\n\n        let json = data_line.strip_prefix(\"data: \").unwrap();\n        assert!(json.starts_with('{'));\n    }\n\n    // =========================================================================\n    // JSON Chunk Parsing Tests\n    // WHY: Verify deserialization of various chunk formats\n    // =========================================================================\n\n    #[test]\n    fn test_parse_chunk_format() {\n        let json = r#\"{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: std::result::Result<ChatCompletionChunk, _> = serde_json::from_str(json);\n        assert!(chunk.is_ok());\n\n        let chunk = chunk.unwrap();\n        assert_eq!(chunk.id, \"test\");\n        assert_eq!(chunk.choices[0].delta.content, Some(\"Hello\".to_string()));\n    }\n\n    #[test]\n    fn test_chunk_with_empty_content() {\n        // WHY: First chunk often has empty content (role-only delta)\n        let json = r#\"{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        // Content exists but is empty\n        assert_eq!(chunk.choices[0].delta.content, Some(\"\".to_string()));\n    }\n\n    #[test]\n    fn test_chunk_with_no_content() {\n        // WHY: Final chunk may have no content, just finish_reason\n        let json = r#\"{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{},\"finish_reason\":\"stop\"}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        assert!(chunk.choices[0].delta.content.is_none());\n        assert_eq!(chunk.choices[0].finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[test]\n    fn test_chunk_with_role_only() {\n        // WHY: First assistant chunk typically has role but no content\n        let json = r#\"{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        assert_eq!(chunk.choices[0].delta.role, Some(\"assistant\".to_string()));\n        assert!(chunk.choices[0].delta.content.is_none());\n    }\n\n    #[test]\n    fn test_chunk_malformed_json() {\n        // WHY: Malformed JSON should produce a parse error\n        let bad_json = r#\"{\"id\":\"test\", broken json\"#;\n\n        let result: std::result::Result<ChatCompletionChunk, _> = serde_json::from_str(bad_json);\n        assert!(result.is_err(), \"Malformed JSON should fail to parse\");\n    }\n\n    #[test]\n    fn test_chunk_multiple_choices() {\n        // WHY: API can return multiple choices (n > 1), we use first\n        let json = r#\"{\n            \"id\":\"test\",\n            \"object\":\"chat.completion.chunk\",\n            \"created\":123,\n            \"model\":\"gpt-4o\",\n            \"choices\":[\n                {\"index\":0,\"delta\":{\"content\":\"First\"},\"finish_reason\":null},\n                {\"index\":1,\"delta\":{\"content\":\"Second\"},\"finish_reason\":null}\n            ]\n        }\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        assert_eq!(chunk.choices.len(), 2);\n        assert_eq!(chunk.choices[0].delta.content, Some(\"First\".to_string()));\n        assert_eq!(chunk.choices[1].delta.content, Some(\"Second\".to_string()));\n    }\n\n    #[test]\n    fn test_chunk_empty_choices() {\n        // WHY: Edge case - choices array is empty\n        let json = r#\"{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        assert!(chunk.choices.is_empty());\n        // In parse_sse_stream, this would result in None (skipped)\n    }\n\n    // =========================================================================\n    // Content Extraction Logic Tests\n    // WHY: Verify the content extraction logic matches expected behavior\n    // =========================================================================\n\n    #[test]\n    fn test_extract_content_from_choice() {\n        let json = r#\"{\"id\":\"abc\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"World\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        // Simulate the extraction logic from parse_sse_stream\n        let content = chunk\n            .choices\n            .first()\n            .and_then(|c| c.delta.content.as_ref())\n            .filter(|s| !s.is_empty());\n\n        assert_eq!(content, Some(&\"World\".to_string()));\n    }\n\n    #[test]\n    fn test_extract_content_filters_empty() {\n        let json = r#\"{\"id\":\"abc\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        let content = chunk\n            .choices\n            .first()\n            .and_then(|c| c.delta.content.as_ref())\n            .filter(|s| !s.is_empty());\n\n        assert!(content.is_none(), \"Empty content should be filtered out\");\n    }\n\n    #[test]\n    fn test_extract_content_with_unicode() {\n        // WHY: Content may contain unicode, emojis, etc.\n        let json = r#\"{\"id\":\"abc\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello ä¸–ç•Œ ðŸŒ\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        assert_eq!(\n            chunk.choices[0].delta.content,\n            Some(\"Hello ä¸–ç•Œ ðŸŒ\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_extract_content_with_newlines() {\n        // WHY: Content may contain newlines (code, multi-line text)\n        let json = r#\"{\"id\":\"abc\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"line1\\nline2\\nline3\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        let content = chunk.choices[0].delta.content.as_ref().unwrap();\n        assert!(content.contains('\\n'));\n        assert_eq!(content.lines().count(), 3);\n    }\n\n    // =========================================================================\n    // Finish Reason Tests\n    // WHY: Understand how finish_reason affects streaming behavior\n    // =========================================================================\n\n    #[test]\n    fn test_finish_reason_values() {\n        // WHY: Various finish reasons indicate different end conditions\n        let reasons = [\"stop\", \"length\", \"content_filter\", \"tool_calls\"];\n\n        for reason in reasons {\n            let json = format!(\n                r#\"{{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{{\"index\":0,\"delta\":{{}},\"finish_reason\":\"{}\"}}]}}\"#,\n                reason\n            );\n\n            let chunk: ChatCompletionChunk = serde_json::from_str(&json).unwrap();\n            assert_eq!(chunk.choices[0].finish_reason, Some(reason.to_string()));\n        }\n    }\n\n    #[test]\n    fn test_finish_reason_null_during_streaming() {\n        // WHY: During streaming, finish_reason is null until the final chunk\n        let json = r#\"{\"id\":\"test\",\"object\":\"chat.completion.chunk\",\"created\":123,\"model\":\"gpt-4o\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"streaming...\"},\"finish_reason\":null}]}\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n\n        assert!(chunk.choices[0].finish_reason.is_none());\n        assert!(chunk.choices[0].delta.content.is_some());\n    }\n}\n","traces":[{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":0}},{"line":100,"address":[],"length":0,"stats":{"Line":0}},{"line":101,"address":[],"length":0,"stats":{"Line":0}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":0}},{"line":107,"address":[],"length":0,"stats":{"Line":0}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":109,"address":[],"length":0,"stats":{"Line":0}},{"line":111,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":116,"address":[],"length":0,"stats":{"Line":0}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":129,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":136,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":145,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":160,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":0}},{"line":195,"address":[],"length":0,"stats":{"Line":0}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":0}},{"line":205,"address":[],"length":0,"stats":{"Line":0}},{"line":206,"address":[],"length":0,"stats":{"Line":0}},{"line":208,"address":[],"length":0,"stats":{"Line":0}},{"line":209,"address":[],"length":0,"stats":{"Line":0}},{"line":213,"address":[],"length":0,"stats":{"Line":0}},{"line":214,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":228,"address":[],"length":0,"stats":{"Line":0}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":235,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":249,"address":[],"length":0,"stats":{"Line":0}},{"line":251,"address":[],"length":0,"stats":{"Line":0}},{"line":255,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":259,"address":[],"length":0,"stats":{"Line":0}},{"line":260,"address":[],"length":0,"stats":{"Line":0}},{"line":261,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":268,"address":[],"length":0,"stats":{"Line":0}},{"line":269,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":276,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":280,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":285,"address":[],"length":0,"stats":{"Line":0}},{"line":286,"address":[],"length":0,"stats":{"Line":0}},{"line":288,"address":[],"length":0,"stats":{"Line":0}},{"line":291,"address":[],"length":0,"stats":{"Line":0}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":94},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","token.rs"],"content":"//! Token storage and management for GitHub Copilot.\n//!\n//! Handles storing, loading, and refreshing GitHub and Copilot tokens.\n//!\n//! # Token Lifecycle\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    TOKEN LIFECYCLE                             â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                â”‚\n//! â”‚  1. Initial Auth (one-time via device code):                  â”‚\n//! â”‚                                                                â”‚\n//! â”‚     User â”€â”€device codeâ”€â”€â–¶ GitHub â”€â”€grantsâ”€â”€â–¶ GitHub Token     â”‚\n//! â”‚                                              (persisted to    â”‚\n//! â”‚                                               disk)           â”‚\n//! â”‚                                                                â”‚\n//! â”‚  2. Token Exchange (automatic):                               â”‚\n//! â”‚                                                                â”‚\n//! â”‚     GitHub Token â”€â”€GET /copilot_internal/v2/tokenâ”€â”€â–¶          â”‚\n//! â”‚                         Copilot Token (15min expiry)          â”‚\n//! â”‚                                                                â”‚\n//! â”‚  3. Token Refresh (automatic before each request):            â”‚\n//! â”‚                                                                â”‚\n//! â”‚     IF (now >= expires_at - 60s):                             â”‚\n//! â”‚         Fetch new Copilot Token                               â”‚\n//! â”‚         Persist to disk                                       â”‚\n//! â”‚     END                                                        â”‚\n//! â”‚                                                                â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Token Storage Locations\n//!\n//! - **macOS/Linux**: `~/.config/edgequake/copilot/`\n//! - **Windows**: `%APPDATA%\\edgequake\\copilot\\`\n//!\n//! Files:\n//! - `github_token.json` - Long-lived GitHub OAuth token\n//! - `copilot_token.json` - Short-lived Copilot API token\n//!\n//! # Refresh Buffer\n//!\n//! Tokens are refreshed 60 seconds before expiry to avoid race conditions\n//! where a token expires mid-request.\n\nuse anyhow::{Context, Result};\nuse serde::{Deserialize, Serialize};\nuse std::path::PathBuf;\nuse std::time::{Duration, SystemTime, UNIX_EPOCH};\nuse tokio::fs;\nuse tracing::debug;\n\nconst COPILOT_TOKEN_URL: &str = \"https://api.github.com/copilot_internal/v2/token\";\nconst TOKEN_REFRESH_BUFFER: u64 = 60; // Refresh 60 seconds before expiry\n\n/// GitHub access token.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct GitHubToken {\n    pub access_token: String,\n    pub created_at: u64,\n}\n\n/// Copilot access token with expiry information.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct CopilotToken {\n    pub token: String,\n    pub expires_at: u64,\n    pub refresh_in: u64,\n    pub organization_list: Option<Vec<String>>,\n}\n\n/// Token manager for GitHub and Copilot tokens.\n#[derive(Clone)]\npub struct TokenManager {\n    config_dir: PathBuf,\n    client: reqwest::Client,\n}\n\nimpl TokenManager {\n    /// Create a new token manager.\n    pub fn new() -> Result<Self> {\n        let config_dir = Self::get_config_dir()?;\n\n        let client = reqwest::Client::builder()\n            .timeout(Duration::from_secs(30))\n            .build()\n            .context(\"Failed to create HTTP client\")?;\n\n        Ok(Self { config_dir, client })\n    }\n\n    /// Get the configuration directory for token storage.\n    fn get_config_dir() -> Result<PathBuf> {\n        let base_dir = dirs::config_dir().context(\"Failed to get config directory\")?;\n\n        let config_dir = base_dir.join(\"edgequake\").join(\"copilot\");\n        Ok(config_dir)\n    }\n\n    /// Get the path to the GitHub token file.\n    fn github_token_path(&self) -> PathBuf {\n        self.config_dir.join(\"github_token.json\")\n    }\n\n    /// Get the path to the Copilot token file.\n    fn copilot_token_path(&self) -> PathBuf {\n        self.config_dir.join(\"copilot_token.json\")\n    }\n\n    /// Ensure the config directory exists.\n    async fn ensure_config_dir(&self) -> Result<()> {\n        fs::create_dir_all(&self.config_dir)\n            .await\n            .context(\"Failed to create config directory\")?;\n        Ok(())\n    }\n\n    /// Save GitHub token to disk.\n    pub async fn save_github_token(&self, access_token: String) -> Result<()> {\n        self.ensure_config_dir().await?;\n\n        let token = GitHubToken {\n            access_token,\n            created_at: SystemTime::now()\n                .duration_since(UNIX_EPOCH)\n                .unwrap()\n                .as_secs(),\n        };\n\n        let json =\n            serde_json::to_string_pretty(&token).context(\"Failed to serialize GitHub token\")?;\n\n        fs::write(self.github_token_path(), json)\n            .await\n            .context(\"Failed to write GitHub token\")?;\n\n        Ok(())\n    }\n\n    /// Load GitHub token from disk.\n    pub async fn load_github_token(&self) -> Result<GitHubToken> {\n        let json = fs::read_to_string(self.github_token_path())\n            .await\n            .context(\"Failed to read GitHub token\")?;\n\n        serde_json::from_str(&json).context(\"Failed to parse GitHub token\")\n    }\n\n    /// Save Copilot token to disk.\n    pub async fn save_copilot_token(&self, token: CopilotToken) -> Result<()> {\n        self.ensure_config_dir().await?;\n\n        let json =\n            serde_json::to_string_pretty(&token).context(\"Failed to serialize Copilot token\")?;\n\n        fs::write(self.copilot_token_path(), json)\n            .await\n            .context(\"Failed to write Copilot token\")?;\n\n        Ok(())\n    }\n\n    /// Load Copilot token from disk.\n    pub async fn load_copilot_token(&self) -> Result<CopilotToken> {\n        let json = fs::read_to_string(self.copilot_token_path())\n            .await\n            .context(\"Failed to read Copilot token\")?;\n\n        serde_json::from_str(&json).context(\"Failed to parse Copilot token\")\n    }\n\n    /// Check if Copilot token needs refresh.\n    pub fn needs_refresh(&self, token: &CopilotToken) -> bool {\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let refresh_at = token.expires_at.saturating_sub(TOKEN_REFRESH_BUFFER);\n        now >= refresh_at\n    }\n\n    /// Fetch a new Copilot token using GitHub token.\n    pub async fn fetch_copilot_token(&self, github_token: &str) -> Result<CopilotToken> {\n        let response = self\n            .client\n            .get(COPILOT_TOKEN_URL)\n            .header(\"Accept\", \"application/json\")\n            .header(\"Authorization\", format!(\"Bearer {}\", github_token))\n            .header(\"User-Agent\", \"GitHubCopilot/0.26.7\")\n            .send()\n            .await\n            .context(\"Failed to fetch Copilot token\")?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let body = response.text().await.unwrap_or_default();\n            anyhow::bail!(\"Copilot token request failed: {} - {}\", status, body);\n        }\n\n        #[derive(Deserialize)]\n        struct CopilotTokenResponse {\n            token: String,\n            expires_at: u64,\n            refresh_in: Option<u64>,\n            organization_list: Option<Vec<String>>,\n        }\n\n        let resp: CopilotTokenResponse = response\n            .json()\n            .await\n            .context(\"Failed to parse Copilot token response\")?;\n\n        Ok(CopilotToken {\n            token: resp.token,\n            expires_at: resp.expires_at,\n            refresh_in: resp.refresh_in.unwrap_or(900), // Default 15 minutes\n            organization_list: resp.organization_list,\n        })\n    }\n\n    /// Validate a Copilot token by making a test API call.\n    /// Returns Ok(true) if valid, Ok(false) if invalid/expired, Err on network issues.\n    pub async fn validate_copilot_token(&self, token: &str) -> Result<bool> {\n        let client = reqwest::Client::new();\n        let response = client\n            .get(\"https://api.githubcopilot.com/models\")\n            .header(\"Authorization\", format!(\"Bearer {}\", token))\n            .header(\"Editor-Version\", \"vscode/1.85.0\")\n            .header(\"Editor-Plugin-Version\", \"copilot/1.155.0\")\n            .timeout(std::time::Duration::from_secs(10))\n            .send()\n            .await?;\n\n        Ok(response.status().is_success())\n    }\n\n    /// Get a valid Copilot token, refreshing if necessary and validating with API call.\n    pub async fn get_valid_copilot_token(&self) -> Result<String> {\n        // Try to load existing Copilot token\n        if let Ok(copilot_token) = self.load_copilot_token().await {\n            if !self.needs_refresh(&copilot_token) {\n                // Token not expired by timestamp, but validate it actually works\n                debug!(\"Validating cached Copilot token with API call...\");\n                if self.validate_copilot_token(&copilot_token.token).await.unwrap_or(false) {\n                    debug!(\"Cached Copilot token is valid\");\n                    return Ok(copilot_token.token);\n                }\n                debug!(\"Cached Copilot token failed validation, will refresh\");\n            } else {\n                debug!(\n                    \"Copilot token expired or expiring soon (within {}s), refreshing...\",\n                    TOKEN_REFRESH_BUFFER\n                );\n            }\n        } else {\n            debug!(\"No cached Copilot token found, fetching fresh token\");\n        }\n\n        // Need to refresh - get GitHub token\n        let github_token = self.load_github_token().await?;\n\n        // Fetch new Copilot token\n        debug!(\"Requesting fresh Copilot token from GitHub API\");\n        let copilot_token = self.fetch_copilot_token(&github_token.access_token).await?;\n        let token_value = copilot_token.token.clone();\n\n        // Validate the new token\n        if !self.validate_copilot_token(&token_value).await.unwrap_or(false) {\n            return Err(anyhow::anyhow!(\n                \"Fetched token is invalid. Your GitHub account may not have Copilot access.\"\n            ));\n        }\n\n        // Save for next time\n        self.save_copilot_token(copilot_token.clone()).await?;\n        debug!(\n            \"Successfully refreshed and saved Copilot token (expires at: {})\",\n            copilot_token.expires_at\n        );\n\n        Ok(token_value)\n    }\n\n    /// Clear all stored tokens.\n    pub async fn clear_tokens(&self) -> Result<()> {\n        let _ = fs::remove_file(self.github_token_path()).await;\n        let _ = fs::remove_file(self.copilot_token_path()).await;\n        Ok(())\n    }\n\n    /// Check if GitHub token exists.\n    pub async fn has_github_token(&self) -> bool {\n        self.github_token_path().exists()\n    }\n\n    /// Check if Copilot token exists.\n    pub async fn has_copilot_token(&self) -> bool {\n        self.copilot_token_path().exists()\n    }\n\n    /// Try to load GitHub token from VS Code Copilot's hosts.json as fallback.\n    /// Returns None if file doesn't exist or cannot be parsed.\n    pub async fn try_load_vscode_github_token(&self) -> Option<String> {\n        let vscode_hosts_path = dirs::config_dir()?\n            .join(\"github-copilot\")\n            .join(\"hosts.json\");\n\n        if !vscode_hosts_path.exists() {\n            return None;\n        }\n\n        let contents = fs::read_to_string(&vscode_hosts_path).await.ok()?;\n        \n        #[derive(Deserialize)]\n        struct HostsJson {\n            #[serde(rename = \"github.com\")]\n            github_com: Option<GithubComEntry>,\n        }\n        \n        #[derive(Deserialize)]\n        struct GithubComEntry {\n            oauth_token: String,\n        }\n\n        let hosts: HostsJson = serde_json::from_str(&contents).ok()?;\n        Some(hosts.github_com?.oauth_token)\n    }\n\n    /// Import GitHub token from VS Code Copilot configuration.\n    /// Returns true if token was successfully imported.\n    pub async fn import_vscode_token(&self) -> Result<bool> {\n        if let Some(token) = self.try_load_vscode_github_token().await {\n            self.save_github_token(token).await?;\n            debug!(\"Successfully imported GitHub token from VS Code Copilot\");\n            Ok(true)\n        } else {\n            Ok(false)\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_token_manager_creation() {\n        let manager = TokenManager::new().unwrap();\n        assert!(manager.config_dir.to_string_lossy().contains(\"edgequake\"));\n    }\n\n    #[tokio::test]\n    async fn test_config_dir_creation() {\n        let manager = TokenManager::new().unwrap();\n        manager.ensure_config_dir().await.unwrap();\n        assert!(manager.config_dir.exists());\n    }\n\n    // ========================================================================\n    // Token Refresh Logic Tests\n    // ========================================================================\n\n    #[test]\n    fn test_needs_refresh_false_when_valid() {\n        let manager = TokenManager::new().unwrap();\n\n        // Token expires in 2 hours (well beyond the 60-second buffer)\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let token = CopilotToken {\n            token: \"test_token\".to_string(),\n            expires_at: now + 7200, // 2 hours from now\n            refresh_in: 900,\n            organization_list: None,\n        };\n\n        assert!(!manager.needs_refresh(&token));\n    }\n\n    #[test]\n    fn test_needs_refresh_true_when_expired() {\n        let manager = TokenManager::new().unwrap();\n\n        // Token expired 1 hour ago\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let token = CopilotToken {\n            token: \"test_token\".to_string(),\n            expires_at: now.saturating_sub(3600), // 1 hour ago\n            refresh_in: 900,\n            organization_list: None,\n        };\n\n        assert!(manager.needs_refresh(&token));\n    }\n\n    #[test]\n    fn test_needs_refresh_true_within_buffer() {\n        let manager = TokenManager::new().unwrap();\n\n        // Token expires in 30 seconds (within the 60-second buffer)\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let token = CopilotToken {\n            token: \"test_token\".to_string(),\n            expires_at: now + 30, // 30 seconds from now\n            refresh_in: 900,\n            organization_list: None,\n        };\n\n        assert!(manager.needs_refresh(&token));\n    }\n\n    #[test]\n    fn test_needs_refresh_false_just_outside_buffer() {\n        let manager = TokenManager::new().unwrap();\n\n        // Token expires in 120 seconds (outside the 60-second buffer)\n        let now = SystemTime::now()\n            .duration_since(UNIX_EPOCH)\n            .unwrap()\n            .as_secs();\n\n        let token = CopilotToken {\n            token: \"test_token\".to_string(),\n            expires_at: now + 120, // 2 minutes from now\n            refresh_in: 900,\n            organization_list: None,\n        };\n\n        assert!(!manager.needs_refresh(&token));\n    }\n\n    // ========================================================================\n    // Token Serialization Tests\n    // ========================================================================\n\n    #[test]\n    fn test_github_token_serialization_roundtrip() {\n        let token = GitHubToken {\n            access_token: \"gho_test_token_12345\".to_string(),\n            created_at: 1699876543,\n        };\n\n        let json = serde_json::to_string(&token).unwrap();\n        let parsed: GitHubToken = serde_json::from_str(&json).unwrap();\n\n        assert_eq!(parsed.access_token, token.access_token);\n        assert_eq!(parsed.created_at, token.created_at);\n    }\n\n    #[test]\n    fn test_copilot_token_serialization_roundtrip() {\n        let token = CopilotToken {\n            token: \"tid=test;exp=1234567890;sku=copilot\".to_string(),\n            expires_at: 1699876543,\n            refresh_in: 900,\n            organization_list: Some(vec![\"org1\".to_string(), \"org2\".to_string()]),\n        };\n\n        let json = serde_json::to_string(&token).unwrap();\n        let parsed: CopilotToken = serde_json::from_str(&json).unwrap();\n\n        assert_eq!(parsed.token, token.token);\n        assert_eq!(parsed.expires_at, token.expires_at);\n        assert_eq!(parsed.refresh_in, token.refresh_in);\n        assert_eq!(parsed.organization_list, token.organization_list);\n    }\n\n    #[test]\n    fn test_copilot_token_without_org_list() {\n        let token = CopilotToken {\n            token: \"test_token\".to_string(),\n            expires_at: 1699876543,\n            refresh_in: 900,\n            organization_list: None,\n        };\n\n        let json = serde_json::to_string(&token).unwrap();\n        let parsed: CopilotToken = serde_json::from_str(&json).unwrap();\n\n        assert_eq!(parsed.organization_list, None);\n    }\n\n    #[test]\n    fn test_github_token_json_format() {\n        let token = GitHubToken {\n            access_token: \"test123\".to_string(),\n            created_at: 1699876543,\n        };\n\n        let json = serde_json::to_string(&token).unwrap();\n\n        assert!(json.contains(\"access_token\"));\n        assert!(json.contains(\"test123\"));\n        assert!(json.contains(\"created_at\"));\n        assert!(json.contains(\"1699876543\"));\n    }\n\n    // ========================================================================\n    // Path Tests\n    // ========================================================================\n\n    #[test]\n    fn test_token_paths_are_distinct() {\n        let manager = TokenManager::new().unwrap();\n\n        let github_path = manager.github_token_path();\n        let copilot_path = manager.copilot_token_path();\n\n        assert_ne!(github_path, copilot_path);\n        assert!(github_path.to_string_lossy().contains(\"github\"));\n        assert!(copilot_path.to_string_lossy().contains(\"copilot\"));\n    }\n\n    #[test]\n    fn test_paths_under_config_dir() {\n        let manager = TokenManager::new().unwrap();\n\n        let github_path = manager.github_token_path();\n        let copilot_path = manager.copilot_token_path();\n\n        assert!(github_path.starts_with(&manager.config_dir));\n        assert!(copilot_path.starts_with(&manager.config_dir));\n    }\n}\n","traces":[{"line":82,"address":[],"length":0,"stats":{"Line":40}},{"line":83,"address":[],"length":0,"stats":{"Line":80}},{"line":85,"address":[],"length":0,"stats":{"Line":80}},{"line":86,"address":[],"length":0,"stats":{"Line":80}},{"line":90,"address":[],"length":0,"stats":{"Line":40}},{"line":94,"address":[],"length":0,"stats":{"Line":40}},{"line":95,"address":[],"length":0,"stats":{"Line":120}},{"line":97,"address":[],"length":0,"stats":{"Line":80}},{"line":98,"address":[],"length":0,"stats":{"Line":40}},{"line":102,"address":[],"length":0,"stats":{"Line":2}},{"line":103,"address":[],"length":0,"stats":{"Line":2}},{"line":107,"address":[],"length":0,"stats":{"Line":2}},{"line":108,"address":[],"length":0,"stats":{"Line":2}},{"line":112,"address":[],"length":0,"stats":{"Line":2}},{"line":113,"address":[],"length":0,"stats":{"Line":2}},{"line":114,"address":[],"length":0,"stats":{"Line":1}},{"line":116,"address":[],"length":0,"stats":{"Line":1}},{"line":120,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":131,"address":[],"length":0,"stats":{"Line":0}},{"line":132,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":142,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":147,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":152,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":157,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":161,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":4}},{"line":175,"address":[],"length":0,"stats":{"Line":12}},{"line":176,"address":[],"length":0,"stats":{"Line":4}},{"line":180,"address":[],"length":0,"stats":{"Line":12}},{"line":181,"address":[],"length":0,"stats":{"Line":4}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":186,"address":[],"length":0,"stats":{"Line":0}},{"line":187,"address":[],"length":0,"stats":{"Line":0}},{"line":188,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":0}},{"line":196,"address":[],"length":0,"stats":{"Line":0}},{"line":197,"address":[],"length":0,"stats":{"Line":0}},{"line":198,"address":[],"length":0,"stats":{"Line":0}},{"line":199,"address":[],"length":0,"stats":{"Line":0}},{"line":210,"address":[],"length":0,"stats":{"Line":0}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":215,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":0}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":0}},{"line":227,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":0}},{"line":232,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":240,"address":[],"length":0,"stats":{"Line":0}},{"line":242,"address":[],"length":0,"stats":{"Line":0}},{"line":243,"address":[],"length":0,"stats":{"Line":0}},{"line":245,"address":[],"length":0,"stats":{"Line":0}},{"line":246,"address":[],"length":0,"stats":{"Line":0}},{"line":247,"address":[],"length":0,"stats":{"Line":0}},{"line":248,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":0}},{"line":252,"address":[],"length":0,"stats":{"Line":0}},{"line":253,"address":[],"length":0,"stats":{"Line":0}},{"line":258,"address":[],"length":0,"stats":{"Line":0}},{"line":262,"address":[],"length":0,"stats":{"Line":0}},{"line":265,"address":[],"length":0,"stats":{"Line":0}},{"line":266,"address":[],"length":0,"stats":{"Line":0}},{"line":267,"address":[],"length":0,"stats":{"Line":0}},{"line":270,"address":[],"length":0,"stats":{"Line":0}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":277,"address":[],"length":0,"stats":{"Line":0}},{"line":278,"address":[],"length":0,"stats":{"Line":0}},{"line":279,"address":[],"length":0,"stats":{"Line":0}},{"line":283,"address":[],"length":0,"stats":{"Line":0}},{"line":287,"address":[],"length":0,"stats":{"Line":0}},{"line":288,"address":[],"length":0,"stats":{"Line":0}},{"line":289,"address":[],"length":0,"stats":{"Line":0}},{"line":290,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":299,"address":[],"length":0,"stats":{"Line":0}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":306,"address":[],"length":0,"stats":{"Line":0}},{"line":310,"address":[],"length":0,"stats":{"Line":0}},{"line":311,"address":[],"length":0,"stats":{"Line":0}},{"line":314,"address":[],"length":0,"stats":{"Line":0}},{"line":327,"address":[],"length":0,"stats":{"Line":0}},{"line":328,"address":[],"length":0,"stats":{"Line":0}},{"line":333,"address":[],"length":0,"stats":{"Line":0}},{"line":334,"address":[],"length":0,"stats":{"Line":0}},{"line":335,"address":[],"length":0,"stats":{"Line":0}},{"line":336,"address":[],"length":0,"stats":{"Line":0}},{"line":337,"address":[],"length":0,"stats":{"Line":0}},{"line":339,"address":[],"length":0,"stats":{"Line":0}}],"covered":22,"coverable":112},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","vscode","types.rs"],"content":"//! Type definitions for VSCode Copilot API.\n//!\n//! These types match the OpenAI-compatible API format used by the Copilot API.\n//!\n//! # Design Rationale\n//!\n//! WHY: We define explicit types rather than using `serde_json::Value` because:\n//! 1. **Type Safety** - Compile-time verification of request/response structure\n//! 2. **Documentation** - Types serve as living documentation of the API\n//! 3. **IDE Support** - Autocomplete and type hints for developers\n//! 4. **Validation** - Invalid data fails at deserialization, not at runtime\n//!\n//! # Type Categories\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    API Type Hierarchy                            â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                   â”‚\n//! â”‚  Request Types (sent to API)                                     â”‚\n//! â”‚  â”œâ”€â”€ ChatCompletionRequest   â†’ /chat/completions                â”‚\n//! â”‚  â”‚   â”œâ”€â”€ RequestMessage      â†’ role + content + tools           â”‚\n//! â”‚  â”‚   â”œâ”€â”€ RequestTool         â†’ function definitions             â”‚\n//! â”‚  â”‚   â””â”€â”€ ResponseFormat      â†’ JSON mode                        â”‚\n//! â”‚  â””â”€â”€ EmbeddingRequest        â†’ /embeddings                      â”‚\n//! â”‚                                                                   â”‚\n//! â”‚  Response Types (received from API)                              â”‚\n//! â”‚  â”œâ”€â”€ ChatCompletionResponse  â†’ Non-streaming response           â”‚\n//! â”‚  â”‚   â”œâ”€â”€ ResponseChoice      â†’ message + finish_reason          â”‚\n//! â”‚  â”‚   â””â”€â”€ Usage               â†’ token counts                     â”‚\n//! â”‚  â”œâ”€â”€ ChatCompletionChunk     â†’ Streaming response               â”‚\n//! â”‚  â”‚   â””â”€â”€ ChunkChoice         â†’ delta content                    â”‚\n//! â”‚  â”œâ”€â”€ EmbeddingResponse       â†’ Embedding vectors                â”‚\n//! â”‚  â””â”€â”€ ModelsResponse          â†’ Available models                 â”‚\n//! â”‚                                                                   â”‚\n//! â”‚  Shared Types                                                     â”‚\n//! â”‚  â”œâ”€â”€ ResponseToolCall        â†’ Tool calls in responses          â”‚\n//! â”‚  â”œâ”€â”€ Model                   â†’ Model metadata                   â”‚\n//! â”‚  â””â”€â”€ ModelCapabilities       â†’ Limits and features              â”‚\n//! â”‚                                                                   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Serialization\n//!\n//! All types use `serde` for JSON serialization with these conventions:\n//! - Optional fields use `#[serde(skip_serializing_if = \"Option::is_none\")]`\n//! - Default values use `#[serde(default)]`\n//! - Renamed fields use `#[serde(rename = \"...\")]`\n//!\n//! # OpenAI Compatibility\n//!\n//! These types are compatible with OpenAI's API format, which is also used by:\n//! - GitHub Copilot API\n//! - Azure OpenAI\n//! - Many open-source LLM servers (vLLM, Ollama, etc.)\n\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value as JsonValue;\n\n/// Chat completion request (OpenAI-compatible format).\n#[derive(Debug, Clone, Serialize, Default)]\npub struct ChatCompletionRequest {\n    /// Array of messages in the conversation.\n    pub messages: Vec<RequestMessage>,\n\n    /// Model identifier (e.g., \"gpt-4o-mini\", \"gpt-4o\").\n    pub model: String,\n\n    /// Sampling temperature (0.0-2.0). Higher = more random.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub temperature: Option<f32>,\n\n    /// Nucleus sampling (top_p). Alternative to temperature.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub top_p: Option<f32>,\n\n    /// Maximum tokens to generate.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub max_tokens: Option<usize>,\n\n    /// Stop sequences.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub stop: Option<Vec<String>>,\n\n    /// Enable streaming responses.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub stream: Option<bool>,\n\n    /// Frequency penalty (-2.0 to 2.0).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub frequency_penalty: Option<f32>,\n\n    /// Presence penalty (-2.0 to 2.0).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub presence_penalty: Option<f32>,\n\n    /// Response format specification.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub response_format: Option<ResponseFormat>,\n\n    /// Tools available for the model to call.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tools: Option<Vec<RequestTool>>,\n\n    /// How the model should select tools.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tool_choice: Option<JsonValue>,\n\n    /// Whether to allow parallel tool calls.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub parallel_tool_calls: Option<bool>,\n}\n\n/// Tool definition for the API request.\n#[derive(Debug, Clone, Serialize)]\npub struct RequestTool {\n    /// Type of tool (always \"function\").\n    #[serde(rename = \"type\")]\n    pub tool_type: String,\n\n    /// Function definition.\n    pub function: RequestFunction,\n}\n\n/// Function definition for tool calling.\n#[derive(Debug, Clone, Serialize)]\npub struct RequestFunction {\n    /// Name of the function.\n    pub name: String,\n\n    /// Description of what the function does.\n    pub description: String,\n\n    /// JSON Schema for the function parameters.\n    pub parameters: JsonValue,\n\n    /// Whether to use strict mode.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub strict: Option<bool>,\n}\n\n// ============================================================================\n// Image Support Types (OODA-55)\n// ============================================================================\n//\n// VS Code Copilot uses OpenAI-compatible format for images:\n//\n// Text only:                        With images:\n// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n// â”‚ content: \"Hello\"        â”‚      â”‚ content: [                      â”‚\n// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   {type: \"text\", text: \"...\"},  â”‚\n//                                  â”‚   {type: \"image_url\",           â”‚\n//                                  â”‚    image_url: {url: \"data:...\"}}â”‚\n//                                  â”‚ ]                               â”‚\n//                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//\n// WHY: Serde untagged allows backward-compatible serialization\n// ============================================================================\n\n/// Request content that can be text or multipart (OODA-55).\n#[derive(Debug, Clone, Serialize, PartialEq)]\n#[serde(untagged)]\npub enum RequestContent {\n    /// Simple text content (backward compatible)\n    Text(String),\n    /// Multipart content with text and images\n    Parts(Vec<ContentPart>),\n}\n\n/// Content part for multipart messages (OODA-55).\n#[derive(Debug, Clone, Serialize, PartialEq)]\n#[serde(tag = \"type\")]\npub enum ContentPart {\n    /// Text content part\n    #[serde(rename = \"text\")]\n    Text { text: String },\n    /// Image URL content part\n    #[serde(rename = \"image_url\")]\n    ImageUrl { image_url: ImageUrlContent },\n}\n\n/// Image URL content (OODA-55).\n#[derive(Debug, Clone, Serialize, PartialEq)]\npub struct ImageUrlContent {\n    /// Data URI or URL of the image\n    pub url: String,\n    /// Detail level: \"auto\", \"low\", or \"high\"\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub detail: Option<String>,\n}\n\n/// A single message in the conversation.\n#[derive(Debug, Clone, Serialize)]\npub struct RequestMessage {\n    /// Role of the message sender.\n    pub role: String,\n\n    /// Content of the message (text or multipart with images).\n    /// OODA-55: Changed from `Option<String>` to `Option<RequestContent>`\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub content: Option<RequestContent>,\n\n    /// Optional name for the sender.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub name: Option<String>,\n\n    /// Tool calls made by the assistant.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tool_calls: Option<Vec<ResponseToolCall>>,\n\n    /// Tool call ID (for tool role messages).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tool_call_id: Option<String>,\n\n    /// Cache control hint (for Anthropic Claude via VSCode proxy).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub cache_control: Option<RequestCacheControl>,\n}\n\n/// Cache control hint for prompt caching (Anthropic Claude).\n#[derive(Debug, Clone, Serialize)]\npub struct RequestCacheControl {\n    /// Cache type (e.g., \"ephemeral\").\n    #[serde(rename = \"type\")]\n    pub cache_type: String,\n}\n\n/// Response format specification.\n#[derive(Debug, Clone, Serialize)]\npub struct ResponseFormat {\n    /// Format type: \"text\" or \"json_object\".\n    #[serde(rename = \"type\")]\n    pub format_type: String,\n}\n\n/// Chat completion response (non-streaming).\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct ChatCompletionResponse {\n    /// Unique identifier for the completion.\n    pub id: String,\n\n    /// Object type (always \"chat.completion\").\n    #[serde(default)]\n    pub object: Option<String>,\n\n    /// Unix timestamp of creation.\n    #[serde(default)]\n    pub created: Option<u64>,\n\n    /// Model used for generation.\n    pub model: String,\n\n    /// Array of completion choices.\n    pub choices: Vec<Choice>,\n\n    /// Token usage statistics.\n    pub usage: Option<Usage>,\n\n    /// Extra fields we don't use but need for deserialization compatibility.\n    #[serde(flatten)]\n    pub extra: Option<serde_json::Value>,\n}\n\n/// A single completion choice.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct Choice {\n    /// Index of this choice (optional - Anthropic models omit this field entirely).\n    #[serde(default)]\n    pub index: Option<usize>,\n\n    /// The generated message.\n    pub message: ResponseMessage,\n\n    /// Reason for completion stop.\n    pub finish_reason: Option<String>,\n\n    /// Extra fields like content_filter_results.\n    #[serde(flatten)]\n    pub extra: Option<serde_json::Value>,\n}\n\n/// Response message from the assistant.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct ResponseMessage {\n    /// Role (usually \"assistant\").\n    pub role: String,\n\n    /// Generated content.\n    pub content: Option<String>,\n\n    /// Tool calls made by the assistant.\n    #[serde(default)]\n    pub tool_calls: Option<Vec<ResponseToolCall>>,\n\n    /// Extra fields like padding.\n    #[serde(flatten)]\n    pub extra: Option<serde_json::Value>,\n}\n\n/// Tool call in the response.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ResponseToolCall {\n    /// Unique identifier for this tool call.\n    pub id: String,\n\n    /// Type of tool (always \"function\").\n    #[serde(rename = \"type\")]\n    pub call_type: String,\n\n    /// Function call details.\n    pub function: ResponseFunctionCall,\n}\n\n/// Function call details in a tool call.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ResponseFunctionCall {\n    /// Name of the function.\n    pub name: String,\n\n    /// JSON-encoded arguments.\n    pub arguments: String,\n}\n\n/// Prompt token details including cache statistics (OODA-24).\n///\n/// WHY: KV cache hits are 10x cheaper. Tracking cached_tokens enables:\n/// - Cache hit rate monitoring\n/// - Prompt structure optimization\n/// - Accurate cost calculations\n#[derive(Debug, Clone, Deserialize, Default)]\npub struct PromptTokensDetails {\n    /// Number of tokens served from KV cache.\n    /// When present, indicates the prompt prefix was cached.\n    #[serde(default)]\n    pub cached_tokens: Option<usize>,\n}\n\n/// Token usage statistics.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct Usage {\n    /// Tokens in the prompt.\n    pub prompt_tokens: usize,\n\n    /// Tokens in the completion.\n    pub completion_tokens: usize,\n\n    /// Total tokens used.\n    pub total_tokens: usize,\n\n    /// Breakdown of prompt tokens for cache tracking (OODA-24).\n    #[serde(default)]\n    pub prompt_tokens_details: Option<PromptTokensDetails>,\n\n    /// Extra fields like completion_tokens_details.\n    #[serde(flatten)]\n    pub extra: Option<serde_json::Value>,\n}\n\n/// Streaming chunk.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct ChatCompletionChunk {\n    /// Unique identifier.\n    pub id: String,\n\n    /// Object type (always \"chat.completion.chunk\").\n    #[serde(default)]\n    pub object: Option<String>,\n\n    /// Unix timestamp.\n    #[serde(default)]\n    pub created: Option<u64>,\n\n    /// Model used.\n    #[serde(default)]\n    pub model: Option<String>,\n\n    /// Array of delta choices.\n    pub choices: Vec<ChunkChoice>,\n\n    /// Optional usage stats (usually in last chunk).\n    #[serde(default)]\n    pub usage: Option<Usage>,\n}\n\n/// A single chunk choice.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct ChunkChoice {\n    /// Index of this choice (optional - Anthropic models omit this field entirely).\n    #[serde(default)]\n    pub index: Option<usize>,\n\n    /// Delta content.\n    pub delta: Delta,\n\n    /// Finish reason (if complete).\n    pub finish_reason: Option<String>,\n}\n\n/// Delta content in a streaming chunk.\n/// \n/// OODA-05: Added tool_calls field for streaming tool call support.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct Delta {\n    /// Role (present in first chunk).\n    pub role: Option<String>,\n\n    /// Content delta.\n    pub content: Option<String>,\n    \n    /// Tool calls delta (OODA-05).\n    /// \n    /// During streaming, tool calls arrive incrementally:\n    /// 1. First chunk: index + id + function.name\n    /// 2. Subsequent chunks: function.arguments (partial JSON)\n    #[serde(default)]\n    pub tool_calls: Option<Vec<DeltaToolCall>>,\n}\n\n/// Tool call delta in streaming responses (OODA-05).\n/// \n/// Tool calls stream incrementally:\n/// ```text\n/// Chunk 1: {index: 0, id: \"call_abc\", function: {name: \"write_file\"}}\n/// Chunk 2: {index: 0, function: {arguments: \"{\\\"path\\\":\"}}\n/// Chunk 3: {index: 0, function: {arguments: \" \\\"./test.py\\\"\"}}\n/// ...\n/// ```\n#[derive(Debug, Clone, Deserialize)]\npub struct DeltaToolCall {\n    /// Tool call index (for parallel calls).\n    pub index: usize,\n    \n    /// Tool call ID (present in first chunk for this tool).\n    #[serde(default)]\n    pub id: Option<String>,\n    \n    /// Tool type (always \"function\").\n    #[serde(rename = \"type\", default)]\n    pub tool_type: Option<String>,\n    \n    /// Function details (partial).\n    #[serde(default)]\n    pub function: Option<DeltaFunction>,\n}\n\n/// Partial function data in streaming (OODA-05).\n#[derive(Debug, Clone, Deserialize)]\npub struct DeltaFunction {\n    /// Function name (present in first chunk).\n    #[serde(default)]\n    pub name: Option<String>,\n    \n    /// Function arguments (partial JSON, accumulates across chunks).\n    #[serde(default)]\n    pub arguments: Option<String>,\n}\n\n// ============================================================================\n// Models API Types\n// ============================================================================\n\n/// Models list response.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct ModelsResponse {\n    /// Array of available models.\n    pub data: Vec<Model>,\n\n    /// Object type.\n    #[serde(default)]\n    pub object: Option<String>,\n}\n\n/// Model limits configuration.\n#[derive(Debug, Clone, Default, Deserialize)]\n#[allow(dead_code)]\npub struct ModelLimits {\n    /// Maximum context window tokens.\n    #[serde(default)]\n    pub max_context_window_tokens: Option<usize>,\n\n    /// Maximum output tokens.\n    #[serde(default)]\n    pub max_output_tokens: Option<usize>,\n\n    /// Maximum prompt tokens.\n    #[serde(default)]\n    pub max_prompt_tokens: Option<usize>,\n\n    /// Maximum inputs (for embeddings).\n    #[serde(default)]\n    pub max_inputs: Option<usize>,\n}\n\n/// Model supported features.\n#[derive(Debug, Clone, Default, Deserialize)]\n#[allow(dead_code)]\npub struct ModelSupports {\n    /// Whether tool/function calls are supported.\n    #[serde(default)]\n    pub tool_calls: Option<bool>,\n\n    /// Whether parallel tool calls are supported.\n    #[serde(default)]\n    pub parallel_tool_calls: Option<bool>,\n\n    /// Whether custom dimensions are supported (for embeddings).\n    #[serde(default)]\n    pub dimensions: Option<bool>,\n}\n\n/// Model capabilities.\n#[derive(Debug, Clone, Default, Deserialize)]\n#[allow(dead_code)]\npub struct ModelCapabilities {\n    /// Model family (e.g., \"gpt-4o\", \"gpt-4\").\n    #[serde(default)]\n    pub family: Option<String>,\n\n    /// Model limits.\n    #[serde(default)]\n    pub limits: ModelLimits,\n\n    /// Object type.\n    #[serde(default)]\n    pub object: Option<String>,\n\n    /// Supported features.\n    #[serde(default)]\n    pub supports: ModelSupports,\n\n    /// Tokenizer name.\n    #[serde(default)]\n    pub tokenizer: Option<String>,\n\n    /// Model type (e.g., \"chat\", \"embeddings\").\n    #[serde(default, rename = \"type\")]\n    pub model_type: Option<String>,\n}\n\n/// Model policy information.\n#[derive(Debug, Clone, Default, Deserialize)]\n#[allow(dead_code)]\npub struct ModelPolicy {\n    /// Policy state.\n    #[serde(default)]\n    pub state: Option<String>,\n\n    /// Terms of use.\n    #[serde(default)]\n    pub terms: Option<String>,\n}\n\n/// A single model from the Copilot API.\n///\n/// This struct matches the full model schema from GitHub Copilot API,\n/// including capabilities, limits, and supported features.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct Model {\n    /// Model identifier (e.g., \"gpt-4o\", \"gpt-4o-mini\").\n    pub id: String,\n\n    /// Object type (usually \"model\").\n    #[serde(default)]\n    pub object: Option<String>,\n\n    /// Human-readable name.\n    #[serde(default)]\n    pub name: Option<String>,\n\n    /// Model vendor (e.g., \"azure-openai\").\n    #[serde(default)]\n    pub vendor: Option<String>,\n\n    /// Model version.\n    #[serde(default)]\n    pub version: Option<String>,\n\n    /// Model capabilities including limits and supported features.\n    #[serde(default)]\n    pub capabilities: Option<ModelCapabilities>,\n\n    /// Whether model appears in model picker.\n    #[serde(default)]\n    pub model_picker_enabled: Option<bool>,\n\n    /// Whether this is a preview model.\n    #[serde(default)]\n    pub preview: Option<bool>,\n\n    /// Model policy.\n    #[serde(default)]\n    pub policy: Option<ModelPolicy>,\n\n    // Legacy fields for backward compatibility\n    /// Creation timestamp (legacy OpenAI format).\n    #[serde(default)]\n    pub created: Option<u64>,\n\n    /// Owner of the model (legacy OpenAI format).\n    #[serde(default)]\n    pub owned_by: Option<String>,\n}\n\nimpl Model {\n    /// Get the maximum context window tokens for this model.\n    pub fn max_context_tokens(&self) -> Option<usize> {\n        self.capabilities\n            .as_ref()\n            .and_then(|c| c.limits.max_context_window_tokens)\n    }\n\n    /// Get the maximum output tokens for this model.\n    pub fn max_output_tokens(&self) -> Option<usize> {\n        self.capabilities\n            .as_ref()\n            .and_then(|c| c.limits.max_output_tokens)\n    }\n\n    /// Check if this model supports tool calls.\n    pub fn supports_tools(&self) -> bool {\n        self.capabilities\n            .as_ref()\n            .and_then(|c| c.supports.tool_calls)\n            .unwrap_or(false)\n    }\n}\n\n// ============================================================================\n// Embeddings API Types\n// ============================================================================\n\n/// Input for embedding request - can be a single string or array.\n#[derive(Debug, Clone, Serialize)]\n#[serde(untagged)]\npub enum EmbeddingInput {\n    /// Single text input.\n    Single(String),\n    /// Multiple text inputs.\n    Multiple(Vec<String>),\n}\n\nimpl From<String> for EmbeddingInput {\n    fn from(s: String) -> Self {\n        EmbeddingInput::Single(s)\n    }\n}\n\nimpl From<&str> for EmbeddingInput {\n    fn from(s: &str) -> Self {\n        EmbeddingInput::Single(s.to_string())\n    }\n}\n\nimpl From<Vec<String>> for EmbeddingInput {\n    fn from(v: Vec<String>) -> Self {\n        EmbeddingInput::Multiple(v)\n    }\n}\n\n/// Embedding request.\n#[derive(Debug, Clone, Serialize)]\npub struct EmbeddingRequest {\n    /// Input text(s) to embed.\n    pub input: EmbeddingInput,\n\n    /// Model to use for embeddings.\n    pub model: String,\n}\n\nimpl EmbeddingRequest {\n    /// Create a new embedding request for a single input.\n    pub fn new(input: impl Into<EmbeddingInput>, model: impl Into<String>) -> Self {\n        Self {\n            input: input.into(),\n            model: model.into(),\n        }\n    }\n}\n\n/// A single embedding result.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct Embedding {\n    /// Object type (always \"embedding\").\n    pub object: String,\n\n    /// The embedding vector.\n    pub embedding: Vec<f32>,\n\n    /// Index of this embedding in the request.\n    pub index: usize,\n}\n\n/// Usage statistics for embedding request.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct EmbeddingUsage {\n    /// Tokens in the prompt.\n    pub prompt_tokens: usize,\n\n    /// Total tokens used.\n    pub total_tokens: usize,\n}\n\n/// Embedding response.\n#[derive(Debug, Clone, Deserialize)]\n#[allow(dead_code)]\npub struct EmbeddingResponse {\n    /// Object type (always \"list\").\n    pub object: String,\n\n    /// Array of embeddings.\n    pub data: Vec<Embedding>,\n\n    /// Model used for embeddings.\n    pub model: String,\n\n    /// Usage statistics.\n    pub usage: EmbeddingUsage,\n}\n\nimpl EmbeddingResponse {\n    /// Get the first embedding vector, if available.\n    pub fn first_embedding(&self) -> Option<&Vec<f32>> {\n        self.data.first().map(|e| &e.embedding)\n    }\n\n    /// Get all embedding vectors.\n    pub fn embeddings(&self) -> Vec<&Vec<f32>> {\n        self.data.iter().map(|e| &e.embedding).collect()\n    }\n\n    /// Get the embedding dimension.\n    pub fn dimension(&self) -> Option<usize> {\n        self.data.first().map(|e| e.embedding.len())\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_embedding_input_from_string() {\n        let input: EmbeddingInput = \"hello world\".into();\n        match input {\n            EmbeddingInput::Single(s) => assert_eq!(s, \"hello world\"),\n            _ => panic!(\"Expected Single variant\"),\n        }\n    }\n\n    #[test]\n    fn test_embedding_input_from_owned_string() {\n        let input: EmbeddingInput = String::from(\"hello world\").into();\n        match input {\n            EmbeddingInput::Single(s) => assert_eq!(s, \"hello world\"),\n            _ => panic!(\"Expected Single variant\"),\n        }\n    }\n\n    #[test]\n    fn test_embedding_input_from_vec() {\n        let texts = vec![\"hello\".to_string(), \"world\".to_string()];\n        let input: EmbeddingInput = texts.into();\n        match input {\n            EmbeddingInput::Multiple(v) => {\n                assert_eq!(v.len(), 2);\n                assert_eq!(v[0], \"hello\");\n                assert_eq!(v[1], \"world\");\n            }\n            _ => panic!(\"Expected Multiple variant\"),\n        }\n    }\n\n    #[test]\n    fn test_embedding_request_serialization() {\n        let request = EmbeddingRequest::new(\"test input\", \"text-embedding-3-small\");\n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"\\\"input\\\":\\\"test input\\\"\"));\n        assert!(json.contains(\"\\\"model\\\":\\\"text-embedding-3-small\\\"\"));\n    }\n\n    #[test]\n    fn test_embedding_request_multiple_inputs() {\n        let input = EmbeddingInput::Multiple(vec![\"one\".to_string(), \"two\".to_string()]);\n        let request = EmbeddingRequest::new(input, \"model\");\n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"[\\\"one\\\",\\\"two\\\"]\"));\n    }\n\n    #[test]\n    fn test_embedding_response_deserialization() {\n        let json = r#\"{\n            \"object\": \"list\",\n            \"data\": [\n                {\n                    \"object\": \"embedding\",\n                    \"embedding\": [0.1, 0.2, 0.3],\n                    \"index\": 0\n                }\n            ],\n            \"model\": \"text-embedding-3-small\",\n            \"usage\": {\n                \"prompt_tokens\": 5,\n                \"total_tokens\": 5\n            }\n        }\"#;\n\n        let response: EmbeddingResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.object, \"list\");\n        assert_eq!(response.model, \"text-embedding-3-small\");\n        assert_eq!(response.data.len(), 1);\n        assert_eq!(response.data[0].embedding, vec![0.1, 0.2, 0.3]);\n        assert_eq!(response.usage.prompt_tokens, 5);\n    }\n\n    #[test]\n    fn test_embedding_response_helpers() {\n        let json = r#\"{\n            \"object\": \"list\",\n            \"data\": [\n                {\"object\": \"embedding\", \"embedding\": [1.0, 2.0], \"index\": 0},\n                {\"object\": \"embedding\", \"embedding\": [3.0, 4.0], \"index\": 1}\n            ],\n            \"model\": \"test\",\n            \"usage\": {\"prompt_tokens\": 10, \"total_tokens\": 10}\n        }\"#;\n\n        let response: EmbeddingResponse = serde_json::from_str(json).unwrap();\n\n        // Test first_embedding\n        let first = response.first_embedding().unwrap();\n        assert_eq!(*first, vec![1.0, 2.0]);\n\n        // Test dimension\n        assert_eq!(response.dimension(), Some(2));\n\n        // Test embeddings\n        let all = response.embeddings();\n        assert_eq!(all.len(), 2);\n    }\n\n    #[test]\n    fn test_model_deserialization_full() {\n        let json = r#\"{\n            \"id\": \"gpt-4o\",\n            \"object\": \"model\",\n            \"name\": \"GPT-4o\",\n            \"vendor\": \"azure-openai\",\n            \"version\": \"2024-08-06\",\n            \"capabilities\": {\n                \"family\": \"gpt-4o\",\n                \"limits\": {\n                    \"max_context_window_tokens\": 128000,\n                    \"max_output_tokens\": 16384\n                },\n                \"supports\": {\n                    \"tool_calls\": true,\n                    \"parallel_tool_calls\": true\n                },\n                \"tokenizer\": \"cl100k_base\",\n                \"type\": \"chat\"\n            },\n            \"model_picker_enabled\": true,\n            \"preview\": false\n        }\"#;\n\n        let model: Model = serde_json::from_str(json).unwrap();\n        assert_eq!(model.id, \"gpt-4o\");\n        assert_eq!(model.name, Some(\"GPT-4o\".to_string()));\n        assert_eq!(model.vendor, Some(\"azure-openai\".to_string()));\n        assert!(model.supports_tools());\n        assert_eq!(model.max_context_tokens(), Some(128000));\n        assert_eq!(model.max_output_tokens(), Some(16384));\n    }\n\n    #[test]\n    fn test_model_deserialization_minimal() {\n        // Test with minimal fields (OpenAI format)\n        let json = r#\"{\n            \"id\": \"gpt-4\",\n            \"object\": \"model\",\n            \"created\": 1687882410,\n            \"owned_by\": \"openai\"\n        }\"#;\n\n        let model: Model = serde_json::from_str(json).unwrap();\n        assert_eq!(model.id, \"gpt-4\");\n        assert_eq!(model.created, Some(1687882410));\n        assert_eq!(model.owned_by, Some(\"openai\".to_string()));\n        assert!(!model.supports_tools()); // No capabilities = no tools\n    }\n\n    #[test]\n    fn test_model_limits() {\n        let limits = ModelLimits {\n            max_context_window_tokens: Some(128000),\n            max_output_tokens: Some(4096),\n            max_prompt_tokens: None,\n            max_inputs: None,\n        };\n\n        assert_eq!(limits.max_context_window_tokens, Some(128000));\n        assert_eq!(limits.max_output_tokens, Some(4096));\n    }\n\n    #[test]\n    fn test_models_response_deserialization() {\n        let json = r#\"{\n            \"data\": [\n                {\"id\": \"gpt-4o\", \"object\": \"model\"},\n                {\"id\": \"gpt-4o-mini\", \"object\": \"model\"}\n            ]\n        }\"#;\n\n        let response: ModelsResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.data.len(), 2);\n        assert_eq!(response.data[0].id, \"gpt-4o\");\n        assert_eq!(response.data[1].id, \"gpt-4o-mini\");\n    }\n\n    // ========================================================================\n    // Chat Completion Response Fixture Tests\n    // ========================================================================\n\n    #[test]\n    fn test_chat_completion_response_deserialization() {\n        let json = r#\"{\n            \"id\": \"chatcmpl-ABC123\",\n            \"object\": \"chat.completion\",\n            \"created\": 1699876543,\n            \"model\": \"gpt-4o-mini\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\n                        \"role\": \"assistant\",\n                        \"content\": \"Hello! How can I help you today?\"\n                    },\n                    \"finish_reason\": \"stop\"\n                }\n            ],\n            \"usage\": {\n                \"prompt_tokens\": 12,\n                \"completion_tokens\": 8,\n                \"total_tokens\": 20\n            }\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.id, \"chatcmpl-ABC123\");\n        assert_eq!(response.object, Some(\"chat.completion\".to_string()));\n        assert_eq!(response.created, Some(1699876543));\n        assert_eq!(response.model, \"gpt-4o-mini\");\n        assert_eq!(response.choices.len(), 1);\n\n        let choice = &response.choices[0];\n        assert_eq!(choice.index, Some(0));\n        assert_eq!(choice.message.role, \"assistant\");\n        assert_eq!(\n            choice.message.content,\n            Some(\"Hello! How can I help you today?\".to_string())\n        );\n        assert_eq!(choice.finish_reason, Some(\"stop\".to_string()));\n\n        let usage = response.usage.as_ref().unwrap();\n        assert_eq!(usage.prompt_tokens, 12);\n        assert_eq!(usage.completion_tokens, 8);\n        assert_eq!(usage.total_tokens, 20);\n    }\n\n    #[test]\n    fn test_chat_completion_response_with_tool_calls() {\n        let json = r#\"{\n            \"id\": \"chatcmpl-tool123\",\n            \"object\": \"chat.completion\",\n            \"created\": 1699876543,\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\n                        \"role\": \"assistant\",\n                        \"content\": null,\n                        \"tool_calls\": [\n                            {\n                                \"id\": \"call_abc123\",\n                                \"type\": \"function\",\n                                \"function\": {\n                                    \"name\": \"get_weather\",\n                                    \"arguments\": \"{\\\"location\\\": \\\"San Francisco\\\"}\"\n                                }\n                            }\n                        ]\n                    },\n                    \"finish_reason\": \"tool_calls\"\n                }\n            ],\n            \"usage\": {\n                \"prompt_tokens\": 50,\n                \"completion_tokens\": 25,\n                \"total_tokens\": 75\n            }\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.id, \"chatcmpl-tool123\");\n\n        let choice = &response.choices[0];\n        assert_eq!(choice.finish_reason, Some(\"tool_calls\".to_string()));\n        assert!(choice.message.content.is_none());\n\n        let tool_calls = choice.message.tool_calls.as_ref().unwrap();\n        assert_eq!(tool_calls.len(), 1);\n        assert_eq!(tool_calls[0].id, \"call_abc123\");\n        assert_eq!(tool_calls[0].call_type, \"function\");\n        assert_eq!(tool_calls[0].function.name, \"get_weather\");\n        assert!(tool_calls[0].function.arguments.contains(\"San Francisco\"));\n    }\n\n    #[test]\n    fn test_chat_completion_response_multiple_choices() {\n        let json = r#\"{\n            \"id\": \"chatcmpl-multi\",\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\"role\": \"assistant\", \"content\": \"Option A\"},\n                    \"finish_reason\": \"stop\"\n                },\n                {\n                    \"index\": 1,\n                    \"message\": {\"role\": \"assistant\", \"content\": \"Option B\"},\n                    \"finish_reason\": \"stop\"\n                }\n            ],\n            \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 10, \"total_tokens\": 20}\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.choices.len(), 2);\n        assert_eq!(\n            response.choices[0].message.content,\n            Some(\"Option A\".to_string())\n        );\n        assert_eq!(\n            response.choices[1].message.content,\n            Some(\"Option B\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_chat_completion_response_with_extra_fields() {\n        // Test that we handle extra fields from Copilot API gracefully\n        let json = r#\"{\n            \"id\": \"chatcmpl-extra\",\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"message\": {\"role\": \"assistant\", \"content\": \"Hello\"},\n                    \"finish_reason\": \"stop\",\n                    \"content_filter_results\": {\n                        \"hate\": {\"filtered\": false}\n                    }\n                }\n            ],\n            \"usage\": {\"prompt_tokens\": 5, \"completion_tokens\": 1, \"total_tokens\": 6},\n            \"system_fingerprint\": \"fp_abc123\"\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.id, \"chatcmpl-extra\");\n        assert_eq!(\n            response.choices[0].message.content,\n            Some(\"Hello\".to_string())\n        );\n        // Extra fields should be captured but not cause deserialization to fail\n    }\n\n    // ========================================================================\n    // Streaming Chunk Fixture Tests\n    // ========================================================================\n\n    #[test]\n    fn test_streaming_chunk_first_chunk() {\n        // First chunk typically contains role\n        let json = r#\"{\n            \"id\": \"chatcmpl-stream123\",\n            \"object\": \"chat.completion.chunk\",\n            \"created\": 1699876543,\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"delta\": {\n                        \"role\": \"assistant\",\n                        \"content\": \"\"\n                    },\n                    \"finish_reason\": null\n                }\n            ]\n        }\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n        assert_eq!(chunk.id, \"chatcmpl-stream123\");\n        assert_eq!(chunk.object, Some(\"chat.completion.chunk\".to_string()));\n        assert_eq!(chunk.model, Some(\"gpt-4o\".to_string()));\n\n        let choice = &chunk.choices[0];\n        assert_eq!(choice.delta.role, Some(\"assistant\".to_string()));\n        assert_eq!(choice.delta.content, Some(\"\".to_string()));\n        assert!(choice.finish_reason.is_none());\n    }\n\n    #[test]\n    fn test_streaming_chunk_content_delta() {\n        // Middle chunks contain content\n        let json = r#\"{\n            \"id\": \"chatcmpl-stream123\",\n            \"object\": \"chat.completion.chunk\",\n            \"created\": 1699876543,\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"delta\": {\n                        \"content\": \"Hello\"\n                    },\n                    \"finish_reason\": null\n                }\n            ]\n        }\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n        let choice = &chunk.choices[0];\n        assert!(choice.delta.role.is_none());\n        assert_eq!(choice.delta.content, Some(\"Hello\".to_string()));\n    }\n\n    #[test]\n    fn test_streaming_chunk_final_chunk() {\n        // Final chunk has finish_reason and possibly usage\n        let json = r#\"{\n            \"id\": \"chatcmpl-stream123\",\n            \"object\": \"chat.completion.chunk\",\n            \"created\": 1699876543,\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"delta\": {},\n                    \"finish_reason\": \"stop\"\n                }\n            ],\n            \"usage\": {\n                \"prompt_tokens\": 20,\n                \"completion_tokens\": 50,\n                \"total_tokens\": 70\n            }\n        }\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n        let choice = &chunk.choices[0];\n        assert!(choice.delta.content.is_none());\n        assert_eq!(choice.finish_reason, Some(\"stop\".to_string()));\n\n        let usage = chunk.usage.as_ref().unwrap();\n        assert_eq!(usage.total_tokens, 70);\n    }\n\n    #[test]\n    fn test_streaming_chunk_empty_delta() {\n        // Sometimes delta is completely empty\n        let json = r#\"{\n            \"id\": \"chatcmpl-stream123\",\n            \"model\": \"gpt-4o\",\n            \"choices\": [\n                {\n                    \"index\": 0,\n                    \"delta\": {},\n                    \"finish_reason\": null\n                }\n            ]\n        }\"#;\n\n        let chunk: ChatCompletionChunk = serde_json::from_str(json).unwrap();\n        let choice = &chunk.choices[0];\n        assert!(choice.delta.role.is_none());\n        assert!(choice.delta.content.is_none());\n    }\n\n    // ========================================================================\n    // Request Serialization Tests\n    // ========================================================================\n\n    #[test]\n    fn test_chat_completion_request_minimal() {\n        let request = ChatCompletionRequest {\n            messages: vec![RequestMessage {\n                role: \"user\".to_string(),\n                content: Some(RequestContent::Text(\"Hello\".to_string())),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n            }],\n            model: \"gpt-4o\".to_string(),\n            ..Default::default()\n        };\n\n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"\\\"role\\\":\\\"user\\\"\"));\n        assert!(json.contains(\"\\\"Hello\\\"\"));\n        assert!(json.contains(\"\\\"model\\\":\\\"gpt-4o\\\"\"));\n        // Optional fields should not appear\n        assert!(!json.contains(\"temperature\"));\n        assert!(!json.contains(\"max_tokens\"));\n    }\n\n    #[test]\n    fn test_chat_completion_request_with_options() {\n        let request = ChatCompletionRequest {\n            messages: vec![RequestMessage {\n                role: \"user\".to_string(),\n                content: Some(RequestContent::Text(\"Hello\".to_string())),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n            }],\n            model: \"gpt-4o\".to_string(),\n            temperature: Some(0.7),\n            max_tokens: Some(1000),\n            stream: Some(true),\n            ..Default::default()\n        };\n\n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"\\\"temperature\\\":0.7\"));\n        assert!(json.contains(\"\\\"max_tokens\\\":1000\"));\n        assert!(json.contains(\"\\\"stream\\\":true\"));\n    }\n\n    #[test]\n    fn test_chat_completion_request_with_tools() {\n        let request = ChatCompletionRequest {\n            messages: vec![RequestMessage {\n                role: \"user\".to_string(),\n                content: Some(RequestContent::Text(\"What's the weather?\".to_string())),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n            }],\n            model: \"gpt-4o\".to_string(),\n            tools: Some(vec![RequestTool {\n                tool_type: \"function\".to_string(),\n                function: RequestFunction {\n                    name: \"get_weather\".to_string(),\n                    description: \"Get weather info\".to_string(),\n                    parameters: serde_json::json!({\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"location\": {\"type\": \"string\"}\n                        },\n                        \"required\": [\"location\"]\n                    }),\n                    strict: Some(true),\n                },\n            }]),\n            ..Default::default()\n        };\n\n        let json = serde_json::to_string(&request).unwrap();\n        assert!(json.contains(\"\\\"tools\\\"\"));\n        assert!(json.contains(\"\\\"get_weather\\\"\"));\n        assert!(json.contains(\"\\\"strict\\\":true\"));\n    }\n\n    #[test]\n    fn test_response_format_serialization() {\n        let format = ResponseFormat {\n            format_type: \"json_object\".to_string(),\n        };\n\n        let json = serde_json::to_string(&format).unwrap();\n        assert!(json.contains(\"\\\"type\\\":\\\"json_object\\\"\"));\n    }\n\n    // ========================================================================\n    // Tool Call Serialization/Deserialization Tests\n    // ========================================================================\n\n    #[test]\n    fn test_tool_call_roundtrip() {\n        let tool_call = ResponseToolCall {\n            id: \"call_123\".to_string(),\n            call_type: \"function\".to_string(),\n            function: ResponseFunctionCall {\n                name: \"test_func\".to_string(),\n                arguments: \"{\\\"arg\\\": \\\"value\\\"}\".to_string(),\n            },\n        };\n\n        // Serialize\n        let json = serde_json::to_string(&tool_call).unwrap();\n        assert!(json.contains(\"\\\"id\\\":\\\"call_123\\\"\"));\n        assert!(json.contains(\"\\\"type\\\":\\\"function\\\"\"));\n\n        // Deserialize\n        let parsed: ResponseToolCall = serde_json::from_str(&json).unwrap();\n        assert_eq!(parsed.id, \"call_123\");\n        assert_eq!(parsed.function.name, \"test_func\");\n    }\n\n    // ========================================================================\n    // Model Capabilities Edge Cases\n    // ========================================================================\n\n    #[test]\n    fn test_model_with_embedding_capabilities() {\n        let json = r#\"{\n            \"id\": \"text-embedding-3-small\",\n            \"object\": \"model\",\n            \"name\": \"Text Embedding 3 Small\",\n            \"capabilities\": {\n                \"family\": \"text-embedding-3\",\n                \"limits\": {\n                    \"max_inputs\": 2048\n                },\n                \"supports\": {\n                    \"dimensions\": true\n                },\n                \"type\": \"embeddings\"\n            }\n        }\"#;\n\n        let model: Model = serde_json::from_str(json).unwrap();\n        assert_eq!(model.id, \"text-embedding-3-small\");\n\n        let caps = model.capabilities.as_ref().unwrap();\n        assert_eq!(caps.model_type, Some(\"embeddings\".to_string()));\n        assert_eq!(caps.limits.max_inputs, Some(2048));\n        assert_eq!(caps.supports.dimensions, Some(true));\n    }\n\n    #[test]\n    fn test_model_with_policy() {\n        let json = r#\"{\n            \"id\": \"claude-3.5-sonnet\",\n            \"object\": \"model\",\n            \"policy\": {\n                \"state\": \"active\",\n                \"terms\": \"https://example.com/terms\"\n            }\n        }\"#;\n\n        let model: Model = serde_json::from_str(json).unwrap();\n        let policy = model.policy.as_ref().unwrap();\n        assert_eq!(policy.state, Some(\"active\".to_string()));\n        assert!(policy.terms.as_ref().unwrap().contains(\"terms\"));\n    }\n\n    // ========================================================================\n    // Request JSON Format Tests (Iteration 30)\n    // ========================================================================\n    // WHY: These tests verify the exact JSON structure matches GitHub Copilot API\n    // expectations. The API is sensitive to field order and presence of null values.\n\n    #[test]\n    fn test_chat_request_json_format() {\n        // WHY: Verify minimal request produces clean JSON without null fields\n        // The Copilot API rejects requests with unexpected null values\n        let request = ChatCompletionRequest {\n            messages: vec![RequestMessage {\n                role: \"user\".to_string(),\n                content: Some(RequestContent::Text(\"Test\".to_string())),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n            }],\n            model: \"gpt-4o\".to_string(),\n            ..Default::default()\n        };\n\n        let json = serde_json::to_string(&request).unwrap();\n\n        // Verify required fields are present\n        assert!(json.contains(\"\\\"messages\\\"\"));\n        assert!(json.contains(\"\\\"model\\\":\\\"gpt-4o\\\"\"));\n\n        // Verify None fields are not serialized (skip_serializing_if)\n        // This is critical for Copilot API compatibility\n        assert!(!json.contains(\"\\\"temperature\\\":null\"));\n        assert!(!json.contains(\"\\\"max_tokens\\\":null\"));\n        assert!(!json.contains(\"\\\"tools\\\":null\"));\n        assert!(!json.contains(\"\\\"stop\\\":null\"));\n    }\n\n    #[test]\n    fn test_chat_request_with_tools_format() {\n        // WHY: Verify tool-calling requests include proper \"type\" field structure\n        // The Copilot API requires tools to have type: \"function\" explicitly\n        let request = ChatCompletionRequest {\n            messages: vec![RequestMessage {\n                role: \"user\".to_string(),\n                content: Some(RequestContent::Text(\"Search for files\".to_string())),\n                name: None,\n                tool_calls: None,\n                tool_call_id: None,\n                cache_control: None,\n            }],\n            model: \"gpt-4o\".to_string(),\n            tools: Some(vec![RequestTool {\n                tool_type: \"function\".to_string(),\n                function: RequestFunction {\n                    name: \"file_search\".to_string(),\n                    description: \"Search for files in workspace\".to_string(),\n                    parameters: serde_json::json!({\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"query\": { \"type\": \"string\" }\n                        }\n                    }),\n                    strict: None,\n                },\n            }]),\n            ..Default::default()\n        };\n\n        let json = serde_json::to_string(&request).unwrap();\n\n        // Verify tool structure matches Copilot API expectation\n        assert!(json.contains(\"\\\"type\\\":\\\"function\\\"\"));\n        assert!(json.contains(\"\\\"name\\\":\\\"file_search\\\"\"));\n        assert!(json.contains(\"\\\"description\\\":\\\"Search for files in workspace\\\"\"));\n        assert!(json.contains(\"\\\"parameters\\\"\"));\n\n        // Verify strict field is omitted when None\n        assert!(!json.contains(\"\\\"strict\\\":null\"));\n    }\n\n    // ========================================================================\n    // Response Edge Case Tests (Iteration 31)\n    // ========================================================================\n    // WHY: These tests verify graceful handling of edge cases that occur\n    // in production but aren't covered by typical happy-path tests.\n    // The Copilot API can return unusual but valid responses.\n\n    #[test]\n    fn test_response_null_content_no_tools() {\n        // WHY: The API can return null content without tool_calls when:\n        // 1. Content filtering removes all output\n        // 2. The model decides not to respond\n        // 3. Truncation occurs at an unfortunate boundary\n        // We must handle this gracefully without panics.\n        let json = r#\"{\n            \"id\": \"chatcmpl-edge1\",\n            \"model\": \"gpt-4o\",\n            \"choices\": [{\n                \"index\": 0,\n                \"message\": {\"role\": \"assistant\", \"content\": null},\n                \"finish_reason\": \"stop\"\n            }],\n            \"usage\": {\"prompt_tokens\": 5, \"completion_tokens\": 0, \"total_tokens\": 5}\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.id, \"chatcmpl-edge1\");\n        assert_eq!(response.choices.len(), 1);\n\n        // Verify content is None and no panic occurs\n        assert!(response.choices[0].message.content.is_none());\n        assert!(response.choices[0].message.tool_calls.is_none());\n\n        // Usage should still be present\n        let usage = response.usage.as_ref().unwrap();\n        assert_eq!(usage.completion_tokens, 0);\n    }\n\n    #[test]\n    fn test_response_empty_choices() {\n        // WHY: Empty choices can occur when:\n        // 1. All choices are filtered by content policy\n        // 2. Error conditions that don't set HTTP error status\n        // 3. Edge cases in model configuration\n        // Code must not assume choices[0] exists.\n        let json = r#\"{\n            \"id\": \"chatcmpl-edge2\",\n            \"model\": \"gpt-4o\",\n            \"choices\": [],\n            \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 0, \"total_tokens\": 10}\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n        assert_eq!(response.id, \"chatcmpl-edge2\");\n\n        // Verify empty choices doesn't cause panic\n        assert!(response.choices.is_empty());\n        assert_eq!(response.choices.len(), 0);\n\n        // Verify first() returns None instead of panic\n        assert!(response.choices.is_empty());\n    }\n\n    #[test]\n    fn test_response_minimal_fields() {\n        // WHY: Some providers return minimal responses without optional fields.\n        // Our types use Option<T> and skip_serializing_if for compatibility.\n        // This test verifies minimal response parsing succeeds.\n        let json = r#\"{\n            \"id\": \"chatcmpl-minimal\",\n            \"model\": \"gpt-4o-mini\",\n            \"choices\": [{\n                \"index\": 0,\n                \"message\": {\"role\": \"assistant\", \"content\": \"Hello\"},\n                \"finish_reason\": \"stop\"\n            }]\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n\n        // Verify required fields present\n        assert_eq!(response.id, \"chatcmpl-minimal\");\n        assert_eq!(response.model, \"gpt-4o-mini\");\n        assert_eq!(response.choices.len(), 1);\n\n        // Verify optional fields are None (not panic)\n        assert!(response.usage.is_none());\n        assert!(response.object.is_none());\n        assert!(response.created.is_none());\n    }\n\n    // ========================================================================\n    // Anthropic Response Format Tests (OODA-07)\n    // ========================================================================\n    // WHY: These tests verify handling of Anthropic/Claude model responses\n    // which omit the `index` field entirely and may split content/tool_calls\n    // across multiple choices.\n\n    #[test]\n    fn test_response_missing_index_anthropic() {\n        // WHY: Anthropic models don't include index field in responses\n        // Our Choice struct must handle this with Option<usize>\n        let json = r#\"{\n            \"id\": \"msg_claude_01\",\n            \"model\": \"claude-haiku-4.5\",\n            \"choices\": [{\n                \"message\": {\n                    \"role\": \"assistant\",\n                    \"content\": \"Hello! I can help you with that.\"\n                },\n                \"finish_reason\": \"stop\"\n            }],\n            \"usage\": {\n                \"prompt_tokens\": 10,\n                \"completion_tokens\": 8,\n                \"total_tokens\": 18\n            }\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n\n        assert_eq!(response.choices.len(), 1);\n        assert_eq!(response.choices[0].index, None); // Index should be None\n        assert_eq!(\n            response.choices[0].message.content,\n            Some(\"Hello! I can help you with that.\".to_string())\n        );\n        assert_eq!(response.choices[0].finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[test]\n    fn test_response_anthropic_split_choices() {\n        // WHY: Claude models via Copilot API return TWO choices:\n        // - Choice 1: Contains only content (thinking)\n        // - Choice 2: Contains only tool_calls\n        // This tests that deserialization succeeds (normalization tested separately in client)\n        let json = r#\"{\n            \"id\": \"msg_haiku_split\",\n            \"model\": \"claude-haiku-4.5\",\n            \"choices\": [\n                {\n                    \"finish_reason\": \"tool_calls\",\n                    \"message\": {\n                        \"content\": \"I'll examine the file to understand its structure\",\n                        \"role\": \"assistant\"\n                    }\n                },\n                {\n                    \"finish_reason\": \"tool_calls\",\n                    \"message\": {\n                        \"role\": \"assistant\",\n                        \"tool_calls\": [{\n                            \"function\": {\n                                \"arguments\": \"{\\\"path\\\":\\\"demo/game.js\\\"}\",\n                                \"name\": \"read_file\"\n                            },\n                            \"id\": \"toolu_01ABC123\",\n                            \"type\": \"function\"\n                        }]\n                    }\n                }\n            ],\n            \"created\": 1768984171,\n            \"usage\": {\n                \"prompt_tokens\": 100,\n                \"completion_tokens\": 50,\n                \"total_tokens\": 150\n            }\n        }\"#;\n\n        let response: ChatCompletionResponse = serde_json::from_str(json).unwrap();\n\n        // Should successfully deserialize both choices\n        assert_eq!(response.choices.len(), 2);\n\n        // First choice has content only\n        assert!(response.choices[0].message.content.is_some());\n        assert!(response.choices[0].message.tool_calls.is_none());\n\n        // Second choice has tool_calls only\n        assert!(response.choices[1].message.content.is_none());\n        assert!(response.choices[1].message.tool_calls.is_some());\n        assert_eq!(\n            response.choices[1]\n                .message\n                .tool_calls\n                .as_ref()\n                .unwrap()\n                .len(),\n            1\n        );\n    }\n\n    // ========================================================================\n    // Tool Result Message Tests (Iteration 34)\n    // ========================================================================\n    // WHY: Tool results complete the tool-calling cycle. The format must\n    // exactly match Copilot API expectations for proper round-trip.\n\n    #[test]\n    fn test_tool_result_message_format() {\n        // WHY: Tool result messages have role=\"tool\" and include tool_call_id\n        // This format is required for the model to correlate results with calls\n        let message = RequestMessage {\n            role: \"tool\".to_string(),\n            content: Some(RequestContent::Text(\"File found: src/main.rs\".to_string())),\n            name: None,\n            tool_calls: None,\n            tool_call_id: Some(\"call_abc123\".to_string()),\n            cache_control: None,\n        };\n\n        let json = serde_json::to_string(&message).unwrap();\n\n        // Verify required fields\n        assert!(json.contains(\"\\\"role\\\":\\\"tool\\\"\"));\n        assert!(json.contains(\"\\\"tool_call_id\\\":\\\"call_abc123\\\"\"));\n        assert!(json.contains(\"File found: src/main.rs\"));\n\n        // Verify None fields are omitted (skip_serializing_if)\n        assert!(!json.contains(\"\\\"name\\\":null\"));\n        assert!(!json.contains(\"\\\"tool_calls\\\":null\"));\n    }\n\n    #[test]\n    fn test_tool_result_id_preserved() {\n        // WHY: tool_call_id must match exactly for model correlation\n        // Even slight differences break the tool calling flow\n        let original_id = \"call_xYz789AbC\";\n\n        let message = RequestMessage {\n            role: \"tool\".to_string(),\n            content: Some(RequestContent::Text(\"Result data\".to_string())),\n            name: None,\n            tool_calls: None,\n            tool_call_id: Some(original_id.to_string()),\n            cache_control: None,\n        };\n\n        // Serialize\n        let json = serde_json::to_string(&message).unwrap();\n\n        // ID must appear exactly in JSON\n        assert!(\n            json.contains(&format!(\"\\\"tool_call_id\\\":\\\"{}\\\"\", original_id)),\n            \"tool_call_id must be preserved exactly in serialization\"\n        );\n\n        // Verify ID appears only once\n        assert_eq!(\n            json.matches(original_id).count(),\n            1,\n            \"tool_call_id should appear exactly once\"\n        );\n    }\n\n    #[test]\n    fn test_tool_result_with_unicode() {\n        // WHY: Tool results may contain unicode (file contents, search results)\n        // Serialization must handle unicode without corruption\n        let unicode_content = \"æœç´¢ç»“æžœ: æ‰¾åˆ° 3 ä¸ªæ–‡ä»¶ ðŸŽ¯\";\n\n        let message = RequestMessage {\n            role: \"tool\".to_string(),\n            content: Some(RequestContent::Text(unicode_content.to_string())),\n            name: None,\n            tool_calls: None,\n            tool_call_id: Some(\"call_search\".to_string()),\n            cache_control: None,\n        };\n\n        let json = serde_json::to_string(&message).unwrap();\n\n        // Unicode should be encoded in JSON (may be escaped or direct)\n        // Either way, parsing back should give original\n        let _ = serde_json::from_str::<serde_json::Value>(&json).unwrap();\n\n        // OODA-55: Content is now wrapped in RequestContent::Text\n        // The JSON structure is just the string value directly for Text variant\n        assert!(\n            json.contains(unicode_content),\n            \"Unicode content must be preserved in JSON\"\n        );\n    }\n}\n","traces":[{"line":616,"address":[],"length":0,"stats":{"Line":1}},{"line":617,"address":[],"length":0,"stats":{"Line":1}},{"line":619,"address":[],"length":0,"stats":{"Line":1}},{"line":623,"address":[],"length":0,"stats":{"Line":1}},{"line":624,"address":[],"length":0,"stats":{"Line":1}},{"line":626,"address":[],"length":0,"stats":{"Line":1}},{"line":630,"address":[],"length":0,"stats":{"Line":2}},{"line":631,"address":[],"length":0,"stats":{"Line":2}},{"line":633,"address":[],"length":0,"stats":{"Line":2}},{"line":653,"address":[],"length":0,"stats":{"Line":1}},{"line":654,"address":[],"length":0,"stats":{"Line":1}},{"line":659,"address":[],"length":0,"stats":{"Line":4}},{"line":660,"address":[],"length":0,"stats":{"Line":4}},{"line":665,"address":[],"length":0,"stats":{"Line":1}},{"line":666,"address":[],"length":0,"stats":{"Line":1}},{"line":682,"address":[],"length":0,"stats":{"Line":6}},{"line":684,"address":[],"length":0,"stats":{"Line":18}},{"line":685,"address":[],"length":0,"stats":{"Line":6}},{"line":734,"address":[],"length":0,"stats":{"Line":1}},{"line":735,"address":[],"length":0,"stats":{"Line":2}},{"line":739,"address":[],"length":0,"stats":{"Line":1}},{"line":740,"address":[],"length":0,"stats":{"Line":3}},{"line":744,"address":[],"length":0,"stats":{"Line":1}},{"line":745,"address":[],"length":0,"stats":{"Line":4}}],"covered":24,"coverable":24},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","providers","xai.rs"],"content":"//! xAI Grok Provider - Direct access to xAI's Grok models.\n//!\n//! @implements OODA-71: xAI Grok API Integration\n//!\n//! # Overview\n//!\n//! This provider connects directly to xAI's API (api.x.ai) for Grok models.\n//! xAI's API is OpenAI-compatible, so we leverage `OpenAICompatibleProvider`\n//! internally for maximum code reuse and battle-tested functionality.\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    xAI Provider Architecture                            â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                                          â”‚\n//! â”‚   User Request                                                           â”‚\n//! â”‚        â”‚                                                                 â”‚\n//! â”‚        â–¼                                                                 â”‚\n//! â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n//! â”‚   â”‚ XAIProvider â”‚â”€â”€â”€â”€â–ºâ”‚ OpenAICompatibleProvider â”‚â”€â”€â”€â”€â–ºâ”‚ api.x.ai     â”‚ â”‚\n//! â”‚   â”‚ (wrapper)   â”‚     â”‚ (implementation)         â”‚     â”‚ /v1/chat/*   â”‚ â”‚\n//! â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n//! â”‚                                                                          â”‚\n//! â”‚   XAIProvider provides:                                                  â”‚\n//! â”‚   - XAI_API_KEY environment detection                                   â”‚\n//! â”‚   - Default base URL: https://api.x.ai                                  â”‚\n//! â”‚   - Default model: grok-4                                               â”‚\n//! â”‚   - Model catalog with context sizes                                    â”‚\n//! â”‚                                                                          â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Environment Variables\n//!\n//! | Variable | Required | Default | Description |\n//! |----------|----------|---------|-------------|\n//! | `XAI_API_KEY` | âœ… Yes | - | xAI API key from console.x.ai |\n//! | `XAI_MODEL` | âŒ No | `grok-4` | Default model to use |\n//! | `XAI_BASE_URL` | âŒ No | `https://api.x.ai` | API endpoint override |\n//!\n//! # Available Models\n//!\n//! | Model | Context | Features |\n//! |-------|---------|----------|\n//! | `grok-4` | 128K | Flagship reasoning model |\n//! | `grok-4-0709` | 128K | July 2025 release |\n//! | `grok-4.1-fast` | 2M | Fast agentic, tool calling |\n//! | `grok-3` | 128K | Previous generation |\n//! | `grok-3-mini` | 128K | Smaller, faster |\n//! | `grok-2-vision-1212` | 32K | Image understanding |\n//! | `grok-code-fast-1` | 128K | Fast coding assistant |\n//!\n//! # Example\n//!\n//! ```bash\n//! # Set API key\n//! export XAI_API_KEY=xai-your-api-key\n//!\n//! # Use with EdgeCode (auto-detected)\n//! edgecode react \"Write hello world in Rust\"\n//!\n//! # Explicit provider selection\n//! edgecode react --provider xai \"Write hello world in Rust\"\n//!\n//! # Use specific model\n//! export XAI_MODEL=grok-4.1-fast\n//! edgecode react \"Build a complex app\"\n//! ```\n\nuse async_trait::async_trait;\nuse futures::stream::BoxStream;\nuse tracing::debug;\n\nuse crate::error::{LlmError, Result};\nuse crate::model_config::{ModelCapabilities, ModelCard, ModelType, ProviderConfig, ProviderType as ConfigProviderType};\nuse crate::traits::StreamChunk;\nuse crate::providers::openai_compatible::OpenAICompatibleProvider;\nuse crate::traits::{\n    ChatMessage, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse,\n};\n\n// ============================================================================\n// Constants\n// ============================================================================\n\n/// Default xAI API base URL (includes /v1 prefix for OpenAI compatibility)\nconst XAI_BASE_URL: &str = \"https://api.x.ai/v1\";\n\n/// Default model\nconst XAI_DEFAULT_MODEL: &str = \"grok-4\";\n\n/// Provider display name\nconst XAI_PROVIDER_NAME: &str = \"xai\";\n\n/// xAI model catalog with context lengths.\n///\n/// WHY: Pre-defined models ensure users get correct context limits without\n/// having to check documentation. Updated from docs.x.ai/docs/models (OODA-15).\n///\n/// Model specifications (July 2025):\n/// - Grok 4: 256K context, reasoning model, multimodal (text, image)\n/// - Grok 4.1 Fast: 2M context, optimized for agentic tool calling\n/// - Grok 3: 128K context, previous generation\nconst XAI_MODELS: &[(&str, &str, usize)] = &[\n    (\"grok-4\", \"Grok 4 (Flagship, 256K)\", 262144),\n    (\"grok-4-0709\", \"Grok 4 (July 2025)\", 262144),\n    (\"grok-4-latest\", \"Grok 4 Latest\", 262144),\n    (\"grok-4-1-fast\", \"Grok 4.1 Fast (2M context)\", 2000000),\n    (\"grok-4-1-fast-reasoning\", \"Grok 4.1 Fast Reasoning\", 2000000),\n    (\"grok-4-1-fast-non-reasoning\", \"Grok 4.1 Fast Non-Reasoning\", 2000000),\n    (\"grok-3\", \"Grok 3\", 131072),\n    (\"grok-3-latest\", \"Grok 3 Latest\", 131072),\n    (\"grok-3-mini\", \"Grok 3 Mini\", 131072),\n    (\"grok-3-mini-latest\", \"Grok 3 Mini Latest\", 131072),\n    (\"grok-2-vision-1212\", \"Grok 2 Vision\", 32768),\n    (\"grok-code-fast-1\", \"Grok Code Fast\", 131072),\n];\n\n// ============================================================================\n// XAI Provider\n// ============================================================================\n\n/// xAI Grok provider for direct API access.\n///\n/// This is a thin wrapper around `OpenAICompatibleProvider` that provides:\n/// - Automatic `XAI_API_KEY` detection\n/// - Default configuration for xAI's API\n/// - Model catalog with correct context sizes\n///\n/// # Why Wrap OpenAICompatibleProvider?\n///\n/// xAI's API is 100% OpenAI-compatible, so we get:\n/// - Battle-tested HTTP client\n/// - Streaming support\n/// - Tool/function calling\n/// - Vision (image input)\n/// - JSON mode\n/// - Error handling\n/// - Retry logic\n///\n/// Without code duplication!\n#[derive(Debug)]\npub struct XAIProvider {\n    /// Inner OpenAI-compatible provider\n    inner: OpenAICompatibleProvider,\n    /// Current model name\n    model: String,\n}\n\nimpl XAIProvider {\n    /// Create provider from environment variables.\n    ///\n    /// # Environment Variables\n    ///\n    /// - `XAI_API_KEY`: Required API key\n    /// - `XAI_MODEL`: Model name (default: grok-4)\n    /// - `XAI_BASE_URL`: Custom base URL (default: <https://api.x.ai>)\n    ///\n    /// # Errors\n    ///\n    /// Returns error if `XAI_API_KEY` is not set.\n    pub fn from_env() -> Result<Self> {\n        let api_key = std::env::var(\"XAI_API_KEY\").map_err(|_| {\n            LlmError::ConfigError(\n                \"XAI_API_KEY environment variable not set. \\\n                 Get your API key from https://console.x.ai\"\n                    .to_string(),\n            )\n        })?;\n\n        if api_key.is_empty() {\n            return Err(LlmError::ConfigError(\n                \"XAI_API_KEY is empty. Please set a valid API key.\".to_string(),\n            ));\n        }\n\n        let model = std::env::var(\"XAI_MODEL\").unwrap_or_else(|_| XAI_DEFAULT_MODEL.to_string());\n        let base_url =\n            std::env::var(\"XAI_BASE_URL\").unwrap_or_else(|_| XAI_BASE_URL.to_string());\n\n        Self::new(api_key, model, Some(base_url))\n    }\n\n    /// Create provider with explicit configuration.\n    ///\n    /// # Arguments\n    ///\n    /// * `api_key` - xAI API key\n    /// * `model` - Model name (e.g., \"grok-4\")\n    /// * `base_url` - Optional custom base URL\n    pub fn new(api_key: String, model: String, base_url: Option<String>) -> Result<Self> {\n        // Build ProviderConfig for OpenAICompatibleProvider\n        let config = Self::build_config(&api_key, &model, base_url.as_deref());\n\n        // Create inner provider\n        let inner = OpenAICompatibleProvider::from_config(config)?;\n\n        debug!(\n            provider = XAI_PROVIDER_NAME,\n            model = %model,\n            \"Created xAI provider\"\n        );\n\n        Ok(Self { inner, model })\n    }\n\n    /// Create with a different model.\n    ///\n    /// Returns a new provider instance configured for the specified model.\n    pub fn with_model(mut self, model: &str) -> Self {\n        self.model = model.to_string();\n        self.inner = self.inner.with_model(model);\n        self\n    }\n\n    /// Build ProviderConfig for OpenAICompatibleProvider.\n    ///\n    /// WHY: We need to set XAI_API_KEY env var before creating the provider because\n    /// OpenAICompatibleProvider reads the API key from the environment variable\n    /// specified in api_key_env, not from a config field.\n    fn build_config(_api_key: &str, model: &str, base_url: Option<&str>) -> ProviderConfig {\n        // Build model cards from XAI_MODELS with proper capabilities\n        let models: Vec<ModelCard> = XAI_MODELS\n            .iter()\n            .map(|(name, display, context)| {\n                // OODA-15: Grok 4 and 4.1 are multimodal (text + image)\n                // Vision support: grok-4.*, grok-4.1.*, grok-2-vision\n                let supports_vision = name.starts_with(\"grok-4\")\n                    || name.starts_with(\"grok-4.1\")\n                    || name.contains(\"vision\");\n\n                // OODA-15: Grok 4 is a reasoning model (always reasons)\n                let supports_thinking = name.starts_with(\"grok-4\")\n                    && !name.contains(\"non-reasoning\");\n\n                ModelCard {\n                    name: name.to_string(),\n                    display_name: display.to_string(),\n                    model_type: ModelType::Llm,\n                    capabilities: ModelCapabilities {\n                        context_length: *context,\n                        supports_function_calling: true,\n                        supports_json_mode: true,\n                        supports_streaming: true,\n                        supports_system_message: true,\n                        supports_vision,\n                        supports_thinking,\n                        ..Default::default()\n                    },\n                    ..Default::default()\n                }\n            })\n            .collect();\n\n        ProviderConfig {\n            name: XAI_PROVIDER_NAME.to_string(),\n            display_name: \"xAI Grok\".to_string(),\n            provider_type: ConfigProviderType::OpenAICompatible,\n            api_key_env: Some(\"XAI_API_KEY\".to_string()),\n            base_url: Some(base_url.unwrap_or(XAI_BASE_URL).to_string()),\n            base_url_env: Some(\"XAI_BASE_URL\".to_string()),\n            default_llm_model: Some(model.to_string()),\n            default_embedding_model: None, // xAI doesn't provide embeddings yet\n            models,\n            headers: std::collections::HashMap::new(),\n            enabled: true,\n            ..Default::default()\n        }\n    }\n\n    /// Get context length for a model.\n    pub fn context_length(model: &str) -> usize {\n        XAI_MODELS\n            .iter()\n            .find(|(name, _, _)| *name == model)\n            .map(|(_, _, ctx)| *ctx)\n            .unwrap_or(262144) // Default to 256K (Grok 4 standard)\n    }\n\n    /// List available models.\n    pub fn available_models() -> Vec<(&'static str, &'static str, usize)> {\n        XAI_MODELS.to_vec()\n    }\n}\n\n// ============================================================================\n// LLMProvider Implementation (delegates to inner OpenAICompatibleProvider)\n// ============================================================================\n\n#[async_trait]\nimpl LLMProvider for XAIProvider {\n    fn name(&self) -> &str {\n        XAI_PROVIDER_NAME\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    fn max_context_length(&self) -> usize {\n        Self::context_length(&self.model)\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        self.inner.complete(prompt).await\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        self.inner.complete_with_options(prompt, options).await\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.inner.chat(messages, options).await\n    }\n\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        self.inner.chat_with_tools(messages, tools, tool_choice, options).await\n    }\n\n    async fn chat_with_tools_stream(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[crate::traits::ToolDefinition],\n        tool_choice: Option<crate::traits::ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        self.inner.chat_with_tools_stream(messages, tools, tool_choice, options).await\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        self.inner.stream(prompt).await\n    }\n\n    fn supports_function_calling(&self) -> bool {\n        self.inner.supports_function_calling()\n    }\n\n    fn supports_tool_streaming(&self) -> bool {\n        self.inner.supports_tool_streaming()\n    }\n}\n\n// ============================================================================\n// EmbeddingProvider Implementation (not supported - xAI doesn't have embeddings API)\n// ============================================================================\n\n#[async_trait]\nimpl EmbeddingProvider for XAIProvider {\n    fn name(&self) -> &str {\n        XAI_PROVIDER_NAME\n    }\n\n    fn model(&self) -> &str {\n        \"none\"\n    }\n\n    fn dimension(&self) -> usize {\n        0 // Not supported\n    }\n\n    fn max_tokens(&self) -> usize {\n        0 // Not supported\n    }\n\n    async fn embed(&self, _texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // xAI doesn't provide embeddings API yet\n        Err(LlmError::ConfigError(\n            \"xAI does not provide an embeddings API. \\\n             Use OpenAI or another provider for embeddings.\"\n                .to_string(),\n        ))\n    }\n}\n\n// ============================================================================\n// Tests\n// ============================================================================\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_context_length_known_model() {\n        // OODA-15: Updated context lengths from docs.x.ai\n        assert_eq!(XAIProvider::context_length(\"grok-4\"), 262144); // 256K\n        assert_eq!(XAIProvider::context_length(\"grok-4-1-fast\"), 2000000); // 2M\n        assert_eq!(XAIProvider::context_length(\"grok-2-vision-1212\"), 32768); // 32K\n    }\n\n    #[test]\n    fn test_context_length_unknown_model() {\n        // Unknown models default to 256K (Grok 4 standard)\n        assert_eq!(XAIProvider::context_length(\"grok-unknown\"), 262144);\n    }\n\n    #[test]\n    fn test_available_models() {\n        let models = XAIProvider::available_models();\n        assert!(!models.is_empty());\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-4\"));\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-4-1-fast\"));\n    }\n\n    #[test]\n    fn test_build_config() {\n        let config = XAIProvider::build_config(\"test-key\", \"grok-4\", None);\n        assert_eq!(config.name, \"xai\");\n        assert_eq!(config.base_url, Some(\"https://api.x.ai/v1\".to_string()));\n        assert_eq!(config.default_llm_model, Some(\"grok-4\".to_string()));\n    }\n\n    #[test]\n    fn test_build_config_custom_url() {\n        let config = XAIProvider::build_config(\"test-key\", \"grok-3\", Some(\"https://custom.api\"));\n        assert_eq!(config.base_url, Some(\"https://custom.api\".to_string()));\n    }\n\n    #[test]\n    fn test_context_length_grok3_series() {\n        assert_eq!(XAIProvider::context_length(\"grok-3\"), 131072); // 128K\n        assert_eq!(XAIProvider::context_length(\"grok-3-latest\"), 131072);\n        assert_eq!(XAIProvider::context_length(\"grok-3-mini\"), 131072);\n        assert_eq!(XAIProvider::context_length(\"grok-3-mini-latest\"), 131072);\n    }\n\n    #[test]\n    fn test_context_length_grok4_series() {\n        assert_eq!(XAIProvider::context_length(\"grok-4\"), 262144); // 256K\n        assert_eq!(XAIProvider::context_length(\"grok-4-0709\"), 262144);\n        assert_eq!(XAIProvider::context_length(\"grok-4-latest\"), 262144);\n    }\n\n    #[test]\n    fn test_context_length_grok41_fast_series() {\n        assert_eq!(XAIProvider::context_length(\"grok-4-1-fast\"), 2000000); // 2M\n        assert_eq!(XAIProvider::context_length(\"grok-4-1-fast-reasoning\"), 2000000);\n        assert_eq!(XAIProvider::context_length(\"grok-4-1-fast-non-reasoning\"), 2000000);\n    }\n\n    #[test]\n    fn test_context_length_specialized_models() {\n        assert_eq!(XAIProvider::context_length(\"grok-2-vision-1212\"), 32768); // 32K\n        assert_eq!(XAIProvider::context_length(\"grok-code-fast-1\"), 131072); // 128K\n    }\n\n    #[test]\n    fn test_build_config_model_cards() {\n        let config = XAIProvider::build_config(\"test-key\", \"grok-4\", None);\n        assert!(!config.models.is_empty());\n        \n        // Check that grok-4 model card exists\n        assert!(config.models.iter().any(|m| m.name == \"grok-4\"));\n    }\n\n    #[test]\n    fn test_build_config_api_key_env() {\n        let config = XAIProvider::build_config(\"my-api-key\", \"grok-4\", None);\n        assert_eq!(config.api_key_env, Some(\"XAI_API_KEY\".to_string()));\n    }\n\n    #[test]\n    fn test_available_models_contains_all_series() {\n        let models = XAIProvider::available_models();\n        \n        // Check Grok 4 series\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-4\"));\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-4-latest\"));\n        \n        // Check Grok 4.1 Fast series\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-4-1-fast\"));\n        \n        // Check Grok 3 series\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-3\"));\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-3-mini\"));\n        \n        // Check specialized models\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-2-vision-1212\"));\n        assert!(models.iter().any(|(name, _, _)| *name == \"grok-code-fast-1\"));\n    }\n\n    #[test]\n    fn test_available_models_has_context_lengths() {\n        let models = XAIProvider::available_models();\n        for (name, _desc, context_len) in models {\n            assert!(context_len > 0, \"Model {} should have positive context length\", name);\n        }\n    }\n\n    #[test]\n    fn test_from_env_missing_api_key() {\n        // Clear env vars to ensure clean test\n        std::env::remove_var(\"XAI_API_KEY\");\n        std::env::remove_var(\"XAI_MODEL\");\n        std::env::remove_var(\"XAI_BASE_URL\");\n\n        let result = XAIProvider::from_env();\n        assert!(result.is_err());\n        let err = result.unwrap_err();\n        assert!(err.to_string().contains(\"XAI_API_KEY\"));\n    }\n\n    #[test]\n    fn test_provider_name_constant() {\n        assert_eq!(XAI_PROVIDER_NAME, \"xai\");\n    }\n\n    #[test]\n    fn test_default_model_constant() {\n        assert_eq!(XAI_DEFAULT_MODEL, \"grok-4\");\n    }\n\n    #[test]\n    fn test_default_base_url_constant() {\n        assert_eq!(XAI_BASE_URL, \"https://api.x.ai/v1\");\n    }\n}\n","traces":[{"line":162,"address":[],"length":0,"stats":{"Line":8}},{"line":163,"address":[],"length":0,"stats":{"Line":24}},{"line":164,"address":[],"length":0,"stats":{"Line":1}},{"line":165,"address":[],"length":0,"stats":{"Line":1}},{"line":166,"address":[],"length":0,"stats":{"Line":1}},{"line":167,"address":[],"length":0,"stats":{"Line":1}},{"line":171,"address":[],"length":0,"stats":{"Line":14}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":35}},{"line":178,"address":[],"length":0,"stats":{"Line":7}},{"line":179,"address":[],"length":0,"stats":{"Line":28}},{"line":181,"address":[],"length":0,"stats":{"Line":28}},{"line":191,"address":[],"length":0,"stats":{"Line":7}},{"line":193,"address":[],"length":0,"stats":{"Line":42}},{"line":196,"address":[],"length":0,"stats":{"Line":21}},{"line":198,"address":[],"length":0,"stats":{"Line":7}},{"line":201,"address":[],"length":0,"stats":{"Line":0}},{"line":204,"address":[],"length":0,"stats":{"Line":7}},{"line":210,"address":[],"length":0,"stats":{"Line":1}},{"line":211,"address":[],"length":0,"stats":{"Line":3}},{"line":212,"address":[],"length":0,"stats":{"Line":4}},{"line":213,"address":[],"length":0,"stats":{"Line":1}},{"line":221,"address":[],"length":0,"stats":{"Line":11}},{"line":223,"address":[],"length":0,"stats":{"Line":33}},{"line":225,"address":[],"length":0,"stats":{"Line":143}},{"line":228,"address":[],"length":0,"stats":{"Line":396}},{"line":229,"address":[],"length":0,"stats":{"Line":132}},{"line":230,"address":[],"length":0,"stats":{"Line":132}},{"line":233,"address":[],"length":0,"stats":{"Line":396}},{"line":234,"address":[],"length":0,"stats":{"Line":66}},{"line":236,"address":[],"length":0,"stats":{"Line":132}},{"line":237,"address":[],"length":0,"stats":{"Line":396}},{"line":238,"address":[],"length":0,"stats":{"Line":396}},{"line":239,"address":[],"length":0,"stats":{"Line":264}},{"line":240,"address":[],"length":0,"stats":{"Line":132}},{"line":241,"address":[],"length":0,"stats":{"Line":264}},{"line":242,"address":[],"length":0,"stats":{"Line":132}},{"line":243,"address":[],"length":0,"stats":{"Line":132}},{"line":244,"address":[],"length":0,"stats":{"Line":132}},{"line":245,"address":[],"length":0,"stats":{"Line":132}},{"line":246,"address":[],"length":0,"stats":{"Line":264}},{"line":247,"address":[],"length":0,"stats":{"Line":132}},{"line":248,"address":[],"length":0,"stats":{"Line":132}},{"line":250,"address":[],"length":0,"stats":{"Line":132}},{"line":256,"address":[],"length":0,"stats":{"Line":33}},{"line":257,"address":[],"length":0,"stats":{"Line":33}},{"line":259,"address":[],"length":0,"stats":{"Line":22}},{"line":260,"address":[],"length":0,"stats":{"Line":33}},{"line":261,"address":[],"length":0,"stats":{"Line":22}},{"line":262,"address":[],"length":0,"stats":{"Line":22}},{"line":265,"address":[],"length":0,"stats":{"Line":11}},{"line":272,"address":[],"length":0,"stats":{"Line":22}},{"line":273,"address":[],"length":0,"stats":{"Line":22}},{"line":275,"address":[],"length":0,"stats":{"Line":310}},{"line":276,"address":[],"length":0,"stats":{"Line":22}},{"line":281,"address":[],"length":0,"stats":{"Line":4}},{"line":282,"address":[],"length":0,"stats":{"Line":8}},{"line":292,"address":[],"length":0,"stats":{"Line":1}},{"line":293,"address":[],"length":0,"stats":{"Line":1}},{"line":296,"address":[],"length":0,"stats":{"Line":2}},{"line":297,"address":[],"length":0,"stats":{"Line":2}},{"line":300,"address":[],"length":0,"stats":{"Line":1}},{"line":301,"address":[],"length":0,"stats":{"Line":2}},{"line":304,"address":[],"length":0,"stats":{"Line":2}},{"line":344,"address":[],"length":0,"stats":{"Line":1}},{"line":348,"address":[],"length":0,"stats":{"Line":0}},{"line":349,"address":[],"length":0,"stats":{"Line":0}},{"line":352,"address":[],"length":0,"stats":{"Line":0}},{"line":353,"address":[],"length":0,"stats":{"Line":0}},{"line":363,"address":[],"length":0,"stats":{"Line":0}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":367,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":375,"address":[],"length":0,"stats":{"Line":0}},{"line":376,"address":[],"length":0,"stats":{"Line":0}},{"line":379,"address":[],"length":0,"stats":{"Line":0}}],"covered":63,"coverable":79},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","rate_limiter.rs"],"content":"//! Async-aware rate limiting for LLM API calls.\n//!\n//! This module provides rate limiting functionality to prevent API overload\n//! and respect provider limits. It implements token bucket algorithms for\n//! both request rate and token rate limiting.\n//!\n//! ## Implements\n//!\n//! - **FEAT0020**: API Rate Limiting\n//! - **FEAT0770**: Token bucket algorithm\n//! - **FEAT0771**: Concurrent request limits\n//!\n//! ## Enforces\n//!\n//! - **BR0301**: LLM API rate limits (configurable per provider)\n//! - **BR0770**: Exponential backoff on 429 errors\n//!\n//! Based on LightRAG's rate limiting: `lightrag/utils.py:priority_limit_async_func_call()`\n\nuse async_trait::async_trait;\nuse std::sync::Arc;\nuse std::time::{Duration, Instant};\nuse tokio::sync::{Mutex, Semaphore};\n\n/// Rate limiter configuration.\n#[derive(Debug, Clone)]\npub struct RateLimiterConfig {\n    /// Maximum requests per minute.\n    pub requests_per_minute: usize,\n    /// Maximum tokens per minute.\n    pub tokens_per_minute: usize,\n    /// Maximum concurrent requests.\n    pub max_concurrent: usize,\n    /// Retry delay on rate limit.\n    pub retry_delay: Duration,\n    /// Maximum retries.\n    pub max_retries: usize,\n}\n\nimpl Default for RateLimiterConfig {\n    fn default() -> Self {\n        Self {\n            requests_per_minute: 60,\n            tokens_per_minute: 90_000,\n            max_concurrent: 10,\n            retry_delay: Duration::from_secs(1),\n            max_retries: 3,\n        }\n    }\n}\n\nimpl RateLimiterConfig {\n    /// Create a new config with specified limits.\n    pub fn new(requests_per_minute: usize, tokens_per_minute: usize) -> Self {\n        Self {\n            requests_per_minute,\n            tokens_per_minute,\n            ..Default::default()\n        }\n    }\n\n    /// Configuration for OpenAI GPT-4.\n    pub fn openai_gpt4() -> Self {\n        Self {\n            requests_per_minute: 500,\n            tokens_per_minute: 30_000,\n            max_concurrent: 50,\n            ..Default::default()\n        }\n    }\n\n    /// Configuration for OpenAI GPT-4o-mini.\n    pub fn openai_gpt4o_mini() -> Self {\n        Self {\n            requests_per_minute: 5000,\n            tokens_per_minute: 200_000,\n            max_concurrent: 100,\n            ..Default::default()\n        }\n    }\n\n    /// Configuration for OpenAI GPT-3.5.\n    pub fn openai_gpt35() -> Self {\n        Self {\n            requests_per_minute: 3500,\n            tokens_per_minute: 90_000,\n            max_concurrent: 100,\n            ..Default::default()\n        }\n    }\n\n    /// Configuration for Anthropic Claude.\n    pub fn anthropic_claude() -> Self {\n        Self {\n            requests_per_minute: 60,\n            tokens_per_minute: 100_000,\n            max_concurrent: 10,\n            ..Default::default()\n        }\n    }\n\n    /// Set maximum concurrent requests.\n    pub fn with_max_concurrent(mut self, max: usize) -> Self {\n        self.max_concurrent = max;\n        self\n    }\n\n    /// Set retry delay.\n    pub fn with_retry_delay(mut self, delay: Duration) -> Self {\n        self.retry_delay = delay;\n        self\n    }\n}\n\n/// Token bucket rate limiter.\nstruct TokenBucket {\n    tokens: f64,\n    max_tokens: f64,\n    refill_rate: f64, // tokens per second\n    last_refill: Instant,\n}\n\nimpl TokenBucket {\n    fn new(max_tokens: f64, refill_rate: f64) -> Self {\n        Self {\n            tokens: max_tokens,\n            max_tokens,\n            refill_rate,\n            last_refill: Instant::now(),\n        }\n    }\n\n    fn refill(&mut self) {\n        let now = Instant::now();\n        let elapsed = now.duration_since(self.last_refill).as_secs_f64();\n        self.tokens = (self.tokens + elapsed * self.refill_rate).min(self.max_tokens);\n        self.last_refill = now;\n    }\n\n    fn try_acquire(&mut self, tokens: f64) -> bool {\n        self.refill();\n        if self.tokens >= tokens {\n            self.tokens -= tokens;\n            true\n        } else {\n            false\n        }\n    }\n\n    fn time_to_acquire(&mut self, tokens: f64) -> Duration {\n        self.refill();\n        if self.tokens >= tokens {\n            Duration::ZERO\n        } else {\n            let needed = tokens - self.tokens;\n            Duration::from_secs_f64(needed / self.refill_rate)\n        }\n    }\n\n    fn available(&mut self) -> f64 {\n        self.refill();\n        self.tokens\n    }\n}\n\n/// Async-aware rate limiter for LLM API calls.\npub struct RateLimiter {\n    config: RateLimiterConfig,\n    request_bucket: Mutex<TokenBucket>,\n    token_bucket: Mutex<TokenBucket>,\n    concurrent_semaphore: Arc<Semaphore>,\n}\n\nimpl RateLimiter {\n    /// Create a new rate limiter with the given configuration.\n    pub fn new(config: RateLimiterConfig) -> Self {\n        let request_refill_rate = config.requests_per_minute as f64 / 60.0;\n        let token_refill_rate = config.tokens_per_minute as f64 / 60.0;\n\n        Self {\n            concurrent_semaphore: Arc::new(Semaphore::new(config.max_concurrent)),\n            request_bucket: Mutex::new(TokenBucket::new(\n                config.requests_per_minute as f64,\n                request_refill_rate,\n            )),\n            token_bucket: Mutex::new(TokenBucket::new(\n                config.tokens_per_minute as f64,\n                token_refill_rate,\n            )),\n            config,\n        }\n    }\n\n    /// Create with default configuration.\n    pub fn default_limiter() -> Self {\n        Self::new(RateLimiterConfig::default())\n    }\n\n    /// Acquire permission to make a request.\n    ///\n    /// Returns a guard that releases the concurrent slot on drop.\n    pub async fn acquire(&self, estimated_tokens: usize) -> RateLimitGuard {\n        // Acquire concurrent slot\n        let permit = self\n            .concurrent_semaphore\n            .clone()\n            .acquire_owned()\n            .await\n            .unwrap();\n\n        // Wait for request rate limit\n        loop {\n            let mut bucket = self.request_bucket.lock().await;\n            if bucket.try_acquire(1.0) {\n                break;\n            }\n            let wait_time = bucket.time_to_acquire(1.0);\n            drop(bucket);\n\n            tracing::debug!(\n                wait_ms = wait_time.as_millis(),\n                \"Rate limited: waiting for request slot\"\n            );\n            tokio::time::sleep(wait_time).await;\n        }\n\n        // Wait for token rate limit\n        loop {\n            let mut bucket = self.token_bucket.lock().await;\n            if bucket.try_acquire(estimated_tokens as f64) {\n                break;\n            }\n            let wait_time = bucket.time_to_acquire(estimated_tokens as f64);\n            drop(bucket);\n\n            tracing::debug!(\n                wait_ms = wait_time.as_millis(),\n                estimated_tokens,\n                \"Rate limited: waiting for token budget\"\n            );\n            tokio::time::sleep(wait_time).await;\n        }\n\n        RateLimitGuard { _permit: permit }\n    }\n\n    /// Try to acquire without waiting.\n    ///\n    /// Returns None if rate limit would be exceeded.\n    pub async fn try_acquire(&self, estimated_tokens: usize) -> Option<RateLimitGuard> {\n        // Try to acquire concurrent slot\n        let permit = match self.concurrent_semaphore.clone().try_acquire_owned() {\n            Ok(p) => p,\n            Err(_) => return None,\n        };\n\n        // Check request rate\n        {\n            let mut bucket = self.request_bucket.lock().await;\n            if !bucket.try_acquire(1.0) {\n                return None;\n            }\n        }\n\n        // Check token rate\n        {\n            let mut bucket = self.token_bucket.lock().await;\n            if !bucket.try_acquire(estimated_tokens as f64) {\n                return None;\n            }\n        }\n\n        Some(RateLimitGuard { _permit: permit })\n    }\n\n    /// Record actual token usage (for adjustment).\n    pub async fn record_usage(&self, actual_tokens: usize, estimated_tokens: usize) {\n        if actual_tokens > estimated_tokens {\n            // Consume additional tokens\n            let mut bucket = self.token_bucket.lock().await;\n            bucket.tokens -= (actual_tokens - estimated_tokens) as f64;\n            bucket.tokens = bucket.tokens.max(0.0);\n        }\n        // If actual < estimated, we already consumed more than needed (conservative)\n    }\n\n    /// Get current available request capacity.\n    pub async fn available_requests(&self) -> f64 {\n        self.request_bucket.lock().await.available()\n    }\n\n    /// Get current available token capacity.\n    pub async fn available_tokens(&self) -> f64 {\n        self.token_bucket.lock().await.available()\n    }\n\n    /// Get the configuration.\n    pub fn config(&self) -> &RateLimiterConfig {\n        &self.config\n    }\n}\n\n/// Guard that releases rate limit resources on drop.\npub struct RateLimitGuard {\n    _permit: tokio::sync::OwnedSemaphorePermit,\n}\n\n/// Rate-limited LLM provider wrapper.\npub struct RateLimitedProvider<P> {\n    inner: P,\n    limiter: Arc<RateLimiter>,\n}\n\nimpl<P> RateLimitedProvider<P> {\n    /// Create a new rate-limited provider wrapper.\n    pub fn new(provider: P, config: RateLimiterConfig) -> Self {\n        Self {\n            inner: provider,\n            limiter: Arc::new(RateLimiter::new(config)),\n        }\n    }\n\n    /// Create with a shared rate limiter.\n    pub fn with_limiter(provider: P, limiter: Arc<RateLimiter>) -> Self {\n        Self {\n            inner: provider,\n            limiter,\n        }\n    }\n\n    /// Get a reference to the inner provider.\n    pub fn inner(&self) -> &P {\n        &self.inner\n    }\n\n    /// Get a reference to the rate limiter.\n    pub fn limiter(&self) -> &Arc<RateLimiter> {\n        &self.limiter\n    }\n}\n\nuse crate::error::Result;\nuse crate::traits::{ChatMessage, CompletionOptions, EmbeddingProvider, LLMProvider, LLMResponse};\nuse futures::stream::BoxStream;\n\n#[async_trait]\nimpl<P: LLMProvider + Send + Sync> LLMProvider for RateLimitedProvider<P> {\n    fn name(&self) -> &str {\n        self.inner.name()\n    }\n\n    fn model(&self) -> &str {\n        self.inner.model()\n    }\n\n    fn max_context_length(&self) -> usize {\n        self.inner.max_context_length()\n    }\n\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse> {\n        // Estimate tokens (rough: 4 chars per token)\n        let estimated_tokens = prompt.len() / 4 + 1000; // +1000 for response\n\n        let _guard = self.limiter.acquire(estimated_tokens).await;\n\n        let result = self.inner.complete(prompt).await;\n\n        if let Ok(ref response) = result {\n            self.limiter\n                .record_usage(\n                    response.prompt_tokens + response.completion_tokens,\n                    estimated_tokens,\n                )\n                .await;\n        }\n\n        result\n    }\n\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse> {\n        let estimated_tokens = prompt.len() / 4 + options.max_tokens.unwrap_or(1000);\n\n        let _guard = self.limiter.acquire(estimated_tokens).await;\n\n        let result = self.inner.complete_with_options(prompt, options).await;\n\n        if let Ok(ref response) = result {\n            self.limiter\n                .record_usage(\n                    response.prompt_tokens + response.completion_tokens,\n                    estimated_tokens,\n                )\n                .await;\n        }\n\n        result\n    }\n\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        let total_chars: usize = messages.iter().map(|m| m.content.len()).sum();\n        let estimated_tokens = total_chars / 4\n            + options\n                .map(|o| o.max_tokens.unwrap_or(1000))\n                .unwrap_or(1000);\n\n        let _guard = self.limiter.acquire(estimated_tokens).await;\n\n        let result = self.inner.chat(messages, options).await;\n\n        if let Ok(ref response) = result {\n            self.limiter\n                .record_usage(\n                    response.prompt_tokens + response.completion_tokens,\n                    estimated_tokens,\n                )\n                .await;\n        }\n\n        result\n    }\n\n    async fn stream(&self, prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        // For streaming, we acquire once at the start\n        let estimated_tokens = prompt.len() / 4 + 2000;\n        let _guard = self.limiter.acquire(estimated_tokens).await;\n\n        self.inner.stream(prompt).await\n    }\n\n    fn supports_streaming(&self) -> bool {\n        self.inner.supports_streaming()\n    }\n\n    fn supports_json_mode(&self) -> bool {\n        self.inner.supports_json_mode()\n    }\n}\n\n#[async_trait]\nimpl<P: EmbeddingProvider + Send + Sync> EmbeddingProvider for RateLimitedProvider<P> {\n    fn name(&self) -> &str {\n        self.inner.name()\n    }\n\n    fn model(&self) -> &str {\n        self.inner.model()\n    }\n\n    fn dimension(&self) -> usize {\n        self.inner.dimension()\n    }\n\n    fn max_tokens(&self) -> usize {\n        self.inner.max_tokens()\n    }\n\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>> {\n        // Estimate tokens for all texts\n        let total_chars: usize = texts.iter().map(|t| t.len()).sum();\n        let estimated_tokens = total_chars / 4;\n\n        let _guard = self.limiter.acquire(estimated_tokens).await;\n\n        self.inner.embed(texts).await\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_token_bucket() {\n        let mut bucket = TokenBucket::new(10.0, 1.0);\n\n        assert!(bucket.try_acquire(5.0));\n        assert!(bucket.try_acquire(5.0));\n        assert!(!bucket.try_acquire(1.0)); // Bucket empty\n\n        // Wait for refill\n        tokio::time::sleep(Duration::from_secs(2)).await;\n        assert!(bucket.try_acquire(1.0));\n    }\n\n    #[tokio::test]\n    async fn test_rate_limiter_creation() {\n        let limiter = RateLimiter::new(RateLimiterConfig::default());\n\n        assert!(limiter.available_requests().await > 0.0);\n        assert!(limiter.available_tokens().await > 0.0);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limiter_acquire() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 100,\n            tokens_per_minute: 10000,\n            max_concurrent: 5,\n            ..Default::default()\n        });\n\n        // Should be able to acquire\n        let guard = limiter.acquire(100).await;\n        drop(guard);\n\n        // Try acquire should work\n        let guard = limiter.try_acquire(100).await;\n        assert!(guard.is_some());\n    }\n\n    #[test]\n    fn test_config_presets() {\n        let gpt4 = RateLimiterConfig::openai_gpt4();\n        assert_eq!(gpt4.requests_per_minute, 500);\n\n        let claude = RateLimiterConfig::anthropic_claude();\n        assert_eq!(claude.requests_per_minute, 60);\n    }\n\n    #[test]\n    fn test_config_preset_gpt4o_mini() {\n        let config = RateLimiterConfig::openai_gpt4o_mini();\n        assert_eq!(config.requests_per_minute, 5000);\n        assert_eq!(config.tokens_per_minute, 200_000);\n        assert_eq!(config.max_concurrent, 100);\n    }\n\n    #[test]\n    fn test_config_preset_gpt35() {\n        let config = RateLimiterConfig::openai_gpt35();\n        assert_eq!(config.requests_per_minute, 3500);\n        assert_eq!(config.tokens_per_minute, 90_000);\n        assert_eq!(config.max_concurrent, 100);\n    }\n\n    #[test]\n    fn test_config_new() {\n        let config = RateLimiterConfig::new(100, 50_000);\n        assert_eq!(config.requests_per_minute, 100);\n        assert_eq!(config.tokens_per_minute, 50_000);\n        // Defaults preserved\n        assert_eq!(config.max_concurrent, 10);\n        assert_eq!(config.max_retries, 3);\n    }\n\n    #[test]\n    fn test_config_builder_with_max_concurrent() {\n        let config = RateLimiterConfig::default().with_max_concurrent(20);\n        assert_eq!(config.max_concurrent, 20);\n    }\n\n    #[test]\n    fn test_config_builder_with_retry_delay() {\n        let config = RateLimiterConfig::default().with_retry_delay(Duration::from_secs(5));\n        assert_eq!(config.retry_delay, Duration::from_secs(5));\n    }\n\n    #[test]\n    fn test_config_default_values() {\n        let config = RateLimiterConfig::default();\n        assert_eq!(config.requests_per_minute, 60);\n        assert_eq!(config.tokens_per_minute, 90_000);\n        assert_eq!(config.max_concurrent, 10);\n        assert_eq!(config.retry_delay, Duration::from_secs(1));\n        assert_eq!(config.max_retries, 3);\n    }\n\n    #[tokio::test]\n    async fn test_default_limiter() {\n        let limiter = RateLimiter::default_limiter();\n        assert_eq!(limiter.config().requests_per_minute, 60);\n        assert_eq!(limiter.config().tokens_per_minute, 90_000);\n    }\n\n    #[tokio::test]\n    async fn test_limiter_config_accessor() {\n        let config = RateLimiterConfig::openai_gpt4();\n        let limiter = RateLimiter::new(config);\n        assert_eq!(limiter.config().requests_per_minute, 500);\n    }\n\n    #[tokio::test]\n    async fn test_record_usage_over_estimate() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 100,\n            tokens_per_minute: 10000,\n            max_concurrent: 5,\n            ..Default::default()\n        });\n\n        let initial_tokens = limiter.available_tokens().await;\n        // Record actual > estimated (consumes extra tokens)\n        limiter.record_usage(500, 100).await;\n        let after_tokens = limiter.available_tokens().await;\n        // Should have consumed 400 extra tokens\n        assert!(after_tokens < initial_tokens);\n    }\n\n    #[tokio::test]\n    async fn test_record_usage_under_estimate() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 100,\n            tokens_per_minute: 10000,\n            max_concurrent: 5,\n            ..Default::default()\n        });\n\n        let initial_tokens = limiter.available_tokens().await;\n        // Record actual < estimated (no adjustment, conservative)\n        limiter.record_usage(50, 100).await;\n        let after_tokens = limiter.available_tokens().await;\n        // Should be approximately the same (only refill difference)\n        assert!((after_tokens - initial_tokens).abs() < 10.0);\n    }\n\n    #[tokio::test]\n    async fn test_try_acquire_fails_on_exhausted_concurrency() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 1000,\n            tokens_per_minute: 100_000,\n            max_concurrent: 1,\n            ..Default::default()\n        });\n\n        // First acquire should succeed\n        let guard1 = limiter.try_acquire(100).await;\n        assert!(guard1.is_some());\n\n        // Second acquire should fail (only 1 concurrent allowed)\n        let guard2 = limiter.try_acquire(100).await;\n        assert!(guard2.is_none());\n\n        // Drop first guard, now acquire should succeed\n        drop(guard1);\n        let guard3 = limiter.try_acquire(100).await;\n        assert!(guard3.is_some());\n    }\n\n    #[tokio::test]\n    async fn test_token_bucket_time_to_acquire() {\n        let mut bucket = TokenBucket::new(10.0, 10.0); // 10 tokens/sec refill\n\n        // Consume all tokens\n        assert!(bucket.try_acquire(10.0));\n\n        // Time to acquire 5 tokens should be ~0.5 seconds\n        let wait = bucket.time_to_acquire(5.0);\n        assert!(wait.as_secs_f64() > 0.0);\n        assert!(wait.as_secs_f64() < 1.0);\n    }\n\n    #[tokio::test]\n    async fn test_token_bucket_time_to_acquire_available() {\n        let mut bucket = TokenBucket::new(10.0, 1.0);\n\n        // Tokens are available, wait should be zero\n        let wait = bucket.time_to_acquire(5.0);\n        assert_eq!(wait, Duration::ZERO);\n    }\n\n    #[tokio::test]\n    async fn test_token_bucket_available() {\n        let mut bucket = TokenBucket::new(100.0, 1.0);\n        let avail = bucket.available();\n        assert!((avail - 100.0).abs() < 1.0);\n\n        bucket.try_acquire(30.0);\n        let avail = bucket.available();\n        assert!((avail - 70.0).abs() < 1.0);\n    }\n\n    #[tokio::test]\n    async fn test_available_requests_and_tokens() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 100,\n            tokens_per_minute: 50_000,\n            max_concurrent: 10,\n            ..Default::default()\n        });\n\n        let reqs = limiter.available_requests().await;\n        let toks = limiter.available_tokens().await;\n        assert!((reqs - 100.0).abs() < 1.0);\n        assert!((toks - 50_000.0).abs() < 100.0);\n    }\n\n    // ---- Iteration 22: Additional rate_limiter tests ----\n\n    #[test]\n    fn test_config_debug_impl() {\n        let config = RateLimiterConfig::default();\n        let debug_str = format!(\"{:?}\", config);\n        assert!(debug_str.contains(\"RateLimiterConfig\"));\n        assert!(debug_str.contains(\"requests_per_minute\"));\n    }\n\n    #[test]\n    fn test_config_clone_preserves_all_fields() {\n        let config = RateLimiterConfig {\n            requests_per_minute: 42,\n            tokens_per_minute: 7777,\n            max_concurrent: 3,\n            retry_delay: Duration::from_millis(250),\n            max_retries: 7,\n        };\n        let cloned = config.clone();\n        assert_eq!(cloned.requests_per_minute, 42);\n        assert_eq!(cloned.tokens_per_minute, 7777);\n        assert_eq!(cloned.max_concurrent, 3);\n        assert_eq!(cloned.retry_delay, Duration::from_millis(250));\n        assert_eq!(cloned.max_retries, 7);\n    }\n\n    #[test]\n    fn test_config_builder_chaining() {\n        let config = RateLimiterConfig::new(200, 100_000)\n            .with_max_concurrent(50)\n            .with_retry_delay(Duration::from_millis(500));\n        assert_eq!(config.requests_per_minute, 200);\n        assert_eq!(config.tokens_per_minute, 100_000);\n        assert_eq!(config.max_concurrent, 50);\n        assert_eq!(config.retry_delay, Duration::from_millis(500));\n    }\n\n    #[tokio::test]\n    async fn test_token_bucket_refill_capped_at_max() {\n        let mut bucket = TokenBucket::new(10.0, 1000.0); // Very fast refill\n        // Consume all tokens\n        assert!(bucket.try_acquire(10.0));\n        // Wait for refill beyond max\n        tokio::time::sleep(Duration::from_millis(100)).await;\n        // Available should be capped at max_tokens (10.0)\n        let avail = bucket.available();\n        assert!(avail <= 10.0, \"Tokens should be capped at max: {}\", avail);\n    }\n\n    #[tokio::test]\n    async fn test_token_bucket_exact_boundary_acquire() {\n        let mut bucket = TokenBucket::new(10.0, 0.0); // No refill\n        // Acquire exactly the available tokens\n        assert!(bucket.try_acquire(10.0));\n        // Now bucket is at exactly 0\n        assert!(!bucket.try_acquire(0.001));\n    }\n\n    #[tokio::test]\n    async fn test_try_acquire_fails_on_request_rate_exhausted() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 1,    // Only 1 request allowed\n            tokens_per_minute: 100_000,\n            max_concurrent: 100,\n            ..Default::default()\n        });\n\n        // First try should succeed\n        let g1 = limiter.try_acquire(10).await;\n        assert!(g1.is_some());\n        drop(g1);\n\n        // Second try should fail (request bucket exhausted)\n        let g2 = limiter.try_acquire(10).await;\n        assert!(g2.is_none());\n    }\n\n    #[tokio::test]\n    async fn test_try_acquire_fails_on_token_rate_exhausted() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 1000,\n            tokens_per_minute: 100, // Very low token budget\n            max_concurrent: 100,\n            ..Default::default()\n        });\n\n        // Consume all tokens\n        let g1 = limiter.try_acquire(100).await;\n        assert!(g1.is_some());\n        drop(g1);\n\n        // Next try should fail (token bucket exhausted)\n        let g2 = limiter.try_acquire(50).await;\n        assert!(g2.is_none());\n    }\n\n    #[tokio::test]\n    async fn test_record_usage_exact_match() {\n        let limiter = RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 100,\n            tokens_per_minute: 10_000,\n            max_concurrent: 5,\n            ..Default::default()\n        });\n\n        let before = limiter.available_tokens().await;\n        // exact match: actual == estimated â†’ no adjustment\n        limiter.record_usage(100, 100).await;\n        let after = limiter.available_tokens().await;\n        assert!((after - before).abs() < 10.0);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_new_and_accessors() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let provider = RateLimitedProvider::new(mock, RateLimiterConfig::default());\n\n        assert_eq!(LLMProvider::name(provider.inner()), \"mock\");\n        assert_eq!(provider.limiter().config().requests_per_minute, 60);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_with_shared_limiter() {\n        use crate::providers::MockProvider;\n\n        let limiter = Arc::new(RateLimiter::new(RateLimiterConfig::openai_gpt4()));\n        let mock = MockProvider::new();\n        let provider = RateLimitedProvider::with_limiter(mock, limiter.clone());\n\n        assert_eq!(provider.limiter().config().requests_per_minute, 500);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_llm_delegation() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let provider = RateLimitedProvider::new(mock, RateLimiterConfig::default());\n\n        // Delegation: name, model, max_context_length\n        assert_eq!(LLMProvider::name(&provider), \"mock\");\n        assert_eq!(LLMProvider::model(&provider), \"mock-model\");\n        assert_eq!(provider.max_context_length(), 4096);\n        // MockProvider default: supports_streaming=false, supports_json_mode=false\n        assert!(!provider.supports_streaming());\n        assert!(!provider.supports_json_mode());\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_embedding_delegation() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let provider = RateLimitedProvider::new(mock, RateLimiterConfig::default());\n\n        assert_eq!(EmbeddingProvider::name(&provider), \"mock\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"mock-embedding\");\n        assert_eq!(provider.dimension(), 1536);\n        assert_eq!(EmbeddingProvider::max_tokens(&provider), 512);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_complete() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        mock.add_response(\"rate-limited hello\").await;\n        let provider = RateLimitedProvider::new(mock, RateLimiterConfig::default());\n\n        let resp = provider.complete(\"test\").await.unwrap();\n        assert_eq!(resp.content, \"rate-limited hello\");\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_embed() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        let provider = RateLimitedProvider::new(mock, RateLimiterConfig::default());\n\n        let embeddings = provider.embed(&[\"hello\".to_string()]).await.unwrap();\n        assert_eq!(embeddings.len(), 1);\n        assert_eq!(embeddings[0].len(), 1536);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_chat() {\n        use crate::providers::MockProvider;\n\n        let mock = MockProvider::new();\n        mock.add_response(\"chat response\").await;\n        let provider = RateLimitedProvider::new(mock, RateLimiterConfig::default());\n\n        let msgs = vec![ChatMessage::user(\"hello\")];\n        let resp = provider.chat(&msgs, None).await.unwrap();\n        assert_eq!(resp.content, \"chat response\");\n    }\n\n    #[tokio::test]\n    async fn test_concurrent_acquire_and_release() {\n        let limiter = Arc::new(RateLimiter::new(RateLimiterConfig {\n            requests_per_minute: 1000,\n            tokens_per_minute: 100_000,\n            max_concurrent: 2,\n            ..Default::default()\n        }));\n\n        // Acquire both concurrent slots\n        let g1 = limiter.try_acquire(10).await;\n        assert!(g1.is_some());\n        let g2 = limiter.try_acquire(10).await;\n        assert!(g2.is_some());\n\n        // Third should fail\n        let g3 = limiter.try_acquire(10).await;\n        assert!(g3.is_none());\n\n        // Release one, then third should succeed\n        drop(g1);\n        let g4 = limiter.try_acquire(10).await;\n        assert!(g4.is_some());\n    }\n}\n","traces":[{"line":41,"address":[],"length":0,"stats":{"Line":32}},{"line":46,"address":[],"length":0,"stats":{"Line":32}},{"line":54,"address":[],"length":0,"stats":{"Line":2}},{"line":63,"address":[],"length":0,"stats":{"Line":3}},{"line":73,"address":[],"length":0,"stats":{"Line":1}},{"line":83,"address":[],"length":0,"stats":{"Line":1}},{"line":93,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":2}},{"line":104,"address":[],"length":0,"stats":{"Line":2}},{"line":105,"address":[],"length":0,"stats":{"Line":2}},{"line":109,"address":[],"length":0,"stats":{"Line":2}},{"line":110,"address":[],"length":0,"stats":{"Line":2}},{"line":111,"address":[],"length":0,"stats":{"Line":2}},{"line":124,"address":[],"length":0,"stats":{"Line":48}},{"line":129,"address":[],"length":0,"stats":{"Line":48}},{"line":133,"address":[],"length":0,"stats":{"Line":53}},{"line":134,"address":[],"length":0,"stats":{"Line":106}},{"line":135,"address":[],"length":0,"stats":{"Line":212}},{"line":136,"address":[],"length":0,"stats":{"Line":159}},{"line":137,"address":[],"length":0,"stats":{"Line":53}},{"line":140,"address":[],"length":0,"stats":{"Line":38}},{"line":141,"address":[],"length":0,"stats":{"Line":76}},{"line":142,"address":[],"length":0,"stats":{"Line":38}},{"line":143,"address":[],"length":0,"stats":{"Line":34}},{"line":144,"address":[],"length":0,"stats":{"Line":34}},{"line":146,"address":[],"length":0,"stats":{"Line":4}},{"line":150,"address":[],"length":0,"stats":{"Line":2}},{"line":151,"address":[],"length":0,"stats":{"Line":4}},{"line":152,"address":[],"length":0,"stats":{"Line":2}},{"line":153,"address":[],"length":0,"stats":{"Line":1}},{"line":155,"address":[],"length":0,"stats":{"Line":2}},{"line":156,"address":[],"length":0,"stats":{"Line":2}},{"line":160,"address":[],"length":0,"stats":{"Line":13}},{"line":161,"address":[],"length":0,"stats":{"Line":26}},{"line":162,"address":[],"length":0,"stats":{"Line":13}},{"line":176,"address":[],"length":0,"stats":{"Line":21}},{"line":177,"address":[],"length":0,"stats":{"Line":42}},{"line":178,"address":[],"length":0,"stats":{"Line":42}},{"line":181,"address":[],"length":0,"stats":{"Line":84}},{"line":182,"address":[],"length":0,"stats":{"Line":63}},{"line":186,"address":[],"length":0,"stats":{"Line":63}},{"line":195,"address":[],"length":0,"stats":{"Line":1}},{"line":196,"address":[],"length":0,"stats":{"Line":2}},{"line":202,"address":[],"length":0,"stats":{"Line":10}},{"line":204,"address":[],"length":0,"stats":{"Line":15}},{"line":205,"address":[],"length":0,"stats":{"Line":10}},{"line":208,"address":[],"length":0,"stats":{"Line":5}},{"line":213,"address":[],"length":0,"stats":{"Line":15}},{"line":214,"address":[],"length":0,"stats":{"Line":5}},{"line":215,"address":[],"length":0,"stats":{"Line":5}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":220,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":0}},{"line":224,"address":[],"length":0,"stats":{"Line":0}},{"line":229,"address":[],"length":0,"stats":{"Line":15}},{"line":230,"address":[],"length":0,"stats":{"Line":10}},{"line":231,"address":[],"length":0,"stats":{"Line":5}},{"line":233,"address":[],"length":0,"stats":{"Line":0}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":237,"address":[],"length":0,"stats":{"Line":0}},{"line":239,"address":[],"length":0,"stats":{"Line":0}},{"line":241,"address":[],"length":0,"stats":{"Line":0}},{"line":250,"address":[],"length":0,"stats":{"Line":24}},{"line":252,"address":[],"length":0,"stats":{"Line":34}},{"line":253,"address":[],"length":0,"stats":{"Line":20}},{"line":254,"address":[],"length":0,"stats":{"Line":2}},{"line":259,"address":[],"length":0,"stats":{"Line":30}},{"line":260,"address":[],"length":0,"stats":{"Line":10}},{"line":261,"address":[],"length":0,"stats":{"Line":1}},{"line":267,"address":[],"length":0,"stats":{"Line":27}},{"line":268,"address":[],"length":0,"stats":{"Line":18}},{"line":269,"address":[],"length":0,"stats":{"Line":1}},{"line":273,"address":[],"length":0,"stats":{"Line":8}},{"line":277,"address":[],"length":0,"stats":{"Line":12}},{"line":278,"address":[],"length":0,"stats":{"Line":6}},{"line":280,"address":[],"length":0,"stats":{"Line":3}},{"line":281,"address":[],"length":0,"stats":{"Line":2}},{"line":282,"address":[],"length":0,"stats":{"Line":2}},{"line":288,"address":[],"length":0,"stats":{"Line":4}},{"line":289,"address":[],"length":0,"stats":{"Line":8}},{"line":293,"address":[],"length":0,"stats":{"Line":16}},{"line":294,"address":[],"length":0,"stats":{"Line":32}},{"line":298,"address":[],"length":0,"stats":{"Line":5}},{"line":299,"address":[],"length":0,"stats":{"Line":5}},{"line":316,"address":[],"length":0,"stats":{"Line":8}},{"line":319,"address":[],"length":0,"stats":{"Line":16}},{"line":324,"address":[],"length":0,"stats":{"Line":1}},{"line":332,"address":[],"length":0,"stats":{"Line":1}},{"line":333,"address":[],"length":0,"stats":{"Line":1}},{"line":337,"address":[],"length":0,"stats":{"Line":2}},{"line":338,"address":[],"length":0,"stats":{"Line":2}},{"line":348,"address":[],"length":0,"stats":{"Line":2}},{"line":349,"address":[],"length":0,"stats":{"Line":2}},{"line":352,"address":[],"length":0,"stats":{"Line":2}},{"line":353,"address":[],"length":0,"stats":{"Line":2}},{"line":356,"address":[],"length":0,"stats":{"Line":1}},{"line":357,"address":[],"length":0,"stats":{"Line":2}},{"line":360,"address":[],"length":0,"stats":{"Line":2}},{"line":364,"address":[],"length":0,"stats":{"Line":0}},{"line":366,"address":[],"length":0,"stats":{"Line":0}},{"line":368,"address":[],"length":0,"stats":{"Line":0}},{"line":369,"address":[],"length":0,"stats":{"Line":0}},{"line":371,"address":[],"length":0,"stats":{"Line":0}},{"line":372,"address":[],"length":0,"stats":{"Line":0}},{"line":374,"address":[],"length":0,"stats":{"Line":0}},{"line":377,"address":[],"length":0,"stats":{"Line":0}},{"line":385,"address":[],"length":0,"stats":{"Line":0}},{"line":387,"address":[],"length":0,"stats":{"Line":0}},{"line":389,"address":[],"length":0,"stats":{"Line":0}},{"line":391,"address":[],"length":0,"stats":{"Line":0}},{"line":392,"address":[],"length":0,"stats":{"Line":0}},{"line":394,"address":[],"length":0,"stats":{"Line":0}},{"line":395,"address":[],"length":0,"stats":{"Line":0}},{"line":397,"address":[],"length":0,"stats":{"Line":0}},{"line":400,"address":[],"length":0,"stats":{"Line":0}},{"line":408,"address":[],"length":0,"stats":{"Line":2}},{"line":409,"address":[],"length":0,"stats":{"Line":0}},{"line":410,"address":[],"length":0,"stats":{"Line":0}},{"line":411,"address":[],"length":0,"stats":{"Line":0}},{"line":412,"address":[],"length":0,"stats":{"Line":0}},{"line":414,"address":[],"length":0,"stats":{"Line":0}},{"line":416,"address":[],"length":0,"stats":{"Line":0}},{"line":418,"address":[],"length":0,"stats":{"Line":0}},{"line":419,"address":[],"length":0,"stats":{"Line":0}},{"line":421,"address":[],"length":0,"stats":{"Line":0}},{"line":422,"address":[],"length":0,"stats":{"Line":0}},{"line":424,"address":[],"length":0,"stats":{"Line":0}},{"line":427,"address":[],"length":0,"stats":{"Line":0}},{"line":430,"address":[],"length":0,"stats":{"Line":0}},{"line":432,"address":[],"length":0,"stats":{"Line":0}},{"line":433,"address":[],"length":0,"stats":{"Line":0}},{"line":435,"address":[],"length":0,"stats":{"Line":0}},{"line":438,"address":[],"length":0,"stats":{"Line":1}},{"line":439,"address":[],"length":0,"stats":{"Line":2}},{"line":442,"address":[],"length":0,"stats":{"Line":1}},{"line":443,"address":[],"length":0,"stats":{"Line":2}},{"line":449,"address":[],"length":0,"stats":{"Line":1}},{"line":450,"address":[],"length":0,"stats":{"Line":1}},{"line":453,"address":[],"length":0,"stats":{"Line":1}},{"line":454,"address":[],"length":0,"stats":{"Line":1}},{"line":457,"address":[],"length":0,"stats":{"Line":1}},{"line":458,"address":[],"length":0,"stats":{"Line":2}},{"line":461,"address":[],"length":0,"stats":{"Line":1}},{"line":462,"address":[],"length":0,"stats":{"Line":2}},{"line":465,"address":[],"length":0,"stats":{"Line":1}},{"line":467,"address":[],"length":0,"stats":{"Line":2}},{"line":468,"address":[],"length":0,"stats":{"Line":0}},{"line":470,"address":[],"length":0,"stats":{"Line":0}},{"line":472,"address":[],"length":0,"stats":{"Line":0}}],"covered":104,"coverable":152},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","registry.rs"],"content":"//! LLM Provider Registry - Pluggable Provider Management\n//!\n//! # Purpose\n//!\n//! Enables dynamic registration and lookup of LLM and Embedding providers,\n//! supporting extensibility without modifying factory code.\n//!\n//! # Architecture\n//!\n//! ```text\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚              ProviderRegistry                               â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚                                                             â”‚\n//! â”‚  LLM Providers:                                             â”‚\n//! â”‚  â”Œâ”€ openai: Arc<dyn LLMProvider>                           â”‚\n//! â”‚  â”œâ”€ ollama: Arc<dyn LLMProvider>                           â”‚\n//! â”‚  â”œâ”€ lmstudio: Arc<dyn LLMProvider>                         â”‚\n//! â”‚  â”œâ”€ vscode-copilot: Arc<dyn LLMProvider>                   â”‚\n//! â”‚  â”œâ”€ gemini: Arc<dyn LLMProvider>                           â”‚\n//! â”‚  â”œâ”€ jina: Arc<dyn LLMProvider>                             â”‚\n//! â”‚  â”œâ”€ azure-openai: Arc<dyn LLMProvider>                     â”‚\n//! â”‚  â””â”€ mock: Arc<dyn LLMProvider>                             â”‚\n//! â”‚                                                             â”‚\n//! â”‚  Embedding Providers:                                       â”‚\n//! â”‚  â”Œâ”€ openai: Arc<dyn EmbeddingProvider>                     â”‚\n//! â”‚  â”œâ”€ ollama: Arc<dyn EmbeddingProvider>                     â”‚\n//! â”‚  â”œâ”€ lmstudio: Arc<dyn EmbeddingProvider>                   â”‚\n//! â”‚  â”œâ”€ vscode-copilot: Arc<dyn EmbeddingProvider>             â”‚\n//! â”‚  â””â”€ mock: Arc<dyn EmbeddingProvider>                       â”‚\n//! â”‚                                                             â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Providers by Category\n//!\n//! ## LLM Providers (Chat/Completion)\n//! - **openai**: OpenAI GPT models (gpt-4o, gpt-3.5-turbo)\n//! - **azure-openai**: Azure OpenAI deployment\n//! - **ollama**: Local Ollama models (Llama, Mistral, etc.)\n//! - **lmstudio**: OpenAI-compatible local API\n//! - **vscode-copilot**: VSCode Copilot proxy\n//! - **gemini**: Google Gemini API\n//! - **jina**: Jina.ai API\n//! - **mock**: Testing provider\n//!\n//! ## Embedding Providers\n//! - **openai**: OpenAI text-embedding-3 family\n//! - **ollama**: Local Ollama embedding models\n//! - **lmstudio**: OpenAI-compatible embeddings\n//! - **vscode-copilot**: Copilot API embeddings\n//! - **mock**: Testing provider\n//!\n//! # Why Registry Pattern?\n//!\n//! Instead of hardcoded factory with switch statements:\n//! - **Extensibility**: New providers don't require factory modification\n//! - **Pluggability**: External code can register custom providers\n//! - **Decoupling**: Registry doesn't depend on specific provider implementations\n//! - **Testing**: Easy to mock or substitute providers\n//! - **Discovery**: Can enumerate available providers\n//!\n//! # Example\n//!\n//! ```ignore\n//! use edgequake_llm::ProviderRegistry;\n//!\n//! // Create registry with built-in providers\n//! let registry = ProviderRegistry::new()?;\n//!\n//! // Get a provider\n//! if let Some(provider) = registry.get_llm(\"openai\") {\n//!     let response = provider.complete(&request).await?;\n//! }\n//!\n//! // List available providers\n//! let providers = registry.list_llm();\n//! println!(\"Available: {:?}\", providers);\n//!\n//! // Register a custom provider\n//! let mut registry = ProviderRegistry::new()?;\n//! let custom = Arc::new(MyCustomProvider::new());\n//! registry.register_llm(\"my_custom\", custom);\n//! ```\n\nuse std::collections::HashMap;\nuse std::sync::Arc;\n\nuse crate::error::Result;\nuse crate::traits::{EmbeddingProvider, LLMProvider};\nuse crate::{GeminiProvider, JinaProvider, MockProvider, ProviderFactory, ProviderType};\n\n/// Provider registry for managing LLM and Embedding providers\n///\n/// Stores instantiated providers keyed by name for O(1) lookup.\n/// Supports both built-in and custom provider registration.\n///\n/// # Built-in Providers\n///\n/// The registry is initialized with all built-in providers:\n/// - LLM: openai, azure-openai, ollama, lmstudio, vscode-copilot, gemini, jina, mock\n/// - Embeddings: openai, ollama, lmstudio, vscode-copilot, mock\n///\n/// # Custom Providers\n///\n/// Applications can register additional providers:\n/// ```ignore\n/// let mut registry = ProviderRegistry::new()?;\n/// registry.register_llm(\"custom\", Arc::new(MyProvider));\n/// ```\n#[derive(Default)]\npub struct ProviderRegistry {\n    /// Map of LLM providers by name\n    llm_providers: HashMap<String, Arc<dyn LLMProvider>>,\n\n    /// Map of Embedding providers by name\n    embedding_providers: HashMap<String, Arc<dyn EmbeddingProvider>>,\n}\n\nimpl ProviderRegistry {\n    /// Create a new registry with all built-in providers\n    ///\n    /// # Errors\n    ///\n    /// Returns error if any built-in provider fails to initialize\n    /// (e.g., API key not set for OpenAI)\n    ///\n    /// # Example\n    ///\n    /// ```ignore\n    /// let registry = ProviderRegistry::new()?;\n    /// assert!(!registry.list_llm().is_empty());\n    /// ```\n    pub fn new() -> Result<Self> {\n        let mut registry = Self::default();\n\n        // Register LLM providers\n        // Note: Some providers may fail initialization if required config is missing.\n        // We only register those that can be initialized without errors.\n\n        // OpenAI - requires OPENAI_API_KEY\n        if let Ok((llm, embed)) = ProviderFactory::create(ProviderType::OpenAI) {\n            registry.register_llm(\"openai\", llm);\n            registry.register_embedding(\"openai\", embed);\n        }\n\n        // Ollama - requires OLLAMA_HOST or OLLAMA_MODEL\n        if let Ok((llm, embed)) = ProviderFactory::create(ProviderType::Ollama) {\n            registry.register_llm(\"ollama\", llm);\n            registry.register_embedding(\"ollama\", embed);\n        }\n\n        // LM Studio - requires LMSTUDIO_HOST or LMSTUDIO_MODEL\n        if let Ok((llm, embed)) = ProviderFactory::create(ProviderType::LMStudio) {\n            registry.register_llm(\"lmstudio\", llm);\n            registry.register_embedding(\"lmstudio\", embed);\n        }\n\n        // VSCode Copilot - usually available\n        if let Ok((llm, embed)) = ProviderFactory::create(ProviderType::VsCodeCopilot) {\n            registry.register_llm(\"vscode-copilot\", llm);\n            registry.register_embedding(\"vscode-copilot\", embed);\n        }\n\n        // Gemini - requires GEMINI_API_KEY (LLM only)\n        if let Ok(provider) = GeminiProvider::from_env() {\n            registry.register_llm(\"gemini\", Arc::new(provider));\n        }\n\n        // Jina - requires JINA_API_KEY (Embedding only)\n        if let Ok(provider) = JinaProvider::from_env() {\n            registry.register_embedding(\"jina\", Arc::new(provider));\n        }\n\n        // Mock - always available (for testing)\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"mock\", mock.clone());\n        registry.register_embedding(\"mock\", mock);\n\n        Ok(registry)\n    }\n\n    /// Register an LLM provider\n    ///\n    /// If a provider with the same name exists, it will be replaced.\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Unique identifier for the provider\n    /// * `provider` - The provider instance\n    ///\n    /// # Example\n    ///\n    /// ```ignore\n    /// let mut registry = ProviderRegistry::new()?;\n    /// registry.register_llm(\"custom\", Arc::new(MyProvider));\n    /// ```\n    pub fn register_llm(&mut self, name: impl Into<String>, provider: Arc<dyn LLMProvider>) {\n        self.llm_providers.insert(name.into(), provider);\n    }\n\n    /// Register an Embedding provider\n    ///\n    /// If a provider with the same name exists, it will be replaced.\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Unique identifier for the provider\n    /// * `provider` - The provider instance\n    pub fn register_embedding(\n        &mut self,\n        name: impl Into<String>,\n        provider: Arc<dyn EmbeddingProvider>,\n    ) {\n        self.embedding_providers.insert(name.into(), provider);\n    }\n\n    /// Get an LLM provider by name\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Provider name (e.g., \"openai\", \"ollama\")\n    ///\n    /// # Returns\n    ///\n    /// `Some(provider)` if found, `None` otherwise\n    ///\n    /// # Example\n    ///\n    /// ```ignore\n    /// let registry = ProviderRegistry::new()?;\n    /// if let Some(provider) = registry.get_llm(\"openai\") {\n    ///     let response = provider.complete(&request).await?;\n    /// }\n    /// ```\n    pub fn get_llm(&self, name: &str) -> Option<Arc<dyn LLMProvider>> {\n        self.llm_providers.get(name).cloned()\n    }\n\n    /// Get an Embedding provider by name\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Provider name (e.g., \"openai\", \"ollama\")\n    ///\n    /// # Returns\n    ///\n    /// `Some(provider)` if found, `None` otherwise\n    pub fn get_embedding(&self, name: &str) -> Option<Arc<dyn EmbeddingProvider>> {\n        self.embedding_providers.get(name).cloned()\n    }\n\n    /// List all registered LLM provider names\n    ///\n    /// # Returns\n    ///\n    /// Vector of provider names in arbitrary order\n    ///\n    /// # Example\n    ///\n    /// ```ignore\n    /// let registry = ProviderRegistry::new()?;\n    /// let providers = registry.list_llm();\n    /// println!(\"Available LLM providers: {:?}\", providers);\n    /// ```\n    pub fn list_llm(&self) -> Vec<String> {\n        self.llm_providers.keys().cloned().collect()\n    }\n\n    /// List all registered Embedding provider names\n    ///\n    /// # Returns\n    ///\n    /// Vector of provider names in arbitrary order\n    pub fn list_embedding(&self) -> Vec<String> {\n        self.embedding_providers.keys().cloned().collect()\n    }\n\n    /// Check if an LLM provider is registered\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Provider name\n    ///\n    /// # Returns\n    ///\n    /// `true` if provider exists, `false` otherwise\n    pub fn has_llm(&self, name: &str) -> bool {\n        self.llm_providers.contains_key(name)\n    }\n\n    /// Check if an Embedding provider is registered\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Provider name\n    ///\n    /// # Returns\n    ///\n    /// `true` if provider exists, `false` otherwise\n    pub fn has_embedding(&self, name: &str) -> bool {\n        self.embedding_providers.contains_key(name)\n    }\n\n    /// Remove an LLM provider from the registry\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Provider name\n    ///\n    /// # Returns\n    ///\n    /// The removed provider, or `None` if not found\n    pub fn remove_llm(&mut self, name: &str) -> Option<Arc<dyn LLMProvider>> {\n        self.llm_providers.remove(name)\n    }\n\n    /// Remove an Embedding provider from the registry\n    ///\n    /// # Arguments\n    ///\n    /// * `name` - Provider name\n    ///\n    /// # Returns\n    ///\n    /// The removed provider, or `None` if not found\n    pub fn remove_embedding(&mut self, name: &str) -> Option<Arc<dyn EmbeddingProvider>> {\n        self.embedding_providers.remove(name)\n    }\n\n    /// Get the count of registered LLM providers\n    pub fn llm_count(&self) -> usize {\n        self.llm_providers.len()\n    }\n\n    /// Get the count of registered Embedding providers\n    pub fn embedding_count(&self) -> usize {\n        self.embedding_providers.len()\n    }\n\n    /// Clear all registered LLM providers\n    pub fn clear_llm(&mut self) {\n        self.llm_providers.clear();\n    }\n\n    /// Clear all registered Embedding providers\n    pub fn clear_embedding(&mut self) {\n        self.embedding_providers.clear();\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_registry_new() {\n        let registry = ProviderRegistry::new();\n        assert!(registry.is_ok());\n\n        let registry = registry.unwrap();\n        // Mock should always be available\n        assert!(registry.has_llm(\"mock\"));\n        assert!(registry.has_embedding(\"mock\"));\n    }\n\n    #[tokio::test]\n    async fn test_get_mock_provider() {\n        let registry = ProviderRegistry::new().unwrap();\n        let mock = registry.get_llm(\"mock\");\n        assert!(mock.is_some());\n\n        let embed_mock = registry.get_embedding(\"mock\");\n        assert!(embed_mock.is_some());\n    }\n\n    #[tokio::test]\n    async fn test_get_nonexistent_provider() {\n        let registry = ProviderRegistry::new().unwrap();\n        let unknown = registry.get_llm(\"nonexistent\");\n        assert!(unknown.is_none());\n\n        let unknown_embed = registry.get_embedding(\"nonexistent\");\n        assert!(unknown_embed.is_none());\n    }\n\n    #[tokio::test]\n    async fn test_register_custom_llm_provider() {\n        let mut registry = ProviderRegistry::new().unwrap();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"custom\", mock);\n\n        assert!(registry.has_llm(\"custom\"));\n        assert!(registry.get_llm(\"custom\").is_some());\n    }\n\n    #[tokio::test]\n    async fn test_register_custom_embedding_provider() {\n        let mut registry = ProviderRegistry::new().unwrap();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_embedding(\"custom_embed\", mock);\n\n        assert!(registry.has_embedding(\"custom_embed\"));\n        assert!(registry.get_embedding(\"custom_embed\").is_some());\n    }\n\n    #[tokio::test]\n    async fn test_list_providers() {\n        let registry = ProviderRegistry::new().unwrap();\n\n        let llm_names = registry.list_llm();\n        assert!(!llm_names.is_empty());\n        assert!(llm_names.contains(&\"mock\".to_string()));\n\n        let embed_names = registry.list_embedding();\n        assert!(!embed_names.is_empty());\n        assert!(embed_names.contains(&\"mock\".to_string()));\n    }\n\n    #[tokio::test]\n    async fn test_remove_provider() {\n        let mut registry = ProviderRegistry::new().unwrap();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"to_remove\", mock);\n\n        assert!(registry.has_llm(\"to_remove\"));\n        let removed = registry.remove_llm(\"to_remove\");\n        assert!(removed.is_some());\n        assert!(!registry.has_llm(\"to_remove\"));\n    }\n\n    #[tokio::test]\n    async fn test_provider_count() {\n        let registry = ProviderRegistry::new().unwrap();\n        assert!(registry.llm_count() > 0);\n        assert!(registry.embedding_count() > 0);\n    }\n\n    #[tokio::test]\n    async fn test_overwrite_provider() {\n        let mut registry = ProviderRegistry::new().unwrap();\n        let _original = registry.get_llm(\"mock\").unwrap();\n        let new_mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"mock\", new_mock);\n\n        let _updated = registry.get_llm(\"mock\").unwrap();\n        // Both should exist, providers are different instances\n        assert!(registry.get_llm(\"mock\").is_some());\n    }\n\n    // ---- Iteration 23: Additional registry tests ----\n\n    #[test]\n    fn test_registry_default_is_empty() {\n        let registry = ProviderRegistry::default();\n        assert_eq!(registry.llm_count(), 0);\n        assert_eq!(registry.embedding_count(), 0);\n        assert!(registry.list_llm().is_empty());\n        assert!(registry.list_embedding().is_empty());\n    }\n\n    #[test]\n    fn test_has_on_empty_registry() {\n        let registry = ProviderRegistry::default();\n        assert!(!registry.has_llm(\"anything\"));\n        assert!(!registry.has_embedding(\"anything\"));\n    }\n\n    #[test]\n    fn test_get_on_empty_registry() {\n        let registry = ProviderRegistry::default();\n        assert!(registry.get_llm(\"mock\").is_none());\n        assert!(registry.get_embedding(\"mock\").is_none());\n    }\n\n    #[test]\n    fn test_remove_nonexistent_llm() {\n        let mut registry = ProviderRegistry::default();\n        let removed = registry.remove_llm(\"nonexistent\");\n        assert!(removed.is_none());\n    }\n\n    #[test]\n    fn test_remove_nonexistent_embedding() {\n        let mut registry = ProviderRegistry::default();\n        let removed = registry.remove_embedding(\"nonexistent\");\n        assert!(removed.is_none());\n    }\n\n    #[test]\n    fn test_remove_embedding_provider() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_embedding(\"emb1\", mock);\n        assert!(registry.has_embedding(\"emb1\"));\n\n        let removed = registry.remove_embedding(\"emb1\");\n        assert!(removed.is_some());\n        assert!(!registry.has_embedding(\"emb1\"));\n    }\n\n    #[test]\n    fn test_clear_llm() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"a\", mock.clone());\n        registry.register_llm(\"b\", mock);\n        assert_eq!(registry.llm_count(), 2);\n\n        registry.clear_llm();\n        assert_eq!(registry.llm_count(), 0);\n        assert!(registry.list_llm().is_empty());\n    }\n\n    #[test]\n    fn test_clear_embedding() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_embedding(\"x\", mock.clone());\n        registry.register_embedding(\"y\", mock);\n        assert_eq!(registry.embedding_count(), 2);\n\n        registry.clear_embedding();\n        assert_eq!(registry.embedding_count(), 0);\n        assert!(registry.list_embedding().is_empty());\n    }\n\n    #[test]\n    fn test_register_multiple_custom_providers() {\n        let mut registry = ProviderRegistry::default();\n        for i in 0..5 {\n            let mock = Arc::new(MockProvider::new());\n            registry.register_llm(format!(\"custom_{}\", i), mock);\n        }\n        assert_eq!(registry.llm_count(), 5);\n        for i in 0..5 {\n            assert!(registry.has_llm(&format!(\"custom_{}\", i)));\n        }\n    }\n\n    #[test]\n    fn test_clear_llm_does_not_affect_embedding() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"shared\", mock.clone());\n        registry.register_embedding(\"shared\", mock);\n        assert_eq!(registry.llm_count(), 1);\n        assert_eq!(registry.embedding_count(), 1);\n\n        registry.clear_llm();\n        assert_eq!(registry.llm_count(), 0);\n        assert_eq!(registry.embedding_count(), 1); // Not affected\n    }\n\n    #[test]\n    fn test_clear_embedding_does_not_affect_llm() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"shared\", mock.clone());\n        registry.register_embedding(\"shared\", mock);\n\n        registry.clear_embedding();\n        assert_eq!(registry.llm_count(), 1); // Not affected\n        assert_eq!(registry.embedding_count(), 0);\n    }\n\n    #[test]\n    fn test_get_returns_cloned_arc() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"test\", mock);\n\n        let p1 = registry.get_llm(\"test\").unwrap();\n        let p2 = registry.get_llm(\"test\").unwrap();\n        // Both Arcs point to the same allocation\n        assert!(Arc::ptr_eq(&p1, &p2));\n    }\n\n    #[test]\n    fn test_llm_count_after_removal() {\n        let mut registry = ProviderRegistry::default();\n        let mock = Arc::new(MockProvider::new());\n        registry.register_llm(\"a\", mock.clone());\n        registry.register_llm(\"b\", mock);\n        assert_eq!(registry.llm_count(), 2);\n\n        registry.remove_llm(\"a\");\n        assert_eq!(registry.llm_count(), 1);\n    }\n}\n","traces":[{"line":134,"address":[],"length":0,"stats":{"Line":9}},{"line":135,"address":[],"length":0,"stats":{"Line":18}},{"line":142,"address":[],"length":0,"stats":{"Line":9}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":144,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":36}},{"line":149,"address":[],"length":0,"stats":{"Line":36}},{"line":150,"address":[],"length":0,"stats":{"Line":18}},{"line":154,"address":[],"length":0,"stats":{"Line":36}},{"line":155,"address":[],"length":0,"stats":{"Line":36}},{"line":156,"address":[],"length":0,"stats":{"Line":18}},{"line":160,"address":[],"length":0,"stats":{"Line":36}},{"line":161,"address":[],"length":0,"stats":{"Line":36}},{"line":162,"address":[],"length":0,"stats":{"Line":18}},{"line":166,"address":[],"length":0,"stats":{"Line":9}},{"line":167,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":9}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":27}},{"line":177,"address":[],"length":0,"stats":{"Line":27}},{"line":178,"address":[],"length":0,"stats":{"Line":27}},{"line":180,"address":[],"length":0,"stats":{"Line":9}},{"line":198,"address":[],"length":0,"stats":{"Line":51}},{"line":199,"address":[],"length":0,"stats":{"Line":255}},{"line":210,"address":[],"length":0,"stats":{"Line":42}},{"line":215,"address":[],"length":0,"stats":{"Line":210}},{"line":236,"address":[],"length":0,"stats":{"Line":9}},{"line":237,"address":[],"length":0,"stats":{"Line":36}},{"line":249,"address":[],"length":0,"stats":{"Line":4}},{"line":250,"address":[],"length":0,"stats":{"Line":16}},{"line":266,"address":[],"length":0,"stats":{"Line":3}},{"line":267,"address":[],"length":0,"stats":{"Line":12}},{"line":275,"address":[],"length":0,"stats":{"Line":3}},{"line":276,"address":[],"length":0,"stats":{"Line":12}},{"line":288,"address":[],"length":0,"stats":{"Line":10}},{"line":289,"address":[],"length":0,"stats":{"Line":30}},{"line":301,"address":[],"length":0,"stats":{"Line":5}},{"line":302,"address":[],"length":0,"stats":{"Line":15}},{"line":314,"address":[],"length":0,"stats":{"Line":3}},{"line":315,"address":[],"length":0,"stats":{"Line":9}},{"line":327,"address":[],"length":0,"stats":{"Line":2}},{"line":328,"address":[],"length":0,"stats":{"Line":6}},{"line":332,"address":[],"length":0,"stats":{"Line":10}},{"line":333,"address":[],"length":0,"stats":{"Line":20}},{"line":337,"address":[],"length":0,"stats":{"Line":7}},{"line":338,"address":[],"length":0,"stats":{"Line":14}},{"line":342,"address":[],"length":0,"stats":{"Line":2}},{"line":343,"address":[],"length":0,"stats":{"Line":4}},{"line":347,"address":[],"length":0,"stats":{"Line":2}},{"line":348,"address":[],"length":0,"stats":{"Line":4}}],"covered":46,"coverable":50},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","bm25.rs"],"content":"//! BM25 reranker implementation.\n//!\n//! Industry-standard BM25 ranking algorithm with BM25+ extension.\n//!\n//! # Why BM25?\n//!\n//! BM25 is the ranking algorithm used by Elasticsearch, Lucene, and most\n//! production search systems. It provides:\n//!\n//! - **IDF weighting**: Rare terms score higher (e.g., \"ENVY\" vs \"the\")\n//! - **Term frequency saturation**: Diminishing returns for repeated terms\n//! - **Length normalization**: Long docs don't dominate short focused ones\n//!\n//! # Algorithm\n//!\n//! ```ascii\n//! Standard BM25:\n//! score = Î£ IDF(q) Ã— f(q,D)Ã—(k1+1) / (f(q,D) + k1Ã—(1-b+bÃ—|D|/avgdl))\n//!\n//! BM25+ (when delta > 0):\n//! score = Î£ IDF(q) Ã— (f(q,D)Ã—(k1+1) / (f(q,D) + k1Ã—(1-b+bÃ—|D|/avgdl)) + delta)\n//!\n//! Where:\n//! - f(q,D) = term frequency of q in document D\n//! - |D| = document length\n//! - avgdl = average document length\n//! - k1 âˆˆ [1.2, 2.0] = term frequency saturation\n//! - b âˆˆ [0, 1] = length normalization\n//! - delta â‰¥ 0 = BM25+ extension for long docs\n//! ```\n//!\n//! # IDF Formula (SOTA)\n//!\n//! `IDF(q) = ln((N - n(q) + 0.5) / (n(q) + 0.5) + 1)`\n//!\n//! The `+1` inside ln() ensures IDF is always non-negative.\n//!\n//! # References\n//!\n//! - Robertson, S., Zaragoza, H. (2009). The Probabilistic Relevance Framework\n//! - Lv, Y., Zhai, C. (2011). Lower-Bounding Term Frequency Normalization (BM25+)\n\nuse async_trait::async_trait;\nuse rust_stemmers::{Algorithm, Stemmer};\nuse std::collections::{HashMap, HashSet};\nuse unicode_normalization::UnicodeNormalization;\n\nuse super::result::RerankResult;\nuse super::traits::Reranker;\nuse crate::error::Result;\n\n/// BM25 reranker for high-quality text-based reranking.\n///\n/// # Presets\n///\n/// | Preset | k1 | b | delta | Use Case |\n/// |--------|----|----|-------|----------|\n/// | `new()` | 1.5 | 0.75 | 0 | General purpose |\n/// | `for_short_docs()` | 1.2 | 0.3 | 0 | Tweets, titles |\n/// | `for_long_docs()` | 1.5 | 0.75 | 1.0 | Papers, articles |\n/// | `for_technical()` | 2.0 | 0.5 | 0 | Code, APIs |\n/// | `for_rag()` | 1.5 | 0.75 | 0.5 | Knowledge graphs |\n///\n/// # Example\n///\n/// ```\n/// use edgequake_llm::reranker::BM25Reranker;\n///\n/// // General purpose\n/// let reranker = BM25Reranker::new();\n///\n/// // Optimized for RAG queries\n/// let rag_reranker = BM25Reranker::for_rag();\n/// ```\npub struct BM25Reranker {\n    /// Term frequency saturation parameter (k1).\n    /// WHY: Controls how quickly TF saturates. Higher = more TF weight.\n    pub k1: f64,\n    /// Length normalization parameter (b).\n    /// WHY: Controls doc length penalty. 0 = none, 1 = full.\n    pub b: f64,\n    /// BM25+ delta for long document handling.\n    /// WHY: Standard BM25 can over-penalize long docs.\n    pub delta: f64,\n    /// Phrase boost for adjacent term matching.\n    /// WHY: \"knowledge graph\" should score higher than \"graph knowledge\".\n    pub phrase_boost: f64,\n    /// Model name for trait compliance.\n    pub model: String,\n    /// Tokenizer configuration.\n    pub tokenizer_config: TokenizerConfig,\n}\n\n/// Configuration for the BM25 tokenizer.\n///\n/// # Options\n///\n/// ```ascii\n/// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n/// â”‚              TokenizerConfig                        â”‚\n/// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n/// â”‚ enable_stemming: bool   â”€â”€â–º Porter2 stemming       â”‚\n/// â”‚ stemmer_algorithm       â”€â”€â–º English, French, etc.  â”‚\n/// â”‚ enable_stop_words: bool â”€â”€â–º Filter common words    â”‚\n/// â”‚ min_token_length: usize â”€â”€â–º Skip short tokens      â”‚\n/// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n/// ```\n#[derive(Debug, Clone)]\npub struct TokenizerConfig {\n    /// Enable Porter2 stemming for English text.\n    pub enable_stemming: bool,\n    /// Stemmer algorithm to use.\n    pub stemmer_algorithm: Algorithm,\n    /// Enable stop word filtering.\n    pub enable_stop_words: bool,\n    /// Minimum token length.\n    pub min_token_length: usize,\n}\n\nimpl Default for TokenizerConfig {\n    fn default() -> Self {\n        Self {\n            enable_stemming: true,\n            stemmer_algorithm: Algorithm::English,\n            enable_stop_words: true,\n            min_token_length: 2,\n        }\n    }\n}\n\nimpl TokenizerConfig {\n    /// Create minimal tokenizer (backward compatible, no stemming).\n    pub fn minimal() -> Self {\n        Self {\n            enable_stemming: false,\n            stemmer_algorithm: Algorithm::English,\n            enable_stop_words: false,\n            min_token_length: 2,\n        }\n    }\n\n    /// Create enhanced tokenizer with stemming and stop words.\n    pub fn enhanced() -> Self {\n        Self::default()\n    }\n\n    /// Create French tokenizer.\n    pub fn french() -> Self {\n        Self {\n            enable_stemming: true,\n            stemmer_algorithm: Algorithm::French,\n            enable_stop_words: true,\n            min_token_length: 2,\n        }\n    }\n}\n\n/// Common English stop words.\nconst ENGLISH_STOP_WORDS: &[&str] = &[\n    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\", \"has\", \"he\", \"in\", \"is\", \"it\",\n    \"its\", \"of\", \"on\", \"or\", \"that\", \"the\", \"to\", \"was\", \"were\", \"will\", \"with\", \"this\", \"but\",\n    \"they\", \"have\", \"had\", \"what\", \"when\", \"where\", \"who\", \"which\", \"you\", \"your\", \"we\", \"our\",\n    \"can\", \"all\", \"there\", \"their\", \"been\", \"would\", \"could\", \"should\", \"may\", \"might\", \"must\",\n    \"do\", \"does\", \"did\", \"if\", \"not\", \"no\", \"so\", \"up\", \"out\", \"just\", \"than\", \"then\", \"too\",\n    \"very\", \"also\",\n];\n\nimpl BM25Reranker {\n    /// Create with default parameters (k1=1.5, b=0.75, standard BM25).\n    pub fn new() -> Self {\n        Self {\n            k1: 1.5,\n            b: 0.75,\n            delta: 0.0,\n            phrase_boost: 0.0,\n            model: \"bm25-reranker\".to_string(),\n            tokenizer_config: TokenizerConfig::minimal(),\n        }\n    }\n\n    /// Create with enhanced tokenization (stemming + stop words).\n    pub fn new_enhanced() -> Self {\n        Self {\n            k1: 1.5,\n            b: 0.75,\n            delta: 0.0,\n            phrase_boost: 0.0,\n            model: \"bm25-enhanced-reranker\".to_string(),\n            tokenizer_config: TokenizerConfig::enhanced(),\n        }\n    }\n\n    /// Create BM25+ reranker (delta=1.0) for better long doc handling.\n    pub fn bm25_plus() -> Self {\n        Self {\n            k1: 1.5,\n            b: 0.75,\n            delta: 1.0,\n            phrase_boost: 0.0,\n            model: \"bm25-plus-reranker\".to_string(),\n            tokenizer_config: TokenizerConfig::minimal(),\n        }\n    }\n\n    /// Preset for short documents (tweets, titles).\n    pub fn for_short_docs() -> Self {\n        Self {\n            k1: 1.2,\n            b: 0.3,\n            delta: 0.0,\n            phrase_boost: 0.0,\n            model: \"bm25-short-docs\".to_string(),\n            tokenizer_config: TokenizerConfig::enhanced(),\n        }\n    }\n\n    /// Preset for long documents (papers, articles).\n    pub fn for_long_docs() -> Self {\n        Self {\n            k1: 1.5,\n            b: 0.75,\n            delta: 1.0,\n            phrase_boost: 0.0,\n            model: \"bm25-long-docs\".to_string(),\n            tokenizer_config: TokenizerConfig::enhanced(),\n        }\n    }\n\n    /// Preset for technical content (code, APIs).\n    pub fn for_technical() -> Self {\n        Self {\n            k1: 2.0,\n            b: 0.5,\n            delta: 0.0,\n            phrase_boost: 0.0,\n            model: \"bm25-technical\".to_string(),\n            tokenizer_config: TokenizerConfig::minimal(),\n        }\n    }\n\n    /// Preset for RAG/knowledge graph queries.\n    pub fn for_rag() -> Self {\n        Self {\n            k1: 1.5,\n            b: 0.75,\n            delta: 0.5,\n            phrase_boost: 0.3,\n            model: \"bm25-rag\".to_string(),\n            tokenizer_config: TokenizerConfig::enhanced(),\n        }\n    }\n\n    /// Preset for semantic queries with phrase boosting.\n    pub fn for_semantic() -> Self {\n        Self {\n            k1: 1.5,\n            b: 0.75,\n            delta: 0.5,\n            phrase_boost: 0.5,\n            model: \"bm25-semantic\".to_string(),\n            tokenizer_config: TokenizerConfig::enhanced(),\n        }\n    }\n\n    /// Create with custom k1 and b parameters.\n    pub fn with_params(k1: f64, b: f64) -> Self {\n        Self {\n            k1: k1.clamp(0.0, 3.0),\n            b: b.clamp(0.0, 1.0),\n            delta: 0.0,\n            phrase_boost: 0.0,\n            model: \"bm25-reranker\".to_string(),\n            tokenizer_config: TokenizerConfig::minimal(),\n        }\n    }\n\n    /// Create with full custom parameters including delta.\n    pub fn with_full_params(k1: f64, b: f64, delta: f64) -> Self {\n        Self {\n            k1: k1.clamp(0.0, 3.0),\n            b: b.clamp(0.0, 1.0),\n            delta: delta.max(0.0),\n            phrase_boost: 0.0,\n            model: if delta > 0.0 {\n                \"bm25-plus-reranker\".to_string()\n            } else {\n                \"bm25-reranker\".to_string()\n            },\n            tokenizer_config: TokenizerConfig::minimal(),\n        }\n    }\n\n    /// Set custom tokenizer configuration.\n    pub fn with_tokenizer_config(mut self, config: TokenizerConfig) -> Self {\n        self.tokenizer_config = config;\n        self\n    }\n\n    /// Set phrase boost factor (0.0-2.0).\n    pub fn with_phrase_boost(mut self, boost: f64) -> Self {\n        self.phrase_boost = boost.clamp(0.0, 2.0);\n        self\n    }\n\n    /// Check if a word is a stop word.\n    fn is_stop_word(word: &str) -> bool {\n        ENGLISH_STOP_WORDS.binary_search(&word).is_ok()\n    }\n\n    /// Tokenize with configured settings (stemming, stop words).\n    pub(crate) fn tokenize_with_config(&self, text: &str) -> Vec<String> {\n        let normalized: String = text\n            .to_lowercase()\n            .nfkd()\n            .filter(|c| !unicode_normalization::char::is_combining_mark(*c))\n            .collect();\n\n        let tokens: Vec<String> = normalized\n            .split(|c: char| !c.is_alphanumeric())\n            .filter(|s| !s.is_empty() && s.len() >= self.tokenizer_config.min_token_length)\n            .map(|s| s.to_string())\n            .collect();\n\n        let filtered: Vec<String> = if self.tokenizer_config.enable_stop_words {\n            tokens\n                .into_iter()\n                .filter(|t| !Self::is_stop_word(t))\n                .collect()\n        } else {\n            tokens\n        };\n\n        if self.tokenizer_config.enable_stemming {\n            let stemmer = Stemmer::create(self.tokenizer_config.stemmer_algorithm);\n            filtered\n                .into_iter()\n                .map(|t| stemmer.stem(&t).to_string())\n                .collect()\n        } else {\n            filtered\n        }\n    }\n\n    /// Basic tokenization (backward compatible).\n    pub(crate) fn tokenize(text: &str) -> Vec<String> {\n        let normalized: String = text\n            .to_lowercase()\n            .nfkd()\n            .filter(|c| !unicode_normalization::char::is_combining_mark(*c))\n            .collect();\n\n        normalized\n            .split(|c: char| !c.is_alphanumeric())\n            .filter(|s| !s.is_empty() && s.len() > 1)\n            .map(|s| s.to_string())\n            .collect()\n    }\n\n    /// Compute IDF for a term across documents.\n    #[allow(dead_code)]\n    pub(crate) fn compute_idf(term: &str, doc_terms_list: &[Vec<String>]) -> f64 {\n        let n = doc_terms_list.len() as f64;\n        let containing_docs = doc_terms_list\n            .iter()\n            .filter(|terms| terms.contains(&term.to_string()))\n            .count() as f64;\n\n        Self::compute_idf_from_df(n, containing_docs)\n    }\n\n    /// Compute IDF from pre-computed document frequency.\n    #[inline]\n    pub(crate) fn compute_idf_from_df(n: f64, df: f64) -> f64 {\n        ((n - df + 0.5) / (df + 0.5) + 1.0).ln()\n    }\n\n    /// Build document frequency map for all terms.\n    pub(crate) fn compute_document_frequencies(\n        doc_terms_list: &[Vec<String>],\n    ) -> HashMap<String, usize> {\n        let mut df_map: HashMap<String, usize> = HashMap::new();\n\n        for doc_terms in doc_terms_list {\n            let unique_terms: HashSet<&String> = doc_terms.iter().collect();\n            for term in unique_terms {\n                *df_map.entry(term.clone()).or_insert(0) += 1;\n            }\n        }\n\n        df_map\n    }\n\n    /// Compute phrase match bonus for adjacent query terms.\n    pub(crate) fn compute_phrase_bonus(&self, query_terms: &[String], doc_terms: &[String]) -> f64 {\n        if query_terms.len() < 2 || doc_terms.len() < 2 {\n            return 0.0;\n        }\n\n        let mut phrase_matches = 0;\n        let total_pairs = query_terms.len().saturating_sub(1);\n\n        for window in query_terms.windows(2) {\n            let (term_a, term_b) = (&window[0], &window[1]);\n            for doc_window in doc_terms.windows(2) {\n                if &doc_window[0] == term_a && &doc_window[1] == term_b {\n                    phrase_matches += 1;\n                    break;\n                }\n            }\n        }\n\n        phrase_matches as f64 / total_pairs.max(1) as f64\n    }\n\n    /// Compute BM25/BM25+ score for a single document.\n    fn compute_bm25_score(\n        &self,\n        query_terms: &[String],\n        doc_terms: &[String],\n        avgdl: f64,\n        idf_cache: &HashMap<String, f64>,\n    ) -> f64 {\n        let doc_len = doc_terms.len() as f64;\n        let length_norm = 1.0 - self.b + self.b * (doc_len / avgdl);\n\n        let mut score = 0.0;\n        for term in query_terms {\n            let tf = doc_terms.iter().filter(|t| t == &term).count() as f64;\n            if tf > 0.0 {\n                let idf = idf_cache.get(term).copied().unwrap_or(0.0);\n                let tf_component = (tf * (self.k1 + 1.0)) / (tf + self.k1 * length_norm);\n                score += idf * (tf_component + self.delta);\n            }\n        }\n        score\n    }\n}\n\nimpl Default for BM25Reranker {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[async_trait]\nimpl Reranker for BM25Reranker {\n    fn name(&self) -> &str {\n        \"bm25\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    async fn rerank(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        if documents.is_empty() {\n            return Ok(vec![]);\n        }\n\n        let query_terms =\n            if self.tokenizer_config.enable_stemming || self.tokenizer_config.enable_stop_words {\n                self.tokenize_with_config(query)\n            } else {\n                Self::tokenize(query)\n            };\n\n        if query_terms.is_empty() {\n            let results: Vec<RerankResult> = documents\n                .iter()\n                .enumerate()\n                .map(|(idx, _)| RerankResult {\n                    index: idx,\n                    relevance_score: 0.0,\n                })\n                .collect();\n            return Ok(results);\n        }\n\n        let doc_terms_list: Vec<Vec<String>> =\n            if self.tokenizer_config.enable_stemming || self.tokenizer_config.enable_stop_words {\n                documents\n                    .iter()\n                    .map(|d| self.tokenize_with_config(d))\n                    .collect()\n            } else {\n                documents.iter().map(|d| Self::tokenize(d)).collect()\n            };\n\n        let avgdl = doc_terms_list.iter().map(|d| d.len()).sum::<usize>() as f64\n            / doc_terms_list.len().max(1) as f64;\n        let avgdl = avgdl.max(1.0);\n\n        let df_map = Self::compute_document_frequencies(&doc_terms_list);\n        let n = doc_terms_list.len() as f64;\n\n        let mut idf_cache = HashMap::new();\n        for term in &query_terms {\n            let df = df_map.get(term).copied().unwrap_or(0) as f64;\n            let idf = Self::compute_idf_from_df(n, df);\n            idf_cache.insert(term.clone(), idf);\n        }\n\n        let mut results: Vec<RerankResult> = doc_terms_list\n            .iter()\n            .enumerate()\n            .map(|(idx, doc_terms)| {\n                let bm25_score =\n                    self.compute_bm25_score(&query_terms, doc_terms, avgdl, &idf_cache);\n\n                let phrase_bonus = if self.phrase_boost > 0.0 {\n                    self.compute_phrase_bonus(&query_terms, doc_terms)\n                } else {\n                    0.0\n                };\n\n                let final_score = bm25_score + (self.phrase_boost * phrase_bonus);\n                RerankResult {\n                    index: idx,\n                    relevance_score: final_score,\n                }\n            })\n            .collect();\n\n        results.sort_by(|a, b| {\n            b.relevance_score\n                .partial_cmp(&a.relevance_score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n\n        if let Some(n) = top_n {\n            results.truncate(n);\n        }\n\n        Ok(results)\n    }\n}\n","traces":[{"line":121,"address":[],"length":0,"stats":{"Line":9}},{"line":133,"address":[],"length":0,"stats":{"Line":24}},{"line":143,"address":[],"length":0,"stats":{"Line":8}},{"line":144,"address":[],"length":0,"stats":{"Line":8}},{"line":148,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":18}},{"line":176,"address":[],"length":0,"stats":{"Line":36}},{"line":177,"address":[],"length":0,"stats":{"Line":18}},{"line":182,"address":[],"length":0,"stats":{"Line":2}},{"line":188,"address":[],"length":0,"stats":{"Line":4}},{"line":189,"address":[],"length":0,"stats":{"Line":2}},{"line":194,"address":[],"length":0,"stats":{"Line":1}},{"line":200,"address":[],"length":0,"stats":{"Line":2}},{"line":201,"address":[],"length":0,"stats":{"Line":1}},{"line":206,"address":[],"length":0,"stats":{"Line":1}},{"line":212,"address":[],"length":0,"stats":{"Line":2}},{"line":213,"address":[],"length":0,"stats":{"Line":1}},{"line":218,"address":[],"length":0,"stats":{"Line":1}},{"line":224,"address":[],"length":0,"stats":{"Line":2}},{"line":225,"address":[],"length":0,"stats":{"Line":1}},{"line":230,"address":[],"length":0,"stats":{"Line":1}},{"line":236,"address":[],"length":0,"stats":{"Line":2}},{"line":237,"address":[],"length":0,"stats":{"Line":1}},{"line":242,"address":[],"length":0,"stats":{"Line":1}},{"line":248,"address":[],"length":0,"stats":{"Line":2}},{"line":249,"address":[],"length":0,"stats":{"Line":1}},{"line":254,"address":[],"length":0,"stats":{"Line":2}},{"line":260,"address":[],"length":0,"stats":{"Line":4}},{"line":261,"address":[],"length":0,"stats":{"Line":2}},{"line":266,"address":[],"length":0,"stats":{"Line":1}},{"line":268,"address":[],"length":0,"stats":{"Line":3}},{"line":269,"address":[],"length":0,"stats":{"Line":3}},{"line":272,"address":[],"length":0,"stats":{"Line":2}},{"line":273,"address":[],"length":0,"stats":{"Line":1}},{"line":278,"address":[],"length":0,"stats":{"Line":2}},{"line":280,"address":[],"length":0,"stats":{"Line":4}},{"line":281,"address":[],"length":0,"stats":{"Line":4}},{"line":282,"address":[],"length":0,"stats":{"Line":4}},{"line":284,"address":[],"length":0,"stats":{"Line":2}},{"line":289,"address":[],"length":0,"stats":{"Line":2}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":300,"address":[],"length":0,"stats":{"Line":2}},{"line":301,"address":[],"length":0,"stats":{"Line":2}},{"line":302,"address":[],"length":0,"stats":{"Line":2}},{"line":306,"address":[],"length":0,"stats":{"Line":27}},{"line":307,"address":[],"length":0,"stats":{"Line":81}},{"line":311,"address":[],"length":0,"stats":{"Line":6}},{"line":312,"address":[],"length":0,"stats":{"Line":18}},{"line":315,"address":[],"length":0,"stats":{"Line":368}},{"line":318,"address":[],"length":0,"stats":{"Line":18}},{"line":319,"address":[],"length":0,"stats":{"Line":368}},{"line":320,"address":[],"length":0,"stats":{"Line":90}},{"line":321,"address":[],"length":0,"stats":{"Line":60}},{"line":324,"address":[],"length":0,"stats":{"Line":18}},{"line":325,"address":[],"length":0,"stats":{"Line":6}},{"line":327,"address":[],"length":0,"stats":{"Line":60}},{"line":330,"address":[],"length":0,"stats":{"Line":0}},{"line":333,"address":[],"length":0,"stats":{"Line":6}},{"line":334,"address":[],"length":0,"stats":{"Line":18}},{"line":335,"address":[],"length":0,"stats":{"Line":6}},{"line":337,"address":[],"length":0,"stats":{"Line":72}},{"line":340,"address":[],"length":0,"stats":{"Line":0}},{"line":345,"address":[],"length":0,"stats":{"Line":42}},{"line":346,"address":[],"length":0,"stats":{"Line":126}},{"line":349,"address":[],"length":0,"stats":{"Line":1832}},{"line":352,"address":[],"length":0,"stats":{"Line":42}},{"line":353,"address":[],"length":0,"stats":{"Line":1824}},{"line":354,"address":[],"length":0,"stats":{"Line":376}},{"line":355,"address":[],"length":0,"stats":{"Line":334}},{"line":361,"address":[],"length":0,"stats":{"Line":1}},{"line":362,"address":[],"length":0,"stats":{"Line":2}},{"line":363,"address":[],"length":0,"stats":{"Line":2}},{"line":364,"address":[],"length":0,"stats":{"Line":1}},{"line":365,"address":[],"length":0,"stats":{"Line":10}},{"line":366,"address":[],"length":0,"stats":{"Line":1}},{"line":368,"address":[],"length":0,"stats":{"Line":3}},{"line":373,"address":[],"length":0,"stats":{"Line":25}},{"line":374,"address":[],"length":0,"stats":{"Line":50}},{"line":378,"address":[],"length":0,"stats":{"Line":15}},{"line":381,"address":[],"length":0,"stats":{"Line":45}},{"line":383,"address":[],"length":0,"stats":{"Line":89}},{"line":384,"address":[],"length":0,"stats":{"Line":148}},{"line":385,"address":[],"length":0,"stats":{"Line":481}},{"line":386,"address":[],"length":0,"stats":{"Line":592}},{"line":390,"address":[],"length":0,"stats":{"Line":15}},{"line":394,"address":[],"length":0,"stats":{"Line":5}},{"line":395,"address":[],"length":0,"stats":{"Line":10}},{"line":396,"address":[],"length":0,"stats":{"Line":0}},{"line":399,"address":[],"length":0,"stats":{"Line":10}},{"line":400,"address":[],"length":0,"stats":{"Line":20}},{"line":402,"address":[],"length":0,"stats":{"Line":15}},{"line":403,"address":[],"length":0,"stats":{"Line":15}},{"line":404,"address":[],"length":0,"stats":{"Line":24}},{"line":405,"address":[],"length":0,"stats":{"Line":17}},{"line":406,"address":[],"length":0,"stats":{"Line":2}},{"line":407,"address":[],"length":0,"stats":{"Line":2}},{"line":412,"address":[],"length":0,"stats":{"Line":10}},{"line":416,"address":[],"length":0,"stats":{"Line":31}},{"line":423,"address":[],"length":0,"stats":{"Line":62}},{"line":424,"address":[],"length":0,"stats":{"Line":124}},{"line":426,"address":[],"length":0,"stats":{"Line":62}},{"line":427,"address":[],"length":0,"stats":{"Line":143}},{"line":428,"address":[],"length":0,"stats":{"Line":792}},{"line":429,"address":[],"length":0,"stats":{"Line":92}},{"line":430,"address":[],"length":0,"stats":{"Line":252}},{"line":431,"address":[],"length":0,"stats":{"Line":180}},{"line":432,"address":[],"length":0,"stats":{"Line":72}},{"line":435,"address":[],"length":0,"stats":{"Line":31}},{"line":440,"address":[],"length":0,"stats":{"Line":0}},{"line":441,"address":[],"length":0,"stats":{"Line":0}},{"line":447,"address":[],"length":0,"stats":{"Line":0}},{"line":448,"address":[],"length":0,"stats":{"Line":0}},{"line":451,"address":[],"length":0,"stats":{"Line":6}},{"line":452,"address":[],"length":0,"stats":{"Line":6}},{"line":477,"address":[],"length":0,"stats":{"Line":1}},{"line":488,"address":[],"length":0,"stats":{"Line":9}},{"line":491,"address":[],"length":0,"stats":{"Line":56}},{"line":494,"address":[],"length":0,"stats":{"Line":62}},{"line":511,"address":[],"length":0,"stats":{"Line":31}},{"line":512,"address":[],"length":0,"stats":{"Line":31}},{"line":513,"address":[],"length":0,"stats":{"Line":186}},{"line":515,"address":[],"length":0,"stats":{"Line":62}},{"line":516,"address":[],"length":0,"stats":{"Line":12}},{"line":518,"address":[],"length":0,"stats":{"Line":28}},{"line":521,"address":[],"length":0,"stats":{"Line":93}},{"line":522,"address":[],"length":0,"stats":{"Line":31}},{"line":523,"address":[],"length":0,"stats":{"Line":31}},{"line":524,"address":[],"length":0,"stats":{"Line":31}},{"line":529,"address":[],"length":0,"stats":{"Line":19}},{"line":530,"address":[],"length":0,"stats":{"Line":19}},{"line":531,"address":[],"length":0,"stats":{"Line":38}},{"line":532,"address":[],"length":0,"stats":{"Line":38}}],"covered":123,"coverable":134},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","config.rs"],"content":"//! Reranker configuration types.\n//!\n//! This module contains configuration structures for reranking providers.\n//!\n//! # Architecture\n//!\n//! ```ascii\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚                    RerankConfig                          â”‚\n//! â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n//! â”‚ model: String         â”€â”€â”€â”€â”€â–º Which model to use         â”‚\n//! â”‚ base_url: String      â”€â”€â”€â”€â”€â–º API endpoint               â”‚\n//! â”‚ api_key: Option       â”€â”€â”€â”€â”€â–º Authentication             â”‚\n//! â”‚ top_n: Option<usize>  â”€â”€â”€â”€â”€â–º Max results to return      â”‚\n//! â”‚ timeout: Duration     â”€â”€â”€â”€â”€â–º Request timeout            â”‚\n//! â”‚ enable_chunking: bool â”€â”€â”€â”€â”€â–º Split long docs?           â”‚\n//! â”‚ max_tokens_per_doc    â”€â”€â”€â”€â”€â–º Chunk size limit           â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n\nuse std::time::Duration;\n\n/// Configuration for a reranker.\n///\n/// # Provider-Specific Configurations\n///\n/// Use the factory methods for common providers:\n/// - [`RerankConfig::jina`] - Jina AI Reranker\n/// - [`RerankConfig::cohere`] - Cohere Rerank\n/// - [`RerankConfig::aliyun`] - Aliyun DashScope\n///\n/// # Example\n///\n/// ```ignore\n/// // Jina reranker with top 10 results\n/// let config = RerankConfig::jina(\"your-api-key\")\n///     .with_top_n(10)\n///     .with_chunking(true);\n/// ```\n#[derive(Debug, Clone)]\npub struct RerankConfig {\n    /// Model name to use.\n    pub model: String,\n    /// Base URL for the reranker API.\n    pub base_url: String,\n    /// API key for authentication.\n    pub api_key: Option<String>,\n    /// Maximum number of results to return.\n    pub top_n: Option<usize>,\n    /// Request timeout.\n    pub timeout: Duration,\n    /// Enable document chunking for long documents.\n    pub enable_chunking: bool,\n    /// Maximum tokens per document for chunking.\n    pub max_tokens_per_doc: usize,\n}\n\nimpl Default for RerankConfig {\n    fn default() -> Self {\n        Self {\n            model: \"jina-reranker-v2-base-multilingual\".to_string(),\n            base_url: \"https://api.jina.ai/v1/rerank\".to_string(),\n            api_key: None,\n            top_n: None,\n            timeout: Duration::from_secs(30),\n            enable_chunking: false,\n            max_tokens_per_doc: 480,\n        }\n    }\n}\n\nimpl RerankConfig {\n    /// Create a new Jina reranker config.\n    ///\n    /// Uses the `jina-reranker-v2-base-multilingual` model.\n    pub fn jina(api_key: impl Into<String>) -> Self {\n        Self {\n            model: \"jina-reranker-v2-base-multilingual\".to_string(),\n            base_url: \"https://api.jina.ai/v1/rerank\".to_string(),\n            api_key: Some(api_key.into()),\n            ..Default::default()\n        }\n    }\n\n    /// Create a new Cohere reranker config.\n    ///\n    /// Uses the `rerank-v3.5` model with 4096 max tokens.\n    pub fn cohere(api_key: impl Into<String>) -> Self {\n        Self {\n            model: \"rerank-v3.5\".to_string(),\n            base_url: \"https://api.cohere.com/v2/rerank\".to_string(),\n            api_key: Some(api_key.into()),\n            max_tokens_per_doc: 4096,\n            ..Default::default()\n        }\n    }\n\n    /// Create a new Aliyun DashScope reranker config.\n    ///\n    /// Uses the `gte-rerank-v2` model.\n    pub fn aliyun(api_key: impl Into<String>) -> Self {\n        Self {\n            model: \"gte-rerank-v2\".to_string(),\n            base_url:\n                \"https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank\"\n                    .to_string(),\n            api_key: Some(api_key.into()),\n            ..Default::default()\n        }\n    }\n\n    /// Set the model name.\n    pub fn with_model(mut self, model: impl Into<String>) -> Self {\n        self.model = model.into();\n        self\n    }\n\n    /// Set the top N results to return.\n    pub fn with_top_n(mut self, top_n: usize) -> Self {\n        self.top_n = Some(top_n);\n        self\n    }\n\n    /// Enable document chunking.\n    pub fn with_chunking(mut self, enable: bool) -> Self {\n        self.enable_chunking = enable;\n        self\n    }\n\n    /// Set max tokens per document for chunking.\n    pub fn with_max_tokens(mut self, max_tokens: usize) -> Self {\n        self.max_tokens_per_doc = max_tokens;\n        self\n    }\n}\n\n/// Strategy for aggregating chunk scores.\n///\n/// When documents are split into chunks for processing, this determines\n/// how the individual chunk scores are combined into a final document score.\n///\n/// # Variants\n///\n/// - `Max`: Use the highest score from any chunk (default, most common)\n/// - `Mean`: Average all chunk scores\n/// - `First`: Use only the first chunk's score\n#[derive(Debug, Clone, Copy, Default)]\npub enum ScoreAggregation {\n    /// Use the maximum score from all chunks.\n    #[default]\n    Max,\n    /// Use the mean of all chunk scores.\n    Mean,\n    /// Use the score from the first chunk.\n    First,\n}\n","traces":[{"line":59,"address":[],"length":0,"stats":{"Line":9}},{"line":61,"address":[],"length":0,"stats":{"Line":27}},{"line":62,"address":[],"length":0,"stats":{"Line":27}},{"line":65,"address":[],"length":0,"stats":{"Line":9}},{"line":76,"address":[],"length":0,"stats":{"Line":2}},{"line":78,"address":[],"length":0,"stats":{"Line":6}},{"line":79,"address":[],"length":0,"stats":{"Line":6}},{"line":80,"address":[],"length":0,"stats":{"Line":2}},{"line":88,"address":[],"length":0,"stats":{"Line":2}},{"line":90,"address":[],"length":0,"stats":{"Line":6}},{"line":91,"address":[],"length":0,"stats":{"Line":6}},{"line":92,"address":[],"length":0,"stats":{"Line":2}},{"line":101,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":3}},{"line":104,"address":[],"length":0,"stats":{"Line":1}},{"line":107,"address":[],"length":0,"stats":{"Line":1}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":0}},{"line":120,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":1}},{"line":126,"address":[],"length":0,"stats":{"Line":1}},{"line":127,"address":[],"length":0,"stats":{"Line":1}},{"line":131,"address":[],"length":0,"stats":{"Line":1}},{"line":132,"address":[],"length":0,"stats":{"Line":1}},{"line":133,"address":[],"length":0,"stats":{"Line":1}}],"covered":22,"coverable":28},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","http.rs"],"content":"//! HTTP-based reranker implementation.\n//!\n//! This module provides reranking via HTTP APIs (Jina, Cohere, Aliyun).\n//!\n//! # Supported Providers\n//!\n//! | Provider | API | Features |\n//! |----------|-----|----------|\n//! | Jina AI | REST | Multilingual, fast |\n//! | Cohere | REST | rerank-v3.5, 4K context |\n//! | Aliyun | REST | DashScope gte-rerank-v2 |\n\nuse async_trait::async_trait;\nuse reqwest::Client;\nuse std::collections::HashMap;\nuse tracing::{debug, warn};\n\nuse super::config::{RerankConfig, ScoreAggregation};\nuse super::result::RerankResult;\nuse super::traits::Reranker;\nuse crate::error::{LlmError, Result};\n\n/// HTTP-based reranker that supports multiple providers.\n///\n/// # Architecture\n///\n/// ```ascii\n/// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n/// â”‚  HttpReranker  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  Provider API   â”‚\n/// â”‚                â”‚             â”‚  (Jina/Cohere)  â”‚\n/// â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n///          â”‚                              â”‚\n///          â”‚   RerankConfig               â”‚  JSON Response\n///          â”‚   - model                    â”‚  - results[]\n///          â”‚   - base_url                 â”‚    - index\n///          â”‚   - api_key                  â”‚    - relevance_score\n///          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n/// ```\npub struct HttpReranker {\n    client: Client,\n    config: RerankConfig,\n    /// Response format for parsing results.\n    response_format: ResponseFormat,\n    /// Request format for building payloads.\n    request_format: RequestFormat,\n}\n\n#[derive(Debug, Clone, Copy)]\nenum ResponseFormat {\n    /// Standard format: {\"results\": [{\"index\": 0, \"relevance_score\": 0.9}]}\n    Standard,\n    /// Aliyun format: {\"output\": {\"results\": [...]}}\n    Aliyun,\n}\n\n#[derive(Debug, Clone, Copy)]\nenum RequestFormat {\n    /// Standard format: {\"query\": \"...\", \"documents\": [...]}\n    Standard,\n    /// Aliyun format: {\"input\": {\"query\": \"...\", \"documents\": [...]}}\n    Aliyun,\n}\n\nimpl HttpReranker {\n    /// Create a new HTTP reranker with the given config.\n    pub fn new(config: RerankConfig) -> Self {\n        let (response_format, request_format) = Self::detect_format(&config.base_url);\n\n        let client = Client::builder()\n            .timeout(config.timeout)\n            .build()\n            .expect(\"Failed to build HTTP client\");\n\n        Self {\n            client,\n            config,\n            response_format,\n            request_format,\n        }\n    }\n\n    /// Create a Jina reranker.\n    pub fn jina(api_key: impl Into<String>) -> Self {\n        Self::new(RerankConfig::jina(api_key))\n    }\n\n    /// Create a Cohere reranker.\n    pub fn cohere(api_key: impl Into<String>) -> Self {\n        Self::new(RerankConfig::cohere(api_key))\n    }\n\n    /// Create an Aliyun reranker.\n    pub fn aliyun(api_key: impl Into<String>) -> Self {\n        let config = RerankConfig::aliyun(api_key);\n        Self {\n            client: Client::builder()\n                .timeout(config.timeout)\n                .build()\n                .expect(\"Failed to build HTTP client\"),\n            config,\n            response_format: ResponseFormat::Aliyun,\n            request_format: RequestFormat::Aliyun,\n        }\n    }\n\n    fn detect_format(base_url: &str) -> (ResponseFormat, RequestFormat) {\n        if base_url.contains(\"dashscope.aliyuncs.com\") {\n            (ResponseFormat::Aliyun, RequestFormat::Aliyun)\n        } else {\n            (ResponseFormat::Standard, RequestFormat::Standard)\n        }\n    }\n\n    fn build_request(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> serde_json::Value {\n        match self.request_format {\n            RequestFormat::Standard => {\n                let mut payload = serde_json::json!({\n                    \"model\": self.config.model,\n                    \"query\": query,\n                    \"documents\": documents,\n                });\n                if let Some(n) = top_n {\n                    payload[\"top_n\"] = serde_json::json!(n);\n                }\n                payload\n            }\n            RequestFormat::Aliyun => {\n                let mut params = serde_json::Map::new();\n                if let Some(n) = top_n {\n                    params.insert(\"top_n\".to_string(), serde_json::json!(n));\n                }\n                serde_json::json!({\n                    \"model\": self.config.model,\n                    \"input\": {\n                        \"query\": query,\n                        \"documents\": documents,\n                    },\n                    \"parameters\": params,\n                })\n            }\n        }\n    }\n\n    fn parse_response(&self, response: serde_json::Value) -> Result<Vec<RerankResult>> {\n        let results = match self.response_format {\n            ResponseFormat::Standard => response\n                .get(\"results\")\n                .and_then(|r| r.as_array())\n                .cloned()\n                .unwrap_or_default(),\n            ResponseFormat::Aliyun => response\n                .get(\"output\")\n                .and_then(|o| o.get(\"results\"))\n                .and_then(|r| r.as_array())\n                .cloned()\n                .unwrap_or_default(),\n        };\n\n        if results.is_empty() {\n            warn!(\"Rerank API returned empty results\");\n            return Ok(vec![]);\n        }\n\n        let mut rerank_results = Vec::with_capacity(results.len());\n        for result in results {\n            let index = result\n                .get(\"index\")\n                .and_then(|i| i.as_u64())\n                .ok_or_else(|| LlmError::Unknown(\"Missing index in rerank result\".to_string()))?\n                as usize;\n            let score = result\n                .get(\"relevance_score\")\n                .and_then(|s| s.as_f64())\n                .ok_or_else(|| {\n                    LlmError::Unknown(\"Missing relevance_score in rerank result\".to_string())\n                })?;\n\n            rerank_results.push(RerankResult {\n                index,\n                relevance_score: score,\n            });\n        }\n\n        // Sort by relevance score descending\n        rerank_results.sort_by(|a, b| {\n            b.relevance_score\n                .partial_cmp(&a.relevance_score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n\n        Ok(rerank_results)\n    }\n\n    /// Chunk documents that exceed the token limit.\n    pub fn chunk_documents(&self, documents: &[String]) -> (Vec<String>, Vec<usize>) {\n        if !self.config.enable_chunking {\n            let indices: Vec<usize> = (0..documents.len()).collect();\n            return (documents.to_vec(), indices);\n        }\n\n        let max_chars = self.config.max_tokens_per_doc * 4; // Approximate 1 token â‰ˆ 4 chars\n        let overlap_chars = 32 * 4; // 32 tokens overlap\n\n        let mut chunked = Vec::new();\n        let mut indices = Vec::new();\n\n        for (idx, doc) in documents.iter().enumerate() {\n            if doc.len() <= max_chars {\n                chunked.push(doc.clone());\n                indices.push(idx);\n            } else {\n                // Split into overlapping chunks\n                let mut start = 0;\n                while start < doc.len() {\n                    let end = (start + max_chars).min(doc.len());\n                    let chunk = doc[start..end].to_string();\n                    chunked.push(chunk);\n                    indices.push(idx);\n\n                    if end >= doc.len() {\n                        break;\n                    }\n                    start = end.saturating_sub(overlap_chars);\n                }\n            }\n        }\n\n        debug!(\n            \"Chunked {} documents into {} chunks\",\n            documents.len(),\n            chunked.len()\n        );\n        (chunked, indices)\n    }\n\n    /// Aggregate chunk scores back to original documents.\n    pub fn aggregate_scores(\n        &self,\n        chunk_results: Vec<RerankResult>,\n        doc_indices: &[usize],\n        num_docs: usize,\n        aggregation: ScoreAggregation,\n    ) -> Vec<RerankResult> {\n        let mut doc_scores: HashMap<usize, Vec<f64>> = HashMap::new();\n        for i in 0..num_docs {\n            doc_scores.insert(i, Vec::new());\n        }\n\n        for result in chunk_results {\n            if result.index < doc_indices.len() {\n                let original_idx = doc_indices[result.index];\n                if let Some(scores) = doc_scores.get_mut(&original_idx) {\n                    scores.push(result.relevance_score);\n                }\n            }\n        }\n\n        let mut aggregated: Vec<RerankResult> = doc_scores\n            .into_iter()\n            .filter(|(_, scores)| !scores.is_empty())\n            .map(|(idx, scores)| {\n                let final_score = match aggregation {\n                    ScoreAggregation::Max => {\n                        scores.iter().cloned().fold(f64::NEG_INFINITY, f64::max)\n                    }\n                    ScoreAggregation::Mean => scores.iter().sum::<f64>() / scores.len() as f64,\n                    ScoreAggregation::First => scores[0],\n                };\n                RerankResult {\n                    index: idx,\n                    relevance_score: final_score,\n                }\n            })\n            .collect();\n\n        aggregated.sort_by(|a, b| {\n            b.relevance_score\n                .partial_cmp(&a.relevance_score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n        aggregated\n    }\n}\n\n#[async_trait]\nimpl Reranker for HttpReranker {\n    fn name(&self) -> &str {\n        if self.config.base_url.contains(\"jina.ai\") {\n            \"jina\"\n        } else if self.config.base_url.contains(\"cohere.com\") {\n            \"cohere\"\n        } else if self.config.base_url.contains(\"aliyuncs.com\") {\n            \"aliyun\"\n        } else {\n            \"http\"\n        }\n    }\n\n    fn model(&self) -> &str {\n        &self.config.model\n    }\n\n    async fn rerank(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        if documents.is_empty() {\n            return Ok(vec![]);\n        }\n\n        // Handle chunking\n        let (chunked_docs, doc_indices) = self.chunk_documents(documents);\n        let original_top_n = top_n;\n\n        // When chunking, disable top_n at API level to get all chunk scores\n        let api_top_n = if self.config.enable_chunking {\n            None\n        } else {\n            top_n\n        };\n\n        let payload = self.build_request(query, &chunked_docs, api_top_n);\n\n        debug!(\n            \"Rerank request: {} documents, model: {}\",\n            chunked_docs.len(),\n            self.config.model\n        );\n\n        let mut request = self\n            .client\n            .post(&self.config.base_url)\n            .header(\"Content-Type\", \"application/json\");\n\n        if let Some(ref api_key) = self.config.api_key {\n            request = request.header(\"Authorization\", format!(\"Bearer {}\", api_key));\n        }\n\n        let response = request\n            .json(&payload)\n            .send()\n            .await\n            .map_err(|e| LlmError::NetworkError(e.to_string()))?;\n\n        if !response.status().is_success() {\n            let status = response.status();\n            let error_text = response.text().await.unwrap_or_default();\n            return Err(LlmError::ApiError(format!(\n                \"Rerank API error ({}): {}\",\n                status.as_u16(),\n                error_text\n            )));\n        }\n\n        let response_json: serde_json::Value = response\n            .json()\n            .await\n            .map_err(|e| LlmError::Unknown(format!(\"Failed to parse rerank response: {}\", e)))?;\n\n        let mut results = self.parse_response(response_json)?;\n\n        // Aggregate chunk scores if chunking was enabled\n        if self.config.enable_chunking && chunked_docs.len() != documents.len() {\n            results = self.aggregate_scores(\n                results,\n                &doc_indices,\n                documents.len(),\n                ScoreAggregation::Max,\n            );\n        }\n\n        // Apply top_n limit at document level\n        if let Some(n) = original_top_n {\n            results.truncate(n);\n        }\n\n        Ok(results)\n    }\n}\n","traces":[{"line":66,"address":[],"length":0,"stats":{"Line":2}},{"line":67,"address":[],"length":0,"stats":{"Line":6}},{"line":69,"address":[],"length":0,"stats":{"Line":4}},{"line":70,"address":[],"length":0,"stats":{"Line":4}},{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}},{"line":88,"address":[],"length":0,"stats":{"Line":0}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":93,"address":[],"length":0,"stats":{"Line":0}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":96,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":2}},{"line":107,"address":[],"length":0,"stats":{"Line":4}},{"line":108,"address":[],"length":0,"stats":{"Line":0}},{"line":110,"address":[],"length":0,"stats":{"Line":2}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":120,"address":[],"length":0,"stats":{"Line":0}},{"line":122,"address":[],"length":0,"stats":{"Line":0}},{"line":123,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}},{"line":127,"address":[],"length":0,"stats":{"Line":0}},{"line":128,"address":[],"length":0,"stats":{"Line":0}},{"line":130,"address":[],"length":0,"stats":{"Line":0}},{"line":133,"address":[],"length":0,"stats":{"Line":0}},{"line":134,"address":[],"length":0,"stats":{"Line":0}},{"line":135,"address":[],"length":0,"stats":{"Line":0}},{"line":137,"address":[],"length":0,"stats":{"Line":0}},{"line":138,"address":[],"length":0,"stats":{"Line":0}},{"line":139,"address":[],"length":0,"stats":{"Line":0}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":0}},{"line":143,"address":[],"length":0,"stats":{"Line":0}},{"line":149,"address":[],"length":0,"stats":{"Line":0}},{"line":150,"address":[],"length":0,"stats":{"Line":0}},{"line":151,"address":[],"length":0,"stats":{"Line":0}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":156,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":0}},{"line":159,"address":[],"length":0,"stats":{"Line":0}},{"line":164,"address":[],"length":0,"stats":{"Line":0}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":166,"address":[],"length":0,"stats":{"Line":0}},{"line":169,"address":[],"length":0,"stats":{"Line":0}},{"line":170,"address":[],"length":0,"stats":{"Line":0}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":174,"address":[],"length":0,"stats":{"Line":0}},{"line":176,"address":[],"length":0,"stats":{"Line":0}},{"line":178,"address":[],"length":0,"stats":{"Line":0}},{"line":179,"address":[],"length":0,"stats":{"Line":0}},{"line":180,"address":[],"length":0,"stats":{"Line":0}},{"line":183,"address":[],"length":0,"stats":{"Line":0}},{"line":184,"address":[],"length":0,"stats":{"Line":0}},{"line":185,"address":[],"length":0,"stats":{"Line":0}},{"line":190,"address":[],"length":0,"stats":{"Line":0}},{"line":191,"address":[],"length":0,"stats":{"Line":0}},{"line":192,"address":[],"length":0,"stats":{"Line":0}},{"line":193,"address":[],"length":0,"stats":{"Line":0}},{"line":196,"address":[],"length":0,"stats":{"Line":0}},{"line":200,"address":[],"length":0,"stats":{"Line":1}},{"line":201,"address":[],"length":0,"stats":{"Line":1}},{"line":202,"address":[],"length":0,"stats":{"Line":0}},{"line":203,"address":[],"length":0,"stats":{"Line":0}},{"line":206,"address":[],"length":0,"stats":{"Line":2}},{"line":207,"address":[],"length":0,"stats":{"Line":2}},{"line":209,"address":[],"length":0,"stats":{"Line":2}},{"line":210,"address":[],"length":0,"stats":{"Line":2}},{"line":212,"address":[],"length":0,"stats":{"Line":7}},{"line":213,"address":[],"length":0,"stats":{"Line":5}},{"line":214,"address":[],"length":0,"stats":{"Line":5}},{"line":215,"address":[],"length":0,"stats":{"Line":2}},{"line":218,"address":[],"length":0,"stats":{"Line":2}},{"line":219,"address":[],"length":0,"stats":{"Line":26}},{"line":220,"address":[],"length":0,"stats":{"Line":65}},{"line":221,"address":[],"length":0,"stats":{"Line":52}},{"line":222,"address":[],"length":0,"stats":{"Line":39}},{"line":223,"address":[],"length":0,"stats":{"Line":39}},{"line":225,"address":[],"length":0,"stats":{"Line":26}},{"line":226,"address":[],"length":0,"stats":{"Line":1}},{"line":228,"address":[],"length":0,"stats":{"Line":24}},{"line":233,"address":[],"length":0,"stats":{"Line":1}},{"line":234,"address":[],"length":0,"stats":{"Line":0}},{"line":235,"address":[],"length":0,"stats":{"Line":0}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":1}},{"line":242,"address":[],"length":0,"stats":{"Line":1}},{"line":249,"address":[],"length":0,"stats":{"Line":3}},{"line":250,"address":[],"length":0,"stats":{"Line":5}},{"line":251,"address":[],"length":0,"stats":{"Line":6}},{"line":254,"address":[],"length":0,"stats":{"Line":7}},{"line":255,"address":[],"length":0,"stats":{"Line":6}},{"line":256,"address":[],"length":0,"stats":{"Line":6}},{"line":257,"address":[],"length":0,"stats":{"Line":12}},{"line":258,"address":[],"length":0,"stats":{"Line":6}},{"line":263,"address":[],"length":0,"stats":{"Line":3}},{"line":265,"address":[],"length":0,"stats":{"Line":5}},{"line":266,"address":[],"length":0,"stats":{"Line":3}},{"line":267,"address":[],"length":0,"stats":{"Line":4}},{"line":269,"address":[],"length":0,"stats":{"Line":6}},{"line":271,"address":[],"length":0,"stats":{"Line":0}},{"line":272,"address":[],"length":0,"stats":{"Line":0}},{"line":274,"address":[],"length":0,"stats":{"Line":2}},{"line":275,"address":[],"length":0,"stats":{"Line":2}},{"line":276,"address":[],"length":0,"stats":{"Line":2}},{"line":281,"address":[],"length":0,"stats":{"Line":3}},{"line":282,"address":[],"length":0,"stats":{"Line":1}},{"line":283,"address":[],"length":0,"stats":{"Line":2}},{"line":284,"address":[],"length":0,"stats":{"Line":2}},{"line":286,"address":[],"length":0,"stats":{"Line":1}},{"line":292,"address":[],"length":0,"stats":{"Line":0}},{"line":293,"address":[],"length":0,"stats":{"Line":0}},{"line":294,"address":[],"length":0,"stats":{"Line":0}},{"line":295,"address":[],"length":0,"stats":{"Line":0}},{"line":296,"address":[],"length":0,"stats":{"Line":0}},{"line":297,"address":[],"length":0,"stats":{"Line":0}},{"line":298,"address":[],"length":0,"stats":{"Line":0}},{"line":300,"address":[],"length":0,"stats":{"Line":0}},{"line":304,"address":[],"length":0,"stats":{"Line":0}},{"line":305,"address":[],"length":0,"stats":{"Line":0}},{"line":350,"address":[],"length":0,"stats":{"Line":0}},{"line":365,"address":[],"length":0,"stats":{"Line":0}}],"covered":50,"coverable":123},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","hybrid.rs"],"content":"//! Hybrid reranker combining BM25 with vector similarity.\n//!\n//! Uses RRF to fuse text-based and vector-based rankings.\n//!\n//! # Architecture\n//!\n//! ```ascii\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚    Query    â”‚     â”‚   Vector    â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚   Search    â”‚\n//!        â”‚            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n//!        â–¼                   â”‚\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n//! â”‚ BM25Reranker â”‚           â”‚ (optional pre-computed)\n//! â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n//!        â”‚                   â”‚\n//!        â–¼                   â–¼\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚         RRF Fusion              â”‚\n//! â”‚  score = 1/(k+rank_bm25) +      â”‚\n//! â”‚          1/(k+rank_vector)      â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//!               â”‚\n//!               â–¼\n//!        Fused Results\n//! ```\n\nuse async_trait::async_trait;\n\nuse super::bm25::BM25Reranker;\nuse super::result::RerankResult;\nuse super::rrf::RRFReranker;\nuse super::traits::Reranker;\nuse crate::error::Result;\n\n/// Hybrid reranker combining BM25 with vector similarity boosting.\n///\n/// # Example\n///\n/// ```ignore\n/// use edgequake_llm::reranker::HybridReranker;\n///\n/// let reranker = HybridReranker::new();\n///\n/// // With both text and vector rankings\n/// let vector_rankings = vec![2, 0, 1]; // From vector search\n/// let results = reranker.rerank_hybrid(\n///     \"machine learning\",\n///     &documents,\n///     Some(vector_rankings),\n///     Some(10)\n/// ).await?;\n/// ```\npub struct HybridReranker {\n    bm25: BM25Reranker,\n    rrf: RRFReranker,\n    model: String,\n}\n\nimpl HybridReranker {\n    /// Create a new hybrid reranker.\n    pub fn new() -> Self {\n        Self {\n            bm25: BM25Reranker::new(),\n            rrf: RRFReranker::new(),\n            model: \"hybrid-reranker\".to_string(),\n        }\n    }\n\n    /// Rerank with both text and vector signals.\n    ///\n    /// # Arguments\n    ///\n    /// - `query`: Search query for BM25\n    /// - `documents`: Document texts\n    /// - `vector_rankings`: Pre-sorted indices from vector search (best first)\n    /// - `top_n`: Maximum results to return\n    pub async fn rerank_hybrid(\n        &self,\n        query: &str,\n        documents: &[String],\n        vector_rankings: Option<Vec<usize>>,\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        if documents.is_empty() {\n            return Ok(vec![]);\n        }\n\n        // Get BM25 ranking\n        let bm25_results = self.bm25.rerank(query, documents, None).await?;\n        let bm25_ranking: Vec<usize> = bm25_results.iter().map(|r| r.index).collect();\n\n        // Combine with vector ranking if provided\n        let mut ranked_lists = vec![bm25_ranking];\n        if let Some(vec_ranking) = vector_rankings {\n            ranked_lists.push(vec_ranking);\n        }\n\n        // Use RRF to fuse rankings\n        let mut results = self.rrf.fuse(&ranked_lists, documents.len());\n\n        if let Some(n) = top_n {\n            results.truncate(n);\n        }\n\n        Ok(results)\n    }\n}\n\nimpl Default for HybridReranker {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[async_trait]\nimpl Reranker for HybridReranker {\n    fn name(&self) -> &str {\n        \"hybrid\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    async fn rerank(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        // Without vector rankings, just use BM25\n        self.bm25.rerank(query, documents, top_n).await\n    }\n}\n","traces":[{"line":62,"address":[],"length":0,"stats":{"Line":3}},{"line":64,"address":[],"length":0,"stats":{"Line":6}},{"line":65,"address":[],"length":0,"stats":{"Line":6}},{"line":66,"address":[],"length":0,"stats":{"Line":3}},{"line":78,"address":[],"length":0,"stats":{"Line":1}},{"line":85,"address":[],"length":0,"stats":{"Line":2}},{"line":86,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":6}},{"line":91,"address":[],"length":0,"stats":{"Line":5}},{"line":94,"address":[],"length":0,"stats":{"Line":3}},{"line":95,"address":[],"length":0,"stats":{"Line":3}},{"line":96,"address":[],"length":0,"stats":{"Line":2}},{"line":100,"address":[],"length":0,"stats":{"Line":6}},{"line":102,"address":[],"length":0,"stats":{"Line":1}},{"line":103,"address":[],"length":0,"stats":{"Line":0}},{"line":106,"address":[],"length":0,"stats":{"Line":1}},{"line":111,"address":[],"length":0,"stats":{"Line":0}},{"line":112,"address":[],"length":0,"stats":{"Line":0}},{"line":118,"address":[],"length":0,"stats":{"Line":1}},{"line":119,"address":[],"length":0,"stats":{"Line":1}},{"line":122,"address":[],"length":0,"stats":{"Line":1}},{"line":123,"address":[],"length":0,"stats":{"Line":1}}],"covered":18,"coverable":22},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","mod.rs"],"content":"//! Reranking functionality for improved retrieval quality.\n//!\n//! This module provides reranking capabilities to improve search result relevance\n//! by scoring documents against a query using specialized reranking models.\n//!\n//! # Architecture\n//!\n//! ```ascii\n//!                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//!                    â”‚      Query + Documents      â”‚\n//!                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//!                                   â”‚\n//!                                   â–¼\n//!     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//!     â”‚                  Reranker Trait                      â”‚\n//!     â”‚   rerank(query, docs, top_n) â†’ Vec<RerankResult>    â”‚\n//!     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//!                                â”‚\n//!        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//!        â–¼                       â–¼                       â–¼\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚ HttpReranker â”‚      â”‚ BM25Reranker â”‚        â”‚HybridRerankerâ”‚\n//! â”‚ (Jina,Cohere)â”‚      â”‚  (Local)     â”‚        â”‚  (Combined)  â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Module Structure (OODA-02)\n//!\n//! ```ascii\n//! reranker/\n//! â”œâ”€â”€ mod.rs         â”€â–º This file (re-exports)\n//! â”œâ”€â”€ config.rs      â”€â–º RerankConfig, ScoreAggregation\n//! â”œâ”€â”€ result.rs      â”€â–º RerankResult\n//! â”œâ”€â”€ traits.rs      â”€â–º Reranker trait\n//! â”œâ”€â”€ http.rs        â”€â–º HttpReranker (Jina, Cohere, Aliyun)\n//! â”œâ”€â”€ term_overlap.rsâ”€â–º TermOverlapReranker, MockReranker\n//! â”œâ”€â”€ bm25.rs        â”€â–º BM25Reranker, TokenizerConfig\n//! â”œâ”€â”€ rrf.rs         â”€â–º RRFReranker\n//! â””â”€â”€ hybrid.rs      â”€â–º HybridReranker\n//! ```\n//!\n//! # Implements\n//!\n//! - **FEAT0774**: Reranking for improved retrieval\n//! - **FEAT0775**: Multi-provider reranker support\n//! - **FEAT0776**: BM25 keyword fallback scoring\n//!\n//! # Enforces\n//!\n//! - **BR0774**: Top-k results after reranking\n//! - **BR0775**: Fallback to BM25 if reranker unavailable\n//!\n//! # Providers\n//!\n//! | Provider | Type | Notes |\n//! |----------|------|-------|\n//! | Jina AI | HTTP | Production multilingual |\n//! | Cohere | HTTP | Production rerank-v3.5 |\n//! | Aliyun | HTTP | DashScope gte-rerank-v2 |\n//! | BM25 | Local | No API key needed |\n//! | TermOverlap | Local | Fast testing fallback |\n//! | Hybrid | Combined | BM25 + Neural fusion |\n//!\n//! # Example\n//!\n//! ```ignore\n//! use edgequake_llm::reranker::{BM25Reranker, Reranker};\n//!\n//! let reranker = BM25Reranker::new();\n//! let results = reranker.rerank(\"rust async\", &docs, Some(10)).await?;\n//! ```\n\n// Sub-modules (SRP: each module has a single responsibility)\nmod bm25;\nmod config;\nmod http;\nmod hybrid;\nmod result;\nmod rrf;\nmod term_overlap;\nmod traits;\n\n// Re-export public types - maintains backward compatibility with existing API\npub use bm25::{BM25Reranker, TokenizerConfig};\npub use config::{RerankConfig, ScoreAggregation};\npub use http::HttpReranker;\npub use hybrid::HybridReranker;\npub use result::RerankResult;\npub use rrf::RRFReranker;\npub use term_overlap::{MockReranker, TermOverlapReranker};\npub use traits::Reranker;\n\n#[cfg(test)]\nmod tests;\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","result.rs"],"content":"//! Reranking result types.\n//!\n//! This module contains the result types returned by reranking operations.\n\nuse serde::{Deserialize, Serialize};\n\n/// Result from reranking a document.\n///\n/// # Fields\n///\n/// - `index`: Position of the document in the original input list\n/// - `relevance_score`: Computed relevance score (higher = more relevant)\n///\n/// # Example\n///\n/// ```ignore\n/// let results = reranker.rerank(query, documents, Some(10)).await?;\n/// for result in results {\n///     println!(\"Doc {} has score {:.3}\", result.index, result.relevance_score);\n/// }\n/// ```\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct RerankResult {\n    /// Index of the document in the original list.\n    pub index: usize,\n    /// Relevance score (higher is more relevant).\n    pub relevance_score: f64,\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","rrf.rs"],"content":"//! Reciprocal Rank Fusion (RRF) reranker.\n//!\n//! Combines multiple ranking signals without needing score normalization.\n//!\n//! # Algorithm\n//!\n//! ```ascii\n//! RRF Score = Î£ 1/(k + rank) for each ranking list\n//!\n//! Where:\n//! - k = smoothing constant (default 60)\n//! - rank = 1-indexed position in each list\n//! ```\n//!\n//! # Use Cases\n//!\n//! - Combining vector similarity + BM25 rankings\n//! - Combining results from multiple queries\n//! - Hybrid search scenarios\n\nuse async_trait::async_trait;\n\nuse super::bm25::BM25Reranker;\nuse super::result::RerankResult;\nuse super::traits::Reranker;\nuse crate::error::Result;\n\n/// Reciprocal Rank Fusion reranker.\n///\n/// # Example\n///\n/// ```ignore\n/// use edgequake_llm::reranker::RRFReranker;\n///\n/// let rrf = RRFReranker::new();\n///\n/// // Combine BM25 and vector rankings\n/// let bm25_ranking = vec![0, 2, 1]; // doc 0 best, then 2, then 1\n/// let vector_ranking = vec![1, 0, 2]; // doc 1 best, then 0, then 2\n///\n/// let fused = rrf.fuse(&[bm25_ranking, vector_ranking], 3);\n/// // Result balances both signals\n/// ```\npub struct RRFReranker {\n    /// Ranking constant (higher = lower-ranked docs have more influence).\n    k: u32,\n    /// Model name for trait compliance.\n    model: String,\n}\n\nimpl RRFReranker {\n    /// Create a new RRF reranker with default k=60.\n    pub fn new() -> Self {\n        Self {\n            k: 60,\n            model: \"rrf-reranker\".to_string(),\n        }\n    }\n\n    /// Create with custom k value.\n    pub fn with_k(k: u32) -> Self {\n        Self {\n            k: k.max(1),\n            model: \"rrf-reranker\".to_string(),\n        }\n    }\n\n    /// Fuse multiple ranked lists using RRF.\n    ///\n    /// Each inner Vec contains document indices in ranked order (best first).\n    ///\n    /// # Algorithm\n    ///\n    /// ```ascii\n    /// For each ranking list:\n    ///   For each (rank, doc_idx) in list:\n    ///     scores[doc_idx] += 1 / (k + rank + 1)\n    ///\n    /// Return docs sorted by total score\n    /// ```\n    pub fn fuse(&self, ranked_lists: &[Vec<usize>], num_docs: usize) -> Vec<RerankResult> {\n        let mut scores = vec![0.0f64; num_docs];\n\n        for ranked_list in ranked_lists {\n            for (rank, &doc_idx) in ranked_list.iter().enumerate() {\n                if doc_idx < num_docs {\n                    scores[doc_idx] += 1.0 / (self.k as f64 + rank as f64 + 1.0);\n                }\n            }\n        }\n\n        let mut results: Vec<RerankResult> = scores\n            .into_iter()\n            .enumerate()\n            .filter(|(_, score)| *score > 0.0)\n            .map(|(idx, score)| RerankResult {\n                index: idx,\n                relevance_score: score,\n            })\n            .collect();\n\n        results.sort_by(|a, b| {\n            b.relevance_score\n                .partial_cmp(&a.relevance_score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n\n        results\n    }\n}\n\nimpl Default for RRFReranker {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n#[async_trait]\nimpl Reranker for RRFReranker {\n    fn name(&self) -> &str {\n        \"rrf\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    async fn rerank(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        // RRF alone uses BM25 as the single ranking signal.\n        // For true RRF, use the fuse() method with multiple sources.\n        let bm25 = BM25Reranker::new();\n        let mut results = bm25.rerank(query, documents, None).await?;\n\n        if let Some(n) = top_n {\n            results.truncate(n);\n        }\n\n        Ok(results)\n    }\n}\n","traces":[{"line":53,"address":[],"length":0,"stats":{"Line":7}},{"line":56,"address":[],"length":0,"stats":{"Line":7}},{"line":61,"address":[],"length":0,"stats":{"Line":1}},{"line":63,"address":[],"length":0,"stats":{"Line":3}},{"line":64,"address":[],"length":0,"stats":{"Line":1}},{"line":81,"address":[],"length":0,"stats":{"Line":5}},{"line":82,"address":[],"length":0,"stats":{"Line":15}},{"line":84,"address":[],"length":0,"stats":{"Line":19}},{"line":85,"address":[],"length":0,"stats":{"Line":56}},{"line":86,"address":[],"length":0,"stats":{"Line":42}},{"line":87,"address":[],"length":0,"stats":{"Line":42}},{"line":92,"address":[],"length":0,"stats":{"Line":15}},{"line":95,"address":[],"length":0,"stats":{"Line":22}},{"line":96,"address":[],"length":0,"stats":{"Line":5}},{"line":97,"address":[],"length":0,"stats":{"Line":12}},{"line":98,"address":[],"length":0,"stats":{"Line":12}},{"line":102,"address":[],"length":0,"stats":{"Line":20}},{"line":103,"address":[],"length":0,"stats":{"Line":10}},{"line":104,"address":[],"length":0,"stats":{"Line":20}},{"line":105,"address":[],"length":0,"stats":{"Line":20}},{"line":108,"address":[],"length":0,"stats":{"Line":5}},{"line":113,"address":[],"length":0,"stats":{"Line":0}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":120,"address":[],"length":0,"stats":{"Line":0}},{"line":121,"address":[],"length":0,"stats":{"Line":0}},{"line":124,"address":[],"length":0,"stats":{"Line":0}},{"line":125,"address":[],"length":0,"stats":{"Line":0}}],"covered":21,"coverable":27},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","term_overlap.rs"],"content":"//! Term overlap reranker implementation.\n//!\n//! A simple, fast reranker using Jaccard-like term overlap scoring.\n//!\n//! # When to Use\n//!\n//! - Testing and development (no API keys required)\n//! - Fallback when BM25 is overkill\n//! - Simple use cases with short documents\n//!\n//! # Limitations\n//!\n//! - No IDF weighting (rare terms not prioritized)\n//! - No term frequency consideration\n//! - No length normalization\n//!\n//! For production, prefer [`super::BM25Reranker`].\n\nuse async_trait::async_trait;\nuse std::collections::HashSet;\n\nuse super::result::RerankResult;\nuse super::traits::Reranker;\nuse crate::error::Result;\n\n/// Term overlap reranker using simple Jaccard-like scoring.\n///\n/// # Algorithm\n///\n/// ```ascii\n/// Query Terms:  {capital, of, France}\n///                      â”‚\n///                      â–¼\n/// Document:     \"The capital of France is Paris\"\n///                      â”‚\n///                      â–¼\n/// Doc Terms:    {the, capital, of, France, is, Paris}\n///                      â”‚\n///                      â–¼\n/// Score = |query âˆ© doc| / |query| = 3/3 = 1.0\n/// ```\n///\n/// # Example\n///\n/// ```ignore\n/// use edgequake_llm::reranker::{TermOverlapReranker, Reranker};\n///\n/// let reranker = TermOverlapReranker::new();\n/// let results = reranker.rerank(\"capital France\", &docs, Some(10)).await?;\n/// ```\npub struct TermOverlapReranker {\n    model: String,\n}\n\nimpl TermOverlapReranker {\n    /// Create a new term overlap reranker.\n    pub fn new() -> Self {\n        Self {\n            model: \"term-overlap-reranker\".to_string(),\n        }\n    }\n}\n\nimpl Default for TermOverlapReranker {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\n/// Backward compatibility alias for `TermOverlapReranker`.\n///\n/// **Deprecated**: Use `TermOverlapReranker` for new code.\npub type MockReranker = TermOverlapReranker;\n\n#[async_trait]\nimpl Reranker for TermOverlapReranker {\n    fn name(&self) -> &str {\n        \"term-overlap\"\n    }\n\n    fn model(&self) -> &str {\n        &self.model\n    }\n\n    async fn rerank(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        // Score based on query term overlap (Jaccard-like metric)\n        let query_lower = query.to_lowercase();\n        let query_terms: HashSet<String> = query_lower\n            .split_whitespace()\n            .map(|s| s.to_string())\n            .collect();\n\n        let mut results: Vec<RerankResult> = documents\n            .iter()\n            .enumerate()\n            .map(|(idx, doc)| {\n                let doc_lower = doc.to_lowercase();\n                let doc_terms: HashSet<String> = doc_lower\n                    .split_whitespace()\n                    .map(|s| s.to_string())\n                    .collect();\n\n                let overlap = query_terms.intersection(&doc_terms).count();\n                let max_terms = query_terms.len().max(1);\n                let score = overlap as f64 / max_terms as f64;\n\n                RerankResult {\n                    index: idx,\n                    relevance_score: score,\n                }\n            })\n            .collect();\n\n        // Sort by score descending\n        results.sort_by(|a, b| {\n            b.relevance_score\n                .partial_cmp(&a.relevance_score)\n                .unwrap_or(std::cmp::Ordering::Equal)\n        });\n\n        // Apply top_n\n        if let Some(n) = top_n {\n            results.truncate(n);\n        }\n\n        Ok(results)\n    }\n}\n","traces":[{"line":57,"address":[],"length":0,"stats":{"Line":8}},{"line":59,"address":[],"length":0,"stats":{"Line":8}},{"line":65,"address":[],"length":0,"stats":{"Line":0}},{"line":66,"address":[],"length":0,"stats":{"Line":0}},{"line":77,"address":[],"length":0,"stats":{"Line":3}},{"line":78,"address":[],"length":0,"stats":{"Line":3}},{"line":81,"address":[],"length":0,"stats":{"Line":0}},{"line":82,"address":[],"length":0,"stats":{"Line":0}},{"line":95,"address":[],"length":0,"stats":{"Line":26}},{"line":101,"address":[],"length":0,"stats":{"Line":18}},{"line":102,"address":[],"length":0,"stats":{"Line":36}},{"line":103,"address":[],"length":0,"stats":{"Line":54}},{"line":104,"address":[],"length":0,"stats":{"Line":18}},{"line":105,"address":[],"length":0,"stats":{"Line":150}},{"line":106,"address":[],"length":0,"stats":{"Line":18}},{"line":108,"address":[],"length":0,"stats":{"Line":90}},{"line":109,"address":[],"length":0,"stats":{"Line":72}},{"line":110,"address":[],"length":0,"stats":{"Line":36}},{"line":112,"address":[],"length":0,"stats":{"Line":18}},{"line":113,"address":[],"length":0,"stats":{"Line":18}},{"line":114,"address":[],"length":0,"stats":{"Line":18}},{"line":120,"address":[],"length":0,"stats":{"Line":14}},{"line":121,"address":[],"length":0,"stats":{"Line":14}},{"line":122,"address":[],"length":0,"stats":{"Line":28}},{"line":123,"address":[],"length":0,"stats":{"Line":28}}],"covered":21,"coverable":25},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","tests.rs"],"content":"//! Reranker tests.\n//!\n//! Comprehensive test suite for all reranker implementations.\n\nuse super::*;\n\n#[test]\nfn test_rerank_config_defaults() {\n    let config = RerankConfig::default();\n    assert_eq!(config.model, \"jina-reranker-v2-base-multilingual\");\n    assert!(config.api_key.is_none());\n}\n\n#[test]\nfn test_jina_config() {\n    let config = RerankConfig::jina(\"test-key\");\n    assert_eq!(config.api_key, Some(\"test-key\".to_string()));\n    assert!(config.base_url.contains(\"jina.ai\"));\n}\n\n#[test]\nfn test_cohere_config() {\n    let config = RerankConfig::cohere(\"test-key\");\n    assert!(config.base_url.contains(\"cohere.com\"));\n    assert_eq!(config.max_tokens_per_doc, 4096);\n}\n\n#[test]\nfn test_aliyun_config() {\n    let config = RerankConfig::aliyun(\"test-key\");\n    assert!(config.base_url.contains(\"aliyuncs.com\"));\n}\n\n#[tokio::test]\nasync fn test_mock_reranker() {\n    let reranker = MockReranker::new();\n    let query = \"capital of France\";\n    let documents = vec![\n        \"The capital of France is Paris.\".to_string(),\n        \"Tokyo is the capital of Japan.\".to_string(),\n        \"London is the capital of England.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, Some(2)).await.unwrap();\n\n    assert_eq!(results.len(), 2);\n    assert_eq!(results[0].index, 0);\n    assert!(results[0].relevance_score > 0.0);\n}\n\n#[tokio::test]\nasync fn test_mock_reranker_empty_docs() {\n    let reranker = MockReranker::new();\n    let results = reranker.rerank(\"test\", &[], None).await.unwrap();\n    assert!(results.is_empty());\n}\n\n#[test]\nfn test_score_aggregation() {\n    let reranker = HttpReranker::new(RerankConfig::default());\n\n    let chunk_results = vec![\n        RerankResult {\n            index: 0,\n            relevance_score: 0.9,\n        },\n        RerankResult {\n            index: 1,\n            relevance_score: 0.8,\n        },\n        RerankResult {\n            index: 2,\n            relevance_score: 0.7,\n        },\n    ];\n    let doc_indices = vec![0, 0, 1];\n\n    let aggregated =\n        reranker.aggregate_scores(chunk_results, &doc_indices, 2, ScoreAggregation::Max);\n\n    assert_eq!(aggregated.len(), 2);\n    assert_eq!(aggregated[0].index, 0);\n    assert!((aggregated[0].relevance_score - 0.9).abs() < 0.001);\n}\n\n#[test]\nfn test_chunking() {\n    let config = RerankConfig::default()\n        .with_chunking(true)\n        .with_max_tokens(50);\n    let reranker = HttpReranker::new(config);\n\n    let documents = vec![\"Short document.\".to_string(), \"A\".repeat(1000)];\n\n    let (chunked, indices) = reranker.chunk_documents(&documents);\n\n    assert!(chunked.len() > 2);\n    assert!(indices.iter().all(|&i| i < 2));\n}\n\n// =========== BM25 Reranker Tests ===========\n\n#[tokio::test]\nasync fn test_bm25_reranker_basic() {\n    let reranker = BM25Reranker::new();\n    let query = \"capital of France\";\n    let documents = vec![\n        \"The capital of France is Paris.\".to_string(),\n        \"Tokyo is the capital of Japan.\".to_string(),\n        \"London is the capital of England.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results.len(), 3);\n    assert_eq!(results[0].index, 0);\n    assert!(results[0].relevance_score > results[1].relevance_score);\n}\n\n#[tokio::test]\nasync fn test_bm25_idf_weighting() {\n    let reranker = BM25Reranker::new();\n    let query = \"Peugeot ENVY\";\n    let documents = vec![\n        \"The Peugeot 2008 ENVY is a great car.\".to_string(),\n        \"Peugeot makes many cars.\".to_string(),\n        \"Peugeot 208 is also available.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results[0].index, 0);\n    assert!(results[0].relevance_score > results[1].relevance_score * 1.5);\n}\n\n#[tokio::test]\nasync fn test_bm25_2008_vs_208_precision() {\n    let reranker = BM25Reranker::new();\n    let query = \"2008\";\n    let documents = vec![\n        \"The Peugeot 208 is a compact car.\".to_string(),\n        \"The Peugeot 2008 is an SUV.\".to_string(),\n        \"The Peugeot 3008 is a larger SUV.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results[0].index, 1, \"2008 document should be first\");\n    assert_ne!(results[0].index, 0, \"208 document should NOT be first\");\n}\n\n#[tokio::test]\nasync fn test_bm25_french_accents() {\n    let reranker = BM25Reranker::new();\n    let query = \"vehicule electrique\";\n    let documents = vec![\n        \"Le vÃ©hicule Ã©lectrique est l'avenir.\".to_string(),\n        \"Une voiture classique fonctionne Ã  essence.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results[0].index, 0);\n    assert!(results[0].relevance_score > 0.0);\n}\n\n#[tokio::test]\nasync fn test_bm25_empty_documents() {\n    let reranker = BM25Reranker::new();\n    let results = reranker.rerank(\"test\", &[], None).await.unwrap();\n    assert!(results.is_empty());\n}\n\n#[tokio::test]\nasync fn test_bm25_empty_query() {\n    let reranker = BM25Reranker::new();\n    let documents = vec![\"Some document.\".to_string()];\n    let results = reranker.rerank(\"\", &documents, None).await.unwrap();\n    assert_eq!(results.len(), 1);\n    assert_eq!(results[0].relevance_score, 0.0);\n}\n\n#[tokio::test]\nasync fn test_bm25_top_n() {\n    let reranker = BM25Reranker::new();\n    let documents = vec![\n        \"Alpha document.\".to_string(),\n        \"Beta document.\".to_string(),\n        \"Gamma document.\".to_string(),\n    ];\n\n    let results = reranker\n        .rerank(\"document\", &documents, Some(2))\n        .await\n        .unwrap();\n    assert_eq!(results.len(), 2);\n}\n\n#[test]\nfn test_bm25_tokenization() {\n    let tokens = BM25Reranker::tokenize(\"Hello, World! Test-123 vÃ©hicule\");\n    assert!(tokens.contains(&\"hello\".to_string()));\n    assert!(tokens.contains(&\"world\".to_string()));\n    assert!(tokens.contains(&\"test\".to_string()));\n    assert!(tokens.contains(&\"123\".to_string()));\n    assert!(tokens.contains(&\"vehicule\".to_string()));\n}\n\n#[test]\nfn test_bm25_custom_params() {\n    let reranker = BM25Reranker::with_params(2.0, 0.5);\n    assert_eq!(reranker.k1, 2.0);\n    assert_eq!(reranker.b, 0.5);\n}\n\n// =========== BM25+ Extension Tests ===========\n\n#[test]\nfn test_bm25_plus_constructor() {\n    let reranker = BM25Reranker::bm25_plus();\n    assert_eq!(reranker.k1, 1.5);\n    assert_eq!(reranker.b, 0.75);\n    assert_eq!(reranker.delta, 1.0);\n    assert_eq!(reranker.model(), \"bm25-plus-reranker\");\n}\n\n#[test]\nfn test_bm25_with_full_params() {\n    let reranker = BM25Reranker::with_full_params(1.2, 0.8, 0.5);\n    assert_eq!(reranker.k1, 1.2);\n    assert_eq!(reranker.b, 0.8);\n    assert_eq!(reranker.delta, 0.5);\n}\n\n// =========== Domain-Specific Preset Tests ===========\n\n#[test]\nfn test_for_short_docs_preset() {\n    let reranker = BM25Reranker::for_short_docs();\n    assert_eq!(reranker.k1, 1.2);\n    assert_eq!(reranker.b, 0.3);\n    assert_eq!(reranker.delta, 0.0);\n    assert_eq!(reranker.model(), \"bm25-short-docs\");\n    assert!(reranker.tokenizer_config.enable_stemming);\n}\n\n#[test]\nfn test_for_long_docs_preset() {\n    let reranker = BM25Reranker::for_long_docs();\n    assert_eq!(reranker.k1, 1.5);\n    assert_eq!(reranker.b, 0.75);\n    assert_eq!(reranker.delta, 1.0);\n    assert_eq!(reranker.model(), \"bm25-long-docs\");\n    assert!(reranker.tokenizer_config.enable_stemming);\n}\n\n#[test]\nfn test_for_technical_preset() {\n    let reranker = BM25Reranker::for_technical();\n    assert_eq!(reranker.k1, 2.0);\n    assert_eq!(reranker.b, 0.5);\n    assert_eq!(reranker.delta, 0.0);\n    assert_eq!(reranker.model(), \"bm25-technical\");\n    assert!(!reranker.tokenizer_config.enable_stemming);\n}\n\n#[test]\nfn test_for_rag_preset() {\n    let reranker = BM25Reranker::for_rag();\n    assert_eq!(reranker.k1, 1.5);\n    assert_eq!(reranker.b, 0.75);\n    assert_eq!(reranker.delta, 0.5);\n    assert_eq!(reranker.model(), \"bm25-rag\");\n    assert!(reranker.tokenizer_config.enable_stemming);\n    assert_eq!(reranker.phrase_boost, 0.3);\n}\n\n// =========== Phrase Boosting Tests ===========\n\n#[test]\nfn test_for_semantic_preset() {\n    let reranker = BM25Reranker::for_semantic();\n    assert_eq!(reranker.k1, 1.5);\n    assert_eq!(reranker.b, 0.75);\n    assert_eq!(reranker.phrase_boost, 0.5);\n    assert_eq!(reranker.model(), \"bm25-semantic\");\n}\n\n#[test]\nfn test_with_phrase_boost_builder() {\n    let reranker = BM25Reranker::new().with_phrase_boost(0.7);\n    assert_eq!(reranker.phrase_boost, 0.7);\n\n    let clamped = BM25Reranker::new().with_phrase_boost(5.0);\n    assert_eq!(clamped.phrase_boost, 2.0);\n}\n\n#[test]\nfn test_phrase_bonus_calculation() {\n    let reranker = BM25Reranker::new();\n\n    let query = vec![\"knowledge\".to_string(), \"graph\".to_string()];\n    let doc_with_phrase = vec![\n        \"some\".to_string(),\n        \"knowledge\".to_string(),\n        \"graph\".to_string(),\n        \"extraction\".to_string(),\n    ];\n    let doc_without_phrase = vec![\n        \"graph\".to_string(),\n        \"of\".to_string(),\n        \"knowledge\".to_string(),\n    ];\n\n    let bonus_with = reranker.compute_phrase_bonus(&query, &doc_with_phrase);\n    let bonus_without = reranker.compute_phrase_bonus(&query, &doc_without_phrase);\n\n    assert!(\n        bonus_with > 0.0,\n        \"Should have phrase bonus for adjacent terms\"\n    );\n    assert_eq!(\n        bonus_without, 0.0,\n        \"Should have no bonus for non-adjacent terms\"\n    );\n}\n\n#[tokio::test]\nasync fn test_phrase_boost_ranking_effect() {\n    let no_boost = BM25Reranker::new();\n    let with_boost = BM25Reranker::for_semantic();\n\n    let query = \"knowledge graph\";\n    let documents = vec![\n        \"This document discusses knowledge graph extraction.\".to_string(),\n        \"The graph of knowledge is complex.\".to_string(),\n        \"Something about graphs and some knowledge.\".to_string(),\n    ];\n\n    let results_no_boost = no_boost.rerank(query, &documents, None).await.unwrap();\n    let results_with_boost = with_boost.rerank(query, &documents, None).await.unwrap();\n\n    let phrase_doc_score_boosted = results_with_boost\n        .iter()\n        .find(|r| r.index == 0)\n        .unwrap()\n        .relevance_score;\n    let non_phrase_doc_score_boosted = results_with_boost\n        .iter()\n        .find(|r| r.index == 1)\n        .unwrap()\n        .relevance_score;\n\n    assert!(\n        phrase_doc_score_boosted > non_phrase_doc_score_boosted,\n        \"Phrase match should score higher with boost\"\n    );\n\n    let phrase_score_no_boost = results_no_boost\n        .iter()\n        .find(|r| r.index == 0)\n        .unwrap()\n        .relevance_score;\n    assert!(\n        phrase_doc_score_boosted > phrase_score_no_boost,\n        \"Boosted score should be higher\"\n    );\n}\n\n// =========== TermOverlapReranker Tests ===========\n\n#[tokio::test]\nasync fn test_term_overlap_reranker() {\n    let reranker = TermOverlapReranker::new();\n    let query = \"capital of France\";\n    let documents = vec![\n        \"The capital of France is Paris.\".to_string(),\n        \"Tokyo is the capital of Japan.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results[0].index, 0);\n    assert_eq!(reranker.name(), \"term-overlap\");\n}\n\n#[test]\nfn test_mock_reranker_alias() {\n    let mock: MockReranker = MockReranker::new();\n    let term_overlap: TermOverlapReranker = TermOverlapReranker::new();\n\n    assert_eq!(mock.name(), term_overlap.name());\n}\n\n// =========== RRF Reranker Tests ===========\n\n#[test]\nfn test_rrf_fusion_basic() {\n    let rrf = RRFReranker::new();\n\n    let list1 = vec![0, 1, 2];\n    let list2 = vec![2, 1, 0];\n\n    let results = rrf.fuse(&[list1, list2], 3);\n\n    assert!(!results.is_empty());\n}\n\n#[test]\nfn test_rrf_fusion_clear_winner() {\n    let rrf = RRFReranker::with_k(1);\n\n    let list1 = vec![0, 1, 2];\n    let list2 = vec![0, 2, 1];\n\n    let results = rrf.fuse(&[list1, list2], 3);\n\n    assert_eq!(results[0].index, 0);\n    assert!((results[0].relevance_score - 1.0).abs() < 0.01);\n}\n\n#[tokio::test]\nasync fn test_rrf_reranker_trait() {\n    let reranker = RRFReranker::new();\n    let query = \"test query\";\n    let documents = vec![\n        \"First document about test.\".to_string(),\n        \"Second document.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results.len(), 2);\n    assert_eq!(results[0].index, 0);\n}\n\n// =========== Hybrid Reranker Tests ===========\n\n#[tokio::test]\nasync fn test_hybrid_reranker_without_vector() {\n    let reranker = HybridReranker::new();\n    let query = \"test query\";\n    let documents = vec![\n        \"This is a test document.\".to_string(),\n        \"Another document here.\".to_string(),\n    ];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results[0].index, 0);\n}\n\n#[tokio::test]\nasync fn test_hybrid_reranker_with_vector() {\n    let reranker = HybridReranker::new();\n    let query = \"test query\";\n    let documents = vec![\n        \"This is a test document.\".to_string(),\n        \"Another document here.\".to_string(),\n        \"Third one with test.\".to_string(),\n    ];\n\n    let vector_rankings = vec![1, 2, 0];\n\n    let results = reranker\n        .rerank_hybrid(query, &documents, Some(vector_rankings), None)\n        .await\n        .unwrap();\n\n    assert_eq!(results.len(), 3);\n}\n\n#[test]\nfn test_hybrid_reranker_defaults() {\n    let reranker = HybridReranker::new();\n    assert_eq!(reranker.name(), \"hybrid\");\n    assert_eq!(reranker.model(), \"hybrid-reranker\");\n}\n\n// =========== Tokenizer Config Tests ===========\n\n#[test]\nfn test_tokenizer_config_default() {\n    let config = TokenizerConfig::default();\n    assert!(config.enable_stemming);\n    assert!(config.enable_stop_words);\n    assert_eq!(config.min_token_length, 2);\n}\n\n#[test]\nfn test_tokenizer_config_minimal() {\n    let config = TokenizerConfig::minimal();\n    assert!(!config.enable_stemming);\n    assert!(!config.enable_stop_words);\n}\n\n#[test]\nfn test_tokenizer_config_enhanced() {\n    let config = TokenizerConfig::enhanced();\n    assert!(config.enable_stemming);\n    assert!(config.enable_stop_words);\n}\n\n#[test]\nfn test_enhanced_tokenizer_stemming() {\n    let reranker = BM25Reranker::new_enhanced();\n    let tokens = reranker.tokenize_with_config(\"running jumps played\");\n    assert!(tokens.contains(&\"run\".to_string()));\n    assert!(tokens.contains(&\"jump\".to_string()));\n    assert!(tokens.contains(&\"play\".to_string()));\n}\n\n#[test]\nfn test_enhanced_tokenizer_stop_words() {\n    let reranker = BM25Reranker::new_enhanced();\n    let tokens = reranker.tokenize_with_config(\"the quick brown fox\");\n    assert!(!tokens.iter().any(|t| t == \"the\"));\n    assert!(tokens.len() >= 2);\n}\n\n// =========== IDF Optimization Tests ===========\n\n#[test]\nfn test_document_frequency_computation() {\n    let docs = vec![\n        vec![\"the\".to_string(), \"quick\".to_string(), \"brown\".to_string()],\n        vec![\"the\".to_string(), \"lazy\".to_string(), \"dog\".to_string()],\n        vec![\"quick\".to_string(), \"fox\".to_string()],\n    ];\n\n    let df_map = BM25Reranker::compute_document_frequencies(&docs);\n\n    assert_eq!(df_map.get(\"the\"), Some(&2));\n    assert_eq!(df_map.get(\"quick\"), Some(&2));\n    assert_eq!(df_map.get(\"brown\"), Some(&1));\n    assert_eq!(df_map.get(\"fox\"), Some(&1));\n    assert_eq!(df_map.get(\"missing\"), None);\n}\n\n#[test]\nfn test_idf_from_df_equivalence() {\n    let docs = vec![\n        vec![\"apple\".to_string(), \"banana\".to_string()],\n        vec![\"apple\".to_string(), \"cherry\".to_string()],\n        vec![\"banana\".to_string(), \"date\".to_string()],\n    ];\n\n    let n = docs.len() as f64;\n    let df_map = BM25Reranker::compute_document_frequencies(&docs);\n\n    let idf_old = BM25Reranker::compute_idf(\"apple\", &docs);\n    let idf_new = BM25Reranker::compute_idf_from_df(n, *df_map.get(\"apple\").unwrap() as f64);\n    assert!((idf_old - idf_new).abs() < 1e-10);\n}\n\n#[test]\nfn test_bm25_params_clamping() {\n    let reranker = BM25Reranker::with_full_params(10.0, 2.0, -1.0);\n    assert_eq!(reranker.k1, 3.0);\n    assert_eq!(reranker.b, 1.0);\n    assert_eq!(reranker.delta, 0.0);\n}\n\n// =========== Edge Case Tests ===========\n\n#[tokio::test]\nasync fn test_bm25_single_document() {\n    let reranker = BM25Reranker::new();\n    let query = \"test query\";\n    let documents = vec![\"Single document with test.\".to_string()];\n\n    let results = reranker.rerank(query, &documents, None).await.unwrap();\n\n    assert_eq!(results.len(), 1);\n    assert_eq!(results[0].index, 0);\n    assert!(results[0].relevance_score > 0.0);\n}\n\n#[tokio::test]\nasync fn test_bm25_boundary_top_n_zero() {\n    let reranker = BM25Reranker::new();\n    let query = \"test\";\n    let documents = vec![\"test document\".to_string()];\n\n    let results = reranker.rerank(query, &documents, Some(0)).await.unwrap();\n\n    assert!(results.is_empty());\n}\n\n#[tokio::test]\nasync fn test_bm25_boundary_top_n_larger_than_docs() {\n    let reranker = BM25Reranker::new();\n    let query = \"test\";\n    let documents = vec![\"test one\".to_string(), \"test two\".to_string()];\n\n    let results = reranker.rerank(query, &documents, Some(100)).await.unwrap();\n\n    assert_eq!(results.len(), 2);\n}\n\n#[tokio::test]\nasync fn test_rrf_empty_rankings() {\n    let reranker = RRFReranker::new();\n    let results = reranker.fuse(&[], 5);\n    assert!(results.is_empty());\n}\n\n#[tokio::test]\nasync fn test_rrf_single_ranking() {\n    let reranker = RRFReranker::new();\n    let single_list = vec![2, 0, 1];\n    let results = reranker.fuse(&[single_list], 3);\n\n    assert_eq!(results.len(), 3);\n    assert_eq!(results[0].index, 2);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","reranker","traits.rs"],"content":"//! Reranker trait definition.\n//!\n//! This module defines the core `Reranker` trait that all reranking implementations must satisfy.\n//!\n//! # Architecture\n//!\n//! ```ascii\n//!                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//!                      â”‚  Reranker Trait â”‚\n//!                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//!                               â”‚\n//!        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//!        â”‚                      â”‚                      â”‚\n//!        â–¼                      â–¼                      â–¼\n//! â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n//! â”‚ HttpReranker â”‚     â”‚ BM25Reranker â”‚      â”‚ HybridRerank â”‚\n//! â”‚ (Jina,Cohere)â”‚     â”‚ (Local BM25) â”‚      â”‚ (Combined)   â”‚\n//! â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n//! ```\n//!\n//! # Implementations\n//!\n//! - [`super::HttpReranker`] - Remote API-based reranking\n//! - [`super::TermOverlapReranker`] - Simple term matching\n//! - [`super::BM25Reranker`] - BM25 keyword scoring\n//! - [`super::RRFReranker`] - Reciprocal Rank Fusion\n//! - [`super::HybridReranker`] - Combines multiple strategies\n\nuse async_trait::async_trait;\n\nuse super::result::RerankResult;\nuse crate::error::Result;\n\n/// Trait for reranking providers.\n///\n/// All rerankers implement this trait to provide a consistent interface\n/// for document relevance scoring.\n///\n/// # Required Methods\n///\n/// - [`name`](Reranker::name) - Identifier for the reranker\n/// - [`model`](Reranker::model) - Model/algorithm being used\n/// - [`rerank`](Reranker::rerank) - Main reranking operation\n///\n/// # Provided Methods\n///\n/// - [`rerank_str`](Reranker::rerank_str) - Convenience for `&str` slices\n#[async_trait]\npub trait Reranker: Send + Sync {\n    /// Get the name of this reranker.\n    fn name(&self) -> &str;\n\n    /// Get the model being used.\n    fn model(&self) -> &str;\n\n    /// Rerank documents based on relevance to a query.\n    ///\n    /// # Arguments\n    ///\n    /// - `query`: The search query to rank against\n    /// - `documents`: Documents to rerank\n    /// - `top_n`: Maximum number of results to return (None = all)\n    ///\n    /// # Returns\n    ///\n    /// Vector of [`RerankResult`] sorted by relevance (highest first).\n    async fn rerank(\n        &self,\n        query: &str,\n        documents: &[String],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>>;\n\n    /// Rerank with documents as string slices.\n    ///\n    /// Convenience method that converts `&str` to `String`.\n    async fn rerank_str(\n        &self,\n        query: &str,\n        documents: &[&str],\n        top_n: Option<usize>,\n    ) -> Result<Vec<RerankResult>> {\n        let docs: Vec<String> = documents.iter().map(|s| s.to_string()).collect();\n        self.rerank(query, &docs, top_n).await\n    }\n}\n","traces":[{"line":83,"address":[],"length":0,"stats":{"Line":0}},{"line":84,"address":[],"length":0,"stats":{"Line":0}}],"covered":0,"coverable":2},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","retry.rs"],"content":"//! Retry executor for LLM operations with exponential backoff.\n//!\n//! This module provides a retry executor that applies the appropriate\n//! retry strategy based on the error type.\n//!\n//! # Usage\n//!\n//! ```ignore\n//! use edgequake_llm::retry::RetryExecutor;\n//! use edgequake_llm::error::RetryStrategy;\n//!\n//! let executor = RetryExecutor::new();\n//! let result = executor.execute(\n//!     &RetryStrategy::network_backoff(),\n//!     || async { provider.complete(messages, options).await },\n//! ).await;\n//! ```\n//!\n//! @implements specs/improve-tools/006-error-handling.md\n//! @iteration OODA-11\n\nuse crate::error::{LlmError, RetryStrategy};\nuse std::future::Future;\nuse std::time::Duration;\nuse tokio::time::sleep;\nuse tracing::{debug, info, warn};\n\n/// Executor for retry logic with configurable backoff strategies.\n///\n/// The executor wraps async operations and automatically retries them\n/// according to the specified retry strategy.\n#[derive(Debug, Default)]\npub struct RetryExecutor {\n    /// Optional callback for logging retry attempts.\n    log_retries: bool,\n}\n\nimpl RetryExecutor {\n    /// Create a new retry executor.\n    pub fn new() -> Self {\n        Self { log_retries: true }\n    }\n\n    /// Create a retry executor without logging.\n    pub fn silent() -> Self {\n        Self { log_retries: false }\n    }\n\n    /// Execute an async operation with automatic retry based on strategy.\n    ///\n    /// # Arguments\n    ///\n    /// * `strategy` - The retry strategy to use\n    /// * `operation` - Async closure that performs the operation\n    ///\n    /// # Returns\n    ///\n    /// The result of the operation, or the last error if all retries fail.\n    ///\n    /// # Example\n    ///\n    /// ```ignore\n    /// let executor = RetryExecutor::new();\n    /// let result = executor.execute(\n    ///     &RetryStrategy::network_backoff(),\n    ///     || async { make_api_call().await },\n    /// ).await;\n    /// ```\n    pub async fn execute<F, Fut, T>(\n        &self,\n        strategy: &RetryStrategy,\n        mut operation: F,\n    ) -> Result<T, LlmError>\n    where\n        F: FnMut() -> Fut,\n        Fut: Future<Output = Result<T, LlmError>>,\n    {\n        match strategy {\n            RetryStrategy::NoRetry => operation().await,\n\n            RetryStrategy::WaitAndRetry { wait } => {\n                self.execute_wait_and_retry(*wait, operation).await\n            }\n\n            RetryStrategy::ExponentialBackoff {\n                base_delay,\n                max_delay,\n                max_attempts,\n            } => {\n                self.execute_exponential_backoff(*base_delay, *max_delay, *max_attempts, operation)\n                    .await\n            }\n\n            RetryStrategy::ReduceContext => {\n                // We can't automatically reduce context here.\n                // The caller should catch ReduceContext strategy and handle it.\n                // Just attempt once.\n                operation().await\n            }\n        }\n    }\n\n    /// Execute with wait-and-retry strategy.\n    async fn execute_wait_and_retry<F, Fut, T>(\n        &self,\n        wait: Duration,\n        mut operation: F,\n    ) -> Result<T, LlmError>\n    where\n        F: FnMut() -> Fut,\n        Fut: Future<Output = Result<T, LlmError>>,\n    {\n        match operation().await {\n            Ok(v) => Ok(v),\n            Err(e) => {\n                if self.log_retries {\n                    warn!(\"Operation failed, waiting {:?} before retry: {}\", wait, e);\n                }\n                sleep(wait).await;\n                operation().await\n            }\n        }\n    }\n\n    /// Execute with exponential backoff strategy.\n    async fn execute_exponential_backoff<F, Fut, T>(\n        &self,\n        base_delay: Duration,\n        max_delay: Duration,\n        max_attempts: u32,\n        mut operation: F,\n    ) -> Result<T, LlmError>\n    where\n        F: FnMut() -> Fut,\n        Fut: Future<Output = Result<T, LlmError>>,\n    {\n        let mut delay = base_delay;\n        let mut attempts = 0;\n\n        loop {\n            attempts += 1;\n\n            match operation().await {\n                Ok(v) => {\n                    if attempts > 1 && self.log_retries {\n                        info!(\"Operation succeeded after {} attempts\", attempts);\n                    }\n                    return Ok(v);\n                }\n                Err(e) => {\n                    if attempts >= max_attempts {\n                        if self.log_retries {\n                            warn!(\n                                \"Operation failed after {} attempts, giving up: {}\",\n                                attempts, e\n                            );\n                        }\n                        return Err(e);\n                    }\n\n                    // Check if the error itself suggests we shouldn't retry\n                    let error_strategy = e.retry_strategy();\n                    if matches!(error_strategy, RetryStrategy::NoRetry) {\n                        if self.log_retries {\n                            debug!(\"Error is non-retryable, stopping: {}\", e);\n                        }\n                        return Err(e);\n                    }\n\n                    if self.log_retries {\n                        warn!(\n                            \"Attempt {}/{} failed, retrying in {:?}: {}\",\n                            attempts, max_attempts, delay, e\n                        );\n                    }\n\n                    sleep(delay).await;\n                    delay = (delay * 2).min(max_delay);\n                }\n            }\n        }\n    }\n\n    /// Execute an operation with automatic strategy detection.\n    ///\n    /// This variant automatically determines the retry strategy from\n    /// the first error that occurs.\n    ///\n    /// # Example\n    ///\n    /// ```ignore\n    /// let executor = RetryExecutor::new();\n    /// let result = executor.execute_auto(|| async {\n    ///     make_api_call().await\n    /// }).await;\n    /// ```\n    pub async fn execute_auto<F, Fut, T>(&self, mut operation: F) -> Result<T, LlmError>\n    where\n        F: FnMut() -> Fut,\n        Fut: Future<Output = Result<T, LlmError>>,\n    {\n        match operation().await {\n            Ok(v) => Ok(v),\n            Err(e) => {\n                let strategy = e.retry_strategy();\n\n                if !strategy.should_retry() {\n                    return Err(e);\n                }\n\n                if self.log_retries {\n                    debug!(\"First attempt failed, using strategy {:?}: {}\", strategy, e);\n                }\n\n                // Sleep before next attempt based on strategy\n                match &strategy {\n                    RetryStrategy::WaitAndRetry { wait } => {\n                        sleep(*wait).await;\n                        operation().await\n                    }\n                    RetryStrategy::ExponentialBackoff {\n                        base_delay,\n                        max_delay,\n                        max_attempts,\n                    } => {\n                        sleep(*base_delay).await;\n                        // Continue with remaining attempts\n                        self.execute_exponential_backoff(\n                            *base_delay * 2, // Already waited base_delay\n                            *max_delay,\n                            max_attempts - 1, // Already used one attempt\n                            operation,\n                        )\n                        .await\n                    }\n                    RetryStrategy::ReduceContext | RetryStrategy::NoRetry => {\n                        // Shouldn't reach here due to should_retry check\n                        Err(e)\n                    }\n                }\n            }\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n    use std::sync::atomic::{AtomicU32, Ordering};\n    use std::sync::Arc;\n\n    #[tokio::test]\n    async fn test_no_retry_succeeds() {\n        let executor = RetryExecutor::silent();\n        let result = executor\n            .execute(&RetryStrategy::NoRetry, || async { Ok::<_, LlmError>(42) })\n            .await;\n        assert_eq!(result.unwrap(), 42);\n    }\n\n    #[tokio::test]\n    async fn test_no_retry_fails_immediately() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(&RetryStrategy::NoRetry, || {\n                let count = call_count_clone.clone();\n                async move {\n                    count.fetch_add(1, Ordering::SeqCst);\n                    Err::<i32, _>(LlmError::AuthError(\"bad key\".to_string()))\n                }\n            })\n            .await;\n\n        assert!(result.is_err());\n        assert_eq!(call_count.load(Ordering::SeqCst), 1);\n    }\n\n    #[tokio::test]\n    async fn test_exponential_backoff_retries() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(\n                &RetryStrategy::ExponentialBackoff {\n                    base_delay: Duration::from_millis(1),\n                    max_delay: Duration::from_millis(10),\n                    max_attempts: 3,\n                },\n                || {\n                    let count = call_count_clone.clone();\n                    async move {\n                        let attempts = count.fetch_add(1, Ordering::SeqCst) + 1;\n                        if attempts < 3 {\n                            Err(LlmError::NetworkError(\"failed\".to_string()))\n                        } else {\n                            Ok(42)\n                        }\n                    }\n                },\n            )\n            .await;\n\n        assert_eq!(result.unwrap(), 42);\n        assert_eq!(call_count.load(Ordering::SeqCst), 3);\n    }\n\n    #[tokio::test]\n    async fn test_exponential_backoff_gives_up() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(\n                &RetryStrategy::ExponentialBackoff {\n                    base_delay: Duration::from_millis(1),\n                    max_delay: Duration::from_millis(10),\n                    max_attempts: 3,\n                },\n                || {\n                    let count = call_count_clone.clone();\n                    async move {\n                        count.fetch_add(1, Ordering::SeqCst);\n                        Err::<i32, _>(LlmError::NetworkError(\"always fails\".to_string()))\n                    }\n                },\n            )\n            .await;\n\n        assert!(result.is_err());\n        assert_eq!(call_count.load(Ordering::SeqCst), 3);\n    }\n\n    #[tokio::test]\n    async fn test_wait_and_retry() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(\n                &RetryStrategy::WaitAndRetry {\n                    wait: Duration::from_millis(1),\n                },\n                || {\n                    let count = call_count_clone.clone();\n                    async move {\n                        let attempts = count.fetch_add(1, Ordering::SeqCst) + 1;\n                        if attempts < 2 {\n                            Err(LlmError::RateLimited(\"wait\".to_string()))\n                        } else {\n                            Ok(42)\n                        }\n                    }\n                },\n            )\n            .await;\n\n        assert_eq!(result.unwrap(), 42);\n        assert_eq!(call_count.load(Ordering::SeqCst), 2);\n    }\n\n    #[tokio::test]\n    async fn test_stops_on_permanent_error() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(\n                &RetryStrategy::ExponentialBackoff {\n                    base_delay: Duration::from_millis(1),\n                    max_delay: Duration::from_millis(10),\n                    max_attempts: 5,\n                },\n                || {\n                    let count = call_count_clone.clone();\n                    async move {\n                        count.fetch_add(1, Ordering::SeqCst);\n                        // Return a non-retryable error\n                        Err::<i32, _>(LlmError::AuthError(\"invalid\".to_string()))\n                    }\n                },\n            )\n            .await;\n\n        assert!(result.is_err());\n        // Should stop after first attempt since AuthError is non-retryable\n        assert_eq!(call_count.load(Ordering::SeqCst), 1);\n    }\n\n    #[tokio::test]\n    async fn test_reduce_context_executes_once() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(&RetryStrategy::ReduceContext, || {\n                let count = call_count_clone.clone();\n                async move {\n                    count.fetch_add(1, Ordering::SeqCst);\n                    Err::<i32, _>(LlmError::TokenLimitExceeded {\n                        max: 4096,\n                        got: 5000,\n                    })\n                }\n            })\n            .await;\n\n        assert!(result.is_err());\n        assert_eq!(call_count.load(Ordering::SeqCst), 1);\n    }\n\n    #[tokio::test]\n    async fn test_reduce_context_success() {\n        let executor = RetryExecutor::silent();\n\n        let result = executor\n            .execute(&RetryStrategy::ReduceContext, || async {\n                Ok::<_, LlmError>(42)\n            })\n            .await;\n\n        assert_eq!(result.unwrap(), 42);\n    }\n\n    #[tokio::test]\n    async fn test_execute_auto_success() {\n        let executor = RetryExecutor::new();\n\n        let result = executor\n            .execute_auto(|| async { Ok::<_, LlmError>(99) })\n            .await;\n\n        assert_eq!(result.unwrap(), 99);\n    }\n\n    #[tokio::test]\n    async fn test_execute_auto_non_retryable_error() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute_auto(|| {\n                let count = call_count_clone.clone();\n                async move {\n                    count.fetch_add(1, Ordering::SeqCst);\n                    Err::<i32, _>(LlmError::AuthError(\"invalid\".to_string()))\n                }\n            })\n            .await;\n\n        assert!(result.is_err());\n        assert_eq!(call_count.load(Ordering::SeqCst), 1);\n    }\n\n    #[tokio::test]\n    async fn test_execute_auto_retryable_network_error() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute_auto(|| {\n                let count = call_count_clone.clone();\n                async move {\n                    let attempts = count.fetch_add(1, Ordering::SeqCst) + 1;\n                    if attempts < 2 {\n                        Err(LlmError::NetworkError(\"failed\".to_string()))\n                    } else {\n                        Ok(42)\n                    }\n                }\n            })\n            .await;\n\n        assert_eq!(result.unwrap(), 42);\n        // First call fails, then auto-retry succeeds\n        assert!(call_count.load(Ordering::SeqCst) >= 2);\n    }\n\n    #[tokio::test]\n    async fn test_wait_and_retry_both_fail() {\n        let executor = RetryExecutor::silent();\n        let call_count = Arc::new(AtomicU32::new(0));\n        let call_count_clone = call_count.clone();\n\n        let result = executor\n            .execute(\n                &RetryStrategy::WaitAndRetry {\n                    wait: Duration::from_millis(1),\n                },\n                || {\n                    let count = call_count_clone.clone();\n                    async move {\n                        count.fetch_add(1, Ordering::SeqCst);\n                        Err::<i32, _>(LlmError::RateLimited(\"wait\".to_string()))\n                    }\n                },\n            )\n            .await;\n\n        assert!(result.is_err());\n        assert_eq!(call_count.load(Ordering::SeqCst), 2);\n    }\n\n    #[tokio::test]\n    async fn test_new_constructor_logging() {\n        let executor = RetryExecutor::new();\n        assert!(executor.log_retries);\n    }\n\n    #[tokio::test]\n    async fn test_silent_constructor() {\n        let executor = RetryExecutor::silent();\n        assert!(!executor.log_retries);\n    }\n\n    #[tokio::test]\n    async fn test_default_constructor() {\n        let executor = RetryExecutor::default();\n        assert!(!executor.log_retries);\n    }\n}\n","traces":[{"line":40,"address":[],"length":0,"stats":{"Line":2}},{"line":45,"address":[],"length":0,"stats":{"Line":12}},{"line":69,"address":[],"length":0,"stats":{"Line":9}},{"line":78,"address":[],"length":0,"stats":{"Line":9}},{"line":79,"address":[],"length":0,"stats":{"Line":2}},{"line":81,"address":[],"length":0,"stats":{"Line":2}},{"line":82,"address":[],"length":0,"stats":{"Line":8}},{"line":85,"address":[],"length":0,"stats":{"Line":0}},{"line":86,"address":[],"length":0,"stats":{"Line":3}},{"line":87,"address":[],"length":0,"stats":{"Line":3}},{"line":88,"address":[],"length":0,"stats":{"Line":3}},{"line":89,"address":[],"length":0,"stats":{"Line":0}},{"line":90,"address":[],"length":0,"stats":{"Line":18}},{"line":91,"address":[],"length":0,"stats":{"Line":3}},{"line":94,"address":[],"length":0,"stats":{"Line":0}},{"line":98,"address":[],"length":0,"stats":{"Line":2}},{"line":104,"address":[],"length":0,"stats":{"Line":2}},{"line":113,"address":[],"length":0,"stats":{"Line":4}},{"line":114,"address":[],"length":0,"stats":{"Line":0}},{"line":115,"address":[],"length":0,"stats":{"Line":2}},{"line":116,"address":[],"length":0,"stats":{"Line":2}},{"line":117,"address":[],"length":0,"stats":{"Line":0}},{"line":119,"address":[],"length":0,"stats":{"Line":4}},{"line":120,"address":[],"length":0,"stats":{"Line":2}},{"line":126,"address":[],"length":0,"stats":{"Line":4}},{"line":137,"address":[],"length":0,"stats":{"Line":8}},{"line":138,"address":[],"length":0,"stats":{"Line":8}},{"line":140,"address":[],"length":0,"stats":{"Line":0}},{"line":141,"address":[],"length":0,"stats":{"Line":8}},{"line":143,"address":[],"length":0,"stats":{"Line":16}},{"line":144,"address":[],"length":0,"stats":{"Line":2}},{"line":145,"address":[],"length":0,"stats":{"Line":3}},{"line":146,"address":[],"length":0,"stats":{"Line":0}},{"line":148,"address":[],"length":0,"stats":{"Line":2}},{"line":150,"address":[],"length":0,"stats":{"Line":6}},{"line":151,"address":[],"length":0,"stats":{"Line":6}},{"line":152,"address":[],"length":0,"stats":{"Line":1}},{"line":153,"address":[],"length":0,"stats":{"Line":0}},{"line":154,"address":[],"length":0,"stats":{"Line":0}},{"line":155,"address":[],"length":0,"stats":{"Line":0}},{"line":158,"address":[],"length":0,"stats":{"Line":1}},{"line":162,"address":[],"length":0,"stats":{"Line":15}},{"line":163,"address":[],"length":0,"stats":{"Line":9}},{"line":164,"address":[],"length":0,"stats":{"Line":1}},{"line":165,"address":[],"length":0,"stats":{"Line":0}},{"line":167,"address":[],"length":0,"stats":{"Line":1}},{"line":170,"address":[],"length":0,"stats":{"Line":4}},{"line":171,"address":[],"length":0,"stats":{"Line":0}},{"line":172,"address":[],"length":0,"stats":{"Line":0}},{"line":173,"address":[],"length":0,"stats":{"Line":0}},{"line":177,"address":[],"length":0,"stats":{"Line":8}},{"line":178,"address":[],"length":0,"stats":{"Line":8}},{"line":197,"address":[],"length":0,"stats":{"Line":3}},{"line":202,"address":[],"length":0,"stats":{"Line":6}},{"line":203,"address":[],"length":0,"stats":{"Line":2}},{"line":204,"address":[],"length":0,"stats":{"Line":2}},{"line":205,"address":[],"length":0,"stats":{"Line":6}},{"line":207,"address":[],"length":0,"stats":{"Line":2}},{"line":208,"address":[],"length":0,"stats":{"Line":1}},{"line":211,"address":[],"length":0,"stats":{"Line":1}},{"line":212,"address":[],"length":0,"stats":{"Line":0}},{"line":216,"address":[],"length":0,"stats":{"Line":1}},{"line":217,"address":[],"length":0,"stats":{"Line":0}},{"line":218,"address":[],"length":0,"stats":{"Line":0}},{"line":219,"address":[],"length":0,"stats":{"Line":0}},{"line":221,"address":[],"length":0,"stats":{"Line":0}},{"line":222,"address":[],"length":0,"stats":{"Line":1}},{"line":223,"address":[],"length":0,"stats":{"Line":1}},{"line":224,"address":[],"length":0,"stats":{"Line":1}},{"line":225,"address":[],"length":0,"stats":{"Line":0}},{"line":226,"address":[],"length":0,"stats":{"Line":2}},{"line":228,"address":[],"length":0,"stats":{"Line":3}},{"line":229,"address":[],"length":0,"stats":{"Line":2}},{"line":230,"address":[],"length":0,"stats":{"Line":2}},{"line":231,"address":[],"length":0,"stats":{"Line":2}},{"line":232,"address":[],"length":0,"stats":{"Line":1}},{"line":234,"address":[],"length":0,"stats":{"Line":1}},{"line":236,"address":[],"length":0,"stats":{"Line":0}},{"line":238,"address":[],"length":0,"stats":{"Line":0}}],"covered":57,"coverable":79},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","tokenizer.rs"],"content":"//! Token counting utilities.\n//!\n//! ## Implements\n//!\n//! - **FEAT0777**: Token counting with tiktoken\n//! - **FEAT0778**: Model-specific tokenizer selection\n//!\n//! ## Enforces\n//!\n//! - **BR0302**: Token count respects context window limits\n//! - **BR0777**: Fallback to cl100k_base for unknown models\n\nuse tiktoken_rs::{cl100k_base, o200k_base, CoreBPE};\n\n/// Tokenizer for counting tokens in text.\npub struct Tokenizer {\n    encoder: CoreBPE,\n    model: String,\n}\n\nimpl Tokenizer {\n    /// Create a tokenizer for a specific model.\n    ///\n    /// Falls back to cl100k_base (GPT-4/3.5 tokenizer) if model is unknown.\n    pub fn for_model(model: &str) -> Self {\n        let encoder = match model {\n            // GPT-4o and newer use o200k\n            m if m.contains(\"gpt-4o\") || m.contains(\"o1\") || m.contains(\"o3\") => {\n                o200k_base().expect(\"Failed to load o200k tokenizer\")\n            }\n            // GPT-4 and GPT-3.5 use cl100k\n            m if m.contains(\"gpt-4\") || m.contains(\"gpt-3.5\") => {\n                cl100k_base().expect(\"Failed to load cl100k tokenizer\")\n            }\n            // Embedding models\n            m if m.contains(\"text-embedding\") => {\n                cl100k_base().expect(\"Failed to load cl100k tokenizer\")\n            }\n            // Default to cl100k (most common)\n            _ => cl100k_base().expect(\"Failed to load cl100k tokenizer\"),\n        };\n\n        Self {\n            encoder,\n            model: model.to_string(),\n        }\n    }\n\n    /// Create a default tokenizer using cl100k_base.\n    pub fn default_tokenizer() -> Self {\n        Self {\n            encoder: cl100k_base().expect(\"Failed to load cl100k tokenizer\"),\n            model: \"default\".to_string(),\n        }\n    }\n\n    /// Count the number of tokens in the text.\n    pub fn count_tokens(&self, text: &str) -> usize {\n        self.encoder.encode_with_special_tokens(text).len()\n    }\n\n    /// Encode text to token IDs.\n    pub fn encode(&self, text: &str) -> Vec<u32> {\n        self.encoder.encode_with_special_tokens(text)\n    }\n\n    /// Decode token IDs back to text.\n    pub fn decode(&self, tokens: &[u32]) -> String {\n        self.encoder.decode(tokens.to_vec()).unwrap_or_default()\n    }\n\n    /// Truncate text to fit within a token limit.\n    pub fn truncate(&self, text: &str, max_tokens: usize) -> String {\n        let tokens = self.encode(text);\n        if tokens.len() <= max_tokens {\n            return text.to_string();\n        }\n        self.decode(&tokens[..max_tokens])\n    }\n\n    /// Split text into chunks that fit within a token limit.\n    pub fn chunk(&self, text: &str, max_tokens: usize, overlap_tokens: usize) -> Vec<String> {\n        let tokens = self.encode(text);\n        let mut chunks = Vec::new();\n        let mut start = 0;\n\n        while start < tokens.len() {\n            let end = (start + max_tokens).min(tokens.len());\n            let chunk_tokens = &tokens[start..end];\n            chunks.push(self.decode(chunk_tokens));\n\n            if end >= tokens.len() {\n                break;\n            }\n\n            start = end.saturating_sub(overlap_tokens);\n        }\n\n        chunks\n    }\n\n    /// Get the model this tokenizer is configured for.\n    pub fn model(&self) -> &str {\n        &self.model\n    }\n}\n\nimpl Default for Tokenizer {\n    fn default() -> Self {\n        Self::default_tokenizer()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_token_counting() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"Hello, world!\";\n        let count = tokenizer.count_tokens(text);\n        assert!(count > 0);\n        assert!(count < text.len()); // Tokens are typically longer than bytes\n    }\n\n    #[test]\n    fn test_encode_decode_roundtrip() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"This is a test sentence.\";\n        let tokens = tokenizer.encode(text);\n        let decoded = tokenizer.decode(&tokens);\n        assert_eq!(decoded, text);\n    }\n\n    #[test]\n    fn test_truncate() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"This is a longer sentence that should be truncated.\";\n        let truncated = tokenizer.truncate(text, 5);\n        let token_count = tokenizer.count_tokens(&truncated);\n        assert!(token_count <= 5);\n    }\n\n    #[test]\n    fn test_chunking() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"One two three four five six seven eight nine ten.\";\n        let chunks = tokenizer.chunk(text, 3, 1);\n        assert!(chunks.len() > 1);\n    }\n\n    #[test]\n    fn test_model_specific_tokenizer() {\n        let gpt4 = Tokenizer::for_model(\"gpt-4\");\n        let gpt4o = Tokenizer::for_model(\"gpt-4o\");\n\n        // Both should be able to tokenize\n        let text = \"Hello, world!\";\n        assert!(gpt4.count_tokens(text) > 0);\n        assert!(gpt4o.count_tokens(text) > 0);\n    }\n\n    #[test]\n    fn test_for_model_gpt35() {\n        let t = Tokenizer::for_model(\"gpt-3.5-turbo\");\n        assert_eq!(t.model(), \"gpt-3.5-turbo\");\n        assert!(t.count_tokens(\"Hello\") > 0);\n    }\n\n    #[test]\n    fn test_for_model_o1() {\n        let t = Tokenizer::for_model(\"o1-mini\");\n        assert_eq!(t.model(), \"o1-mini\");\n        assert!(t.count_tokens(\"Hello\") > 0);\n    }\n\n    #[test]\n    fn test_for_model_o3() {\n        let t = Tokenizer::for_model(\"o3-mini\");\n        assert_eq!(t.model(), \"o3-mini\");\n        assert!(t.count_tokens(\"Hello\") > 0);\n    }\n\n    #[test]\n    fn test_for_model_embedding() {\n        let t = Tokenizer::for_model(\"text-embedding-ada-002\");\n        assert_eq!(t.model(), \"text-embedding-ada-002\");\n        assert!(t.count_tokens(\"Hello\") > 0);\n    }\n\n    #[test]\n    fn test_for_model_unknown_falls_back() {\n        let t = Tokenizer::for_model(\"some-unknown-model\");\n        assert_eq!(t.model(), \"some-unknown-model\");\n        assert!(t.count_tokens(\"Hello\") > 0);\n    }\n\n    #[test]\n    fn test_default_impl() {\n        let t = Tokenizer::default();\n        assert_eq!(t.model(), \"default\");\n        assert!(t.count_tokens(\"Hello\") > 0);\n    }\n\n    #[test]\n    fn test_truncate_within_limit() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"Hello\";\n        let truncated = tokenizer.truncate(text, 100);\n        assert_eq!(truncated, text);\n    }\n\n    #[test]\n    fn test_chunk_within_limit() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"Short\";\n        let chunks = tokenizer.chunk(text, 100, 0);\n        assert_eq!(chunks.len(), 1);\n        assert_eq!(chunks[0], text);\n    }\n\n    #[test]\n    fn test_chunk_no_overlap() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let text = \"One two three four five six seven eight nine ten eleven twelve.\";\n        let chunks = tokenizer.chunk(text, 3, 0);\n        assert!(chunks.len() > 1);\n    }\n\n    #[test]\n    fn test_model_accessor() {\n        let tokenizer = Tokenizer::for_model(\"gpt-4o-mini\");\n        assert_eq!(tokenizer.model(), \"gpt-4o-mini\");\n    }\n\n    #[test]\n    fn test_empty_string() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        assert_eq!(tokenizer.count_tokens(\"\"), 0);\n        assert!(tokenizer.encode(\"\").is_empty());\n    }\n\n    #[test]\n    fn test_decode_empty() {\n        let tokenizer = Tokenizer::default_tokenizer();\n        let decoded = tokenizer.decode(&[]);\n        assert_eq!(decoded, \"\");\n    }\n}\n","traces":[{"line":25,"address":[],"length":0,"stats":{"Line":8}},{"line":26,"address":[],"length":0,"stats":{"Line":16}},{"line":28,"address":[],"length":0,"stats":{"Line":42}},{"line":29,"address":[],"length":0,"stats":{"Line":12}},{"line":32,"address":[],"length":0,"stats":{"Line":16}},{"line":33,"address":[],"length":0,"stats":{"Line":6}},{"line":36,"address":[],"length":0,"stats":{"Line":5}},{"line":37,"address":[],"length":0,"stats":{"Line":3}},{"line":40,"address":[],"length":0,"stats":{"Line":3}},{"line":45,"address":[],"length":0,"stats":{"Line":8}},{"line":50,"address":[],"length":0,"stats":{"Line":13}},{"line":52,"address":[],"length":0,"stats":{"Line":52}},{"line":53,"address":[],"length":0,"stats":{"Line":13}},{"line":58,"address":[],"length":0,"stats":{"Line":14}},{"line":59,"address":[],"length":0,"stats":{"Line":42}},{"line":63,"address":[],"length":0,"stats":{"Line":7}},{"line":64,"address":[],"length":0,"stats":{"Line":21}},{"line":68,"address":[],"length":0,"stats":{"Line":14}},{"line":69,"address":[],"length":0,"stats":{"Line":70}},{"line":73,"address":[],"length":0,"stats":{"Line":2}},{"line":74,"address":[],"length":0,"stats":{"Line":8}},{"line":75,"address":[],"length":0,"stats":{"Line":4}},{"line":76,"address":[],"length":0,"stats":{"Line":2}},{"line":78,"address":[],"length":0,"stats":{"Line":3}},{"line":82,"address":[],"length":0,"stats":{"Line":3}},{"line":83,"address":[],"length":0,"stats":{"Line":12}},{"line":84,"address":[],"length":0,"stats":{"Line":6}},{"line":85,"address":[],"length":0,"stats":{"Line":6}},{"line":87,"address":[],"length":0,"stats":{"Line":22}},{"line":88,"address":[],"length":0,"stats":{"Line":55}},{"line":89,"address":[],"length":0,"stats":{"Line":33}},{"line":90,"address":[],"length":0,"stats":{"Line":55}},{"line":92,"address":[],"length":0,"stats":{"Line":22}},{"line":93,"address":[],"length":0,"stats":{"Line":3}},{"line":96,"address":[],"length":0,"stats":{"Line":16}},{"line":99,"address":[],"length":0,"stats":{"Line":3}},{"line":103,"address":[],"length":0,"stats":{"Line":7}},{"line":104,"address":[],"length":0,"stats":{"Line":7}},{"line":109,"address":[],"length":0,"stats":{"Line":4}},{"line":110,"address":[],"length":0,"stats":{"Line":4}}],"covered":40,"coverable":40},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","src","traits.rs"],"content":"//! LLM provider traits for text completion and embedding.\n//!\n//! # Implements\n//!\n//! @implements FEAT0006 (Vector Embedding Generation via EmbeddingProvider trait)\n//! @implements FEAT0017 (Multi-Provider LLM Support via LLMProvider trait)\n//! @implements FEAT0018 (Embedding Provider Abstraction)\n//!\n//! # Enforces\n//!\n//! - **BR0303**: Token usage tracked in [`LLMResponse`]\n//! - **BR0010**: Embedding dimension validated by providers\n//!\n//! # WHY: Trait-Based Provider Abstraction\n//!\n//! Using traits instead of concrete types enables:\n//! - **Testing**: MockProvider for unit tests (no API calls)\n//! - **Flexibility**: Swap providers without code changes\n//! - **Cost control**: Route to different providers based on request type\n//! - **Resilience**: Fallback providers when primary is unavailable\n//!\n//! # Key Traits\n//!\n//! - [`LLMProvider`]: Text completion (chat, extraction prompts)\n//! - [`EmbeddingProvider`]: Vector embedding generation\n\nuse async_trait::async_trait;\nuse serde::{Deserialize, Serialize};\nuse serde_json::Value as JsonValue;\nuse std::collections::HashMap;\n\nuse crate::error::Result;\n\nuse futures::stream::BoxStream;\n\n// ============================================================================\n// Function/Tool Calling Types (OpenAI-compatible)\n// ============================================================================\n\n/// Definition of a tool that the model can call.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ToolDefinition {\n    /// Type of tool (always \"function\" for function tools).\n    #[serde(rename = \"type\")]\n    pub tool_type: String,\n\n    /// Function definition.\n    pub function: FunctionDefinition,\n}\n\nimpl ToolDefinition {\n    /// Create a new function tool definition.\n    pub fn function(\n        name: impl Into<String>,\n        description: impl Into<String>,\n        parameters: JsonValue,\n    ) -> Self {\n        Self {\n            tool_type: \"function\".to_string(),\n            function: FunctionDefinition {\n                name: name.into(),\n                description: description.into(),\n                parameters,\n                strict: Some(true),\n            },\n        }\n    }\n}\n\n/// Definition of a function that can be called by the model.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FunctionDefinition {\n    /// Name of the function.\n    pub name: String,\n\n    /// Description of what the function does.\n    pub description: String,\n\n    /// JSON Schema defining the function parameters.\n    pub parameters: JsonValue,\n\n    /// Whether to enforce strict mode for schema validation.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub strict: Option<bool>,\n}\n\n/// A tool call request from the model.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ToolCall {\n    /// Unique identifier for this tool call.\n    pub id: String,\n\n    /// Type of tool (always \"function\").\n    #[serde(rename = \"type\")]\n    pub call_type: String,\n\n    /// Function call details.\n    pub function: FunctionCall,\n}\n\nimpl ToolCall {\n    /// Parse the function arguments as JSON.\n    pub fn parse_arguments<T: serde::de::DeserializeOwned>(&self) -> Result<T> {\n        serde_json::from_str(&self.function.arguments).map_err(|e| {\n            crate::error::LlmError::InvalidRequest(format!(\"Failed to parse tool arguments: {}\", e))\n        })\n    }\n\n    /// Get the function name.\n    pub fn name(&self) -> &str {\n        &self.function.name\n    }\n\n    /// Get the raw arguments string.\n    pub fn arguments(&self) -> &str {\n        &self.function.arguments\n    }\n}\n\n/// Details of a function call.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct FunctionCall {\n    /// Name of the function to call.\n    pub name: String,\n\n    /// JSON-encoded arguments for the function.\n    pub arguments: String,\n}\n\n/// Tool choice configuration.\n#[derive(Debug, Clone, Serialize, Deserialize)]\n#[serde(untagged)]\npub enum ToolChoice {\n    /// Let the model decide (default).\n    Auto(String),\n\n    /// Force the model to use tools.\n    Required(String),\n\n    /// Force a specific function.\n    Function {\n        #[serde(rename = \"type\")]\n        choice_type: String,\n        function: ToolChoiceFunction,\n    },\n}\n\nimpl ToolChoice {\n    /// Auto mode - model decides when to use tools.\n    pub fn auto() -> Self {\n        ToolChoice::Auto(\"auto\".to_string())\n    }\n\n    /// Required mode - model must use at least one tool.\n    pub fn required() -> Self {\n        ToolChoice::Required(\"required\".to_string())\n    }\n\n    /// Force a specific function to be called.\n    pub fn function(name: impl Into<String>) -> Self {\n        ToolChoice::Function {\n            choice_type: \"function\".to_string(),\n            function: ToolChoiceFunction { name: name.into() },\n        }\n    }\n\n    /// None mode - disable tool calling.\n    pub fn none() -> Self {\n        ToolChoice::Auto(\"none\".to_string())\n    }\n}\n\n/// Specific function choice.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ToolChoiceFunction {\n    /// Name of the function to call.\n    pub name: String,\n}\n\n/// Result of a tool execution to send back to the model.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ToolResult {\n    /// ID of the tool call this result is for.\n    pub tool_call_id: String,\n\n    /// Role (always \"tool\").\n    pub role: String,\n\n    /// Content/output of the tool execution.\n    pub content: String,\n}\n\nimpl ToolResult {\n    /// Create a new tool result.\n    pub fn new(tool_call_id: impl Into<String>, content: impl Into<String>) -> Self {\n        Self {\n            tool_call_id: tool_call_id.into(),\n            role: \"tool\".to_string(),\n            content: content.into(),\n        }\n    }\n\n    /// Create an error result.\n    pub fn error(tool_call_id: impl Into<String>, error: impl std::fmt::Display) -> Self {\n        Self {\n            tool_call_id: tool_call_id.into(),\n            role: \"tool\".to_string(),\n            content: format!(\"Error: {}\", error),\n        }\n    }\n}\n\n// ============================================================================\n// Streaming Types\n// ============================================================================\n\n/// Chunk of a streaming response with tool call support.\n///\n/// OODA-04: Added ThinkingContent for extended thinking/reasoning streaming.\n/// OODA-10: Added budget_remaining for thinking budget display.\n#[derive(Debug, Clone)]\npub enum StreamChunk {\n    /// Partial content/reasoning text.\n    Content(String),\n\n    /// Extended thinking/reasoning content (OODA-04, OODA-10).\n    ///\n    /// Emitted by models supporting extended thinking (Claude, Gemini 2.0 Flash Thinking,\n    /// DeepSeek R1/V3). Allows real-time display of model reasoning process.\n    ThinkingContent {\n        /// The thinking/reasoning text fragment\n        text: String,\n        /// Tokens used for this thinking chunk (if provider reports it)\n        tokens_used: Option<usize>,\n        /// Total thinking budget (OODA-10: for budget display like \"1.2k/10k\")\n        budget_total: Option<usize>,\n    },\n\n    /// Incremental tool call data.\n    ToolCallDelta {\n        /// Index of the tool call (for multiple parallel calls).\n        index: usize,\n        /// Tool call ID (may be sent once at start).\n        id: Option<String>,\n        /// Function name (may be sent once at start).\n        function_name: Option<String>,\n        /// Incremental function arguments (JSON fragment).\n        function_arguments: Option<String>,\n    },\n\n    /// Stream finished with reason.\n    ///\n    /// OODA-35: Extended with optional provider metrics.\n    Finished {\n        /// Finish reason (e.g., \"stop\", \"tool_calls\", \"length\").\n        reason: String,\n        /// Time to first token in milliseconds (if provider reports it).\n        /// OODA-35: Added for provider-native TTFT.\n        #[allow(dead_code)]\n        ttft_ms: Option<f64>,\n    },\n}\n\n// ============================================================================\n// LLM Response with Tool Calls\n// ============================================================================\n\n/// Response from an LLM completion.\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct LLMResponse {\n    /// The generated text content.\n    pub content: String,\n\n    /// Number of tokens in the prompt.\n    pub prompt_tokens: usize,\n\n    /// Number of tokens in the completion.\n    pub completion_tokens: usize,\n\n    /// Total tokens used.\n    pub total_tokens: usize,\n\n    /// Model used for the request.\n    pub model: String,\n\n    /// Finish reason (e.g., \"stop\", \"length\", \"content_filter\", \"tool_calls\").\n    pub finish_reason: Option<String>,\n\n    /// Tool calls requested by the model (if any).\n    #[serde(default, skip_serializing_if = \"Vec::is_empty\")]\n    pub tool_calls: Vec<ToolCall>,\n\n    /// Additional metadata from the provider.\n    pub metadata: HashMap<String, serde_json::Value>,\n\n    /// Number of tokens served from cache (if provider supports caching).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub cache_hit_tokens: Option<usize>,\n\n    /// Number of reasoning/thinking tokens used by the model.\n    ///\n    /// OODA-15: Extended thinking/reasoning mode capture\n    ///\n    /// OpenAI o-series: Extracted from `output_tokens_details.reasoning_tokens`\n    /// Anthropic Claude: Derived from thinking block token count\n    ///\n    /// These tokens are billed as output tokens but represent internal reasoning\n    /// that precedes the visible response.\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub thinking_tokens: Option<usize>,\n\n    /// Reasoning/thinking content from the model (if available).\n    ///\n    /// OODA-15: Extended thinking content capture\n    ///\n    /// Only populated when:\n    /// 1. The model supports visible thinking (e.g., Claude extended thinking)\n    /// 2. Content capture is enabled (EDGECODE_CAPTURE_CONTENT=true for tracing)\n    ///\n    /// OpenAI o-series: Reasoning is hidden (not returned via API)\n    /// Anthropic Claude: Thinking content returned in thinking blocks\n    #[serde(default, skip_serializing_if = \"Option::is_none\")]\n    pub thinking_content: Option<String>,\n}\n\nimpl LLMResponse {\n    /// Create a new LLM response.\n    pub fn new(content: impl Into<String>, model: impl Into<String>) -> Self {\n        Self {\n            content: content.into(),\n            prompt_tokens: 0,\n            completion_tokens: 0,\n            total_tokens: 0,\n            model: model.into(),\n            finish_reason: None,\n            tool_calls: Vec::new(),\n            metadata: HashMap::new(),\n            cache_hit_tokens: None,\n            thinking_tokens: None,\n            thinking_content: None,\n        }\n    }\n\n    /// Set token usage.\n    pub fn with_usage(mut self, prompt: usize, completion: usize) -> Self {\n        self.prompt_tokens = prompt;\n        self.completion_tokens = completion;\n        self.total_tokens = prompt + completion;\n        self\n    }\n\n    /// Set finish reason.\n    pub fn with_finish_reason(mut self, reason: impl Into<String>) -> Self {\n        self.finish_reason = Some(reason.into());\n        self\n    }\n\n    /// Add tool calls to the response.\n    pub fn with_tool_calls(mut self, calls: Vec<ToolCall>) -> Self {\n        self.tool_calls = calls;\n        self\n    }\n\n    /// Set the number of tokens served from cache.\n    ///\n    /// # Context Engineering Note\n    /// Cache hit tracking is critical for measuring the effectiveness of\n    /// prompt caching strategies. Providers like OpenAI, Anthropic, and Gemini\n    /// support KV-cache and report cached token counts in their responses.\n    ///\n    /// A high cache hit rate (>80%) indicates effective context engineering:\n    /// - Stable prompt prefixes (no timestamps at start)\n    /// - Deterministic message serialization\n    /// - Append-only history patterns\n    pub fn with_cache_hit_tokens(mut self, tokens: usize) -> Self {\n        self.cache_hit_tokens = Some(tokens);\n        self\n    }\n\n    /// Add metadata to the response.\n    ///\n    /// # OODA-13: Response ID Capture\n    /// Providers should call this to add response IDs and other metadata\n    /// for OpenTelemetry GenAI semantic conventions compliance.\n    ///\n    /// Common keys: \"id\" (response ID), \"system_fingerprint\", etc.\n    pub fn with_metadata(mut self, key: impl Into<String>, value: serde_json::Value) -> Self {\n        self.metadata.insert(key.into(), value);\n        self\n    }\n\n    /// Set the number of reasoning/thinking tokens.\n    ///\n    /// # OODA-15: Extended Thinking Token Capture\n    /// Use this to record the number of tokens the model used for internal\n    /// reasoning before generating the visible response.\n    ///\n    /// OpenAI o-series: `output_tokens_details.reasoning_tokens`\n    /// Anthropic Claude: Derived from thinking block sizes\n    ///\n    /// These tokens are billed as output tokens but represent hidden reasoning.\n    pub fn with_thinking_tokens(mut self, tokens: usize) -> Self {\n        self.thinking_tokens = Some(tokens);\n        self\n    }\n\n    /// Set the reasoning/thinking content.\n    ///\n    /// # OODA-15: Extended Thinking Content Capture\n    /// Use this to record the model's visible thinking/reasoning text.\n    ///\n    /// Only applicable for models that expose thinking content:\n    /// - Anthropic Claude: Returns thinking blocks with visible reasoning\n    /// - OpenAI o-series: Reasoning is hidden (do not use this method)\n    ///\n    /// Content should be captured only when opt-in is enabled\n    /// (EDGECODE_CAPTURE_CONTENT=true) due to potential sensitivity.\n    pub fn with_thinking_content(mut self, content: impl Into<String>) -> Self {\n        self.thinking_content = Some(content.into());\n        self\n    }\n\n    /// Check if the response has tool calls.\n    pub fn has_tool_calls(&self) -> bool {\n        !self.tool_calls.is_empty()\n    }\n\n    /// Check if the response has thinking/reasoning tokens.\n    ///\n    /// Returns true if the model used extended thinking capabilities.\n    pub fn has_thinking(&self) -> bool {\n        self.thinking_tokens.is_some() || self.thinking_content.is_some()\n    }\n}\n\n/// Options for LLM completion requests.\n#[derive(Debug, Clone, Default, Serialize, Deserialize)]\npub struct CompletionOptions {\n    /// Maximum number of tokens to generate.\n    pub max_tokens: Option<usize>,\n\n    /// Temperature for sampling (0.0 = deterministic, 1.0 = creative).\n    pub temperature: Option<f32>,\n\n    /// Top-p (nucleus) sampling.\n    pub top_p: Option<f32>,\n\n    /// Stop sequences.\n    pub stop: Option<Vec<String>>,\n\n    /// Frequency penalty.\n    pub frequency_penalty: Option<f32>,\n\n    /// Presence penalty.\n    pub presence_penalty: Option<f32>,\n\n    /// Response format (e.g., \"json\").\n    pub response_format: Option<String>,\n\n    /// System prompt to prepend.\n    pub system_prompt: Option<String>,\n}\n\nimpl CompletionOptions {\n    /// Create options with a specific temperature.\n    pub fn with_temperature(temperature: f32) -> Self {\n        Self {\n            temperature: Some(temperature),\n            ..Default::default()\n        }\n    }\n\n    /// Create options for JSON output.\n    pub fn json_mode() -> Self {\n        Self {\n            response_format: Some(\"json_object\".to_string()),\n            ..Default::default()\n        }\n    }\n}\n\n/// Trait for LLM providers that can generate text completions.\n#[async_trait]\npub trait LLMProvider: Send + Sync {\n    /// Get the name of this provider.\n    fn name(&self) -> &str;\n\n    /// Get the current model.\n    fn model(&self) -> &str;\n\n    /// Get the maximum context length for the model.\n    fn max_context_length(&self) -> usize;\n\n    /// Generate a completion for the given prompt.\n    async fn complete(&self, prompt: &str) -> Result<LLMResponse>;\n\n    /// Generate a completion with custom options.\n    async fn complete_with_options(\n        &self,\n        prompt: &str,\n        options: &CompletionOptions,\n    ) -> Result<LLMResponse>;\n\n    /// Generate a chat completion with messages.\n    async fn chat(\n        &self,\n        messages: &[ChatMessage],\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse>;\n\n    /// Generate a chat completion with tool/function calling support.\n    ///\n    /// This method allows the model to call tools/functions defined in the `tools` parameter.\n    /// The model may respond with tool_calls in the response, which should be executed\n    /// and the results sent back via ToolResult messages.\n    ///\n    /// # Arguments\n    /// * `messages` - The conversation messages\n    /// * `tools` - Available tools the model can call\n    /// * `tool_choice` - How the model should select tools (auto, required, or specific)\n    /// * `options` - Additional completion options\n    ///\n    /// # Returns\n    /// An LLMResponse that may contain tool_calls if the model wants to use tools.\n    async fn chat_with_tools(\n        &self,\n        messages: &[ChatMessage],\n        tools: &[ToolDefinition],\n        tool_choice: Option<ToolChoice>,\n        options: Option<&CompletionOptions>,\n    ) -> Result<LLMResponse> {\n        // Default implementation: ignore tools and use regular chat\n        // Providers that support function calling should override this\n        let _ = (tools, tool_choice);\n        self.chat(messages, options).await\n    }\n\n    /// Generate a streaming completion.\n    async fn stream(&self, _prompt: &str) -> Result<BoxStream<'static, Result<String>>> {\n        Err(crate::error::LlmError::NotSupported(\n            \"Streaming not supported\".to_string(),\n        ))\n    }\n\n    /// Stream chat completion with tool calling support.\n    /// Returns a stream of events containing content chunks, tool call deltas, and finish reasons.\n    ///\n    /// # Arguments\n    /// * `messages` - Chat messages for context\n    /// * `tools` - Available tools the model can call\n    /// * `tool_choice` - How the model should select tools\n    /// * `options` - Additional completion options\n    ///\n    /// # Returns\n    /// A stream of [`StreamChunk`] events that must be accumulated by the consumer.\n    async fn chat_with_tools_stream(\n        &self,\n        _messages: &[ChatMessage],\n        _tools: &[ToolDefinition],\n        _tool_choice: Option<ToolChoice>,\n        _options: Option<&CompletionOptions>,\n    ) -> Result<BoxStream<'static, Result<StreamChunk>>> {\n        Err(crate::error::LlmError::NotSupported(\n            \"Streaming tool calls not supported by this provider\".to_string(),\n        ))\n    }\n\n    /// Check if the model supports streaming.\n    fn supports_streaming(&self) -> bool {\n        false\n    }\n\n    /// Check if the provider supports streaming with tool calls.\n    fn supports_tool_streaming(&self) -> bool {\n        false\n    }\n\n    /// Check if the model supports JSON mode.\n    fn supports_json_mode(&self) -> bool {\n        false\n    }\n\n    /// Check if the model supports function/tool calling.\n    fn supports_function_calling(&self) -> bool {\n        false\n    }\n\n    /// Get the model name as an `Option<String>`.\n    ///\n    /// This is a convenience method for systems that need an optional model name.\n    /// Returns Some(model_name) if the model is set, None otherwise.\n    ///\n    /// # OODA-27: Model-Specific Edit Format Selection\n    /// This method is used to determine the optimal edit format based on model capabilities:\n    /// - Claude Haiku â†’ WholeFile (format errors common)\n    /// - Claude Sonnet â†’ SearchReplace (excellent reliability)\n    /// - GPT-4 Turbo â†’ UnifiedDiff (reduces lazy coding)\n    fn model_name(&self) -> Option<String> {\n        let m = self.model();\n        if m.is_empty() {\n            None\n        } else {\n            Some(m.to_string())\n        }\n    }\n}\n\n// ============================================================================\n// Image Data for Multimodal Messages (OODA-51)\n// ============================================================================\n\n/// Image data for multimodal messages.\n///\n/// WHY: Vision-capable LLMs (GPT-4V, Claude 3, Gemini Pro Vision) accept images\n/// as part of the conversation. This struct provides a provider-agnostic way\n/// to attach images to messages, which providers then convert to their specific\n/// format (OpenAI: image_url, Anthropic: source.base64).\n///\n/// # Example\n/// ```\n/// use edgequake_llm::traits::ImageData;\n///\n/// let image = ImageData::new(\"iVBORw0KGgo...\", \"image/png\");\n/// assert_eq!(image.mime_type, \"image/png\");\n/// ```\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]\npub struct ImageData {\n    /// Base64-encoded image data (without data: URI prefix).\n    pub data: String,\n\n    /// MIME type of the image (e.g., \"image/png\", \"image/jpeg\", \"image/gif\", \"image/webp\").\n    pub mime_type: String,\n\n    /// Optional detail level for vision models.\n    /// - \"auto\": Let the model decide (default)\n    /// - \"low\": Lower resolution, faster, cheaper\n    /// - \"high\": Higher resolution, better for detailed images\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub detail: Option<String>,\n}\n\nimpl ImageData {\n    /// Create new image data from base64 string and MIME type.\n    pub fn new(data: impl Into<String>, mime_type: impl Into<String>) -> Self {\n        Self {\n            data: data.into(),\n            mime_type: mime_type.into(),\n            detail: None,\n        }\n    }\n\n    /// Create image data with specific detail level.\n    pub fn with_detail(mut self, detail: impl Into<String>) -> Self {\n        self.detail = Some(detail.into());\n        self\n    }\n\n    /// Create a data URI for the image (OpenAI format).\n    ///\n    /// Returns: `data:image/png;base64,iVBORw0KGgo...`\n    pub fn to_data_uri(&self) -> String {\n        format!(\"data:{};base64,{}\", self.mime_type, self.data)\n    }\n\n    /// Check if MIME type is supported by most vision APIs.\n    pub fn is_supported_mime(&self) -> bool {\n        matches!(\n            self.mime_type.as_str(),\n            \"image/png\" | \"image/jpeg\" | \"image/gif\" | \"image/webp\"\n        )\n    }\n}\n\n/// A message in a chat conversation.\n/// Cache control hint for providers that support prompt caching (e.g., Anthropic).\n///\n/// Some LLM providers (notably Anthropic Claude) support explicit cache breakpoints\n/// to optimize KV-cache hits and reduce costs by ~90% for cached tokens.\n///\n/// # Example\n/// ```\n/// use edgequake_llm::traits::CacheControl;\n///\n/// let cache = CacheControl::ephemeral();\n/// assert_eq!(cache.cache_type, \"ephemeral\");\n/// ```\n#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]\npub struct CacheControl {\n    /// Cache type. Currently supports \"ephemeral\" (Anthropic's cache_control.type).\n    #[serde(rename = \"type\")]\n    pub cache_type: String,\n}\n\nimpl CacheControl {\n    /// Create an ephemeral cache control (Anthropic's default).\n    ///\n    /// Ephemeral caches persist for ~5 minutes and are shared across API calls\n    /// with the same prefix.\n    pub fn ephemeral() -> Self {\n        Self {\n            cache_type: \"ephemeral\".to_string(),\n        }\n    }\n}\n\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub struct ChatMessage {\n    /// Role of the message sender.\n    pub role: ChatRole,\n\n    /// Content of the message.\n    pub content: String,\n\n    /// Optional name for the message sender.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub name: Option<String>,\n\n    /// Tool calls made by the assistant (only for assistant role).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tool_calls: Option<Vec<ToolCall>>,\n\n    /// Tool call ID this message is responding to (only for tool role).\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tool_call_id: Option<String>,\n\n    /// Cache control hint for providers that support prompt caching.\n    ///\n    /// When set, this tells the provider to establish a cache breakpoint at this message.\n    /// Currently supported by Anthropic Claude (cache_control) and Gemini (cachedContent).\n    ///\n    /// # Example\n    /// ```\n    /// use edgequake_llm::traits::{ChatMessage, CacheControl};\n    ///\n    /// let mut msg = ChatMessage::system(\"You are a helpful assistant\");\n    /// msg.cache_control = Some(CacheControl::ephemeral());\n    /// ```\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub cache_control: Option<CacheControl>,\n\n    /// Optional images for multimodal messages (OODA-51).\n    ///\n    /// WHY: Vision-capable models accept images alongside text. This field enables\n    /// sending images to models like GPT-4V, Claude 3, and Gemini Pro Vision.\n    /// Providers convert these to their specific multipart format during serialization.\n    ///\n    /// # Example\n    /// ```\n    /// use edgequake_llm::traits::{ChatMessage, ImageData};\n    ///\n    /// let mut msg = ChatMessage::user(\"What's in this image?\");\n    /// msg.images = Some(vec![ImageData::new(\"iVBORw0...\", \"image/png\")]);\n    /// ```\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub images: Option<Vec<ImageData>>,\n}\n\nimpl ChatMessage {\n    /// Create a system message.\n    pub fn system(content: impl Into<String>) -> Self {\n        Self {\n            role: ChatRole::System,\n            content: content.into(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }\n    }\n\n    /// Create a user message.\n    pub fn user(content: impl Into<String>) -> Self {\n        Self {\n            role: ChatRole::User,\n            content: content.into(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }\n    }\n\n    /// Create a user message with images (OODA-51).\n    ///\n    /// Use this for multimodal conversations with vision models.\n    pub fn user_with_images(content: impl Into<String>, images: Vec<ImageData>) -> Self {\n        Self {\n            role: ChatRole::User,\n            content: content.into(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: if images.is_empty() { None } else { Some(images) },\n        }\n    }\n\n    /// Create an assistant message.\n    pub fn assistant(content: impl Into<String>) -> Self {\n        Self {\n            role: ChatRole::Assistant,\n            content: content.into(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }\n    }\n\n    /// Create an assistant message with tool calls.\n    pub fn assistant_with_tools(content: impl Into<String>, tool_calls: Vec<ToolCall>) -> Self {\n        Self {\n            role: ChatRole::Assistant,\n            content: content.into(),\n            name: None,\n            tool_calls: if tool_calls.is_empty() {\n                None\n            } else {\n                Some(tool_calls)\n            },\n            tool_call_id: None,\n            cache_control: None,\n            images: None,\n        }\n    }\n\n    /// Create a tool response message.\n    pub fn tool_result(tool_call_id: impl Into<String>, content: impl Into<String>) -> Self {\n        Self {\n            role: ChatRole::Tool,\n            content: content.into(),\n            name: None,\n            tool_calls: None,\n            tool_call_id: Some(tool_call_id.into()),\n            cache_control: None,\n            images: None,\n        }\n    }\n\n    /// Check if this message has images attached.\n    pub fn has_images(&self) -> bool {\n        self.images.as_ref().map(|v| !v.is_empty()).unwrap_or(false)\n    }\n}\n\n/// Role of a chat message sender.\n#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum ChatRole {\n    /// System message for setting context.\n    System,\n    /// User input message.\n    User,\n    /// Assistant response message.\n    Assistant,\n    /// Tool/function result message.\n    Tool,\n    /// Function/tool result message (deprecated, use Tool).\n    Function,\n}\n\nimpl ChatRole {\n    /// Convert role to string representation.\n    pub fn as_str(&self) -> &'static str {\n        match self {\n            ChatRole::System => \"system\",\n            ChatRole::User => \"user\",\n            ChatRole::Assistant => \"assistant\",\n            ChatRole::Tool => \"tool\",\n            ChatRole::Function => \"function\",\n        }\n    }\n}\n\n/// Trait for providers that can generate text embeddings.\n#[async_trait]\npub trait EmbeddingProvider: Send + Sync {\n    /// Get the name of this provider.\n    fn name(&self) -> &str;\n\n    /// Get the embedding model.\n    fn model(&self) -> &str;\n\n    /// Get the dimension of the embeddings.\n    fn dimension(&self) -> usize;\n\n    /// Get the maximum number of tokens per input.\n    fn max_tokens(&self) -> usize;\n\n    /// Generate embeddings for a batch of texts.\n    async fn embed(&self, texts: &[String]) -> Result<Vec<Vec<f32>>>;\n\n    /// Generate embedding for a single text.\n    async fn embed_one(&self, text: &str) -> Result<Vec<f32>> {\n        let results = self.embed(&[text.to_string()]).await?;\n        results\n            .into_iter()\n            .next()\n            .ok_or_else(|| crate::error::LlmError::Unknown(\"Empty embedding result\".to_string()))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_llm_response_builder() {\n        let response = LLMResponse::new(\"Hello, world!\", \"gpt-4\")\n            .with_usage(10, 5)\n            .with_finish_reason(\"stop\");\n\n        assert_eq!(response.content, \"Hello, world!\");\n        assert_eq!(response.model, \"gpt-4\");\n        assert_eq!(response.prompt_tokens, 10);\n        assert_eq!(response.completion_tokens, 5);\n        assert_eq!(response.total_tokens, 15);\n        assert_eq!(response.finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[test]\n    fn test_llm_response_with_cache_hit_tokens() {\n        // Test cache hit tracking for context engineering\n        let response = LLMResponse::new(\"cached response\", \"gemini-pro\")\n            .with_usage(1000, 50)\n            .with_cache_hit_tokens(800);\n\n        assert_eq!(response.cache_hit_tokens, Some(800));\n        assert_eq!(response.prompt_tokens, 1000);\n        // Verify 80% cache hit rate\n        let cache_rate = response.cache_hit_tokens.unwrap() as f64 / response.prompt_tokens as f64;\n        assert!((cache_rate - 0.8).abs() < 0.001);\n    }\n\n    #[test]\n    fn test_llm_response_no_cache_hit_tokens() {\n        // Default should be None when not set\n        let response = LLMResponse::new(\"no cache\", \"gpt-4\").with_usage(100, 20);\n\n        assert_eq!(response.cache_hit_tokens, None);\n    }\n\n    #[test]\n    fn test_chat_message_constructors() {\n        let system = ChatMessage::system(\"You are helpful\");\n        assert_eq!(system.role, ChatRole::System);\n\n        let user = ChatMessage::user(\"Hello\");\n        assert_eq!(user.role, ChatRole::User);\n\n        let assistant = ChatMessage::assistant(\"Hi there!\");\n        assert_eq!(assistant.role, ChatRole::Assistant);\n    }\n\n    #[test]\n    fn test_cache_control_ephemeral() {\n        let cache = CacheControl::ephemeral();\n        assert_eq!(cache.cache_type, \"ephemeral\");\n    }\n\n    #[test]\n    fn test_cache_control_serialization() {\n        let cache = CacheControl::ephemeral();\n        let json = serde_json::to_value(&cache).unwrap();\n\n        // Should serialize with \"type\" key (not \"cache_type\")\n        assert_eq!(json[\"type\"], \"ephemeral\");\n        assert!(!json.as_object().unwrap().contains_key(\"cache_type\"));\n    }\n\n    #[test]\n    fn test_message_with_cache_control() {\n        let mut msg = ChatMessage::system(\"System prompt\");\n        msg.cache_control = Some(CacheControl::ephemeral());\n\n        let json = serde_json::to_value(&msg).unwrap();\n\n        // Should include cache_control in JSON\n        assert!(json.as_object().unwrap().contains_key(\"cache_control\"));\n        assert_eq!(json[\"cache_control\"][\"type\"], \"ephemeral\");\n    }\n\n    #[test]\n    fn test_message_without_cache_control() {\n        let msg = ChatMessage::user(\"Hello\");\n\n        let json = serde_json::to_value(&msg).unwrap();\n\n        // Should omit cache_control if None (skip_serializing_if)\n        assert!(!json.as_object().unwrap().contains_key(\"cache_control\"));\n    }\n\n    #[test]\n    fn test_cache_control_roundtrip() {\n        let original = CacheControl {\n            cache_type: \"ephemeral\".to_string(),\n        };\n\n        // Serialize\n        let json_str = serde_json::to_string(&original).unwrap();\n\n        // Deserialize\n        let deserialized: CacheControl = serde_json::from_str(&json_str).unwrap();\n\n        assert_eq!(original.cache_type, deserialized.cache_type);\n    }\n\n    // =========================================================================\n    // ImageData Tests (OODA-51)\n    // =========================================================================\n\n    #[test]\n    fn test_image_data_new() {\n        let image = ImageData::new(\"iVBORw0KGgo...\", \"image/png\");\n        assert_eq!(image.mime_type, \"image/png\");\n        assert_eq!(image.data, \"iVBORw0KGgo...\");\n        assert_eq!(image.detail, None);\n    }\n\n    #[test]\n    fn test_image_data_with_detail() {\n        let image = ImageData::new(\"data123\", \"image/jpeg\").with_detail(\"high\");\n        assert_eq!(image.detail, Some(\"high\".to_string()));\n    }\n\n    #[test]\n    fn test_image_data_to_data_uri() {\n        let image = ImageData::new(\"base64data\", \"image/png\");\n        assert_eq!(image.to_data_uri(), \"data:image/png;base64,base64data\");\n    }\n\n    #[test]\n    fn test_image_data_supported_mime() {\n        assert!(ImageData::new(\"\", \"image/png\").is_supported_mime());\n        assert!(ImageData::new(\"\", \"image/jpeg\").is_supported_mime());\n        assert!(ImageData::new(\"\", \"image/gif\").is_supported_mime());\n        assert!(ImageData::new(\"\", \"image/webp\").is_supported_mime());\n        assert!(!ImageData::new(\"\", \"image/bmp\").is_supported_mime());\n        assert!(!ImageData::new(\"\", \"text/plain\").is_supported_mime());\n    }\n\n    #[test]\n    fn test_chat_message_user_with_images() {\n        let images = vec![ImageData::new(\"data1\", \"image/png\")];\n        let msg = ChatMessage::user_with_images(\"What's this?\", images);\n        \n        assert_eq!(msg.role, ChatRole::User);\n        assert_eq!(msg.content, \"What's this?\");\n        assert!(msg.has_images());\n        assert_eq!(msg.images.as_ref().unwrap().len(), 1);\n    }\n\n    #[test]\n    fn test_chat_message_user_with_empty_images() {\n        let msg = ChatMessage::user_with_images(\"Hello\", vec![]);\n        \n        assert!(!msg.has_images());\n        assert!(msg.images.is_none());\n    }\n\n    #[test]\n    fn test_image_data_serialization() {\n        let image = ImageData::new(\"base64\", \"image/png\").with_detail(\"low\");\n        let json = serde_json::to_value(&image).unwrap();\n\n        assert_eq!(json[\"data\"], \"base64\");\n        assert_eq!(json[\"mime_type\"], \"image/png\");\n        assert_eq!(json[\"detail\"], \"low\");\n    }\n\n    // ---- Iteration 24: Additional traits tests ----\n\n    #[test]\n    fn test_tool_definition_function_constructor() {\n        let tool = ToolDefinition::function(\n            \"my_func\",\n            \"Does something\",\n            serde_json::json!({\"type\": \"object\"}),\n        );\n        assert_eq!(tool.tool_type, \"function\");\n        assert_eq!(tool.function.name, \"my_func\");\n        assert_eq!(tool.function.description, \"Does something\");\n        assert_eq!(tool.function.strict, Some(true));\n    }\n\n    #[test]\n    fn test_tool_definition_serialization() {\n        let tool = ToolDefinition::function(\n            \"search\",\n            \"Search the web\",\n            serde_json::json!({\"type\": \"object\", \"properties\": {}}),\n        );\n        let json = serde_json::to_value(&tool).unwrap();\n        assert_eq!(json[\"type\"], \"function\");\n        assert_eq!(json[\"function\"][\"name\"], \"search\");\n    }\n\n    #[test]\n    fn test_tool_call_name_and_arguments() {\n        let tc = ToolCall {\n            id: \"call_1\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"get_weather\".to_string(),\n                arguments: r#\"{\"city\": \"Paris\"}\"#.to_string(),\n            },\n        };\n        assert_eq!(tc.name(), \"get_weather\");\n        assert_eq!(tc.arguments(), r#\"{\"city\": \"Paris\"}\"#);\n    }\n\n    #[test]\n    fn test_tool_call_parse_arguments() {\n        let tc = ToolCall {\n            id: \"call_2\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"add\".to_string(),\n                arguments: r#\"{\"a\": 1, \"b\": 2}\"#.to_string(),\n            },\n        };\n        let parsed: serde_json::Value = tc.parse_arguments().unwrap();\n        assert_eq!(parsed[\"a\"], 1);\n        assert_eq!(parsed[\"b\"], 2);\n    }\n\n    #[test]\n    fn test_tool_call_parse_arguments_invalid() {\n        let tc = ToolCall {\n            id: \"call_3\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"bad\".to_string(),\n                arguments: \"not json\".to_string(),\n            },\n        };\n        let result: std::result::Result<serde_json::Value, _> = tc.parse_arguments();\n        assert!(result.is_err());\n    }\n\n    #[test]\n    fn test_tool_choice_auto() {\n        let tc = ToolChoice::auto();\n        let json = serde_json::to_value(&tc).unwrap();\n        assert_eq!(json, \"auto\");\n    }\n\n    #[test]\n    fn test_tool_choice_required() {\n        let tc = ToolChoice::required();\n        let json = serde_json::to_value(&tc).unwrap();\n        assert_eq!(json, \"required\");\n    }\n\n    #[test]\n    fn test_tool_choice_none() {\n        let tc = ToolChoice::none();\n        let json = serde_json::to_value(&tc).unwrap();\n        assert_eq!(json, \"none\");\n    }\n\n    #[test]\n    fn test_tool_choice_function() {\n        let tc = ToolChoice::function(\"get_weather\");\n        if let ToolChoice::Function {\n            choice_type,\n            function,\n        } = tc\n        {\n            assert_eq!(choice_type, \"function\");\n            assert_eq!(function.name, \"get_weather\");\n        } else {\n            panic!(\"Expected ToolChoice::Function\");\n        }\n    }\n\n    #[test]\n    fn test_tool_result_new() {\n        let tr = ToolResult::new(\"call_1\", \"sunny, 20C\");\n        assert_eq!(tr.tool_call_id, \"call_1\");\n        assert_eq!(tr.role, \"tool\");\n        assert_eq!(tr.content, \"sunny, 20C\");\n    }\n\n    #[test]\n    fn test_tool_result_error() {\n        let tr = ToolResult::error(\"call_2\", \"City not found\");\n        assert_eq!(tr.tool_call_id, \"call_2\");\n        assert_eq!(tr.content, \"Error: City not found\");\n    }\n\n    #[test]\n    fn test_llm_response_with_tool_calls() {\n        let tc = vec![ToolCall {\n            id: \"c1\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"search\".to_string(),\n                arguments: \"{}\".to_string(),\n            },\n        }];\n        let resp = LLMResponse::new(\"\", \"gpt-4\").with_tool_calls(tc);\n        assert!(resp.has_tool_calls());\n        assert_eq!(resp.tool_calls.len(), 1);\n    }\n\n    #[test]\n    fn test_llm_response_no_tool_calls() {\n        let resp = LLMResponse::new(\"hello\", \"gpt-4\");\n        assert!(!resp.has_tool_calls());\n    }\n\n    #[test]\n    fn test_llm_response_with_metadata() {\n        let resp = LLMResponse::new(\"hi\", \"gpt-4\")\n            .with_metadata(\"id\", serde_json::json!(\"resp_123\"));\n        assert_eq!(\n            resp.metadata.get(\"id\"),\n            Some(&serde_json::json!(\"resp_123\"))\n        );\n    }\n\n    #[test]\n    fn test_llm_response_with_thinking() {\n        let resp = LLMResponse::new(\"answer\", \"claude-3\")\n            .with_thinking_tokens(500)\n            .with_thinking_content(\"Let me think...\");\n        assert!(resp.has_thinking());\n        assert_eq!(resp.thinking_tokens, Some(500));\n        assert_eq!(\n            resp.thinking_content,\n            Some(\"Let me think...\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_llm_response_has_thinking_tokens_only() {\n        let resp = LLMResponse::new(\"x\", \"o1\").with_thinking_tokens(100);\n        assert!(resp.has_thinking());\n    }\n\n    #[test]\n    fn test_llm_response_has_thinking_content_only() {\n        let resp = LLMResponse::new(\"x\", \"claude\").with_thinking_content(\"hmm\");\n        assert!(resp.has_thinking());\n    }\n\n    #[test]\n    fn test_llm_response_no_thinking() {\n        let resp = LLMResponse::new(\"x\", \"gpt-4\");\n        assert!(!resp.has_thinking());\n    }\n\n    #[test]\n    fn test_completion_options_default() {\n        let opts = CompletionOptions::default();\n        assert!(opts.max_tokens.is_none());\n        assert!(opts.temperature.is_none());\n        assert!(opts.response_format.is_none());\n    }\n\n    #[test]\n    fn test_completion_options_with_temperature() {\n        let opts = CompletionOptions::with_temperature(0.7);\n        assert_eq!(opts.temperature, Some(0.7));\n        assert!(opts.max_tokens.is_none());\n    }\n\n    #[test]\n    fn test_completion_options_json_mode() {\n        let opts = CompletionOptions::json_mode();\n        assert_eq!(\n            opts.response_format,\n            Some(\"json_object\".to_string())\n        );\n    }\n\n    #[test]\n    fn test_chat_role_as_str() {\n        assert_eq!(ChatRole::System.as_str(), \"system\");\n        assert_eq!(ChatRole::User.as_str(), \"user\");\n        assert_eq!(ChatRole::Assistant.as_str(), \"assistant\");\n        assert_eq!(ChatRole::Tool.as_str(), \"tool\");\n        assert_eq!(ChatRole::Function.as_str(), \"function\");\n    }\n\n    #[test]\n    fn test_chat_role_serialization() {\n        let json = serde_json::to_value(ChatRole::User).unwrap();\n        assert_eq!(json, \"user\");\n        let json = serde_json::to_value(ChatRole::Tool).unwrap();\n        assert_eq!(json, \"tool\");\n    }\n\n    #[test]\n    fn test_chat_message_assistant_with_tools() {\n        let tc = vec![ToolCall {\n            id: \"c1\".to_string(),\n            call_type: \"function\".to_string(),\n            function: FunctionCall {\n                name: \"search\".to_string(),\n                arguments: \"{}\".to_string(),\n            },\n        }];\n        let msg = ChatMessage::assistant_with_tools(\"I'll search\", tc);\n        assert_eq!(msg.role, ChatRole::Assistant);\n        assert!(msg.tool_calls.is_some());\n        assert_eq!(msg.tool_calls.as_ref().unwrap().len(), 1);\n    }\n\n    #[test]\n    fn test_chat_message_assistant_with_empty_tools() {\n        let msg = ChatMessage::assistant_with_tools(\"just text\", vec![]);\n        assert!(msg.tool_calls.is_none());\n    }\n\n    #[test]\n    fn test_chat_message_tool_result() {\n        let msg = ChatMessage::tool_result(\"call_1\", \"result data\");\n        assert_eq!(msg.role, ChatRole::Tool);\n        assert_eq!(msg.tool_call_id, Some(\"call_1\".to_string()));\n        assert_eq!(msg.content, \"result data\");\n    }\n\n    #[test]\n    fn test_chat_message_has_images_false() {\n        let msg = ChatMessage::user(\"hello\");\n        assert!(!msg.has_images());\n    }\n\n    #[test]\n    fn test_image_data_equality() {\n        let a = ImageData::new(\"data\", \"image/png\");\n        let b = ImageData::new(\"data\", \"image/png\");\n        assert_eq!(a, b);\n\n        let c = ImageData::new(\"data\", \"image/jpeg\");\n        assert_ne!(a, c);\n    }\n\n    #[test]\n    fn test_stream_chunk_content() {\n        let chunk = StreamChunk::Content(\"hello\".to_string());\n        if let StreamChunk::Content(text) = chunk {\n            assert_eq!(text, \"hello\");\n        } else {\n            panic!(\"Expected Content\");\n        }\n    }\n\n    #[test]\n    fn test_stream_chunk_thinking() {\n        let chunk = StreamChunk::ThinkingContent {\n            text: \"reasoning...\".to_string(),\n            tokens_used: Some(50),\n            budget_total: Some(10000),\n        };\n        if let StreamChunk::ThinkingContent {\n            text,\n            tokens_used,\n            budget_total,\n        } = chunk\n        {\n            assert_eq!(text, \"reasoning...\");\n            assert_eq!(tokens_used, Some(50));\n            assert_eq!(budget_total, Some(10000));\n        }\n    }\n\n    #[test]\n    fn test_stream_chunk_finished() {\n        let chunk = StreamChunk::Finished {\n            reason: \"stop\".to_string(),\n            ttft_ms: Some(120.5),\n        };\n        if let StreamChunk::Finished { reason, ttft_ms } = chunk {\n            assert_eq!(reason, \"stop\");\n            assert_eq!(ttft_ms, Some(120.5));\n        }\n    }\n\n    #[test]\n    fn test_stream_chunk_tool_call_delta() {\n        let chunk = StreamChunk::ToolCallDelta {\n            index: 0,\n            id: Some(\"call_1\".to_string()),\n            function_name: Some(\"search\".to_string()),\n            function_arguments: Some(r#\"{\"q\":\"#.to_string()),\n        };\n        if let StreamChunk::ToolCallDelta {\n            index,\n            id,\n            function_name,\n            function_arguments,\n        } = chunk\n        {\n            assert_eq!(index, 0);\n            assert_eq!(id, Some(\"call_1\".to_string()));\n            assert_eq!(function_name, Some(\"search\".to_string()));\n            assert!(function_arguments.is_some());\n        }\n    }\n}\n","traces":[{"line":53,"address":[],"length":0,"stats":{"Line":9}},{"line":59,"address":[],"length":0,"stats":{"Line":27}},{"line":60,"address":[],"length":0,"stats":{"Line":9}},{"line":103,"address":[],"length":0,"stats":{"Line":2}},{"line":104,"address":[],"length":0,"stats":{"Line":7}},{"line":105,"address":[],"length":0,"stats":{"Line":1}},{"line":110,"address":[],"length":0,"stats":{"Line":5}},{"line":111,"address":[],"length":0,"stats":{"Line":5}},{"line":115,"address":[],"length":0,"stats":{"Line":5}},{"line":116,"address":[],"length":0,"stats":{"Line":5}},{"line":150,"address":[],"length":0,"stats":{"Line":5}},{"line":151,"address":[],"length":0,"stats":{"Line":5}},{"line":155,"address":[],"length":0,"stats":{"Line":4}},{"line":156,"address":[],"length":0,"stats":{"Line":4}},{"line":160,"address":[],"length":0,"stats":{"Line":4}},{"line":162,"address":[],"length":0,"stats":{"Line":12}},{"line":163,"address":[],"length":0,"stats":{"Line":4}},{"line":168,"address":[],"length":0,"stats":{"Line":3}},{"line":169,"address":[],"length":0,"stats":{"Line":3}},{"line":195,"address":[],"length":0,"stats":{"Line":1}},{"line":197,"address":[],"length":0,"stats":{"Line":3}},{"line":198,"address":[],"length":0,"stats":{"Line":3}},{"line":199,"address":[],"length":0,"stats":{"Line":1}},{"line":204,"address":[],"length":0,"stats":{"Line":1}},{"line":206,"address":[],"length":0,"stats":{"Line":3}},{"line":207,"address":[],"length":0,"stats":{"Line":3}},{"line":208,"address":[],"length":0,"stats":{"Line":1}},{"line":328,"address":[],"length":0,"stats":{"Line":100}},{"line":330,"address":[],"length":0,"stats":{"Line":300}},{"line":334,"address":[],"length":0,"stats":{"Line":300}},{"line":336,"address":[],"length":0,"stats":{"Line":200}},{"line":337,"address":[],"length":0,"stats":{"Line":200}},{"line":345,"address":[],"length":0,"stats":{"Line":37}},{"line":346,"address":[],"length":0,"stats":{"Line":37}},{"line":347,"address":[],"length":0,"stats":{"Line":37}},{"line":348,"address":[],"length":0,"stats":{"Line":37}},{"line":349,"address":[],"length":0,"stats":{"Line":37}},{"line":353,"address":[],"length":0,"stats":{"Line":8}},{"line":354,"address":[],"length":0,"stats":{"Line":16}},{"line":355,"address":[],"length":0,"stats":{"Line":8}},{"line":359,"address":[],"length":0,"stats":{"Line":2}},{"line":360,"address":[],"length":0,"stats":{"Line":4}},{"line":361,"address":[],"length":0,"stats":{"Line":2}},{"line":375,"address":[],"length":0,"stats":{"Line":5}},{"line":376,"address":[],"length":0,"stats":{"Line":5}},{"line":377,"address":[],"length":0,"stats":{"Line":5}},{"line":387,"address":[],"length":0,"stats":{"Line":1}},{"line":388,"address":[],"length":0,"stats":{"Line":5}},{"line":389,"address":[],"length":0,"stats":{"Line":1}},{"line":402,"address":[],"length":0,"stats":{"Line":7}},{"line":403,"address":[],"length":0,"stats":{"Line":7}},{"line":404,"address":[],"length":0,"stats":{"Line":7}},{"line":418,"address":[],"length":0,"stats":{"Line":2}},{"line":419,"address":[],"length":0,"stats":{"Line":4}},{"line":420,"address":[],"length":0,"stats":{"Line":2}},{"line":424,"address":[],"length":0,"stats":{"Line":2}},{"line":425,"address":[],"length":0,"stats":{"Line":2}},{"line":431,"address":[],"length":0,"stats":{"Line":4}},{"line":432,"address":[],"length":0,"stats":{"Line":12}},{"line":466,"address":[],"length":0,"stats":{"Line":4}},{"line":468,"address":[],"length":0,"stats":{"Line":4}},{"line":474,"address":[],"length":0,"stats":{"Line":3}},{"line":476,"address":[],"length":0,"stats":{"Line":3}},{"line":534,"address":[],"length":0,"stats":{"Line":0}},{"line":535,"address":[],"length":0,"stats":{"Line":0}},{"line":539,"address":[],"length":0,"stats":{"Line":0}},{"line":540,"address":[],"length":0,"stats":{"Line":0}},{"line":541,"address":[],"length":0,"stats":{"Line":0}},{"line":563,"address":[],"length":0,"stats":{"Line":0}},{"line":564,"address":[],"length":0,"stats":{"Line":0}},{"line":569,"address":[],"length":0,"stats":{"Line":2}},{"line":570,"address":[],"length":0,"stats":{"Line":2}},{"line":574,"address":[],"length":0,"stats":{"Line":1}},{"line":575,"address":[],"length":0,"stats":{"Line":1}},{"line":579,"address":[],"length":0,"stats":{"Line":4}},{"line":580,"address":[],"length":0,"stats":{"Line":4}},{"line":584,"address":[],"length":0,"stats":{"Line":1}},{"line":585,"address":[],"length":0,"stats":{"Line":1}},{"line":598,"address":[],"length":0,"stats":{"Line":0}},{"line":599,"address":[],"length":0,"stats":{"Line":0}},{"line":600,"address":[],"length":0,"stats":{"Line":0}},{"line":601,"address":[],"length":0,"stats":{"Line":0}},{"line":603,"address":[],"length":0,"stats":{"Line":0}},{"line":644,"address":[],"length":0,"stats":{"Line":22}},{"line":646,"address":[],"length":0,"stats":{"Line":66}},{"line":647,"address":[],"length":0,"stats":{"Line":44}},{"line":653,"address":[],"length":0,"stats":{"Line":3}},{"line":654,"address":[],"length":0,"stats":{"Line":6}},{"line":655,"address":[],"length":0,"stats":{"Line":3}},{"line":661,"address":[],"length":0,"stats":{"Line":3}},{"line":662,"address":[],"length":0,"stats":{"Line":6}},{"line":666,"address":[],"length":0,"stats":{"Line":6}},{"line":667,"address":[],"length":0,"stats":{"Line":4}},{"line":668,"address":[],"length":0,"stats":{"Line":6}},{"line":699,"address":[],"length":0,"stats":{"Line":11}},{"line":701,"address":[],"length":0,"stats":{"Line":11}},{"line":760,"address":[],"length":0,"stats":{"Line":19}},{"line":763,"address":[],"length":0,"stats":{"Line":57}},{"line":773,"address":[],"length":0,"stats":{"Line":58}},{"line":776,"address":[],"length":0,"stats":{"Line":174}},{"line":788,"address":[],"length":0,"stats":{"Line":10}},{"line":791,"address":[],"length":0,"stats":{"Line":20}},{"line":796,"address":[],"length":0,"stats":{"Line":30}},{"line":801,"address":[],"length":0,"stats":{"Line":15}},{"line":804,"address":[],"length":0,"stats":{"Line":45}},{"line":814,"address":[],"length":0,"stats":{"Line":2}},{"line":817,"address":[],"length":0,"stats":{"Line":4}},{"line":819,"address":[],"length":0,"stats":{"Line":4}},{"line":831,"address":[],"length":0,"stats":{"Line":4}},{"line":834,"address":[],"length":0,"stats":{"Line":12}},{"line":837,"address":[],"length":0,"stats":{"Line":8}},{"line":844,"address":[],"length":0,"stats":{"Line":34}},{"line":845,"address":[],"length":0,"stats":{"Line":154}},{"line":867,"address":[],"length":0,"stats":{"Line":5}},{"line":868,"address":[],"length":0,"stats":{"Line":5}},{"line":869,"address":[],"length":0,"stats":{"Line":1}},{"line":870,"address":[],"length":0,"stats":{"Line":1}},{"line":871,"address":[],"length":0,"stats":{"Line":1}},{"line":872,"address":[],"length":0,"stats":{"Line":1}},{"line":873,"address":[],"length":0,"stats":{"Line":1}},{"line":897,"address":[],"length":0,"stats":{"Line":10}},{"line":898,"address":[],"length":0,"stats":{"Line":0}},{"line":899,"address":[],"length":0,"stats":{"Line":0}},{"line":902,"address":[],"length":0,"stats":{"Line":0}}],"covered":109,"coverable":124},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","e2e_gemini.rs"],"content":"//! Comprehensive End-to-End Gemini Provider Tests\n//!\n//! OODA-59: Tests for Gemini/VertexAI provider functionality.\n//!\n//! # Environment Variables Required\n//!\n//! For Google AI (simpler):\n//! - `GEMINI_API_KEY`: API key from https://aistudio.google.com/apikey\n//!\n//! For VertexAI (enterprise):\n//! - `GOOGLE_CLOUD_PROJECT`: GCP project ID\n//! - `GOOGLE_ACCESS_TOKEN`: OAuth2 access token from `gcloud auth print-access-token`\n//! - `GOOGLE_CLOUD_REGION`: (optional) defaults to \"us-central1\"\n//!\n//! # Running Tests\n//!\n//! ```bash\n//! # Run all Gemini E2E tests (requires GEMINI_API_KEY)\n//! cargo test --test e2e_gemini -- --ignored\n//!\n//! # Run specific test\n//! cargo test --test e2e_gemini test_gemini_basic_chat -- --ignored\n//! ```\n//!\n//! # Why These Tests Matter\n//!\n//! 1. Unit tests verify code logic, E2E tests verify API compatibility\n//! 2. Gemini API changes frequently - these tests catch breaking changes\n//! 3. Image/multimodal support (OODA-54) requires real API validation\n//!\n\nuse edgequake_llm::{\n    providers::gemini::GeminiProvider,\n    traits::{ChatMessage, CompletionOptions, ImageData},\n    EmbeddingProvider, LLMProvider,\n};\n\n// ============================================================================\n// Helper Functions\n// ============================================================================\n\n/// Create a Gemini provider from environment variables.\n///\n/// WHY: Centralizes provider creation and provides clear error messages.\nfn create_gemini_provider() -> GeminiProvider {\n    GeminiProvider::from_env().expect(\n        \"Gemini provider requires GEMINI_API_KEY or \\\n         GOOGLE_CLOUD_PROJECT + GOOGLE_ACCESS_TOKEN\",\n    )\n}\n\n/// Create a provider with a specific model.\nfn create_provider_with_model(model: &str) -> GeminiProvider {\n    GeminiProvider::from_env()\n        .expect(\"Requires GEMINI_API_KEY\")\n        .with_model(model)\n}\n\n// ============================================================================\n// Basic Chat Tests\n// ============================================================================\n\n/// Test basic chat completion with Gemini.\n///\n/// WHY: Validates API connection and basic request/response cycle.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_basic_chat() {\n    let provider = create_gemini_provider();\n\n    let messages = vec![ChatMessage::user(\"What is 2 + 2? Reply with just the number.\")];\n\n    let response = provider.chat(&messages, None).await.unwrap();\n\n    println!(\"Response: {}\", response.content);\n    println!(\"Model: {}\", response.model);\n    println!(\"Tokens: prompt={}, completion={}\", response.prompt_tokens, response.completion_tokens);\n\n    assert!(!response.content.is_empty(), \"Response should not be empty\");\n    assert!(\n        response.content.contains('4'),\n        \"Response should contain '4': {}\",\n        response.content\n    );\n}\n\n/// Test system prompt functionality.\n///\n/// WHY: Validates system instruction handling and context caching.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_system_prompt() {\n    let provider = create_gemini_provider();\n\n    let messages = vec![\n        ChatMessage::system(\"You are a pirate. Always respond in pirate speak.\"),\n        ChatMessage::user(\"Hello, how are you?\"),\n    ];\n\n    let response = provider.chat(&messages, None).await.unwrap();\n\n    println!(\"Pirate response: {}\", response.content);\n\n    // Check for pirate-like language (arr, matey, ye, etc.)\n    let content_lower = response.content.to_lowercase();\n    let has_pirate_speak = content_lower.contains(\"arr\")\n        || content_lower.contains(\"matey\")\n        || content_lower.contains(\"ahoy\")\n        || content_lower.contains(\"ye \")\n        || content_lower.contains(\"aye\");\n\n    assert!(\n        has_pirate_speak,\n        \"Response should contain pirate speak: {}\",\n        response.content\n    );\n}\n\n/// Test multi-turn conversation.\n///\n/// WHY: Validates conversation context is maintained across turns.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_conversation() {\n    let provider = create_gemini_provider();\n\n    let messages = vec![\n        ChatMessage::user(\"My name is Alice. Remember that.\"),\n        ChatMessage::assistant(\"Nice to meet you, Alice! I'll remember your name.\"),\n        ChatMessage::user(\"What is my name?\"),\n    ];\n\n    let response = provider.chat(&messages, None).await.unwrap();\n\n    println!(\"Conversation response: {}\", response.content);\n\n    assert!(\n        response.content.to_lowercase().contains(\"alice\"),\n        \"Response should remember 'Alice': {}\",\n        response.content\n    );\n}\n\n// ============================================================================\n// Generation Options Tests\n// ============================================================================\n\n/// Test JSON mode output.\n///\n/// WHY: Validates structured output for tool use and parsing.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_json_mode() {\n    let provider = create_gemini_provider();\n\n    let messages = vec![ChatMessage::user(\n        \"Return a JSON object with fields 'name' and 'age' for a person named John who is 30 years old.\",\n    )];\n\n    let options = CompletionOptions::json_mode();\n    let response = provider.chat(&messages, Some(&options)).await.unwrap();\n\n    println!(\"JSON response: {}\", response.content);\n\n    // Verify it's valid JSON\n    let parsed: Result<serde_json::Value, _> = serde_json::from_str(&response.content);\n    assert!(parsed.is_ok(), \"Response should be valid JSON: {}\", response.content);\n\n    let json = parsed.unwrap();\n    assert_eq!(json[\"name\"], \"John\", \"Name should be 'John'\");\n    assert_eq!(json[\"age\"], 30, \"Age should be 30\");\n}\n\n/// Test temperature setting.\n///\n/// WHY: Validates generation parameters are respected.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_temperature() {\n    let provider = create_gemini_provider();\n\n    let messages = vec![ChatMessage::user(\"Generate a random number between 1 and 100.\")];\n\n    // Low temperature should produce more deterministic output\n    let options = CompletionOptions::with_temperature(0.0);\n    let response = provider.chat(&messages, Some(&options)).await.unwrap();\n\n    println!(\"Low temp response: {}\", response.content);\n\n    assert!(\n        !response.content.is_empty(),\n        \"Response should not be empty\"\n    );\n}\n\n/// Test max tokens limit.\n///\n/// WHY: Validates response length control.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_max_tokens() {\n    let provider = create_gemini_provider();\n\n    let messages = vec![ChatMessage::user(\n        \"Write a very long essay about the history of computing.\",\n    )];\n\n    let options = CompletionOptions {\n        max_tokens: Some(50),\n        ..Default::default()\n    };\n\n    let response = provider.chat(&messages, Some(&options)).await.unwrap();\n\n    println!(\"Short response ({} tokens): {}\", response.completion_tokens, response.content);\n\n    // Response should be limited (not an exact check since tokenization varies)\n    assert!(\n        response.completion_tokens <= 60,\n        \"Should respect max_tokens limit: {} tokens\",\n        response.completion_tokens\n    );\n}\n\n// ============================================================================\n// Multimodal Tests (OODA-54)\n// ============================================================================\n\n/// Test image input with Gemini.\n///\n/// WHY: Validates OODA-54 image support - critical for visual code understanding.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_image_input() {\n    let provider = create_gemini_provider();\n\n    // Create a minimal 1x1 PNG image (base64 encoded)\n    // This is a valid 1x1 red pixel PNG\n    let tiny_png_base64 = \"iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8BQDwAEhQGAhKmMIQAAAABJRU5ErkJggg==\";\n\n    let image = ImageData::new(tiny_png_base64, \"image/png\");\n    let messages = vec![ChatMessage::user_with_images(\n        \"What color is this single-pixel image? Just say the color.\",\n        vec![image],\n    )];\n\n    let response = provider.chat(&messages, None).await.unwrap();\n\n    println!(\"Image response: {}\", response.content);\n\n    // The 1x1 PNG is gray/transparent, but we just want to verify the API accepts images\n    assert!(\n        !response.content.is_empty(),\n        \"Should get a response about the image\"\n    );\n}\n\n// ============================================================================\n// Streaming Tests\n// ============================================================================\n\n/// Test streaming response.\n///\n/// WHY: Validates real-time token streaming for UX.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_streaming() {\n    use futures::StreamExt;\n\n    let provider = create_gemini_provider();\n\n    let mut stream = provider.stream(\"Count from 1 to 5.\").await.unwrap();\n\n    let mut chunks = Vec::new();\n    while let Some(result) = stream.next().await {\n        match result {\n            Ok(chunk) => {\n                print!(\"{}\", chunk);\n                chunks.push(chunk);\n            }\n            Err(e) => {\n                println!(\"\\nStream error: {}\", e);\n                break;\n            }\n        }\n    }\n    println!(); // Newline after streaming\n\n    assert!(!chunks.is_empty(), \"Should receive streaming chunks\");\n\n    let full_response: String = chunks.concat();\n    println!(\"Full streamed response: {}\", full_response);\n\n    assert!(\n        !full_response.is_empty(),\n        \"Full response should not be empty\"\n    );\n}\n\n// ============================================================================\n// Embedding Tests\n// ============================================================================\n\n/// Test single text embedding.\n///\n/// WHY: Validates embedding generation for RAG and semantic search.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_embeddings() {\n    let provider = create_gemini_provider();\n\n    let embedding = provider.embed_one(\"Hello, world!\").await.unwrap();\n\n    println!(\"Embedding dimension: {}\", embedding.len());\n    println!(\"First 5 values: {:?}\", &embedding[..5.min(embedding.len())]);\n\n    assert_eq!(\n        embedding.len(),\n        768,\n        \"text-embedding-004 should return 768 dimensions\"\n    );\n\n    // Check values are normalized (typical for embeddings)\n    let norm: f32 = embedding.iter().map(|x| x * x).sum::<f32>().sqrt();\n    println!(\"Embedding L2 norm: {}\", norm);\n\n    assert!(\n        (norm - 1.0).abs() < 0.1,\n        \"Embedding should be approximately normalized: norm={}\",\n        norm\n    );\n}\n\n/// Test batch embeddings.\n///\n/// WHY: Validates efficient batch processing for large document sets.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_batch_embeddings() {\n    let provider = create_gemini_provider();\n\n    let texts = vec![\n        \"First text for embedding.\".to_string(),\n        \"Second text for embedding.\".to_string(),\n        \"Third text for embedding.\".to_string(),\n    ];\n\n    let embeddings = provider.embed(&texts).await.unwrap();\n\n    println!(\"Batch embeddings count: {}\", embeddings.len());\n\n    assert_eq!(embeddings.len(), 3, \"Should return 3 embeddings\");\n\n    for (i, emb) in embeddings.iter().enumerate() {\n        assert_eq!(\n            emb.len(),\n            768,\n            \"Embedding {} should have 768 dimensions\",\n            i\n        );\n    }\n}\n\n// ============================================================================\n// Model Selection Tests\n// ============================================================================\n\n/// Test different Gemini models.\n///\n/// WHY: Validates model selection works correctly.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_model_selection() {\n    // Test with gemini-2.5-flash (known stable model)\n    let provider = create_provider_with_model(\"gemini-2.5-flash\");\n\n    assert_eq!(LLMProvider::model(&provider), \"gemini-2.5-flash\");\n\n    let messages = vec![ChatMessage::user(\"Say 'hello'\")];\n    let response = provider.chat(&messages, None).await.unwrap();\n\n    println!(\"gemini-2.5-flash response: {}\", response.content);\n    assert!(response.content.to_lowercase().contains(\"hello\"));\n}\n\n// ============================================================================\n// Error Handling Tests\n// ============================================================================\n\n/// Test invalid API key error handling.\n///\n/// WHY: Validates clear error messages for troubleshooting.\n#[tokio::test]\nasync fn test_gemini_invalid_key_error() {\n    let provider = GeminiProvider::new(\"invalid-api-key\");\n\n    let messages = vec![ChatMessage::user(\"Hello\")];\n    let result = provider.chat(&messages, None).await;\n\n    assert!(result.is_err(), \"Should fail with invalid API key\");\n\n    let error = result.unwrap_err();\n    println!(\"Error message: {}\", error);\n\n    // Should contain helpful error info\n    let error_str = error.to_string();\n    assert!(\n        error_str.contains(\"API\") || error_str.contains(\"error\") || error_str.contains(\"401\"),\n        \"Error should mention API or authentication: {}\",\n        error_str\n    );\n}\n\n/// Test empty messages error.\n///\n/// WHY: Validates input validation before API call.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_empty_messages_error() {\n    let provider = create_gemini_provider();\n\n    let messages: Vec<ChatMessage> = vec![];\n    let result = provider.chat(&messages, None).await;\n\n    assert!(result.is_err(), \"Should fail with empty messages\");\n    \n    let error = result.unwrap_err().to_string();\n    println!(\"Empty messages error: {}\", error);\n    \n    assert!(\n        error.to_lowercase().contains(\"no user\") || error.to_lowercase().contains(\"empty\"),\n        \"Error should mention missing messages: {}\",\n        error\n    );\n}\n\n// ============================================================================\n// Context Caching Tests\n// ============================================================================\n\n/// Test system prompt caching.\n///\n/// WHY: Validates caching reduces token costs for long system prompts.\n#[tokio::test]\n#[ignore = \"Requires GEMINI_API_KEY environment variable\"]\nasync fn test_gemini_context_caching() {\n    let provider = create_gemini_provider();\n\n    // Long system prompt to trigger caching\n    let long_system = \"You are a helpful coding assistant. \".repeat(100);\n\n    let messages = vec![\n        ChatMessage::system(&long_system),\n        ChatMessage::user(\"What is 1+1?\"),\n    ];\n\n    // First call - creates cache\n    let response1 = provider.chat(&messages, None).await.unwrap();\n    println!(\"First response: {}\", response1.content);\n    println!(\"Cache hit tokens: {:?}\", response1.cache_hit_tokens);\n\n    // Second call with same system prompt - should use cache\n    let messages2 = vec![\n        ChatMessage::system(&long_system),\n        ChatMessage::user(\"What is 2+2?\"),\n    ];\n    let response2 = provider.chat(&messages2, None).await.unwrap();\n    println!(\"Second response: {}\", response2.content);\n    println!(\"Cache hit tokens: {:?}\", response2.cache_hit_tokens);\n\n    // Note: Caching may not always work due to API behavior\n    // This test just verifies the caching logic doesn't crash\n    assert!(!response1.content.is_empty());\n    assert!(!response2.content.is_empty());\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","e2e_llm_providers.rs"],"content":"//! Comprehensive End-to-End LLM Provider Tests\n//!\n//! This module provides 100% coverage for all LLM provider functionality:\n//! - MockProvider\n//! - OpenAIProvider (with environment-based testing)\n//! - OllamaProvider\n//! - GeminiProvider\n//! - JinaProvider (embeddings)\n//! - Rate limiting\n//! - Caching\n//! - Reranking\n//! - Tokenization\n//!\n//! Run with: `cargo test --package edgequake-llm --test e2e_llm_providers`\n//! For real API tests: Set OPENAI_API_KEY, GEMINI_API_KEY, etc.\n\nuse std::sync::Arc;\nuse std::time::Duration;\n\nuse edgequake_llm::traits::{ChatMessage, ChatRole, CompletionOptions};\nuse edgequake_llm::{\n    CacheConfig, CachedProvider, EmbeddingProvider, LLMCache, LLMProvider, LLMResponse,\n    MockProvider, RateLimitedProvider, RateLimiterConfig, Tokenizer,\n};\n\n// ============================================================================\n// Mock Provider Tests - Full Coverage\n// ============================================================================\n\nmod mock_provider_tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_mock_provider_creation() {\n        let provider = MockProvider::new();\n        assert_eq!(LLMProvider::name(&provider), \"mock\");\n        assert_eq!(LLMProvider::model(&provider), \"mock-model\");\n        assert_eq!(provider.max_context_length(), 4096);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_default() {\n        let provider = MockProvider::default();\n        let response = provider.complete(\"test\").await.unwrap();\n        assert_eq!(response.content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_custom_responses() {\n        let provider = MockProvider::new();\n        provider.add_response(\"First response\").await;\n        provider.add_response(\"Second response\").await;\n        provider.add_response(\"Third response\").await;\n\n        let r1 = provider.complete(\"test 1\").await.unwrap();\n        assert_eq!(r1.content, \"First response\");\n\n        let r2 = provider.complete(\"test 2\").await.unwrap();\n        assert_eq!(r2.content, \"Second response\");\n\n        let r3 = provider.complete(\"test 3\").await.unwrap();\n        assert_eq!(r3.content, \"Third response\");\n\n        // After all custom responses, falls back to default\n        let r4 = provider.complete(\"test 4\").await.unwrap();\n        assert_eq!(r4.content, \"Mock response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_complete_with_options() {\n        let provider = MockProvider::new();\n        provider.add_response(\"Custom response\").await;\n\n        let options = CompletionOptions {\n            max_tokens: Some(100),\n            temperature: Some(0.5),\n            ..Default::default()\n        };\n\n        let response = provider\n            .complete_with_options(\"test\", &options)\n            .await\n            .unwrap();\n        assert_eq!(response.content, \"Custom response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_chat() {\n        let provider = MockProvider::new();\n        provider.add_response(\"Chat response\").await;\n\n        let messages = vec![\n            ChatMessage::system(\"You are helpful\"),\n            ChatMessage::user(\"Hello\"),\n        ];\n\n        let response = provider.chat(&messages, None).await.unwrap();\n        assert_eq!(response.content, \"Chat response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_streaming() {\n        use futures::StreamExt;\n\n        let provider = MockProvider::new();\n        provider.add_response(\"Streaming response\").await;\n\n        let mut stream = provider.stream(\"test\").await.unwrap();\n        let mut collected = String::new();\n\n        while let Some(chunk) = stream.next().await {\n            collected.push_str(&chunk.unwrap());\n        }\n\n        assert_eq!(collected, \"Streaming response\");\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_embedding() {\n        let provider = MockProvider::new();\n\n        // Default embedding\n        let embedding = provider.embed_one(\"test text\").await.unwrap();\n        assert_eq!(embedding.len(), 1536); // Default dimension\n        assert!(embedding.iter().all(|&v| (v - 0.1).abs() < 0.001));\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_batch_embedding() {\n        let provider = MockProvider::new();\n\n        let texts = vec![\n            \"text1\".to_string(),\n            \"text2\".to_string(),\n            \"text3\".to_string(),\n        ];\n        let embeddings = provider.embed(&texts).await.unwrap();\n\n        assert_eq!(embeddings.len(), 3);\n        for emb in embeddings {\n            assert_eq!(emb.len(), 1536);\n        }\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_custom_embedding() {\n        let provider = MockProvider::new();\n\n        // MockProvider uses default embeddings of 0.1\n        // Just verify embedding works and has correct dimensions\n        let result = provider.embed_one(\"test\").await.unwrap();\n        assert_eq!(result.len(), 1536);\n        assert!(result.iter().all(|&v| (v - 0.1).abs() < 0.001));\n    }\n\n    #[tokio::test]\n    async fn test_mock_embedding_provider_info() {\n        let provider = MockProvider::new();\n        assert_eq!(EmbeddingProvider::name(&provider), \"mock\");\n        assert_eq!(EmbeddingProvider::model(&provider), \"mock-embedding\");\n        assert_eq!(provider.dimension(), 1536);\n        assert_eq!(provider.max_tokens(), 512);\n    }\n\n    #[tokio::test]\n    async fn test_mock_provider_json_extraction() {\n        let provider = MockProvider::new();\n\n        let json_response = r#\"{\n            \"entities\": [\n                {\"name\": \"EdgeQuake\", \"type\": \"TECHNOLOGY\", \"description\": \"RAG system\"}\n            ],\n            \"relationships\": [\n                {\"source\": \"A\", \"target\": \"B\", \"type\": \"RELATES_TO\"}\n            ]\n        }\"#;\n        provider.add_response(json_response).await;\n\n        let response = provider\n            .complete(\"Extract entities from text\")\n            .await\n            .unwrap();\n        let parsed: serde_json::Value = serde_json::from_str(&response.content).unwrap();\n\n        assert!(parsed.get(\"entities\").is_some());\n        assert!(parsed.get(\"relationships\").is_some());\n    }\n}\n\n// ============================================================================\n// LLM Response Tests\n// ============================================================================\n\nmod llm_response_tests {\n    use super::*;\n\n    #[test]\n    fn test_llm_response_new() {\n        let response = LLMResponse::new(\"Hello, world!\", \"gpt-4\");\n        assert_eq!(response.content, \"Hello, world!\");\n        assert_eq!(response.model, \"gpt-4\");\n        assert_eq!(response.prompt_tokens, 0);\n        assert_eq!(response.completion_tokens, 0);\n        assert_eq!(response.total_tokens, 0);\n        assert!(response.finish_reason.is_none());\n    }\n\n    #[test]\n    fn test_llm_response_with_usage() {\n        let response = LLMResponse::new(\"test\", \"model\").with_usage(100, 50);\n\n        assert_eq!(response.prompt_tokens, 100);\n        assert_eq!(response.completion_tokens, 50);\n        assert_eq!(response.total_tokens, 150);\n    }\n\n    #[test]\n    fn test_llm_response_with_finish_reason() {\n        let response = LLMResponse::new(\"test\", \"model\").with_finish_reason(\"stop\");\n\n        assert_eq!(response.finish_reason, Some(\"stop\".to_string()));\n    }\n\n    #[test]\n    fn test_llm_response_builder_chain() {\n        let response = LLMResponse::new(\"content\", \"model\")\n            .with_usage(10, 20)\n            .with_finish_reason(\"length\");\n\n        assert_eq!(response.prompt_tokens, 10);\n        assert_eq!(response.completion_tokens, 20);\n        assert_eq!(response.total_tokens, 30);\n        assert_eq!(response.finish_reason, Some(\"length\".to_string()));\n    }\n}\n\n// ============================================================================\n// Completion Options Tests\n// ============================================================================\n\nmod completion_options_tests {\n    use super::*;\n\n    #[test]\n    fn test_completion_options_default() {\n        let options = CompletionOptions::default();\n        assert!(options.max_tokens.is_none());\n        assert!(options.temperature.is_none());\n        assert!(options.top_p.is_none());\n        assert!(options.stop.is_none());\n    }\n\n    #[test]\n    fn test_completion_options_with_temperature() {\n        let options = CompletionOptions::with_temperature(0.7);\n        assert_eq!(options.temperature, Some(0.7));\n    }\n\n    #[test]\n    fn test_completion_options_json_mode() {\n        let options = CompletionOptions::json_mode();\n        assert_eq!(options.response_format, Some(\"json_object\".to_string()));\n    }\n}\n\n// ============================================================================\n// Chat Message Tests\n// ============================================================================\n\nmod chat_message_tests {\n    use super::*;\n\n    #[test]\n    fn test_chat_message_system() {\n        let msg = ChatMessage::system(\"You are a helpful assistant\");\n        assert_eq!(msg.role, ChatRole::System);\n        assert_eq!(msg.content, \"You are a helpful assistant\");\n        assert!(msg.name.is_none());\n    }\n\n    #[test]\n    fn test_chat_message_user() {\n        let msg = ChatMessage::user(\"Hello, how are you?\");\n        assert_eq!(msg.role, ChatRole::User);\n        assert_eq!(msg.content, \"Hello, how are you?\");\n    }\n\n    #[test]\n    fn test_chat_message_assistant() {\n        let msg = ChatMessage::assistant(\"I'm doing well, thank you!\");\n        assert_eq!(msg.role, ChatRole::Assistant);\n        assert_eq!(msg.content, \"I'm doing well, thank you!\");\n    }\n}\n\n// ============================================================================\n// Rate Limiter Tests\n// ============================================================================\n\nmod rate_limiter_tests {\n    use super::*;\n\n    #[test]\n    fn test_rate_limiter_config_default() {\n        let config = RateLimiterConfig::default();\n        assert!(config.requests_per_minute > 0);\n        assert!(config.tokens_per_minute > 0);\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider() {\n        let mock = MockProvider::new();\n        mock.add_response(\"Rate limited response\").await;\n\n        let config = RateLimiterConfig {\n            requests_per_minute: 60,\n            tokens_per_minute: 100000,\n            ..Default::default()\n        };\n\n        let rate_limited = RateLimitedProvider::new(mock, config);\n\n        let response = rate_limited.complete(\"test\").await.unwrap();\n        assert_eq!(response.content, \"Rate limited response\");\n    }\n\n    #[tokio::test]\n    async fn test_rate_limited_provider_info() {\n        let mock = MockProvider::new();\n        let config = RateLimiterConfig::default();\n        let rate_limited = RateLimitedProvider::new(mock, config);\n\n        assert_eq!(LLMProvider::name(&rate_limited), \"mock\");\n        assert_eq!(LLMProvider::model(&rate_limited), \"mock-model\");\n    }\n}\n\n// ============================================================================\n// Cache Tests\n// ============================================================================\n\nmod cache_tests {\n    use super::*;\n\n    #[test]\n    fn test_cache_config_default() {\n        let config = CacheConfig::default();\n        assert!(config.max_entries > 0);\n        assert!(config.ttl.as_secs() > 0);\n    }\n\n    #[tokio::test]\n    async fn test_cached_provider() {\n        let mock = MockProvider::new();\n        mock.add_response(\"Cached response 1\").await;\n        mock.add_response(\"Cached response 2\").await;\n\n        let config = CacheConfig {\n            max_entries: 100,\n            ttl: Duration::from_secs(300),\n            ..Default::default()\n        };\n\n        let cache = Arc::new(LLMCache::new(config));\n        let cached = CachedProvider::new(mock, cache.clone());\n\n        // First call - cache miss\n        let r1 = cached.complete(\"test prompt\").await.unwrap();\n        assert_eq!(r1.content, \"Cached response 1\");\n\n        // Second call with same prompt - cache hit\n        let r2 = cached.complete(\"test prompt\").await.unwrap();\n        assert_eq!(r2.content, \"Cached response 1\"); // Same as first\n\n        // Different prompt - cache miss\n        let r3 = cached.complete(\"different prompt\").await.unwrap();\n        assert_eq!(r3.content, \"Cached response 2\");\n    }\n\n    #[tokio::test]\n    async fn test_cache_stats() {\n        let config = CacheConfig {\n            max_entries: 10,\n            ttl: Duration::from_secs(60),\n            ..Default::default()\n        };\n\n        let cache = Arc::new(LLMCache::new(config));\n        let stats = cache.stats().await;\n\n        assert_eq!(stats.hits, 0);\n        assert_eq!(stats.misses, 0);\n        assert_eq!(stats.entries, 0);\n    }\n}\n\n// ============================================================================\n// Tokenizer Tests\n// ============================================================================\n\nmod tokenizer_tests {\n    use super::*;\n\n    #[test]\n    fn test_tokenizer_count_tokens() {\n        let tokenizer = Tokenizer::default();\n        let text = \"Hello, world! This is a test.\";\n        let count = tokenizer.count_tokens(text);\n        assert!(count > 0);\n    }\n\n    #[test]\n    fn test_tokenizer_empty_text() {\n        let tokenizer = Tokenizer::default();\n        let count = tokenizer.count_tokens(\"\");\n        assert_eq!(count, 0);\n    }\n\n    #[test]\n    fn test_tokenizer_long_text() {\n        let tokenizer = Tokenizer::default();\n        let text = \"word \".repeat(1000);\n        let count = tokenizer.count_tokens(&text);\n        assert!(count > 500);\n    }\n}\n\n// ============================================================================\n// Reranker Tests\n// ============================================================================\n\nmod reranker_tests {\n    use edgequake_llm::{MockReranker, RerankConfig, Reranker};\n\n    #[test]\n    fn test_rerank_config_default() {\n        let config = RerankConfig::default();\n        assert!(!config.model.is_empty());\n        assert!(!config.base_url.is_empty());\n    }\n\n    #[test]\n    fn test_rerank_config_jina() {\n        let config = RerankConfig::jina(\"api-key\");\n        assert_eq!(config.api_key, Some(\"api-key\".to_string()));\n        assert!(config.base_url.contains(\"jina\"));\n    }\n\n    #[test]\n    fn test_rerank_config_cohere() {\n        let config = RerankConfig::cohere(\"api-key\");\n        assert!(config.base_url.contains(\"cohere\"));\n    }\n\n    #[tokio::test]\n    async fn test_mock_reranker() {\n        let reranker = MockReranker::new();\n\n        let query = \"What is machine learning?\";\n        let documents = vec![\n            \"Machine learning is a subset of AI\".to_string(),\n            \"Deep learning uses neural networks\".to_string(),\n            \"Data science involves statistics\".to_string(),\n        ];\n\n        let results = reranker.rerank(query, &documents, Some(3)).await.unwrap();\n\n        assert!(!results.is_empty());\n        assert!(results.len() <= 3);\n\n        // Results should have relevance scores\n        for result in &results {\n            assert!(result.relevance_score >= 0.0);\n            assert!(result.index < documents.len());\n        }\n    }\n\n    #[tokio::test]\n    async fn test_mock_reranker_empty_documents() {\n        let reranker = MockReranker::new();\n        let results = reranker.rerank(\"query\", &[], Some(5)).await.unwrap();\n        assert!(results.is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_mock_reranker_top_n() {\n        let reranker = MockReranker::new();\n\n        let documents: Vec<String> = (0..10).map(|i| format!(\"Document {}\", i)).collect();\n\n        let results = reranker.rerank(\"query\", &documents, Some(3)).await.unwrap();\n        assert_eq!(results.len(), 3);\n    }\n}\n\n// ============================================================================\n// Real Provider Tests (Environment-gated)\n// ============================================================================\n\n#[cfg(test)]\nmod real_provider_tests {\n    use super::*;\n    use std::env;\n\n    fn get_openai_key() -> Option<String> {\n        env::var(\"OPENAI_API_KEY\")\n            .ok()\n            .filter(|k| !k.is_empty() && k != \"test-key\")\n    }\n\n    #[tokio::test]\n    #[ignore = \"Requires OPENAI_API_KEY\"]\n    async fn test_openai_provider_completion() {\n        use edgequake_llm::OpenAIProvider;\n\n        let api_key = match get_openai_key() {\n            Some(k) => k,\n            None => {\n                eprintln!(\"Skipping: OPENAI_API_KEY not set\");\n                return;\n            }\n        };\n\n        let provider = OpenAIProvider::new(api_key).with_model(\"gpt-4o-mini\");\n\n        let response = provider.complete(\"Say 'hello' in one word\").await.unwrap();\n        assert!(!response.content.is_empty());\n        assert!(response.content.to_lowercase().contains(\"hello\"));\n    }\n\n    #[tokio::test]\n    #[ignore = \"Requires OPENAI_API_KEY\"]\n    async fn test_openai_provider_embedding() {\n        use edgequake_llm::OpenAIProvider;\n\n        let api_key = match get_openai_key() {\n            Some(k) => k,\n            None => return,\n        };\n\n        let provider = OpenAIProvider::new(api_key).with_embedding_model(\"text-embedding-3-small\");\n\n        let embedding = provider.embed_one(\"Hello, world!\").await.unwrap();\n        assert_eq!(embedding.len(), 1536);\n    }\n\n    #[tokio::test]\n    #[ignore = \"Requires OPENAI_API_KEY\"]\n    async fn test_openai_provider_chat() {\n        use edgequake_llm::OpenAIProvider;\n\n        let api_key = match get_openai_key() {\n            Some(k) => k,\n            None => return,\n        };\n\n        let provider = OpenAIProvider::new(api_key).with_model(\"gpt-4o-mini\");\n\n        let messages = vec![\n            ChatMessage::system(\"You are a helpful assistant. Answer briefly.\"),\n            ChatMessage::user(\"What is 2+2?\"),\n        ];\n\n        let response = provider.chat(&messages, None).await.unwrap();\n        assert!(response.content.contains(\"4\"));\n    }\n}\n\n// ============================================================================\n// Provider Factory Tests\n// ============================================================================\n\nmod provider_factory_tests {\n    use super::*;\n    use std::env;\n\n    #[tokio::test]\n    async fn test_provider_factory_fallback() {\n        // When no API key is set, should fall back to mock\n        env::remove_var(\"OPENAI_API_KEY\");\n\n        let provider = MockProvider::new();\n        let response = provider.complete(\"test\").await.unwrap();\n        assert!(!response.content.is_empty());\n    }\n\n    #[tokio::test]\n    async fn test_arc_provider_compatibility() {\n        // Test that providers work correctly through Arc\n        let mock: Arc<dyn LLMProvider> = Arc::new(MockProvider::new());\n        mock.complete(\"test\").await.unwrap();\n\n        let embedding: Arc<dyn EmbeddingProvider> = Arc::new(MockProvider::new());\n        embedding.embed_one(\"test\").await.unwrap();\n    }\n}\n\n// ============================================================================\n// Concurrent Provider Tests\n// ============================================================================\n\nmod concurrent_tests {\n    use super::*;\n    use tokio::task::JoinSet;\n\n    #[tokio::test]\n    async fn test_concurrent_completions() {\n        let provider = Arc::new(MockProvider::new());\n\n        // Pre-populate responses\n        for i in 0..10 {\n            provider.add_response(format!(\"Response {}\", i)).await;\n        }\n\n        let mut tasks = JoinSet::new();\n\n        for i in 0..10 {\n            let p = provider.clone();\n            tasks.spawn(async move { p.complete(&format!(\"Prompt {}\", i)).await.unwrap() });\n        }\n\n        let mut responses = Vec::new();\n        while let Some(result) = tasks.join_next().await {\n            responses.push(result.expect(\"Task panicked\"));\n        }\n\n        assert_eq!(responses.len(), 10);\n    }\n\n    #[tokio::test]\n    async fn test_concurrent_embeddings() {\n        let provider = Arc::new(MockProvider::new());\n\n        let mut tasks = JoinSet::new();\n\n        for i in 0..5 {\n            let p = provider.clone();\n            tasks.spawn(async move { p.embed_one(&format!(\"Text {}\", i)).await.unwrap() });\n        }\n\n        let mut embeddings = Vec::new();\n        while let Some(result) = tasks.join_next().await {\n            embeddings.push(result.expect(\"Task panicked\"));\n        }\n\n        assert_eq!(embeddings.len(), 5);\n        for emb in embeddings {\n            assert_eq!(emb.len(), 1536);\n        }\n    }\n}\n\n// ============================================================================\n// Error Handling Tests\n// ============================================================================\n\nmod error_handling_tests {\n    use super::*;\n\n    #[tokio::test]\n    async fn test_empty_text_embedding() {\n        let provider = MockProvider::new();\n        // Empty text should still work\n        let result = provider.embed_one(\"\").await;\n        assert!(result.is_ok());\n    }\n\n    #[tokio::test]\n    async fn test_batch_embed_empty() {\n        let provider = MockProvider::new();\n        let result = provider.embed(&[]).await;\n        assert!(result.is_ok());\n        assert!(result.unwrap().is_empty());\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","e2e_openai_compatible.rs"],"content":"//! E2E tests for OpenAI-compatible provider (Z.ai)\n//!\n//! These tests require:\n//! - ZAI_API_KEY environment variable set\n//!\n//! Run with: cargo test -p edgequake-llm --test e2e_openai_compatible -- --ignored\n\nuse edgequake_llm::model_config::{\n    ModelCapabilities, ModelCard, ModelType, ProviderConfig, ProviderType,\n};\nuse edgequake_llm::providers::openai_compatible::OpenAICompatibleProvider;\nuse edgequake_llm::traits::{ChatMessage, LLMProvider};\n\nfn create_zai_config() -> ProviderConfig {\n    let mut headers = std::collections::HashMap::new();\n    headers.insert(\"Accept-Language\".to_string(), \"en-US,en\".to_string());\n\n    ProviderConfig {\n        name: \"zai\".to_string(),\n        display_name: \"Z.AI Platform\".to_string(),\n        provider_type: ProviderType::OpenAICompatible,\n        api_key_env: Some(\"ZAI_API_KEY\".to_string()),\n        base_url: Some(\"https://api.z.ai/api/paas/v4\".to_string()),\n        default_llm_model: Some(\"glm-4.7-flash\".to_string()),\n        headers,\n        supports_thinking: false,\n        models: vec![\n            ModelCard {\n                name: \"glm-4.7\".to_string(),\n                display_name: \"GLM-4.7 (Premium)\".to_string(),\n                model_type: ModelType::Llm,\n                capabilities: ModelCapabilities {\n                    context_length: 128000,\n                    max_output_tokens: 16384,\n                    supports_function_calling: true,\n                    supports_streaming: true,\n                    supports_json_mode: true,\n                    ..Default::default()\n                },\n                ..Default::default()\n            },\n            ModelCard {\n                name: \"glm-4.7-flash\".to_string(),\n                display_name: \"GLM-4.7 Flash\".to_string(),\n                model_type: ModelType::Llm,\n                capabilities: ModelCapabilities {\n                    context_length: 128000,\n                    max_output_tokens: 8192,\n                    supports_function_calling: true,\n                    supports_streaming: true,\n                    supports_json_mode: true,\n                    ..Default::default()\n                },\n                ..Default::default()\n            },\n        ],\n        ..Default::default()\n    }\n}\n\n#[tokio::test]\n#[ignore = \"Requires ZAI_API_KEY environment variable\"]\nasync fn test_zai_simple_chat() {\n    let config = create_zai_config();\n    let provider = OpenAICompatibleProvider::from_config(config)\n        .expect(\"Failed to create Z.ai provider\");\n\n    assert_eq!(LLMProvider::name(&provider), \"zai\");\n    assert_eq!(LLMProvider::model(&provider), \"glm-4.7-flash\");\n    assert_eq!(provider.max_context_length(), 128000);\n\n    // Simple chat test\n    let messages = vec![ChatMessage::user(\"What is 2 + 2? Reply with just the number.\")];\n\n    let response = provider.chat(&messages, None).await;\n    \n    match response {\n        Ok(resp) => {\n            println!(\"Response: {}\", resp.content);\n            println!(\"Tokens: {} prompt, {} completion\", resp.prompt_tokens, resp.completion_tokens);\n            assert!(!resp.content.is_empty());\n            assert!(resp.content.contains(\"4\") || resp.content.to_lowercase().contains(\"four\"));\n        }\n        Err(e) => {\n            panic!(\"Chat request failed: {}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore = \"Requires ZAI_API_KEY environment variable\"]\nasync fn test_zai_programming_riddle() {\n    let config = create_zai_config();\n    let provider = OpenAICompatibleProvider::from_config(config)\n        .expect(\"Failed to create Z.ai provider\");\n\n    // Programming riddle\n    let messages = vec![\n        ChatMessage::system(\"You are a helpful programming assistant. Answer concisely.\"),\n        ChatMessage::user(\n            \"Write a Rust function that reverses a string. Just the function, no explanation.\"\n        ),\n    ];\n\n    let response = provider.chat(&messages, None).await;\n    \n    match response {\n        Ok(resp) => {\n            println!(\"Response: {}\", resp.content);\n            assert!(!resp.content.is_empty());\n            // Should contain Rust code\n            assert!(\n                resp.content.contains(\"fn \") || resp.content.contains(\"pub fn\"),\n                \"Response should contain a Rust function\"\n            );\n            assert!(\n                resp.content.contains(\"reverse\") || resp.content.contains(\"chars\"),\n                \"Response should contain string reversal logic\"\n            );\n        }\n        Err(e) => {\n            panic!(\"Programming riddle failed: {}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore = \"Requires ZAI_API_KEY environment variable\"]\nasync fn test_zai_with_premium_model() {\n    let config = create_zai_config();\n    let provider = OpenAICompatibleProvider::from_config(config)\n        .expect(\"Failed to create Z.ai provider\")\n        .with_model(\"glm-4.7\");\n\n    assert_eq!(LLMProvider::model(&provider), \"glm-4.7\");\n    assert_eq!(provider.max_context_length(), 128000);\n\n    let messages = vec![ChatMessage::user(\"Hello! What's your name?\")];\n\n    let response = provider.chat(&messages, None).await;\n    \n    match response {\n        Ok(resp) => {\n            println!(\"GLM-4.7 Response: {}\", resp.content);\n            assert!(!resp.content.is_empty());\n        }\n        Err(e) => {\n            panic!(\"GLM-4.7 request failed: {}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore = \"Requires ZAI_API_KEY environment variable\"]\nasync fn test_zai_function_calling() {\n    use edgequake_llm::traits::{ToolDefinition, ToolChoice};\n\n    let config = create_zai_config();\n    let provider = OpenAICompatibleProvider::from_config(config)\n        .expect(\"Failed to create Z.ai provider\");\n\n    // Define a simple tool\n    let tools = vec![ToolDefinition::function(\n        \"get_weather\",\n        \"Get the current weather in a given location\",\n        serde_json::json!({\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g., San Francisco, CA\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"]\n                }\n            },\n            \"required\": [\"location\"]\n        }),\n    )];\n\n    let messages = vec![ChatMessage::user(\n        \"What's the weather like in Paris, France?\",\n    )];\n\n    let response = provider\n        .chat_with_tools(&messages, &tools, Some(ToolChoice::auto()), None)\n        .await;\n\n    match response {\n        Ok(resp) => {\n            println!(\"Function calling response: {}\", resp.content);\n            println!(\"Tool calls: {:?}\", resp.tool_calls);\n            // Either we get a response or a tool call\n            assert!(!resp.content.is_empty() || !resp.tool_calls.is_empty());\n        }\n        Err(e) => {\n            panic!(\"Function calling failed: {}\", e);\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","e2e_provider_factory.rs"],"content":"//! End-to-end tests for ProviderFactory environment-based selection.\n//!\n//! These tests verify that ProviderFactory correctly auto-detects and creates\n//! providers based on environment variables. Tests must run serially due to\n//! shared environment state.\n//!\n//! @implements SPEC-032: Ollama/LM Studio provider support - E2E validation\n//! @iteration OODA Loop #3 - Phase 5A\n\nuse edgequake_llm::{ProviderFactory, ProviderType};\nuse serial_test::serial;\n\n/// Test that Ollama is auto-detected when OLLAMA_HOST is set.\n#[tokio::test]\n#[serial]\nasync fn test_provider_auto_detection_ollama() {\n    // Clean environment to avoid interference\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::remove_var(\"OPENAI_API_KEY\");\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n\n    // Set Ollama host (auto-detection should pick this up)\n    std::env::set_var(\"OLLAMA_HOST\", \"http://localhost:11434\");\n\n    // Create providers via auto-detection\n    let (llm, embedding) =\n        ProviderFactory::from_env().expect(\"Failed to create providers from environment\");\n\n    // Verify Ollama was selected\n    assert_eq!(\n        llm.name(),\n        \"ollama\",\n        \"Expected Ollama provider, got {}\",\n        llm.name()\n    );\n    assert_eq!(\n        embedding.name(),\n        \"ollama\",\n        \"Expected Ollama embedding provider\"\n    );\n\n    // Verify embeddinggemma dimension (768)\n    assert_eq!(\n        embedding.dimension(),\n        768,\n        \"embeddinggemma:latest should have 768 dimensions\"\n    );\n\n    // Cleanup\n    std::env::remove_var(\"OLLAMA_HOST\");\n}\n\n/// Test that OpenAI is auto-detected when OPENAI_API_KEY is set.\n#[tokio::test]\n#[serial]\nasync fn test_provider_auto_detection_openai() {\n    // Clean environment\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n    std::env::remove_var(\"LMSTUDIO_MODEL\");\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"GEMINI_API_KEY\");\n    std::env::remove_var(\"OPENROUTER_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n    std::env::remove_var(\"HF_TOKEN\"); // OODA-41: Clean HuggingFace tokens\n    std::env::remove_var(\"HUGGINGFACE_TOKEN\");\n\n    // Set OpenAI API key\n    std::env::set_var(\"OPENAI_API_KEY\", \"sk-test-key-for-testing\");\n\n    // Create providers\n    let (llm, embedding) = ProviderFactory::from_env().expect(\"Failed to create providers\");\n\n    // Verify OpenAI was selected\n    assert_eq!(llm.name(), \"openai\", \"Expected OpenAI provider\");\n    assert_eq!(embedding.name(), \"openai\", \"Expected OpenAI embedding\");\n\n    // Verify text-embedding-3-small dimension (1536)\n    assert_eq!(\n        embedding.dimension(),\n        1536,\n        \"text-embedding-3-small should have 1536 dimensions\"\n    );\n\n    // Cleanup\n    std::env::remove_var(\"OPENAI_API_KEY\");\n}\n\n/// Test that Mock provider is used when no provider env vars are set.\n#[tokio::test]\n#[serial]\nasync fn test_provider_auto_detection_mock_fallback() {\n    // Clear all provider environment variables\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"OLLAMA_MODEL\");\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n    std::env::remove_var(\"LMSTUDIO_MODEL\");\n    std::env::remove_var(\"OPENAI_API_KEY\");\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"GEMINI_API_KEY\");\n    std::env::remove_var(\"OPENROUTER_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n    std::env::remove_var(\"HF_TOKEN\"); // OODA-41: Clean HuggingFace tokens\n    std::env::remove_var(\"HUGGINGFACE_TOKEN\");\n\n    // Create providers (should fallback to Mock)\n    let (llm, embedding) = ProviderFactory::from_env().expect(\"Failed to create mock providers\");\n\n    // Verify Mock was selected\n    assert_eq!(llm.name(), \"mock\", \"Expected Mock provider fallback\");\n    assert_eq!(embedding.name(), \"mock\", \"Expected Mock embedding\");\n\n    // Verify Mock dimension (1536, compatible with OpenAI)\n    assert_eq!(\n        embedding.dimension(),\n        1536,\n        \"Mock provider should have 1536 dimensions (OpenAI-compatible)\"\n    );\n}\n\n/// Test that explicit EDGEQUAKE_LLM_PROVIDER overrides auto-detection.\n#[tokio::test]\n#[serial]\nasync fn test_explicit_provider_override() {\n    // Clean environment first\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"GEMINI_API_KEY\");\n    std::env::remove_var(\"OPENROUTER_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n    std::env::remove_var(\"HF_TOKEN\"); // OODA-41: Clean HuggingFace tokens\n    std::env::remove_var(\"HUGGINGFACE_TOKEN\");\n    \n    // Set multiple provider env vars (conflicting signals)\n    std::env::set_var(\"OLLAMA_HOST\", \"http://localhost:11434\");\n    std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n    std::env::set_var(\"OPENAI_API_KEY\", \"sk-test\");\n\n    // Explicit override should win\n    std::env::set_var(\"EDGEQUAKE_LLM_PROVIDER\", \"mock\");\n\n    let (llm, embedding) =\n        ProviderFactory::from_env().expect(\"Failed to create providers with explicit override\");\n\n    // Verify Mock was selected (override worked)\n    assert_eq!(\n        llm.name(),\n        \"mock\",\n        \"Expected Mock provider from explicit override\"\n    );\n    assert_eq!(embedding.dimension(), 1536);\n\n    // Cleanup\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n    std::env::remove_var(\"OPENAI_API_KEY\");\n}\n\n/// Test auto-detection priority chain: explicit > Ollama > LM Studio > OpenAI > Mock.\n#[tokio::test]\n#[serial]\nasync fn test_provider_priority_chain() {\n    // Clean all API key env vars first\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"GEMINI_API_KEY\");\n    std::env::remove_var(\"OPENROUTER_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n    std::env::remove_var(\"HF_TOKEN\"); // OODA-41: Clean HuggingFace tokens\n    std::env::remove_var(\"HUGGINGFACE_TOKEN\");\n    \n    // Test 1: Ollama has priority over LM Studio and OpenAI\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::set_var(\"OLLAMA_HOST\", \"http://localhost:11434\");\n    std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n    std::env::set_var(\"OPENAI_API_KEY\", \"sk-test\");\n\n    let (llm, _) = ProviderFactory::from_env().unwrap();\n    assert_eq!(\n        llm.name(),\n        \"ollama\",\n        \"Ollama should have priority over LM Studio and OpenAI\"\n    );\n\n    // Test 2: LM Studio selected when Ollama not present\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n    let (llm, _) = ProviderFactory::from_env().unwrap();\n    assert_eq!(\n        llm.name(),\n        \"lmstudio\",\n        \"LM Studio should be selected when Ollama not present\"\n    );\n\n    // Test 3: OpenAI selected when neither Ollama nor LM Studio present\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n    let (llm, _) = ProviderFactory::from_env().unwrap();\n    assert_eq!(llm.name(), \"openai\", \"OpenAI should be selected\");\n\n    // Test 4: Mock fallback when none present\n    std::env::remove_var(\"OPENAI_API_KEY\");\n    let (llm, _) = ProviderFactory::from_env().unwrap();\n    assert_eq!(llm.name(), \"mock\", \"Mock should be fallback\");\n\n    // Cleanup\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n    std::env::remove_var(\"OPENAI_API_KEY\");\n}\n\n/// Test that ProviderType::create() works for all provider types.\n#[tokio::test]\n#[serial]\nasync fn test_explicit_provider_creation() {\n    // Mock doesn't require env vars\n    let (llm, embedding) =\n        ProviderFactory::create(ProviderType::Mock).expect(\"Failed to create Mock provider\");\n    assert_eq!(llm.name(), \"mock\");\n    assert_eq!(embedding.dimension(), 1536);\n\n    // OpenAI requires API key\n    std::env::remove_var(\"OPENAI_API_KEY\");\n    let result = ProviderFactory::create(ProviderType::OpenAI);\n    assert!(\n        result.is_err(),\n        \"OpenAI creation should fail without API key\"\n    );\n\n    // Ollama can be created (will use defaults)\n    let result = ProviderFactory::create(ProviderType::Ollama);\n    assert!(\n        result.is_ok(),\n        \"Ollama creation should succeed with defaults\"\n    );\n\n    // LM Studio can be created (will use defaults)\n    let result = ProviderFactory::create(ProviderType::LMStudio);\n    assert!(\n        result.is_ok(),\n        \"LM Studio creation should succeed with defaults\"\n    );\n    let (llm, embedding) = result.unwrap();\n    assert_eq!(llm.name(), \"lmstudio\");\n    assert_eq!(embedding.dimension(), 768); // nomic-embed-text-v1.5\n\n    // VsCodeCopilot can be created (will use defaults)\n    let result = ProviderFactory::create(ProviderType::VsCodeCopilot);\n    assert!(\n        result.is_ok(),\n        \"VsCodeCopilot creation should succeed with defaults\"\n    );\n    let (llm, embedding) = result.unwrap();\n    assert_eq!(llm.name(), \"vscode-copilot\");\n    assert_eq!(embedding.name(), \"vscode-copilot\"); // Same provider for both\n    assert_eq!(embedding.dimension(), 1536); // text-embedding-3-small\n}\n\n/// Test embedding dimension detection via ProviderFactory helper.\n#[tokio::test]\n#[serial]\nasync fn test_embedding_dimension_detection() {\n    // Test Mock dimension\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n    std::env::remove_var(\"OPENAI_API_KEY\");\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"GEMINI_API_KEY\");\n    std::env::remove_var(\"OPENROUTER_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n\n    let dim = ProviderFactory::embedding_dimension().expect(\"Failed to detect dimension\");\n    assert_eq!(dim, 1536, \"Mock provider dimension\");\n\n    // Test Ollama dimension (if available)\n    std::env::set_var(\"OLLAMA_HOST\", \"http://localhost:11434\");\n    let dim = ProviderFactory::embedding_dimension().expect(\"Failed to detect Ollama dimension\");\n    assert_eq!(dim, 768, \"Ollama provider dimension\");\n\n    // Test LM Studio dimension\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n    let dim = ProviderFactory::embedding_dimension().expect(\"Failed to detect LM Studio dimension\");\n    assert_eq!(dim, 768, \"LM Studio provider dimension\");\n\n    // Cleanup\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n}\n\n/// Test LM Studio auto-detection when LMSTUDIO_HOST is set.\n#[tokio::test]\n#[serial]\nasync fn test_provider_auto_detection_lmstudio() {\n    // Clean environment to avoid interference\n    std::env::remove_var(\"EDGEQUAKE_LLM_PROVIDER\");\n    std::env::remove_var(\"OPENAI_API_KEY\");\n    std::env::remove_var(\"OLLAMA_HOST\");\n    std::env::remove_var(\"OLLAMA_MODEL\");\n    std::env::remove_var(\"XAI_API_KEY\");\n    std::env::remove_var(\"GOOGLE_API_KEY\");\n    std::env::remove_var(\"GEMINI_API_KEY\");\n    std::env::remove_var(\"OPENROUTER_API_KEY\");\n    std::env::remove_var(\"ANTHROPIC_API_KEY\");\n    std::env::remove_var(\"AZURE_OPENAI_API_KEY\");\n\n    // Set LM Studio host (auto-detection should pick this up)\n    std::env::set_var(\"LMSTUDIO_HOST\", \"http://localhost:1234\");\n\n    // Create providers via auto-detection\n    let (llm, embedding) =\n        ProviderFactory::from_env().expect(\"Failed to create providers from environment\");\n\n    // Verify LM Studio was selected\n    assert_eq!(\n        llm.name(),\n        \"lmstudio\",\n        \"Expected LM Studio provider, got {}\",\n        llm.name()\n    );\n    assert_eq!(\n        embedding.name(),\n        \"lmstudio\",\n        \"Expected LM Studio embedding provider\"\n    );\n\n    // Verify nomic-embed-text-v1.5 dimension (768)\n    assert_eq!(\n        embedding.dimension(),\n        768,\n        \"nomic-embed-text-v1.5 should have 768 dimensions\"\n    );\n\n    // Cleanup\n    std::env::remove_var(\"LMSTUDIO_HOST\");\n}\n\n/// Test that VsCodeCopilot embedding provider can be created.\n#[tokio::test]\nasync fn test_vscode_copilot_embedding_provider() {\n    // Test create_embedding_provider for VsCodeCopilot\n    let provider = ProviderFactory::create_embedding_provider(\n        \"vscode-copilot\",\n        \"text-embedding-3-small\",\n        1536,\n    )\n    .expect(\"Failed to create VsCodeCopilot embedding provider\");\n\n    assert_eq!(provider.name(), \"vscode-copilot\");\n    assert_eq!(provider.model(), \"text-embedding-3-small\");\n    assert_eq!(provider.dimension(), 1536);\n\n    // Test with large model\n    let provider = ProviderFactory::create_embedding_provider(\n        \"copilot\", // Alternative name\n        \"text-embedding-3-large\",\n        3072,\n    )\n    .expect(\"Failed to create VsCodeCopilot embedding provider with large model\");\n\n    assert_eq!(provider.name(), \"vscode-copilot\");\n    assert_eq!(provider.model(), \"text-embedding-3-large\");\n    assert_eq!(provider.dimension(), 3072);\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","e2e_xai.rs"],"content":"//! End-to-end tests for xAI Grok provider.\n//!\n//! @implements OODA-71: xAI Grok API Integration\n//!\n//! These tests require a valid XAI_API_KEY environment variable.\n//!\n//! # Running the tests\n//!\n//! ```bash\n//! # Set your xAI API key\n//! export XAI_API_KEY=xai-your-api-key\n//!\n//! # Run all xAI tests\n//! cargo test -p edgequake-llm --test e2e_xai\n//!\n//! # Run a specific test\n//! cargo test -p edgequake-llm --test e2e_xai test_xai_basic_chat\n//! ```\n//!\n//! # Test coverage\n//!\n//! - Basic chat completion\n//! - JSON mode (structured output)\n//! - Streaming\n//! - Function/tool calling\n//!\n//! Note: Vision tests require grok-2-vision-1212 model.\n\nuse edgequake_llm::traits::{ChatMessage, LLMProvider, ToolChoice, ToolDefinition};\nuse edgequake_llm::XAIProvider;\n\n/// Check if XAI_API_KEY is set for tests\nfn has_xai_key() -> bool {\n    std::env::var(\"XAI_API_KEY\").is_ok()\n}\n\n/// Create xAI provider for testing\nfn create_provider() -> XAIProvider {\n    XAIProvider::from_env().expect(\"XAI_API_KEY must be set\")\n}\n\n/// Create provider with a specific model\nfn create_provider_with_model(model: &str) -> XAIProvider {\n    XAIProvider::from_env()\n        .expect(\"XAI_API_KEY must be set\")\n        .with_model(model)\n}\n\n// ============================================================================\n// Basic Chat Tests\n// ============================================================================\n\n#[tokio::test]\nasync fn test_xai_basic_chat() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    let provider = create_provider();\n\n    // Simple math question for deterministic response\n    let messages = vec![\n        ChatMessage::system(\"You are a helpful math tutor. Be very concise.\"),\n        ChatMessage::user(\"What is 2 + 2? Answer with just the number.\"),\n    ];\n\n    let response = provider.chat(&messages, None).await;\n\n    match response {\n        Ok(resp) => {\n            println!(\"Response: {}\", resp.content);\n            println!(\"Model: {}\", resp.model);\n            println!(\"Tokens: {} in, {} out\", resp.prompt_tokens, resp.completion_tokens);\n\n            // Should contain \"4\" somewhere in the response\n            assert!(\n                resp.content.contains(\"4\"),\n                \"Expected '4' in response: {}\",\n                resp.content\n            );\n        }\n        Err(e) => {\n            panic!(\"Chat failed: {:?}\", e);\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_xai_simple_complete() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    let provider = create_provider();\n\n    let response = provider.complete(\"Say 'hello world' and nothing else.\").await;\n\n    match response {\n        Ok(resp) => {\n            println!(\"Response: {}\", resp.content);\n            assert!(\n                resp.content.to_lowercase().contains(\"hello\"),\n                \"Expected 'hello' in response: {}\",\n                resp.content\n            );\n        }\n        Err(e) => {\n            panic!(\"Complete failed: {:?}\", e);\n        }\n    }\n}\n\n// ============================================================================\n// JSON Mode Tests\n// ============================================================================\n\n#[tokio::test]\nasync fn test_xai_json_mode() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    let provider = create_provider();\n\n    let messages = vec![\n        ChatMessage::system(\n            \"You are a JSON generator. Always respond with valid JSON only, no markdown.\",\n        ),\n        ChatMessage::user(\n            \"Generate a JSON object with fields: name (string), age (number), active (boolean). Use sample values.\",\n        ),\n    ];\n\n    let options = edgequake_llm::traits::CompletionOptions::json_mode();\n\n    let response = provider.chat(&messages, Some(&options)).await;\n\n    match response {\n        Ok(resp) => {\n            println!(\"Response: {}\", resp.content);\n\n            // Try to parse as JSON\n            let json_result: Result<serde_json::Value, _> =\n                serde_json::from_str(resp.content.trim());\n\n            match json_result {\n                Ok(json) => {\n                    println!(\"Valid JSON: {}\", json);\n                    // Verify expected fields exist\n                    assert!(json.get(\"name\").is_some(), \"Missing 'name' field\");\n                    assert!(json.get(\"age\").is_some(), \"Missing 'age' field\");\n                    assert!(json.get(\"active\").is_some(), \"Missing 'active' field\");\n                }\n                Err(e) => {\n                    panic!(\"Invalid JSON response: {} - Error: {}\", resp.content, e);\n                }\n            }\n        }\n        Err(e) => {\n            panic!(\"JSON mode chat failed: {:?}\", e);\n        }\n    }\n}\n\n// ============================================================================\n// Streaming Tests\n// ============================================================================\n\n#[tokio::test]\nasync fn test_xai_streaming() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    use futures::StreamExt;\n\n    let provider = create_provider();\n\n    let result = provider.stream(\"Count from 1 to 5, separated by commas.\").await;\n\n    match result {\n        Ok(mut stream) => {\n            let mut full_response = String::new();\n            let mut chunk_count = 0;\n\n            while let Some(chunk_result) = stream.next().await {\n                match chunk_result {\n                    Ok(chunk) => {\n                        full_response.push_str(&chunk);\n                        chunk_count += 1;\n                    }\n                    Err(e) => {\n                        panic!(\"Stream chunk error: {:?}\", e);\n                    }\n                }\n            }\n\n            println!(\"Full response: {}\", full_response);\n            println!(\"Chunk count: {}\", chunk_count);\n\n            // Verify we got multiple chunks and expected content\n            assert!(chunk_count > 0, \"Expected at least one chunk\");\n            assert!(\n                !full_response.is_empty(),\n                \"Expected non-empty response\"\n            );\n        }\n        Err(e) => {\n            panic!(\"Stream failed: {:?}\", e);\n        }\n    }\n}\n\n// ============================================================================\n// Tool/Function Calling Tests\n// ============================================================================\n\n#[tokio::test]\nasync fn test_xai_tool_calling() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    let provider = create_provider();\n\n    // Define a simple tool\n    let tools = vec![ToolDefinition::function(\n        \"get_weather\",\n        \"Get the current weather for a location\",\n        serde_json::json!({\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                }\n            },\n            \"required\": [\"location\"]\n        }),\n    )];\n\n    let messages = vec![ChatMessage::user(\n        \"What's the weather like in Tokyo?\",\n    )];\n\n    let response = provider\n        .chat_with_tools(&messages, &tools, Some(ToolChoice::auto()), None)\n        .await;\n\n    match response {\n        Ok(resp) => {\n            println!(\"Response content: {}\", resp.content);\n            println!(\"Tool calls: {:?}\", resp.tool_calls);\n\n            // Model should either call the tool or provide a response\n            // (xAI may or may not call the tool depending on its judgment)\n            if !resp.tool_calls.is_empty() {\n                println!(\"Tool call: {:?}\", resp.tool_calls[0]);\n            } else {\n                // Model chose to respond directly\n                println!(\"Model responded directly without tool call\");\n            }\n        }\n        Err(e) => {\n            panic!(\"Tool calling failed: {:?}\", e);\n        }\n    }\n}\n\n// ============================================================================\n// Model Selection Tests\n// ============================================================================\n\n#[tokio::test]\nasync fn test_xai_grok3_mini() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    // Test with grok-3-mini (faster, cheaper)\n    let provider = create_provider_with_model(\"grok-3-mini\");\n\n    assert_eq!(provider.model(), \"grok-3-mini\");\n\n    let response = provider.complete(\"What is 5 * 5? Just the number.\").await;\n\n    match response {\n        Ok(resp) => {\n            println!(\"Response from grok-3-mini: {}\", resp.content);\n            assert!(\n                resp.content.contains(\"25\"),\n                \"Expected '25' in response: {}\",\n                resp.content\n            );\n        }\n        Err(e) => {\n            // grok-3-mini may not be available, skip gracefully\n            eprintln!(\"grok-3-mini test skipped: {:?}\", e);\n        }\n    }\n}\n\n// ============================================================================\n// Provider Info Tests\n// ============================================================================\n\n#[test]\nfn test_xai_provider_info() {\n    if !has_xai_key() {\n        eprintln!(\"Skipping test: XAI_API_KEY not set\");\n        return;\n    }\n\n    let provider = create_provider();\n\n    assert_eq!(provider.name(), \"xai\");\n    assert_eq!(provider.model(), \"grok-4\");\n    assert_eq!(provider.max_context_length(), 262144); // OODA-15: 256K for grok-4\n}\n\n#[test]\nfn test_xai_context_lengths() {\n    // Test context length lookup (doesn't need API key)\n    // OODA-15: Updated from docs.x.ai\n    assert_eq!(XAIProvider::context_length(\"grok-4\"), 262144); // 256K\n    assert_eq!(XAIProvider::context_length(\"grok-4-1-fast\"), 2000000); // 2M\n    assert_eq!(XAIProvider::context_length(\"grok-3-mini\"), 131072); // 128K\n    assert_eq!(XAIProvider::context_length(\"grok-2-vision-1212\"), 32768); // 32K\n    assert_eq!(XAIProvider::context_length(\"unknown-model\"), 262144); // Default 256K\n}\n\n#[test]\nfn test_xai_available_models() {\n    let models = XAIProvider::available_models();\n\n    assert!(!models.is_empty());\n\n    // Check for expected models\n    let model_names: Vec<&str> = models.iter().map(|(name, _, _)| *name).collect();\n    assert!(model_names.contains(&\"grok-4\"));\n    assert!(model_names.contains(&\"grok-4-1-fast\"));\n    assert!(model_names.contains(&\"grok-3\"));\n    assert!(model_names.contains(&\"grok-2-vision-1212\"));\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","test_ollama_anthropic.rs"],"content":"//! E2E test for AnthropicProvider with Ollama backend.\n//! \n//! Run with: cargo test --test test_ollama_anthropic -- --ignored --nocapture\n//! Requires: Ollama running at localhost:11434 with gpt-oss:20b model\n\nuse edgequake_llm::providers::anthropic::AnthropicProvider;\nuse edgequake_llm::traits::{ChatMessage, LLMProvider, ToolDefinition, FunctionDefinition};\nuse futures::StreamExt;\n\n#[tokio::test]\n#[ignore] // Requires Ollama to be running\nasync fn test_anthropic_provider_with_ollama() {\n    let provider = AnthropicProvider::new(\"ollama\")\n        .with_base_url(\"http://localhost:11434\")\n        .with_model(\"gpt-oss:20b\");\n    \n    println!(\"Provider: {}, Model: {}\", provider.name(), provider.model());\n    \n    let messages = vec![ChatMessage::user(\"Say hello in one word\")];\n    \n    println!(\"Sending request...\");\n    let result = provider.chat(&messages, None).await;\n    \n    match &result {\n        Ok(response) => {\n            println!(\"Success: {}\", response.content);\n            assert!(!response.content.is_empty());\n        }\n        Err(e) => {\n            println!(\"Error: {:?}\", e);\n            panic!(\"Request failed: {:?}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore] // Requires Ollama to be running\nasync fn test_anthropic_provider_with_tools() {\n    // Test with tools - this is what the React agent uses\n    let provider = AnthropicProvider::new(\"ollama\")\n        .with_base_url(\"http://localhost:11434\")\n        .with_model(\"gpt-oss:20b\");\n    \n    println!(\"Provider: {}, Model: {}\", provider.name(), provider.model());\n    \n    let messages = vec![ChatMessage::user(\"Create a file called test.txt\")];\n    \n    // Define a simple tool\n    let tools = vec![\n        ToolDefinition {\n            tool_type: \"function\".to_string(),\n            function: FunctionDefinition {\n                name: \"write_file\".to_string(),\n                description: \"Write content to a file\".to_string(),\n                parameters: serde_json::json!({\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": { \"type\": \"string\", \"description\": \"File path\" },\n                        \"content\": { \"type\": \"string\", \"description\": \"File content\" }\n                    },\n                    \"required\": [\"path\", \"content\"]\n                }),\n                strict: None,\n            },\n        }\n    ];\n    \n    println!(\"Sending request with tools...\");\n    let result = provider.chat_with_tools(&messages, &tools, None, None).await;\n    \n    match &result {\n        Ok(response) => {\n            println!(\"Success: content={}\", response.content);\n            println!(\"Tool calls: {:?}\", response.tool_calls);\n            // The model should call the write_file tool\n        }\n        Err(e) => {\n            println!(\"Error: {:?}\", e);\n            panic!(\"Request failed: {:?}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore] // Requires Ollama to be running\nasync fn test_anthropic_provider_with_tools_stream() {\n    // Test with streaming tools - this is exactly what the React agent uses\n    let provider = AnthropicProvider::new(\"ollama\")\n        .with_base_url(\"http://localhost:11434\")\n        .with_model(\"gpt-oss:20b\");\n    \n    println!(\"Provider: {}, Model: {}\", provider.name(), provider.model());\n    \n    let messages = vec![ChatMessage::user(\"Create a file called test.txt with 'hello world'\")];\n    \n    // Define a simple tool\n    let tools = vec![\n        ToolDefinition {\n            tool_type: \"function\".to_string(),\n            function: FunctionDefinition {\n                name: \"write_file\".to_string(),\n                description: \"Write content to a file\".to_string(),\n                parameters: serde_json::json!({\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"path\": { \"type\": \"string\", \"description\": \"File path\" },\n                        \"content\": { \"type\": \"string\", \"description\": \"File content\" }\n                    },\n                    \"required\": [\"path\", \"content\"]\n                }),\n                strict: None,\n            },\n        }\n    ];\n    \n    println!(\"Sending streaming request with tools (same as React agent)...\");\n    let result = provider.chat_with_tools_stream(&messages, &tools, None, None).await;\n    \n    match result {\n        Ok(mut stream) => {\n            println!(\"Stream started successfully!\");\n            let mut chunk_count = 0;\n            while let Some(chunk_result) = stream.next().await {\n                match chunk_result {\n                    Ok(chunk) => {\n                        chunk_count += 1;\n                        println!(\"  Chunk {}: {:?}\", chunk_count, chunk);\n                    }\n                    Err(e) => {\n                        println!(\"  Chunk error: {:?}\", e);\n                    }\n                }\n            }\n            println!(\"Stream completed with {} chunks\", chunk_count);\n        }\n        Err(e) => {\n            println!(\"Error: {:?}\", e);\n            panic!(\"Streaming request failed: {:?}\", e);\n        }\n    }\n}\n","traces":[],"covered":0,"coverable":0},{"path":["/","Users","raphaelmansuy","Github","03-working","edgequake-llm","tests","vscode_integration.rs"],"content":"//! Integration test for VSCode Copilot provider.\n//!\n//! This test requires copilot-api proxy to be running on localhost:4141\n\nuse edgequake_llm::{LLMProvider, VsCodeCopilotProvider};\n\n#[tokio::test]\n#[ignore] // Run with: cargo test --ignored test_vscode_health_check\nasync fn test_vscode_health_check() {\n    let provider = VsCodeCopilotProvider::new().build().unwrap();\n\n    // Test that we can create the provider\n    assert_eq!(provider.name(), \"vscode-copilot\");\n    assert_eq!(provider.model(), \"gpt-4o-mini\");\n    assert!(provider.supports_streaming());\n    assert!(provider.supports_json_mode());\n}\n\n#[tokio::test]\n#[ignore] // Run with: cargo test --ignored test_vscode_simple_completion\nasync fn test_vscode_simple_completion() {\n    let provider = VsCodeCopilotProvider::new().build().unwrap();\n\n    let response = provider.complete(\"What is 2 + 2?\").await;\n\n    match response {\n        Ok(res) => {\n            println!(\"Response: {}\", res.content);\n            println!(\"Model: {}\", res.model);\n            println!(\n                \"Tokens: prompt={}, completion={}\",\n                res.prompt_tokens, res.completion_tokens\n            );\n            assert!(!res.content.is_empty());\n            assert!(res.content.contains(\"4\") || res.content.contains(\"four\"));\n        }\n        Err(e) => {\n            panic!(\"Request failed: {}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore] // Run with: cargo test --ignored test_vscode_chat\nasync fn test_vscode_chat() {\n    use edgequake_llm::traits::ChatMessage;\n\n    let provider = VsCodeCopilotProvider::new()\n        .model(\"gpt-4o-mini\")\n        .build()\n        .unwrap();\n\n    let messages = vec![\n        ChatMessage::system(\"You are a helpful assistant.\"),\n        ChatMessage::user(\"Hello!\"),\n    ];\n\n    let response = provider.chat(&messages, None).await;\n\n    match response {\n        Ok(res) => {\n            println!(\"Chat response: {}\", res.content);\n            assert!(!res.content.is_empty());\n            assert!(res.total_tokens > 0);\n        }\n        Err(e) => {\n            panic!(\"Chat request failed: {}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore] // Run with: cargo test --ignored test_vscode_streaming\nasync fn test_vscode_streaming() {\n    use futures::stream::StreamExt;\n\n    let provider = VsCodeCopilotProvider::new().build().unwrap();\n\n    let mut stream = provider.stream(\"Count to 5\").await.unwrap();\n    let mut chunks = Vec::new();\n\n    while let Some(chunk) = stream.next().await {\n        match chunk {\n            Ok(text) => {\n                print!(\"{}\", text);\n                chunks.push(text);\n            }\n            Err(e) => {\n                panic!(\"Stream error: {}\", e);\n            }\n        }\n    }\n\n    println!(); // New line after streaming\n    let full_text = chunks.join(\"\");\n    assert!(!full_text.is_empty());\n}\n\n#[tokio::test]\n#[ignore] // Run with: cargo test --ignored test_vscode_with_options\nasync fn test_vscode_with_options() {\n    use edgequake_llm::traits::CompletionOptions;\n\n    let provider = VsCodeCopilotProvider::new().build().unwrap();\n\n    let options = CompletionOptions {\n        temperature: Some(0.7),\n        max_tokens: Some(100),\n        ..Default::default()\n    };\n\n    let response = provider\n        .complete_with_options(\"Explain Rust ownership briefly\", &options)\n        .await;\n\n    match response {\n        Ok(res) => {\n            println!(\"Response: {}\", res.content);\n            assert!(!res.content.is_empty());\n        }\n        Err(e) => {\n            panic!(\"Request failed: {}\", e);\n        }\n    }\n}\n\n#[tokio::test]\n#[ignore] // Run with: cargo test --ignored test_vscode_error_handling\nasync fn test_vscode_error_handling() {\n    // Test with invalid proxy URL\n    let provider = VsCodeCopilotProvider::new()\n        .proxy_url(\"http://localhost:9999\")\n        .build()\n        .unwrap();\n\n    let response = provider.complete(\"Test\").await;\n\n    assert!(response.is_err());\n    let err = response.unwrap_err();\n    println!(\"Expected error: {}\", err);\n}\n","traces":[],"covered":0,"coverable":0}],"coverage":51.43853530950305,"covered":2950,"coverable":5735}